<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Kubernetes K8S之Helm部署ELK日志分析系统]]></title>
    <url>%2F2021%2F01%2F18%2Fkubernetes30%2F</url>
    <content type="text"><![CDATA[Kubernetes K8S之Helm部署ELK日志分析系统；由于Logstash比较消耗资源，因此本次我们使用Fluentd实现日志收集（EFK）。 主机配置规划 服务器名称(hostname) 系统版本 配置 内网IP 外网IP(模拟) k8s-master CentOS7.7 2C/4G/20G 172.16.1.110 10.0.0.110 k8s-node01 CentOS7.7 2C/4G/20G 172.16.1.111 10.0.0.111 k8s-node02 CentOS7.7 2C/4G/20G 172.16.1.112 10.0.0.112 备注：由于EFK部署比较消耗内存；所以每台机器的内存最好大于等于4G。 ELK概述ELK是Elasticsearch、Logstash、Kibana的简称，这三者是核心套件，但并非全部。 Elasticsearch是实时全文搜索和分析引擎，提供搜集、分析、存储数据三大功能；是一套开放REST和JAVA API等接口，提供高效搜索功能，可扩展的分布式系统。它构建于Apache Lucene搜索引擎库之上。 Logstash是一个用来搜集、分析、过滤日志的工具。它支持几乎任何类型的日志，包括系统日志、错误日志和自定义应用程序日志。它可以从许多来源接收日志，这些来源包括 syslog、消息传递（例如 RabbitMQ）和JMX，它能够以多种方式输出数据，包括电子邮件、websockets和Elasticsearch。 Kibana是一个基于Web的图形界面，用于搜索、分析和可视化存储在 Elasticsearch指标中的日志数据。它利用Elasticsearch的REST接口来检索数据，不仅允许用户创建他们自己数据定制仪表板的视图，还允许他们以特殊的方式查询和过滤数据。 由于Logstash比较消耗资源，因此本次我们使用Fluentd实现日志收集（EFK）。 EFK镜像下载由于镜像都在国外，因此我们在国内下载镜像，然后tag为对应的镜像名称。执行如下脚本【集群所有机器都执行】： 12345678910111213141516171819202122232425262728293031323334353637[root@k8s-node02 software]# vim download_efk_image.sh #!/bin/sh##### 在 master 节点和 worker 节点都要执行 【所有机器执行】# 加载环境变量. /etc/profile. /etc/bashrc# 变量设置elasticsearch_iamge=&quot;elasticsearch-oss:6.7.0&quot;busybox_image=&quot;busybox:latest&quot;bats_image=&quot;bats:0.4.0&quot;fluentd_image=&quot;fluentd-elasticsearch:v2.3.2&quot;kibana_image=&quot;kibana-oss:6.7.0&quot;# 集群所有机器执行# elasticsearch镜像下载docker pull registry.cn-beijing.aliyuncs.com/google_registry/$&#123;elasticsearch_iamge&#125;docker tag registry.cn-beijing.aliyuncs.com/google_registry/$&#123;elasticsearch_iamge&#125; docker.elastic.co/elasticsearch/$&#123;elasticsearch_iamge&#125;docker rmi registry.cn-beijing.aliyuncs.com/google_registry/$&#123;elasticsearch_iamge&#125;# busybox镜像下载docker pull registry.cn-beijing.aliyuncs.com/google_registry/$&#123;busybox_image&#125;docker tag registry.cn-beijing.aliyuncs.com/google_registry/$&#123;busybox_image&#125; $&#123;busybox_image&#125;docker rmi registry.cn-beijing.aliyuncs.com/google_registry/$&#123;busybox_image&#125;# bats镜像下载docker pull registry.cn-beijing.aliyuncs.com/google_registry/$&#123;bats_image&#125;docker tag registry.cn-beijing.aliyuncs.com/google_registry/$&#123;bats_image&#125; dduportal/$&#123;bats_image&#125;docker rmi registry.cn-beijing.aliyuncs.com/google_registry/$&#123;bats_image&#125;# fluentd-elasticsearch镜像下载docker pull registry.cn-beijing.aliyuncs.com/google_registry/$&#123;fluentd_image&#125;docker tag registry.cn-beijing.aliyuncs.com/google_registry/$&#123;fluentd_image&#125; gcr.io/google-containers/$&#123;fluentd_image&#125;docker rmi registry.cn-beijing.aliyuncs.com/google_registry/$&#123;fluentd_image&#125;# kibana-oss镜像下载docker pull registry.cn-beijing.aliyuncs.com/google_registry/$&#123;kibana_image&#125;docker tag registry.cn-beijing.aliyuncs.com/google_registry/$&#123;kibana_image&#125; docker.elastic.co/kibana/$&#123;kibana_image&#125;docker rmi registry.cn-beijing.aliyuncs.com/google_registry/$&#123;kibana_image&#125; Elasticsearch部署本次部署EFK，创建一个efk名称空间。 chart下载与配置修改1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495# 当前目录[root@k8s-master efk]# pwd/root/k8s_practice/efk# 创建 efk 名称空间[root@k8s-master efk]# kubectl create namespace efk[root@k8s-master efk]# # ES版本查看，本次我们部署chart 1.30.0版本，ES 6.7.0版本[root@k8s-master efk]# helm search stable/elasticsearch -lNAME CHART VERSION APP VERSION DESCRIPTION stable/elasticsearch 1.32.5 6.8.6 DEPRECATED Flexible and powerful open source, distributed...stable/elasticsearch 1.32.4 6.8.6 Flexible and powerful open source, distributed real-time ...stable/elasticsearch 1.32.3 6.8.6 Flexible and powerful open source, distributed real-time ...………………[root@k8s-master efk]# [root@k8s-master efk]# helm fetch stable/elasticsearch --version 1.30.0[root@k8s-master efk]# tar xf elasticsearch-1.30.0.tgz# 修改配置文件1[root@k8s-master efk]# vim elasticsearch/values.yamlinitImage: repository: &quot;busybox&quot; tag: &quot;latest&quot; pullPolicy: &quot;IfNotPresent&quot; # 从Always 改为IfNotPresent………………client: name: client replicas: 1 # 从2改为1，因为是在自己PC机操作的，内存有限 serviceType: ClusterIP………………master: name: master exposeHttp: false replicas: 3 # 不要修改 heapSize: &quot;512m&quot; persistence: enabled: false # 没有多余的PVC，因此从true改为false accessMode: ReadWriteOnce………………data: name: data exposeHttp: false replicas: 1 # 从2改为1，因为是在自己PC机操作的，内存有限 heapSize: &quot;1024m&quot; # 从1536m改为1024m，因为是在自己PC机操作的，内存有限 persistence: enabled: false # 没有多余的PVC，因此从true改为false accessMode: ReadWriteOnce[root@k8s-master efk]# # 修改配置文件2[root@k8s-master efk]# vim elasticsearch/templates/client-deployment.yamlapiVersion: apps/v1 # 从 apps/v1beta1 改为 apps/v1kind: Deployment………………spec: replicas: &#123;&#123; .Values.client.replicas &#125;&#125; # 添加信息 如下 selector: matchLabels: app: &#123;&#123; template &quot;elasticsearch.name&quot; . &#125;&#125; component: &quot;&#123;&#123; .Values.client.name &#125;&#125;&quot; release: &#123;&#123; .Release.Name &#125;&#125; # 添加信息 如上[root@k8s-master efk]# # 修改配置文件3[root@k8s-master efk]# vim elasticsearch/templates/data-statefulset.yamlapiVersion: apps/v1 # 从 apps/v1beta1 改为 apps/v1kind: StatefulSet………………spec: serviceName: &#123;&#123; template &quot;elasticsearch.data.fullname&quot; . &#125;&#125; replicas: &#123;&#123; .Values.data.replicas &#125;&#125; # 添加信息 如下 selector: matchLabels: app: &#123;&#123; template &quot;elasticsearch.name&quot; . &#125;&#125; component: &quot;&#123;&#123; .Values.data.name &#125;&#125;&quot; release: &#123;&#123; .Release.Name &#125;&#125; role: data # 添加信息 如上[root@k8s-master efk]# # 修改配置文件4[root@k8s-master efk]# vim elasticsearch/templates/master-statefulset.yaml apiVersion: apps/v1 # 从 apps/v1beta1 改为 apps/v1kind: StatefulSet………………spec: serviceName: &#123;&#123; template &quot;elasticsearch.master.fullname&quot; . &#125;&#125; replicas: &#123;&#123; .Values.master.replicas &#125;&#125; # 添加信息 如下 selector: matchLabels: app: &#123;&#123; template &quot;elasticsearch.name&quot; . &#125;&#125; component: &quot;&#123;&#123; .Values.master.name &#125;&#125;&quot; release: &#123;&#123; .Release.Name &#125;&#125; role: master # 添加信息 如上[root@k8s-master efk]# Elasticsearch部署步骤如下： 12345678910111213141516171819202122232425262728293031323334# 当前目录[root@k8s-master efk]# pwd/root/k8s_practice/efk# 部署ES[root@k8s-master efk]# helm install --name es01 --namespace=efk elasticsearch/[root@k8s-master efk]# # 状态查看[root@k8s-master ~]# helm listNAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACE es01 1 Sat Jul 25 12:18:50 2020 DEPLOYED elasticsearch-1.30.0 6.7.0 efk [root@k8s-master efk]# # 等待一会儿后【估计几分钟】，查看pod状态信息如下[root@k8s-master ~]# kubectl get deploy -n efkNAME READY UP-TO-DATE AVAILABLE AGEes01-elasticsearch-client 1/1 1 1 6m13s[root@k8s-master ~]# [root@k8s-master ~]# kubectl get pod -n efkNAME READY STATUS RESTARTS AGEes01-elasticsearch-client-646f8f866d-rt2wp 1/1 Running 0 6m21ses01-elasticsearch-data-0 1/1 Running 0 6m21ses01-elasticsearch-master-0 1/1 Running 0 6m21ses01-elasticsearch-master-1 1/1 Running 0 5m30ses01-elasticsearch-master-2 1/1 Running 0 5m3s[root@k8s-master efk]# [root@k8s-master efk]# kubectl get svc -n efkNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEes01-elasticsearch-client ClusterIP 10.100.237.152 &lt;none&gt; 9200/TCP 6m34ses01-elasticsearch-discovery ClusterIP None &lt;none&gt; 9300/TCP 6m42s[root@k8s-master efk]# [root@k8s-master efk]# kubectl get sts -n efkNAME READY AGEes01-elasticsearch-data 1/1 7m4ses01-elasticsearch-master 3/3 7m4s[root@k8s-master efk]# Elasticsearch访问其中IP来源于ES的svc。 12345678910111213141516171819202122232425262728293031323334353637[root@k8s-master ~]# curl 10.100.237.152:9200/&#123; &quot;name&quot; : &quot;es01-elasticsearch-client-646f8f866d-rt2wp&quot;, &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;cluster_uuid&quot; : &quot;S4t_UDOuRye9mtK22VWxLw&quot;, &quot;version&quot; : &#123; &quot;number&quot; : &quot;6.7.0&quot;, &quot;build_flavor&quot; : &quot;oss&quot;, &quot;build_type&quot; : &quot;docker&quot;, &quot;build_hash&quot; : &quot;8453f77&quot;, &quot;build_date&quot; : &quot;2019-03-21T15:32:29.844721Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;7.7.0&quot;, &quot;minimum_wire_compatibility_version&quot; : &quot;5.6.0&quot;, &quot;minimum_index_compatibility_version&quot; : &quot;5.0.0&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125;[root@k8s-master ~]# [root@k8s-master ~]# curl 10.100.237.152:9200/_cluster/health?pretty&#123; &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;status&quot; : &quot;green&quot;, # 可见状态正常 &quot;timed_out&quot; : false, &quot;number_of_nodes&quot; : 5, &quot;number_of_data_nodes&quot; : 1, &quot;active_primary_shards&quot; : 0, &quot;active_shards&quot; : 0, &quot;relocating_shards&quot; : 0, &quot;initializing_shards&quot; : 0, &quot;unassigned_shards&quot; : 0, &quot;delayed_unassigned_shards&quot; : 0, &quot;number_of_pending_tasks&quot; : 0, &quot;number_of_in_flight_fetch&quot; : 0, &quot;task_max_waiting_in_queue_millis&quot; : 0, &quot;active_shards_percent_as_number&quot; : 100.0&#125; 至此，elasticsearch部署完毕 elasticsearch-client域名获取根据es01-elasticsearch-client的svc信息获取到es01-elasticsearch-client的域名；用于后面的fluentd 和kibana。 启动一个pod 1234567891011121314151617181920212223242526[root@k8s-master test]# pwd/root/k8s_practice/test[root@k8s-master test]# cat myapp_demo.yamlapiVersion: v1kind: Podmetadata: name: myapp-demo namespace: default labels: k8s-app: myappspec: containers: - name: myapp image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 imagePullPolicy: IfNotPresent ports: - name: httpd containerPort: 80 protocol: TCP[root@k8s-master test]# [root@k8s-master test]# kubectl apply -f myapp_demo.yamlpod/myapp-demo created[root@k8s-master test]# [root@k8s-master test]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmyapp-demo 1/1 Running 0 6s 10.244.2.84 k8s-node02 &lt;none&gt; &lt;none&gt; 进入pod并得到elasticsearch-client域名信息 12345678910111213141516# 进入一个pod容器[root@k8s-master test]# kubectl exec -it myapp-demo sh ##### 格式 nslookup svc-cluster-ip/ # nslookup 10.100.237.152nslookup: can&apos;t resolve &apos;(null)&apos;: Name does not resolveName: 10.100.237.152Address 1: 10.100.237.152 es01-elasticsearch-client.efk.svc.cluster.local/ # / # ### 通过ping判断域名是否通畅/ # ping es01-elasticsearch-client.efk.svc.cluster.localPING es01-elasticsearch-client.efk.svc.cluster.local (10.100.237.152): 56 data bytes64 bytes from 10.100.237.152: seq=0 ttl=64 time=0.094 ms64 bytes from 10.100.237.152: seq=1 ttl=64 time=0.081 ms64 bytes from 10.100.237.152: seq=2 ttl=64 time=0.243 ms 由上可得，Service中es01-elasticsearch-client的域名为：es01-elasticsearch-client.efk.svc.cluster.local Service的域名格式为：$(service name).$(namespace).svc.cluster.local，其中 cluster.local 指定的集群的域名 Fluentd部署chart下载与配置修改123456789101112131415161718192021[root@k8s-master efk]# pwd/root/k8s_practice/efk[root@k8s-master efk]## fluentd版本信息查看[root@k8s-master efk]# helm search stable/fluentd-elasticsearch -lNAME CHART VERSION APP VERSION DESCRIPTION stable/fluentd-elasticsearch 2.0.7 2.3.2 DEPRECATED! - A Fluentd Helm chart for Kubernetes with El...stable/fluentd-elasticsearch 2.0.6 2.3.2 A Fluentd Helm chart for Kubernetes with Elasticsearch ou...stable/fluentd-elasticsearch 2.0.5 2.3.2 A Fluentd Helm chart for Kubernetes with Elasticsearch ou...………………# 获取fluentd-elasticsearch 并解压[root@k8s-master efk]# helm fetch stable/fluentd-elasticsearch --version 2.0.7[root@k8s-master efk]# tar xf fluentd-elasticsearch-2.0.7.tgz# 配置修改[root@k8s-master efk]# vim fluentd-elasticsearch/values.yaml### 为什么使用域名而不是IP，因此每次重启ES的svc，对应IP都会改变。而域名是不变的elasticsearch: host: &apos;es01-elasticsearch-client.efk.svc.cluster.local&apos; # 修改处，域名获取参见上文 port: 9200 scheme: &apos;http&apos;[root@k8s-master efk]# fluentd-elasticsearch部署步骤如下： 123456789101112131415161718192021222324252627################ 部署fluentd-elasticsearch# 当前目录[root@k8s-master efk]# pwd/root/k8s_practice/efk# 部署fluentd-elasticsearch[root@k8s-master efk]# helm install --name fluentd-es01 --namespace=efk fluentd-elasticsearch[root@k8s-master efk]# # 状态查看[root@k8s-master efk]# helm listNAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACE es01 1 Sat Jul 25 12:18:50 2020 DEPLOYED elasticsearch-1.30.0 6.7.0 efk fluentd-es01 1 Sat Jul 25 12:36:01 2020 DEPLOYED fluentd-elasticsearch-2.0.7 2.3.2 efk[root@k8s-master efk]# #查看pod状态信息如下[root@k8s-master efk]# kubectl get ds -n efkNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGEfluentd-es01-fluentd-elasticsearch 2 2 2 2 2 &lt;none&gt; 113s[root@k8s-master efk]# [root@k8s-master efk]# kubectl get pod -n efk -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESes01-elasticsearch-client-646f8f866d-rt2wp 1/1 Running 0 17m 10.244.2.57 k8s-node02 &lt;none&gt; &lt;none&gt;es01-elasticsearch-data-0 1/1 Running 0 17m 10.244.2.58 k8s-node02 &lt;none&gt; &lt;none&gt;es01-elasticsearch-master-0 1/1 Running 0 17m 10.244.4.241 k8s-node01 &lt;none&gt; &lt;none&gt;es01-elasticsearch-master-1 1/1 Running 0 17m 10.244.2.59 k8s-node02 &lt;none&gt; &lt;none&gt;es01-elasticsearch-master-2 1/1 Running 0 16m 10.244.4.242 k8s-node01 &lt;none&gt; &lt;none&gt;fluentd-es01-fluentd-elasticsearch-qnmf9 1/1 Running 0 43s 10.244.4.243 k8s-node01 &lt;none&gt; &lt;none&gt;fluentd-es01-fluentd-elasticsearch-xmw5f 1/1 Running 0 43s 10.244.2.60 k8s-node02 &lt;none&gt; &lt;none&gt; Kibana部署kibana的主版本和大版本必须和elasticsearch（ES）一致，小版本可以不同；但两者版本最好一致，这样可以避免一些因版本不同导致的意外情况。 由于elasticsearch（ES）使用的是6.7.0，因此kibana我们也是用该版本。 chart下载与配置修改123456789101112131415161718192021222324252627282930313233343536373839404142[root@k8s-master efk]# pwd/root/k8s_practice/efk# 所有版本查看[root@k8s-master efk]# helm search stable/kibana -lNAME CHART VERSION APP VERSION DESCRIPTION stable/kibana 3.2.7 6.7.0 Kibana is an open source data visualization plugin for El...stable/kibana 3.2.6 6.7.0 Kibana is an open source data visualization plugin for El...………………# 获取kibana，并解压缩[root@k8s-master efk]# helm fetch stable/kibana --version 3.2.7 [root@k8s-master efk]# tar xf kibana-3.2.7.tgz# 配置修改1[root@k8s-master efk]# vim kibana/values.yaml### 为什么使用域名而不是IP，因此每次重启ES的svc，对应IP都会改变。而域名是不变的files: kibana.yml: ## Default Kibana configuration from kibana-docker. server.name: kibana server.host: &quot;0&quot; elasticsearch.url: http://es01-elasticsearch-client.efk.svc.cluster.local:9200 # 修改处，域名获取参见上文………………service: type: NodePort # 修改内容 从ClusterIP改为NodePort externalPort: 443 internalPort: 5601 nodePort: 30601 # 添加处，Service端口范围：30000-32767[root@k8s-master efk]# # 配置修改2[root@k8s-master efk]# vim kibana/templates/deployment.yaml apiVersion: apps/v1 # 从 apps/v1beta1 改为 apps/v1kind: Deploymentmetadata:………………spec: replicas: &#123;&#123; .Values.replicaCount &#125;&#125; revisionHistoryLimit: &#123;&#123; .Values.revisionHistoryLimit &#125;&#125; # 添加信息 如下 selector: matchLabels: app: &#123;&#123; template &quot;kibana.name&quot; . &#125;&#125; release: &quot;&#123;&#123; .Release.Name &#125;&#125;&quot; # 添加信息 如上 kibana部署步骤如下： 12345678910111213141516171819202122232425262728293031323334353637################ 部署kibana-oss# 当前目录[root@k8s-master efk]# pwd/root/k8s_practice/efk# 部署kibana-oss[root@k8s-master efk]# helm install --name kibana01 --namespace=efk kibana[root@k8s-master efk]## 状态查看[root@k8s-master efk]# helm listNAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACE es01 1 Sat Jul 25 12:18:50 2020 DEPLOYED elasticsearch-1.30.0 6.7.0 efk fluentd-es01 1 Sat Jul 25 12:36:01 2020 DEPLOYED fluentd-elasticsearch-2.0.7 2.3.2 efk kibana01 1 Sat Jul 25 12:38:18 2020 DEPLOYED kibana-3.2.7 6.7.0 efk[root@k8s-master efk]# #查看pod状态信息如下[root@k8s-master efk]# kubectl get deploy -n efkNAME READY UP-TO-DATE AVAILABLE AGEes01-elasticsearch-client 1/1 1 1 19mkibana01 1/1 1 1 27s[root@k8s-master efk]# [root@k8s-master efk]# kubectl get pod -n efk -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESes01-elasticsearch-client-646f8f866d-rt2wp 1/1 Running 0 20m 10.244.2.57 k8s-node02 &lt;none&gt; &lt;none&gt;es01-elasticsearch-data-0 1/1 Running 0 20m 10.244.2.58 k8s-node02 &lt;none&gt; &lt;none&gt;es01-elasticsearch-master-0 1/1 Running 0 20m 10.244.4.241 k8s-node01 &lt;none&gt; &lt;none&gt;es01-elasticsearch-master-1 1/1 Running 0 19m 10.244.2.59 k8s-node02 &lt;none&gt; &lt;none&gt;es01-elasticsearch-master-2 1/1 Running 0 19m 10.244.4.242 k8s-node01 &lt;none&gt; &lt;none&gt;fluentd-es01-fluentd-elasticsearch-qnmf9 1/1 Running 0 3m10s 10.244.4.243 k8s-node01 &lt;none&gt; &lt;none&gt;fluentd-es01-fluentd-elasticsearch-xmw5f 1/1 Running 0 3m10s 10.244.2.60 k8s-node02 &lt;none&gt; &lt;none&gt;kibana01-bc479f8c7-kr2n2 1/1 Running 0 53s 10.244.4.244 k8s-node01 &lt;none&gt; &lt;none&gt;[root@k8s-master efk]# # 查看svc信息[root@k8s-master efk]# kubectl get svc -n efk -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORes01-elasticsearch-client ClusterIP 10.100.237.152 &lt;none&gt; 9200/TCP 20m app=elasticsearch,component=client,release=es01es01-elasticsearch-discovery ClusterIP None &lt;none&gt; 9300/TCP 20m app=elasticsearch,component=master,release=es01kibana01 NodePort 10.101.200.177 &lt;none&gt; 443:30601/TCP 71s app=kibana,release=kibana01 浏览器访问1http://172.16.1.110:30601/ 页面访问 创建索引 数据查看 相关阅读1、Kubernetes K8S之Helm部署、使用与示例 完毕！]]></content>
      <categories>
        <category>Kubernetes</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
        <tag>K8S</tag>
        <tag>CI/CD</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes K8S之kube-prometheus概述与部署]]></title>
    <url>%2F2021%2F01%2F10%2Fkubernetes29%2F</url>
    <content type="text"><![CDATA[Kubernetes K8S之kube-prometheus概述与部署 主机配置规划 服务器名称(hostname) 系统版本 配置 内网IP 外网IP(模拟) k8s-master CentOS7.7 2C/4G/20G 172.16.1.110 10.0.0.110 k8s-node01 CentOS7.7 2C/4G/20G 172.16.1.111 10.0.0.111 k8s-node02 CentOS7.7 2C/4G/20G 172.16.1.112 10.0.0.112 prometheus概述Prometheus是一个开源的系统监控和警报工具包，自2012成立以来，许多公司和组织采用了Prometheus。它现在是一个独立的开源项目，并独立于任何公司维护。在2016年，Prometheus加入云计算基金会作为Kubernetes之后的第二托管项目。 Prometheus性能也足够支撑上万台规模的集群。 Prometheus的关键特性 多维度数据模型 灵活的查询语言 不依赖于分布式存储；单服务器节点是自治的 通过基于HTTP的pull方式采集时序数据 可以通过中间网关进行时序列数据推送 通过服务发现或者静态配置来发现目标服务对象 支持多种多样的图表和界面展示，比如Grafana等 架构图 基本原理Prometheus的基本原理是通过HTTP协议周期性抓取被监控组件的状态，任意组件只要提供对应的HTTP接口就可以接入监控。不需要任何SDK或者其他的集成过程。 这样做非常适合做虚拟化环境监控系统，比如VM、Docker、Kubernetes等。输出被监控组件信息的HTTP接口被叫做exporter 。目前互联网公司常用的组件大部分都有exporter可以直接使用，比如Varnish、Haproxy、Nginx、MySQL、Linux系统信息(包括磁盘、内存、CPU、网络等等)。 Prometheus三大套件 Server 主要负责数据采集和存储，提供PromQL查询语言的支持。 Alertmanager 警告管理器，用来进行报警。 Push Gateway 支持临时性Job主动推送指标的中间网关。 服务过程 Prometheus Daemon负责定时去目标上抓取metrics(指标)数据，每个抓取目标需要暴露一个http服务的接口给它定时抓取。Prometheus支持通过配置文件、文本文件、Zookeeper、Consul、DNS SRV Lookup等方式指定抓取目标。Prometheus采用PULL的方式进行监控，即服务器可以直接通过目标PULL数据或者间接地通过中间网关来Push数据。 Prometheus在本地存储抓取的所有数据，并通过一定规则进行清理和整理数据，并把得到的结果存储到新的时间序列中。 Prometheus通过PromQL和其他API可视化地展示收集的数据。Prometheus支持很多方式的图表可视化，例如Grafana、自带的Promdash以及自身提供的模版引擎等等。Prometheus还提供HTTP API的查询方式，自定义所需要的输出。 PushGateway支持Client主动推送metrics到PushGateway，而Prometheus只是定时去Gateway上抓取数据。 Alertmanager是独立于Prometheus的一个组件，可以支持Prometheus的查询语句，提供十分灵活的报警方式。 kube-prometheus部署kube-prometheus的GitHub地址： 1https://github.com/coreos/kube-prometheus/ 本次我们选择release-0.2版本，而不是其他版本。 kube-prometheus下载与配置修改下载 123456789[root@k8s-master prometheus]# pwd/root/k8s_practice/prometheus[root@k8s-master prometheus]# [root@k8s-master prometheus]# wget https://github.com/coreos/kube-prometheus/archive/v0.2.0.tar.gz [root@k8s-master prometheus]# tar xf v0.2.0.tar.gz [root@k8s-master prometheus]# lltotal 432drwxrwxr-x 10 root root 4096 Sep 13 2019 kube-prometheus-0.2.0-rw-r--r-- 1 root root 200048 Jul 19 11:41 v0.2.0.tar.gz 配置修改 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879# 当前所在目录[root@k8s-master manifests]# pwd/root/k8s_practice/prometheus/kube-prometheus-0.2.0/manifests[root@k8s-master manifests]# # 配置修改1[root@k8s-master manifests]# vim grafana-service.yaml apiVersion: v1kind: Servicemetadata: labels: app: grafana name: grafana namespace: monitoringspec: type: NodePort # 添加内容 ports: - name: http port: 3000 targetPort: http nodePort: 30100 # 添加内容 selector: app: grafana[root@k8s-master manifests]# # 配置修改2 [root@k8s-master manifests]# vim prometheus-service.yaml apiVersion: v1kind: Servicemetadata: labels: prometheus: k8s name: prometheus-k8s namespace: monitoringspec: type: NodePort # 添加内容 ports: - name: web port: 9090 targetPort: web nodePort: 30200 # 添加内容 selector: app: prometheus prometheus: k8s sessionAffinity: ClientIP[root@k8s-master manifests]## 配置修改3[root@k8s-master manifests]# vim alertmanager-service.yaml apiVersion: v1kind: Servicemetadata: labels: alertmanager: main name: alertmanager-main namespace: monitoringspec: type: NodePort # 添加内容 ports: - name: web port: 9093 targetPort: web nodePort: 30300 # 添加内容 selector: alertmanager: main app: alertmanager sessionAffinity: ClientIP[root@k8s-master manifests]## 配置修改4[root@k8s-master manifests]# vim grafana-deployment.yaml # 将apps/v1beta2 改为 apps/v1apiVersion: apps/v1kind: Deploymentmetadata: labels: app: grafana name: grafana namespace: monitoringspec: replicas: 1 selector:……………… kube-prometheus镜像版本查看与下载由于镜像都在国外，因此经常会下载失败。为了快速下载镜像，这里我们下载国内的镜像，然后tag为配置文件中的国外镜像名即可。 查看kube-prometheus的镜像信息 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677# 当前工作目录[root@k8s-master manifests]# pwd/root/k8s_practice/prometheus/kube-prometheus-0.2.0/manifests[root@k8s-master manifests]# # 所有镜像信息如下[root@k8s-master manifests]# grep -riE &apos;quay.io|k8s.gcr|grafana/&apos; *0prometheus-operator-deployment.yaml: - --config-reloader-image=quay.io/coreos/configmap-reload:v0.0.10prometheus-operator-deployment.yaml: - --prometheus-config-reloader=quay.io/coreos/prometheus-config-reloader:v0.33.00prometheus-operator-deployment.yaml: image: quay.io/coreos/prometheus-operator:v0.33.0alertmanager-alertmanager.yaml: baseImage: quay.io/prometheus/alertmanagergrafana-deployment.yaml: - image: grafana/grafana:6.2.2kube-state-metrics-deployment.yaml: image: quay.io/coreos/kube-rbac-proxy:v0.4.1kube-state-metrics-deployment.yaml: image: quay.io/coreos/kube-rbac-proxy:v0.4.1kube-state-metrics-deployment.yaml: image: quay.io/coreos/kube-state-metrics:v1.7.2kube-state-metrics-deployment.yaml: image: k8s.gcr.io/addon-resizer:1.8.4node-exporter-daemonset.yaml: image: quay.io/prometheus/node-exporter:v0.18.1node-exporter-daemonset.yaml: image: quay.io/coreos/kube-rbac-proxy:v0.4.1prometheus-adapter-deployment.yaml: image: quay.io/coreos/k8s-prometheus-adapter-amd64:v0.4.1prometheus-prometheus.yaml: baseImage: quay.io/prometheus/prometheus##### 由上可知alertmanager和prometheus的镜像版本未显示### 获取alertmanager镜像版本信息[root@k8s-master manifests]# cat alertmanager-alertmanager.yaml apiVersion: monitoring.coreos.com/v1kind: Alertmanagermetadata: labels: alertmanager: main name: main namespace: monitoringspec: baseImage: quay.io/prometheus/alertmanager nodeSelector: kubernetes.io/os: linux replicas: 3 securityContext: fsGroup: 2000 runAsNonRoot: true runAsUser: 1000 serviceAccountName: alertmanager-main version: v0.18.0##### 由上可见alertmanager的镜像版本为v0.18.0### 获取prometheus镜像版本信息[root@k8s-master manifests]# cat prometheus-prometheus.yamlapiVersion: monitoring.coreos.com/v1kind: Prometheusmetadata: labels: prometheus: k8s name: k8s namespace: monitoringspec: alerting: alertmanagers: - name: alertmanager-main namespace: monitoring port: web baseImage: quay.io/prometheus/prometheus nodeSelector: kubernetes.io/os: linux podMonitorSelector: &#123;&#125; replicas: 2 resources: requests: memory: 400Mi ruleSelector: matchLabels: prometheus: k8s role: alert-rules securityContext: fsGroup: 2000 runAsNonRoot: true runAsUser: 1000 serviceAccountName: prometheus-k8s serviceMonitorNamespaceSelector: &#123;&#125; serviceMonitorSelector: &#123;&#125; version: v2.11.0##### 由上可见prometheus的镜像版本为v2.11.0 执行脚本：镜像下载并重命名【集群所有机器执行】 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657[root@k8s-master software]# vim download_prometheus_image.sh #!/bin/sh##### 在 master 节点和 worker 节点都要执行 【所有机器执行】# 加载环境变量. /etc/profile. /etc/bashrc################################################ 从国内下载 prometheus 所需镜像，并对镜像重命名 src_registry=&quot;registry.cn-beijing.aliyuncs.com/cloud_registry&quot;# 定义镜像集合数组 images=( kube-rbac-proxy:v0.4.1 kube-state-metrics:v1.7.2 k8s-prometheus-adapter-amd64:v0.4.1 configmap-reload:v0.0.1 prometheus-config-reloader:v0.33.0 prometheus-operator:v0.33.0)# 循环从国内获取的Docker镜像for img in $&#123;images[@]&#125;;do # 从国内源下载镜像 docker pull $&#123;src_registry&#125;/$img # 改变镜像名称 docker tag $&#123;src_registry&#125;/$img quay.io/coreos/$img # 删除源始镜像 docker rmi $&#123;src_registry&#125;/$img # 打印分割线 echo &quot;======== $img download OK ========&quot;done##### 其他镜像下载image_name=&quot;alertmanager:v0.18.0&quot;docker pull $&#123;src_registry&#125;/$&#123;image_name&#125; &amp;&amp; docker tag $&#123;src_registry&#125;/$&#123;image_name&#125; quay.io/prometheus/$&#123;image_name&#125; &amp;&amp; docker rmi $&#123;src_registry&#125;/$&#123;image_name&#125;echo &quot;======== $&#123;image_name&#125; download OK ========&quot;image_name=&quot;node-exporter:v0.18.1&quot;docker pull $&#123;src_registry&#125;/$&#123;image_name&#125; &amp;&amp; docker tag $&#123;src_registry&#125;/$&#123;image_name&#125; quay.io/prometheus/$&#123;image_name&#125; &amp;&amp; docker rmi $&#123;src_registry&#125;/$&#123;image_name&#125;echo &quot;======== $&#123;image_name&#125; download OK ========&quot;image_name=&quot;prometheus:v2.11.0&quot;docker pull $&#123;src_registry&#125;/$&#123;image_name&#125; &amp;&amp; docker tag $&#123;src_registry&#125;/$&#123;image_name&#125; quay.io/prometheus/$&#123;image_name&#125; &amp;&amp; docker rmi $&#123;src_registry&#125;/$&#123;image_name&#125;echo &quot;======== $&#123;image_name&#125; download OK ========&quot;image_name=&quot;grafana:6.2.2&quot;docker pull $&#123;src_registry&#125;/$&#123;image_name&#125; &amp;&amp; docker tag $&#123;src_registry&#125;/$&#123;image_name&#125; grafana/$&#123;image_name&#125; &amp;&amp; docker rmi $&#123;src_registry&#125;/$&#123;image_name&#125;echo &quot;======== $&#123;image_name&#125; download OK ========&quot;image_name=&quot;addon-resizer:1.8.4&quot;docker pull $&#123;src_registry&#125;/$&#123;image_name&#125; &amp;&amp; docker tag $&#123;src_registry&#125;/$&#123;image_name&#125; k8s.gcr.io/$&#123;image_name&#125; &amp;&amp; docker rmi $&#123;src_registry&#125;/$&#123;image_name&#125;echo &quot;======== $&#123;image_name&#125; download OK ========&quot;echo &quot;********** prometheus docker images OK! **********&quot; 执行脚本后得到如下镜像 12345678910111213141516171819[root@k8s-master software]# docker images | grep &apos;quay.io/coreos&apos;quay.io/coreos/kube-rbac-proxy v0.4.1 a9d1a87e4379 6 days ago 41.3MBquay.io/coreos/flannel v0.12.0-amd64 4e9f801d2217 4 months ago 52.8MB ## 之前已存在quay.io/coreos/kube-state-metrics v1.7.2 3fd71b84d250 6 months ago 33.1MBquay.io/coreos/prometheus-config-reloader v0.33.0 64751efb2200 8 months ago 17.6MBquay.io/coreos/prometheus-operator v0.33.0 8f2f814d33e1 8 months ago 42.1MBquay.io/coreos/k8s-prometheus-adapter-amd64 v0.4.1 5f0fc84e586c 15 months ago 60.7MBquay.io/coreos/configmap-reload v0.0.1 3129a2ca29d7 3 years ago 4.79MB[root@k8s-master software]# [root@k8s-master software]# docker images | grep &apos;quay.io/prometheus&apos;quay.io/prometheus/node-exporter v0.18.1 d7707e6f5e95 11 days ago 22.9MBquay.io/prometheus/prometheus v2.11.0 de242295e225 2 months ago 126MBquay.io/prometheus/alertmanager v0.18.0 30594e96cbe8 10 months ago 51.9MB[root@k8s-master software]# [root@k8s-master software]# docker images | grep &apos;grafana&apos;grafana/grafana 6.2.2 a532fe3b344a 9 months ago 248MB[root@k8s-node01 software]# [root@k8s-node01 software]# docker images | grep &apos;addon-resizer&apos;k8s.gcr.io/addon-resizer 1.8.4 5ec630648120 20 months ago 38.3MB kube-prometheus启动启动prometheus 12345[root@k8s-master kube-prometheus-0.2.0]# pwd/root/k8s_practice/prometheus/kube-prometheus-0.2.0[root@k8s-master kube-prometheus-0.2.0]# ### 如果出现异常，可以再重复执行一次或多次[root@k8s-master kube-prometheus-0.2.0]# kubectl apply -f manifests/ 启动后svc与pod状态查看 1234567891011121314151617181920212223242526272829303132[root@k8s-master ~]# kubectl top nodeNAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-master 152m 7% 1311Mi 35% k8s-node01 100m 5% 928Mi 54% k8s-node02 93m 4% 979Mi 56% [root@k8s-master ~]# [root@k8s-master ~]# kubectl get svc -n monitoringNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEalertmanager-main NodePort 10.97.249.249 &lt;none&gt; 9093:30300/TCP 7m21salertmanager-operated ClusterIP None &lt;none&gt; 9093/TCP,9094/TCP,9094/UDP 7m13sgrafana NodePort 10.101.183.103 &lt;none&gt; 3000:30100/TCP 7m20skube-state-metrics ClusterIP None &lt;none&gt; 8443/TCP,9443/TCP 7m20snode-exporter ClusterIP None &lt;none&gt; 9100/TCP 7m20sprometheus-adapter ClusterIP 10.105.174.86 &lt;none&gt; 443/TCP 7m19sprometheus-k8s NodePort 10.109.179.233 &lt;none&gt; 9090:30200/TCP 7m19sprometheus-operated ClusterIP None &lt;none&gt; 9090/TCP 7m3sprometheus-operator ClusterIP None &lt;none&gt; 8080/TCP 7m21s[root@k8s-master ~]# [root@k8s-master ~]# kubectl get pod -n monitoring -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESalertmanager-main-0 2/2 Running 0 2m11s 10.244.4.164 k8s-node01 &lt;none&gt; &lt;none&gt;alertmanager-main-1 2/2 Running 0 2m11s 10.244.2.225 k8s-node02 &lt;none&gt; &lt;none&gt;alertmanager-main-2 2/2 Running 0 2m11s 10.244.4.163 k8s-node01 &lt;none&gt; &lt;none&gt;grafana-5cd56df4cd-6d75r 1/1 Running 0 29s 10.244.2.227 k8s-node02 &lt;none&gt; &lt;none&gt;kube-state-metrics-7d4bb66d8d-gx7w4 4/4 Running 0 2m18s 10.244.2.223 k8s-node02 &lt;none&gt; &lt;none&gt;node-exporter-pl47v 2/2 Running 0 2m17s 172.16.1.110 k8s-master &lt;none&gt; &lt;none&gt;node-exporter-tmmbw 2/2 Running 0 2m17s 172.16.1.111 k8s-node01 &lt;none&gt; &lt;none&gt;node-exporter-w8wd9 2/2 Running 0 2m17s 172.16.1.112 k8s-node02 &lt;none&gt; &lt;none&gt;prometheus-adapter-c676d8764-phj69 1/1 Running 0 2m17s 10.244.2.224 k8s-node02 &lt;none&gt; &lt;none&gt;prometheus-k8s-0 3/3 Running 1 2m1s 10.244.2.226 k8s-node02 &lt;none&gt; &lt;none&gt;prometheus-k8s-1 3/3 Running 0 2m1s 10.244.4.165 k8s-node01 &lt;none&gt; &lt;none&gt;prometheus-operator-7559d67ff-lk86l 1/1 Running 0 2m18s 10.244.4.162 k8s-node01 &lt;none&gt; &lt;none&gt; kube-prometheus访问prometheus-service访问访问地址如下： 1http://172.16.1.110:30200/ 通过访问如下地址，可以看到prometheus已经成功连接上了k8s的apiserver。 1http://172.16.1.110:30200/targets 查看service-discovery 1http://172.16.1.110:30200/service-discovery prometheus自己指标查看 1http://172.16.1.110:30200/metrics prometheus的WEB界面上提供了基本的查询，例如查询K8S集群中每个POD的CPU使用情况，可以使用如下查询条件查询： 12# 直接使用 container_cpu_usage_seconds_total 可以看见有哪些字段信息sum(rate(container_cpu_usage_seconds_total&#123;image!=&quot;&quot;, pod!=&quot;&quot;&#125;[1m] )) by (pod) 列表页面 图形页面 grafana-service访问访问地址如下： 1http://172.16.1.110:30100/ 首次登录时账号密码默认为：admin/admin 添加数据来源 得到如下页面 如上，数据来源默认是已经添加好了的 点击进入，拉到下面，再点击Test按钮，测验数据来源是否正常 之后可导入一些模板 数据信息图像化查看 异常问题解决如果 kubectl apply -f manifests/ 出现类似如下提示： 1234567891011121314unable to recognize &quot;manifests/alertmanager-alertmanager.yaml&quot;: no matches for kind &quot;Alertmanager&quot; in version &quot;monitoring.coreos.com/v1&quot;unable to recognize &quot;manifests/alertmanager-serviceMonitor.yaml&quot;: no matches for kind &quot;ServiceMonitor&quot; in version &quot;monitoring.coreos.com/v1&quot;unable to recognize &quot;manifests/grafana-serviceMonitor.yaml&quot;: no matches for kind &quot;ServiceMonitor&quot; in version &quot;monitoring.coreos.com/v1&quot;unable to recognize &quot;manifests/kube-state-metrics-serviceMonitor.yaml&quot;: no matches for kind &quot;ServiceMonitor&quot; in version &quot;monitoring.coreos.com/v1&quot;unable to recognize &quot;manifests/node-exporter-serviceMonitor.yaml&quot;: no matches for kind &quot;ServiceMonitor&quot; in version &quot;monitoring.coreos.com/v1&quot;unable to recognize &quot;manifests/prometheus-operator-serviceMonitor.yaml&quot;: no matches for kind &quot;ServiceMonitor&quot; in version &quot;monitoring.coreos.com/v1&quot;unable to recognize &quot;manifests/prometheus-prometheus.yaml&quot;: no matches for kind &quot;Prometheus&quot; in version &quot;monitoring.coreos.com/v1&quot;unable to recognize &quot;manifests/prometheus-rules.yaml&quot;: no matches for kind &quot;PrometheusRule&quot; in version &quot;monitoring.coreos.com/v1&quot;unable to recognize &quot;manifests/prometheus-serviceMonitor.yaml&quot;: no matches for kind &quot;ServiceMonitor&quot; in version &quot;monitoring.coreos.com/v1&quot;unable to recognize &quot;manifests/prometheus-serviceMonitorApiserver.yaml&quot;: no matches for kind &quot;ServiceMonitor&quot; in version &quot;monitoring.coreos.com/v1&quot;unable to recognize &quot;manifests/prometheus-serviceMonitorCoreDNS.yaml&quot;: no matches for kind &quot;ServiceMonitor&quot; in version &quot;monitoring.coreos.com/v1&quot;unable to recognize &quot;manifests/prometheus-serviceMonitorKubeControllerManager.yaml&quot;: no matches for kind &quot;ServiceMonitor&quot; in version &quot;monitoring.coreos.com/v1&quot;unable to recognize &quot;manifests/prometheus-serviceMonitorKubeScheduler.yaml&quot;: no matches for kind &quot;ServiceMonitor&quot; in version &quot;monitoring.coreos.com/v1&quot;unable to recognize &quot;manifests/prometheus-serviceMonitorKubelet.yaml&quot;: no matches for kind &quot;ServiceMonitor&quot; in version &quot;monitoring.coreos.com/v1&quot; 那么再次 kubectl apply -f manifests/ 即可；因为存在依赖。 但如果使用的是kube-prometheus：v0.3.0、v0.4.0、v0.5.0版本并出现了上面的提示【反复执行kubectl apply -f manifests/，但一直存在】，原因暂不清楚。 完毕！]]></content>
      <categories>
        <category>Kubernetes</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
        <tag>K8S</tag>
        <tag>CI/CD</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes K8S之CPU和内存资源限制详解]]></title>
    <url>%2F2021%2F01%2F04%2Fkubernetes28%2F</url>
    <content type="text"><![CDATA[Kubernetes K8S之CPU和内存资源限制详解 Pod资源限制备注：CPU单位换算：100m CPU，100 milliCPU 和 0.1 CPU 都相同；精度不能超过 1m。1000m CPU = 1 CPU。 官网地址： 12https://kubernetes.io/zh/docs/tasks/configure-pod-container/assign-cpu-resource/https://kubernetes.io/zh/docs/tasks/configure-pod-container/assign-memory-resource/ Kubernetes对资源的限制实际上是通过cgroup来控制的，cgroup是容器的一组用来控制内核如何运行进程的相关属性集合。针对内存、CPU和各种设备都有对应的cgroup。 默认情况下，Pod运行没有CPU和内存的限额。这意味着系统中的任何Pod将能够像执行Pod所在节点机器一样，可以消耗足够多的CPU和内存。一般会针对某些应用的Pod资源进行资源限制，这个资源限制是通过resources的requests【要分配的资源】和limits【最大使用资源】来实现的。 CPU资源限制示例 123456789101112131415161718# cat cpu-request-limit.yamlapiVersion: v1kind: Podmetadata: name: cpu-demo namespace: cpu-examplespec: containers: - name: cpu-demo-ctr image: vish/stress resources: limits: cpu: &quot;1&quot; requests: cpu: &quot;0.5&quot; args: - -cpus - &quot;2&quot; 配置文件的 args 部分提供了容器启动时的参数。-cpus “2”参数告诉容器尝试使用 2 个 CPU。 内存资源限制示例 1234567891011121314151617# memory-request-limit.yamlapiVersion: v1kind: Podmetadata: name: memory-demo namespace: mem-examplespec: containers: - name: memory-demo-ctr image: polinux/stress resources: limits: memory: &quot;200Mi&quot; requests: memory: &quot;100Mi&quot; command: [&quot;stress&quot;] args: [&quot;--vm&quot;, &quot;1&quot;, &quot;--vm-bytes&quot;, &quot;150M&quot;, &quot;--vm-hang&quot;, &quot;1&quot;] 配置文件的 args 部分提供了容器启动时的参数。 &quot;--vm-bytes&quot;, &quot;150M&quot; 参数告知容器尝试分配 150 MiB 内存。不允许args中的启动内存大于limits限制内存。 namespace资源限制备注：CPU单位换算：100m CPU，100 milliCPU 和 0.1 CPU 都相同；精度不能超过 1m。1000m CPU = 1 CPU。 官网地址： 1https://kubernetes.io/zh/docs/tasks/administer-cluster/manage-resources/ 为命名空间配置内存和 CPU 配额怎么为命名空间设置容器可用的内存和 CPU 总量。你可以通过 ResourceQuota 对象设置配额，使用 ResourceQuota 限制命名空间中所有容器的内存请求总量、内存限制总量、CPU 请求总量和CPU 限制总量。 如果你想对单个容器而不是所有容器进行限制，就请使用 LimitRange。 示例： 1234567891011# cat quota-mem-cpu.yamlapiVersion: v1kind: ResourceQuotametadata: name: mem-cpu-demospec: hard: requests.cpu: &quot;1&quot; requests.memory: 1Gi limits.cpu: &quot;2&quot; limits.memory: 2Gi 应用如下【命名空间quota-mem-cpu-example已提前创建完毕】： 1kubectl create -f quota-mem-cpu.yaml --namespace=quota-mem-cpu-example 查看 ResourceQuota 详情： 1kubectl get resourcequota mem-cpu-demo --namespace=quota-mem-cpu-example --output=yaml 输出部分结果如下： 1234567891011121314151617spec: hard: limits.cpu: &quot;2&quot; limits.memory: 2Gi requests.cpu: &quot;1&quot; requests.memory: 1Gistatus: hard: limits.cpu: &quot;2&quot; limits.memory: 2Gi requests.cpu: &quot;1&quot; requests.memory: 1Gi used: limits.cpu: &quot;0&quot; limits.memory: &quot;0&quot; requests.cpu: &quot;0&quot; requests.memory: &quot;0&quot; ResourceQuota 在 quota-mem-cpu-example 命名空间中设置了如下要求： 每个容器必须有内存请求和限制，以及 CPU 请求和限制。 所有容器的内存请求总和不能超过1 GiB。 所有容器的内存限制总和不能超过2 GiB。 所有容器的 CPU 请求总和不能超过1 cpu。 所有容器的 CPU 限制总和不能超过2 cpu。 为命名空间配置默认的内存请求和限制示例： 123456789101112# cat memory-defaults.yamlapiVersion: v1kind: LimitRangemetadata: name: mem-limit-rangespec: limits: - default: memory: 512Mi defaultRequest: memory: 256Mi type: Container default类似于之前的limit；defaultRequest类似于之前的request。 应用如下： 1kubectl create -f memory-defaults.yaml --namespace=default-mem-example 命名空间default-mem-example已提前创建完毕 现在，如果在 default-mem-example 命名空间创建容器，并且该容器没有声明自己的内存请求和限制值，那么它将被指定一个默认的内存请求256 MiB和一个默认的内存限制512 Mib。 为命名空间配置默认的CPU请求和限制示例： 123456789101112# cpu-defaults.yamlapiVersion: v1kind: LimitRangemetadata: name: cpu-limit-rangespec: limits: - default: cpu: 1 defaultRequest: cpu: 0.5 type: Container 应用如下： 1kubectl create -f cpu-defaults.yaml --namespace=default-cpu-example 其中default-cpu-example名称空间已被提前创建 现在如果在 default-cpu-example 命名空间创建一个容器，该容器没有声明自己的 CPU 请求和限制时，那么将会给它指定默认的 CPU 请求0.5和默认的 CPU 限制值1。 配置命名空间的最小和最大内存约束示例： 123456789101112# cat memory-constraints.yamlapiVersion: v1kind: LimitRangemetadata: name: mem-min-max-demo-lrspec: limits: - max: memory: 1Gi min: memory: 500Mi type: Container 应用如下： 1kubectl create -f memory-constraints.yaml --namespace=constraints-mem-example 其中constraints-mem-example名称空间已被提前创建 查看 LimitRange 的详情： 1kubectl get limitrange mem-min-max-demo-lr --namespace=constraints-mem-example --output=yaml 输出显示预期的最小和最大内存约束。但请注意，即使您没有在 LimitRange 的配置文件中指定默认值，默认值也会被自动创建。 12345678910limits:- default: memory: 1Gi defaultRequest: memory: 1Gi max: memory: 1Gi min: memory: 500Mi type: Container 现在，只要在 constraints-mem-example 命名空间中创建容器，Kubernetes 就会执行下面的步骤： 如果 Container 未指定自己的内存请求和限制，将为它指定默认的内存请求和限制。 验证 Container 的内存请求是否大于或等于500 MiB【超出范围容器创建失败】。 验证 Container 的内存限制是否小于或等于1 GiB【超出范围容器创建失败】。 配置命名空间的最小和最大CPU约束示例： 123456789101112# cpu-constraints.yamlapiVersion: v1kind: LimitRangemetadata: name: cpu-min-max-demo-lrspec: limits: - max: cpu: &quot;800m&quot; min: cpu: &quot;200m&quot; type: Container 应用如下： 1kubectl create -f cpu-constraints.yaml --namespace=constraints-cpu-example 其中constraints-cpu-example名称空间已被提前创建 查看 LimitRange 详情： 1kubectl get limitrange cpu-min-max-demo-lr --output=yaml --namespace=constraints-cpu-example 输出结果显示 CPU 的最小和最大限制符合预期。但需要注意的是，尽管你在 LimitRange 的配置文件中你没有声明默认值，默认值也会被自动创建。 12345678910limits:- default: cpu: 800m defaultRequest: cpu: 800m max: cpu: 800m min: cpu: 200m type: Container 现在不管什么时候在 constraints-cpu-example 命名空间中创建容器，Kubernetes 都会执行下面这些步骤： 如果容器没有声明自己的 CPU 请求和限制，将为容器指定默认 CPU 请求和限制。 核查容器声明的 CPU 请求确保其大于或者等于200 millicpu。 核查容器声明的 CPU 限制确保其小于或者等于800 millicpu。 配置命名空间下pod总数示例： 12345678# cat quota-pod.yamlapiVersion: v1kind: ResourceQuotametadata: name: pod-demospec: hard: pods: &quot;2&quot; 应用如下【命名空间quota-pod-example已提前创建完毕】： 1kubectl apply -f quota-pod.yaml --namespace=quota-pod-example 查看资源配额的详细信息： 1kubectl get resourcequota pod-demo --namespace=quota-pod-example --output=yaml 从输出的信息我们可以看到，该命名空间下pod的配额是2个，目前创建的pods数为0，配额使用率为0。 12345678spec: hard: pods: &quot;2&quot;status: hard: pods: &quot;2&quot; used: pods: &quot;0&quot; 相关阅读1、官网：Pod的CPU资源分配限制 2、官网：Pod的内存资源分配限制 3、官网：管理内存、CPU 和 API 资源 完毕！]]></content>
      <categories>
        <category>Kubernetes</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
        <tag>K8S</tag>
        <tag>CI/CD</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes K8S之通过helm部署metrics-server与HPA详解]]></title>
    <url>%2F2020%2F12%2F20%2Fkubernetes27%2F</url>
    <content type="text"><![CDATA[Kubernetes K8S之通过helm部署metrics-server与 Horizontal Pod Autoscaling (HPA)详解 主机配置规划 服务器名称(hostname) 系统版本 配置 内网IP 外网IP(模拟) k8s-master CentOS7.7 2C/4G/20G 172.16.1.110 10.0.0.110 k8s-node01 CentOS7.7 2C/4G/20G 172.16.1.111 10.0.0.111 k8s-node02 CentOS7.7 2C/4G/20G 172.16.1.112 10.0.0.112 使用Helm部署metrics-server正常情况下，如果没有部署metrics-server那么我们使用如下命令是收集不到信息的 12kubectl top nodekubectl top pod -A 首先完成「Kubernetes K8S之Helm部署、使用与示例」。 从Heapster的 GitHub地址：https://github.com/kubernetes-retired/heapster 中可以看到，heapster 已经 RETIRED【过时】。从Kubernetes 1.12开始将从Kubernetes各种安装脚本中移除。Kubernetes推荐使用metrics-server。我们这里使用Helm来部署metrics-server。 GitHub地址： 1https://github.com/kubernetes-sigs/metrics-server metrics-server-amd64镜像下载 1234# 在集群所有节点都需要执行docker pull registry.cn-beijing.aliyuncs.com/google_registry/metrics-server-amd64:v0.3.6docker tag registry.cn-beijing.aliyuncs.com/google_registry/metrics-server-amd64:v0.3.6 k8s.gcr.io/metrics-server-amd64:v0.3.6docker rmi registry.cn-beijing.aliyuncs.com/google_registry/metrics-server-amd64:v0.3.6 metrics-server.yaml文件 12345678[root@k8s-master helm]# pwd/root/k8s_practice/helm[root@k8s-master helm]# [root@k8s-master helm]# cat metrics-server.yaml args:- --logtostderr- --kubelet-insecure-tls- --kubelet-preferred-address-types=InternalIP 通过helm部署metrics-server 1234# 查询metrics-server的各个版本信息helm search stable/metrics-server -l# 通过helm部署metrics-server并指定了版本helm install stable/metrics-server --version 2.11.1 -n metrics-server --namespace kube-system -f metrics-server.yaml 查看helm和pod信息 123456789[root@k8s-master ~]# helm listNAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACE metrics-server 1 Mon Jul 20 10:06:58 2020 DEPLOYED metrics-server-2.11.1 0.3.6 kube-system[root@k8s-master ~]# [root@k8s-master ~]# kubectl get deploy -A | grep &apos;metrics-server&apos;kube-system metrics-server 1/1 1 1 94s[root@k8s-master ~]# [root@k8s-master ~]# kubectl get pod -A | grep &apos;metrics-server&apos;kube-system metrics-server-6796d97d6b-wvd48 1/1 Running 0 18s 现在使用下面的命令可以获取到关于集群Node节点和Pod的指标信息： 1234567891011121314[root@k8s-master ~]# kubectl top nodeNAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-master 205m 10% 2122Mi 57% k8s-node01 130m 6% 1060Mi 61% k8s-node02 193m 9% 1093Mi 63% [root@k8s-master ~]# [root@k8s-master ~]# kubectl top pod -ANAMESPACE NAME CPU(cores) MEMORY(bytes) default load-generator-57d5fb67c7-xx2wj 0m 0Mi default php-apache-5db466758-k2vxj 1m 20Mi kube-system coredns-6955765f44-c9zfh 5m 20Mi kube-system coredns-6955765f44-lrz5q 5m 20Mi kube-system etcd-k8s-master 21m 88Mi ……………… Horizontal Pod Autoscaling(HPA)Horizontal Pod Autoscaling(HPA) 可以根据CPU利用率自动伸缩一个Replication Controller、Deployment或者ReplicaSet中的Pod数量。 案例镜像下载 123# gcr.io/google_containers/hpa-example 在国外，这里我们下载国内镜像# 在集群所有节点都需要执行【主要是node节点】docker pull registry.cn-beijing.aliyuncs.com/google_registry/hpa-example 启动pod 1kubectl run php-apache --image=registry.cn-beijing.aliyuncs.com/google_registry/hpa-example --requests=cpu=200m --expose --port=80 创建HPA控制器，相关算法参见如下地址： 123# 官方地址：https://v1-17.docs.kubernetes.io/zh/docs/tasks/run-application/horizontal-pod-autoscale/# 当pod中CPU使用率达50%就扩容。最小1个，最大10个kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10 此时查询相关信息 12345678910[root@k8s-master ~]# kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEphp-apache Deployment/php-apache 0%/50% 1 10 1 3m16s[root@k8s-master ~]# [root@k8s-master ~]# kubectl get deploy -A | grep &apos;php-apache&apos;default php-apache 1/1 1 1 10h[root@k8s-master ~]# [root@k8s-master ~]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESphp-apache-5db466758-k2vxj 1/1 Running 0 8m8s 10.244.4.176 k8s-node01 &lt;none&gt; &lt;none&gt; 增加负载 123456# 启动一个pod并进入一个终端kubectl run -i --tty load-generator --image=registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 /bin/sh# 再次进入pod【注意pod名称，根据实际情况修改】kubectl exec -it load-generator-57d5fb67c7-xx2wj -- /bin/sh# 在pod中循环访问while true; do wget -q -O- http://php-apache.default.svc.cluster.local; done 如果需要多个终端访问，那么只需要修改tty终端名称【如：load-generator01】。进入后再次循环访问即可。 增加负载后【可能需要等会儿】，php-apache的pod数量达到了最大值10。此时的pod信息为： 123456789101112131415161718192021[root@k8s-master ~]# kubectl get hpaNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEphp-apache Deployment/php-apache 465%/50% 1 10 10 10h[root@k8s-master ~]# [root@k8s-master ~]# kubectl get deploy -A | grep &apos;php-apache&apos;default php-apache 10/10 10 10 10h[root@k8s-master ~]# [root@k8s-master ~]# kubectl get podNAME READY STATUS RESTARTS AGEload-generator-57d5fb67c7-xx2wj 1/1 Running 0 48mload-generator02-6c857bf9b5-cfmhj 1/1 Running 1 22sphp-apache-5db466758-2jv4x 1/1 Running 0 5m13sphp-apache-5db466758-4cxxm 1/1 Running 0 5m28sphp-apache-5db466758-6vz2b 1/1 Running 0 5m13sphp-apache-5db466758-98sqk 1/1 Running 0 4m58sphp-apache-5db466758-hdvrs 1/1 Running 0 5m13sphp-apache-5db466758-k2vxj 1/1 Running 0 62mphp-apache-5db466758-srctq 1/1 Running 0 4m58sphp-apache-5db466758-vkr5d 1/1 Running 0 5m28sphp-apache-5db466758-vmhlv 1/1 Running 0 5m13sphp-apache-5db466758-x8kms 1/1 Running 0 5m28s 此时，如果我们停止负载访问压测，pod数量也不会立即降下来。而是过段时间后才会慢慢降下来。 这也是为了安全起见，防止由于网络原因或者间歇性流量突增、突降，导致pod回收太快后面流量上来后Pod数量不够。 相关阅读1、Kubernetes K8S之Helm部署、使用与示例 2、Pod 水平自动伸缩 完毕！]]></content>
      <categories>
        <category>Kubernetes</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
        <tag>K8S</tag>
        <tag>CI/CD</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes K8S之Helm部署、使用与示例]]></title>
    <url>%2F2020%2F12%2F14%2Fkubernetes26%2F</url>
    <content type="text"><![CDATA[Kubernetes K8S之Helm部署、使用、常见操作与示例 主机配置规划 服务器名称(hostname) 系统版本 配置 内网IP 外网IP(模拟) k8s-master CentOS7.7 2C/4G/20G 172.16.1.110 10.0.0.110 k8s-node01 CentOS7.7 2C/4G/20G 172.16.1.111 10.0.0.111 k8s-node02 CentOS7.7 2C/4G/20G 172.16.1.112 10.0.0.112 Helm是什么没有使用Helm之前，在Kubernetes部署应用，我们要依次部署deployment、service等，步骤比较繁琐。况且随着很多项目微服务化，复杂的应用在容器中部署以及管理显得较为复杂。 helm通过打包的方式，支持发布的版本管理和控制，很大程度上简化了Kubernetes应用的部署和管理。 Helm本质就是让k8s的应用管理（Deployment、Service等）可配置，能动态生成。通过动态生成K8S资源清单文件（deployment.yaml、service.yaml）。然后kubectl自动调用K8S资源部署。 Helm是官方提供类似于YUM的包管理，是部署环境的流程封装，Helm有三个重要的概念：chart、release和Repository chart是创建一个应用的信息集合，包括各种Kubernetes对象的配置模板、参数定义、依赖关系、文档说明等。可以将chart想象成apt、yum中的软件安装包。 release是chart的运行实例，代表一个正在运行的应用。当chart被安装到Kubernetes集群，就生成一个release。chart能多次安装到同一个集群，每次安装都是一个release【根据chart赋值不同，完全可以部署出多个release出来】。 Repository用于发布和存储 Chart 的存储库。 Helm包含两个组件：Helm客户端和Tiller服务端，如下图所示： Helm 客户端负责 chart 和 release 的创建和管理以及和 Tiller 的交互。Tiller 服务端运行在 Kubernetes 集群中，它会处理Helm客户端的请求，与 Kubernetes API Server 交互。 Helm部署现在越来越多的公司和团队开始使用Helm这个Kubernetes的包管理器，我们也会使用Helm安装Kubernetes的常用组件。Helm由客户端命令helm工具和服务端tiller组成。 helm的GitHub地址 1https://github.com/helm/helm 本次部署版本 Helm安装部署 1234567891011[root@k8s-master software]# pwd/root/software [root@k8s-master software]# wget https://get.helm.sh/helm-v2.16.9-linux-amd64.tar.gz [root@k8s-master software]# [root@k8s-master software]# tar xf helm-v2.16.9-linux-amd64.tar.gz[root@k8s-master software]# lltotal 12624-rw-r--r-- 1 root root 12926032 Jun 16 06:55 helm-v3.2.4-linux-amd64.tar.gzdrwxr-xr-x 2 3434 3434 50 Jun 16 06:55 linux-amd64[root@k8s-master software]# [root@k8s-master software]# cp -a linux-amd64/helm /usr/bin/helm 因为Kubernetes API Server开启了RBAC访问控制，所以需要创建tiller的service account:tiller并分配合适的角色给它。这里为了简单起见我们直接分配cluster-admin这个集群内置的ClusterRole给它。 1234567891011121314151617181920212223242526[root@k8s-master helm]# pwd/root/k8s_practice/helm[root@k8s-master helm]# [root@k8s-master helm]# cat rbac-helm.yamlapiVersion: v1kind: ServiceAccountmetadata: name: tiller namespace: kube-system---apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: tillerroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: tiller namespace: kube-system[root@k8s-master helm]# [root@k8s-master helm]# kubectl apply -f rbac-helm.yaml serviceaccount/tiller createdclusterrolebinding.rbac.authorization.k8s.io/tiller created 初始化Helm的client 和 server 123456789101112131415161718192021222324252627282930313233[root@k8s-master helm]# helm init --service-account tiller………………[root@k8s-master helm]# kubectl get pod -n kube-system -o wide | grep &apos;tiller&apos;tiller-deploy-8488d98b4c-j8txs 0/1 Pending 0 38m &lt;none&gt; &lt;none&gt; &lt;none&gt; &lt;none&gt;[root@k8s-master helm]# ##### 之所有没有调度成功，就是因为拉取镜像包失败；查看需要拉取的镜像包[root@k8s-master helm]# kubectl describe pod tiller-deploy-8488d98b4c-j8txs -n kube-systemName: tiller-deploy-8488d98b4c-j8txsNamespace: kube-systemPriority: 0Node: &lt;none&gt;Labels: app=helm name=tiller pod-template-hash=8488d98b4cAnnotations: &lt;none&gt;Status: PendingIP: IPs: &lt;none&gt;Controlled By: ReplicaSet/tiller-deploy-8488d98b4cContainers: tiller: Image: gcr.io/kubernetes-helm/tiller:v2.16.9 Ports: 44134/TCP, 44135/TCP Host Ports: 0/TCP, 0/TCP Liveness: http-get http://:44135/liveness delay=1s timeout=1s period=10s #success=1 #failure=3 Readiness: http-get http://:44135/readiness delay=1s timeout=1s period=10s #success=1 #failure=3 Environment: TILLER_NAMESPACE: kube-system TILLER_HISTORY_MAX: 0 Mounts: /var/run/secrets/kubernetes.io/serviceaccount from tiller-token-kjqb7 (ro)Conditions:……………… 由上可见，镜像下载失败。原因是镜像在国外，因此这里需要修改镜像地址 12345[root@k8s-master helm]# helm init --upgrade --tiller-image registry.cn-beijing.aliyuncs.com/google_registry/tiller:v2.16.9[root@k8s-master helm]# ### 等待一会儿后[root@k8s-master helm]# kubectl get pod -o wide -A | grep &apos;till&apos;kube-system tiller-deploy-7b7787d77-zln6t 1/1 Running 0 8m43s 10.244.4.123 k8s-node01 &lt;none&gt; &lt;none&gt; 由上可见，Helm服务端tiller部署成功 helm版本信息查看 123[root@k8s-master helm]# helm versionClient: &amp;version.Version&#123;SemVer:&quot;v2.16.9&quot;, GitCommit:&quot;8ad7037828e5a0fca1009dabe290130da6368e39&quot;, GitTreeState:&quot;clean&quot;&#125;Server: &amp;version.Version&#123;SemVer:&quot;v2.16.9&quot;, GitCommit:&quot;8ad7037828e5a0fca1009dabe290130da6368e39&quot;, GitTreeState:&quot;dirty&quot;&#125; Helm使用helm源地址helm默认使用的charts源地址 1234[root@k8s-master helm]# helm repo listNAME URL stable https://kubernetes-charts.storage.googleapis.comlocal http://127.0.0.1:8879/charts 改变helm源【是否改变helm源，根据实际情况而定，一般不需要修改】 1234helm repo remove stablehelm repo add stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/chartshelm repo updatehelm repo list helm安装包下载存放位置 1/root/.helm/cache/archive helm常见应用操作123456789101112# 列出charts仓库中所有可用的应用helm search# 查询指定应用helm search memcached# 查询指定应用的具体信息helm inspect stable/memcached# 用helm安装软件包,--name:指定release名字helm install --name memcached1 stable/memcached# 查看安装的软件包helm list# 删除指定引用helm delete memcached1 helm常用命令chart管理 123456create：根据给定的name创建一个新chartfetch：从仓库下载chart，并(可选项)将其解压缩到本地目录中inspect：chart详情package：打包chart目录到一个chart归档lint：语法检测verify：验证位于给定路径的chart已被签名且有效 release管理 12345678get：下载一个releasedelete：根据给定的release name，从Kubernetes中删除指定的releaseinstall：安装一个chartlist：显示release列表upgrade：升级releaserollback：回滚release到之前的一个版本status：显示release状态信息history：Fetch release历史信息 helm常见操作123456789101112131415161718192021222324252627282930313233# 添加仓库helm repo add REPO_INFO # 如：helm repo add incubator http://storage.googleapis.com/kubernetes-charts-incubator##### 示例helm repo add incubator http://storage.googleapis.com/kubernetes-charts-incubatorhelm repo add elastic https://helm.elastic.co# 查看helm仓库列表helm repo list# 创建chart【可供参考，一般都是自己手动创建chart】helm create CHART_PATH# 根据指定chart部署一个releasehelm install --name RELEASE_NAME CHART_PATH# 根据指定chart模拟安装一个release，并打印处debug信息helm install --dry-run --debug --name RELEASE_NAME CHART_PATH# 列出已经部署的releasehelm list# 列出所有的releasehelm list --all# 查询指定release的状态helm status Release_NAME# 回滚到指定版本的release，这里指定的helm release版本helm rollback Release_NAME REVISION_NUM# 查看指定release的历史信息helm history Release_NAME# 对指定chart打包helm package CHART_PATH 如：helm package my-test-app/# 对指定chart进行语法检测helm lint CHART_PATH# 查看指定chart详情helm inspect CHART_PATH# 从Kubernetes中删除指定release相关的资源【helm list --all 中仍然可见release记录信息】helm delete RELEASE_NAME# 从Kubernetes中删除指定release相关的资源，并删除release记录helm delete --purge RELEASE_NAME 上述操作可结合下文示例，这样能看到更多细节。 helm示例chart文件信息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081[root@k8s-master helm]# pwd/root/k8s_practice/helm[root@k8s-master helm]# [root@k8s-master helm]# mkdir my-test-app[root@k8s-master helm]# cd my-test-app[root@k8s-master my-test-app]# [root@k8s-master my-test-app]# lltotal 8-rw-r--r-- 1 root root 158 Jul 16 17:53 Chart.yamldrwxr-xr-x 2 root root 49 Jul 16 21:04 templates-rw-r--r-- 1 root root 129 Jul 16 21:04 values.yaml[root@k8s-master my-test-app]# [root@k8s-master my-test-app]# cat Chart.yaml apiVersion: v1appVersion: v2.2description: my test appkeywords:- myappmaintainers:- email: zhang@test.com name: zhang# 该name值与上级目录名相同name: my-test-appversion: v1.0.0[root@k8s-master my-test-app]# [root@k8s-master my-test-app]# cat values.yaml deployname: my-test-app02replicaCount: 2images: repository: registry.cn-beijing.aliyuncs.com/google_registry/myapp tag: v2[root@k8s-master my-test-app]# [root@k8s-master my-test-app]# ll templates/total 8-rw-r--r-- 1 root root 544 Jul 16 21:04 deployment.yaml-rw-r--r-- 1 root root 222 Jul 16 20:41 service.yaml[root@k8s-master my-test-app]# [root@k8s-master my-test-app]# cat templates/deployment.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: &#123;&#123; .Values.deployname &#125;&#125; labels: app: mytestapp-deployspec: replicas: &#123;&#123; .Values.replicaCount &#125;&#125; selector: matchLabels: app: mytestapp env: test template: metadata: labels: app: mytestapp env: test description: mytest spec: containers: - name: myapp-pod image: &#123;&#123; .Values.images.repository &#125;&#125;:&#123;&#123; .Values.images.tag &#125;&#125; imagePullPolicy: IfNotPresent ports: - containerPort: 80[root@k8s-master my-test-app]# [root@k8s-master my-test-app]# cat templates/service.yaml apiVersion: v1kind: Servicemetadata: name: my-test-app namespace: defaultspec: type: NodePort selector: app: mytestapp env: test ports: - name: http port: 80 targetPort: 80 protocol: TCP 生成release12345678910111213141516171819202122232425262728293031[root@k8s-master my-test-app]# pwd/root/k8s_practice/helm/my-test-app[root@k8s-master my-test-app]# lltotal 8-rw-r--r-- 1 root root 160 Jul 16 21:15 Chart.yamldrwxr-xr-x 2 root root 49 Jul 16 21:04 templates-rw-r--r-- 1 root root 129 Jul 16 21:04 values.yaml[root@k8s-master my-test-app]# [root@k8s-master my-test-app]# helm install --name mytest-app01 . ### 如果在上级目录则为 helm install --name mytest-app01 my-test-app/NAME: mytest-app01LAST DEPLOYED: Thu Jul 16 21:18:08 2020NAMESPACE: defaultSTATUS: DEPLOYEDRESOURCES:==&gt; v1/DeploymentNAME READY UP-TO-DATE AVAILABLE AGEmy-test-app02 0/2 2 0 0s==&gt; v1/Pod(related)NAME READY STATUS RESTARTS AGEmy-test-app02-58cb6b67fc-4ss4v 0/1 ContainerCreating 0 0smy-test-app02-58cb6b67fc-w2nhc 0/1 ContainerCreating 0 0s==&gt; v1/ServiceNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEmy-test-app NodePort 10.110.82.62 &lt;none&gt; 80:30965/TCP 0s[root@k8s-master my-test-app]# helm listNAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACEmytest-app01 1 Thu Jul 16 21:18:08 2020 DEPLOYED my-test-app-v1.0.0 v2.2 default curl访问1234567891011121314151617181920212223242526[root@k8s-master ~]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmy-test-app02-58cb6b67fc-4ss4v 1/1 Running 0 9m3s 10.244.2.187 k8s-node02 &lt;none&gt; &lt;none&gt;my-test-app02-58cb6b67fc-w2nhc 1/1 Running 0 9m3s 10.244.4.134 k8s-node01 &lt;none&gt; &lt;none&gt;[root@k8s-master ~]# [root@k8s-master ~]# kubectl get svc -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 65d &lt;none&gt;my-test-app NodePort 10.110.82.62 &lt;none&gt; 80:30965/TCP 9m8s app=mytestapp,env=test[root@k8s-master ~]###### 根据svc的IP访问[root@k8s-master ~]# curl 10.110.82.62Hello MyApp | Version: v2 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;[root@k8s-master ~]# [root@k8s-master ~]# curl 10.110.82.62/hostname.htmlmy-test-app02-58cb6b67fc-4ss4v[root@k8s-master ~]# [root@k8s-master ~]# curl 10.110.82.62/hostname.htmlmy-test-app02-58cb6b67fc-w2nhc[root@k8s-master ~]# ##### 根据本机的IP访问[root@k8s-master ~]# curl 172.16.1.110:30965/hostname.htmlmy-test-app02-58cb6b67fc-w2nhc[root@k8s-master ~]# [root@k8s-master ~]# curl 172.16.1.110:30965/hostname.htmlmy-test-app02-58cb6b67fc-4ss4v chart更新values.yaml文件修改 12345678910[root@k8s-master my-test-app]# pwd/root/k8s_practice/helm/my-test-app[root@k8s-master my-test-app]# [root@k8s-master my-test-app]# cat values.yaml deployname: my-test-app02replicaCount: 2images: repository: registry.cn-beijing.aliyuncs.com/google_registry/myapp # 改了tag tag: v3 重新release发布 123456789101112131415161718192021222324252627282930[root@k8s-master my-test-app]# helm listNAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACEmytest-app01 1 Thu Jul 16 21:18:08 2020 DEPLOYED my-test-app-v1.0.0 v2.2 default [root@k8s-master my-test-app]# [root@k8s-master my-test-app]# helm upgrade mytest-app01 . ### 如果在上级目录则为 helm upgrade mytest-app01 my-test-app/Release &quot;mytest-app01&quot; has been upgraded.LAST DEPLOYED: Thu Jul 16 21:32:25 2020NAMESPACE: defaultSTATUS: DEPLOYEDRESOURCES:==&gt; v1/DeploymentNAME READY UP-TO-DATE AVAILABLE AGEmy-test-app02 2/2 1 2 14m==&gt; v1/Pod(related)NAME READY STATUS RESTARTS AGEmy-test-app02-58cb6b67fc-4ss4v 1/1 Running 0 14mmy-test-app02-58cb6b67fc-w2nhc 1/1 Running 0 14mmy-test-app02-6b84df49bb-lpww7 0/1 ContainerCreating 0 0s==&gt; v1/ServiceNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEmy-test-app NodePort 10.110.82.62 &lt;none&gt; 80:30965/TCP 14m[root@k8s-master my-test-app]# [root@k8s-master my-test-app]# helm listNAME REVISION UPDATED STATUS CHART APP VERSION NAMESPACEmytest-app01 2 Thu Jul 16 21:32:25 2020 DEPLOYED my-test-app-v1.0.0 v2.2 default curl访问，可参见上面。可见app version已从v2改为了v3。 相关阅读1、Helm官网地址 2、Helm官网部署helm 3、Helm的GitHub地址 完毕！]]></content>
      <categories>
        <category>Kubernetes</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
        <tag>K8S</tag>
        <tag>CI/CD</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes K8S之鉴权RBAC详解]]></title>
    <url>%2F2020%2F12%2F06%2Fkubernetes25%2F</url>
    <content type="text"><![CDATA[Kubernetes K8S之鉴权概述与RBAC详解 K8S认证与授权 认证「Authentication」认证有如下几种方式： 1、HTTP Token认证：通过一个Token来识别合法用户。 HTTP Token的认证是用一个很长的特殊编码方式的并且难以被模仿的字符串来表达客户的一种方式。每一个Token对应一个用户名，存储在API Server能访问的文件中。当客户端发起API调用请求时，需要在HTTP Header里放入Token。 2、HTTP Base认证：通过用户名+密码的方式认证 用户名:密码 用base64算法进行编码后的字符串放在HTTP Request中的Heather Authorization 域里发送给服务端，服务端收到后进行解码，获取用户名和密码。 3、最严格的HTTPS证书认证：基于CA根证书签名的客户端身份认证方式 授权「Authorization」认证只是确认通信的双方都是可信的，可以相互通信。而授权是确定请求方有哪些资源的权限。API Server目前支持如下几种授权策略（通过API Server的启动参数 --authorization-mode 设置） AlwaysDeny：表示拒绝所有请求。仅用于测试 AlwaysAllow：表示允许所有请求。如果有集群不需要授权流程，则可以采用该策略 Node：节点授权是一种特殊用途的授权模式，专门授权由 kubelet 发出的 API 请求 Webhook：是一种 HTTP 回调模式，允许使用远程 REST 端点管理授权 ABAC：基于属性的访问控制，表示使用用户配置的授权规则对用户请求进行匹配和控制 RBAC：基于角色的访问控制，默认使用该规则 RBAC授权模式RBAC（Role-Based Access Control）基于角色的访问控制，在Kubernetes 1.5 中引入，现为默认标准。相对其他访问控制方式，拥有如下优势： 1、对集群中的资源和非资源均拥有完整的覆盖 2、整个RBAC完全由几个API对象完成，同其他API对象一样，可以用kubectl或API进行操作 3、可以在运行时进行操作，无需重启API Server RBAC API类型RBAC API 所声明的四种顶级类型【Role、ClusterRole、RoleBinding 和 ClusterRoleBinding】。用户可以像与其他 API 资源交互一样，（通过 kubectl API 调用等方式）与这些资源交互。 Role 和 ClusterRole在 RBAC API 中，一个角色包含一组相关权限的规则。权限是纯粹累加的（不存在拒绝某操作的规则），即只能给权限累加，不存在给了XX权限，然后去掉XX01权限的情况。角色可以用 Role 来定义到某个命名空间（namespace）上， 或者用 ClusterRole 来定义到整个集群作用域（所有namespace）。 一个 Role 只可以用来对某一命名空间中的资源赋予访问权限。 Role示例： 定义到名称为 “default” 的命名空间，可以用来授予对该命名空间中的 Pods 的读取权限： 123456789apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: name: pod-reader namespace: defaultrules:- apiGroups: [&quot;&quot;] # &quot;&quot; 指定核心 API 组 resources: [&quot;pods&quot;] verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] ClusterRole 可以授予的权限和 Role 相同，但是因为 ClusterRole 属于集群范围，所以它也可以授予以下访问权限： 集群范围资源 （比如 nodes访问） 非资源端点（比如 “/healthz” 访问） 跨命名空间访问的有名称空间作用域的资源（如 Pods），比如运行命令kubectl get pods --all-namespaces 时需要此能力 ClusterRole示例 可用来对某特定命名空间下的 Secrets 的读取操作授权，或者跨所有名称空间执行授权（取决于它是如何绑定的）： 123456789apiVersion: rbac.authorization.k8s.io/v1kind: ClusterRolemetadata: name: secret-reader # 此处的 &quot;namespace&quot; 被省略掉是因为 ClusterRoles 是没有命名空间的。rules:- apiGroups: [&quot;&quot;] resources: [&quot;secrets&quot;] verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] RoleBinding 和 ClusterRoleBinding角色绑定（RoleBinding）是将角色中定义的权限赋予一个用户或者一组用户。 它包含若干主体【subjects】（users、groups或 service accounts）的列表和对这些主体所获得的角色引用。 可以使用 RoleBinding 在指定的命名空间中执行授权，或者在集群范围的命名空间使用 ClusterRoleBinding 来执行授权。 一个 RoleBinding 可以引用同一的命名空间中的 Role。 RoleBinding示例 将 “pod-reader” 角色授予在 “default” 命名空间中的用户 “jane”； 这样，用户 “jane” 就具有了读取 “default” 命名空间中 pods 的权限。 在下面的例子中，角色绑定使用 roleRef 将用户 “jane” 绑定到前文创建的角色 Role，其名称是 pod-reader。 1234567891011121314apiVersion: rbac.authorization.k8s.io/v1# 此角色绑定，使得用户 &quot;jane&quot; 能够读取 &quot;default&quot; 命名空间中的 Podskind: RoleBindingmetadata: name: read-pods namespace: defaultsubjects:- kind: User name: jane # 名称大小写敏感 apiGroup: rbac.authorization.k8s.ioroleRef: kind: Role #this must be Role or ClusterRole name: pod-reader # 这里的名称必须与你想要绑定的 Role 或 ClusterRole 名称一致 apiGroup: rbac.authorization.k8s.io roleRef 里的内容决定了实际创建绑定的方法。kind 可以是 Role 或 ClusterRole，name 是你要引用的 Role 或 ClusterRole 的名称。 RoleBinding 也可以引用 ClusterRole，这可以允许管理者在 整个集群中定义一组通用的角色，然后在多个命名空间中重用它们。 RoleBinding示例2 下面的例子，RoleBinding 引用的是 ClusterRole， “dave” （subjects区分大小写）将只可以读取在”development” 名称空间（ RoleBinding 的命名空间）中的”secrets” 。 1234567891011121314apiVersion: rbac.authorization.k8s.io/v1# 这个角色绑定允许 &quot;dave&quot; 用户在 &quot;development&quot; 命名空间中有读取 secrets 的权限。 kind: RoleBindingmetadata: name: read-secrets namespace: development # 这里只授予 &quot;development&quot; 命名空间的权限。subjects:- kind: User name: dave # 名称区分大小写 apiGroup: rbac.authorization.k8s.ioroleRef: kind: ClusterRole name: secret-reader apiGroup: rbac.authorization.k8s.io 最后，ClusterRoleBinding 可用来在集群级别并对所有命名空间执行授权。 ClusterRoleBinding示例 12345678910111213apiVersion: rbac.authorization.k8s.io/v1# 这个集群角色绑定允许 &quot;manager&quot; 组中的任何用户读取任意命名空间中 &quot;secrets&quot;。kind: ClusterRoleBindingmetadata: name: read-secrets-globalsubjects:- kind: Group name: manager # 名称区分大小写 apiGroup: rbac.authorization.k8s.ioroleRef: kind: ClusterRole name: secret-reader apiGroup: rbac.authorization.k8s.io 当我们创建binding后，则不能修改binding所引用的Role或ClusterRole。尝试修改会导致验证错误；如果要改变binding的roleRef，那么应该删除该binding对象并且创建一个新的用来替换原来的。 Referring to resources【资源引用】Kubernetes集群内一些资源一般以其名称字符串来表示，这些字符串一般会在API的URL地址中出现；同时某些资源也会包含子资源，例如pod的logs资源就属于pods的子资源，API中URL样例如下： 1GET /api/v1/namespaces/&#123;namespace&#125;/pods/&#123;name&#125;/log 在这种情况下，”pods” 是有名称空间的资源，而 “log” 是 pods 的子资源。在 RBAC 角色中，使用”/“分隔资源和子资源。 允许一个主体（subject）要同时读取 pods 和 pod logs，你可以这么写： 123456789apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: namespace: default name: pod-and-pod-logs-readerrules:- apiGroups: [&quot;&quot;] resources: [&quot;pods&quot;, &quot;pods/log&quot;] verbs: [&quot;get&quot;, &quot;list&quot;] 对于某些请求，也可以通过 resourceNames 列表按名称引用资源。 例如：在指定时，可以将请求类型限制到资源的单个实例。限制只可以 “get” 和 “update” 到单个configmap，则可以这么写： 12345678910apiVersion: rbac.authorization.k8s.io/v1kind: Rolemetadata: namespace: default name: configmap-updaterrules:- apiGroups: [&quot;&quot;] resources: [&quot;configmaps&quot;] resourceNames: [&quot;my-configmap&quot;] verbs: [&quot;update&quot;, &quot;get&quot;] 需要注意的是，create 请求不能被 resourceName 限制，因为在鉴权时还不知道对象名称。 另一个例外是 deletecollection。 Referring to subjects【主体引用】RoleBinding或ClusterRoleBinding绑定一个role到主体（subjects）。主体（subjects）可以是groups，users或ServiceAccounts。 Kubernetes将用户名表示为字符串。这些可以是：普通名称，比如“alice” ；邮件风格的名字，比如“bob@example.com” ；或表示为字符串的数字用户id。 注意：前缀 system: 是保留给Kubernetes系统使用的，因此应该确保不会出现名称以system: 开头的用户或组。除了这个特殊的前缀，RBAC授权系统不要求用户名使用任何格式。 ServiceAccounts具有前缀为system:serviceaccount: 的名称，属于具有前缀为system:serviceaccounts:的名称的组。 RoleBinding的示例下面的示例只是展示 RoleBinding 中 subjects 的部分。 用户的名称为 “alice@example.com” ： 1234subjects:- kind: User name: &quot;alice@example.com&quot; apiGroup: rbac.authorization.k8s.io 组的名称为 “frontend-admins” ： 1234subjects:- kind: Group name: &quot;frontend-admins&quot; apiGroup: rbac.authorization.k8s.io 默认service account在 kube-system 命名空间中： 1234subjects:- kind: ServiceAccount name: default namespace: kube-system 在名称为 “qa” 命名空间中所有的服务账号： 1234subjects:- kind: Group name: system:serviceaccounts:qa apiGroup: rbac.authorization.k8s.io 在任意名称空间的所有service accounts： 1234subjects:- kind: Group name: system:serviceaccounts apiGroup: rbac.authorization.k8s.io 所有认证过的用户（版本 1.5+）： 1234subjects:- kind: Group name: system:authenticated apiGroup: rbac.authorization.k8s.io 所有未认证的用户（版本 1.5+）： 1234subjects:- kind: Group name: system:unauthenticated apiGroup: rbac.authorization.k8s.io 所有用户 （版本 1.5+）： 1234567subjects:- kind: Group name: system:authenticated apiGroup: rbac.authorization.k8s.io- kind: Group name: system:unauthenticated apiGroup: rbac.authorization.k8s.io 准入控制准入控制是API Server的插件集合，通过添加不同的插件，实现额外的准入控制规则。甚至于API Server的一些主要的功能都需要通过Admission Controllers实现，比如：ServiceAccount。 查看哪些插件是默认启用的： 1kube-apiserver -h | grep enable-admission-plugins 在 1.17 中，它们是： 1234NamespaceLifecycle, LimitRanger, ServiceAccount, TaintNodesByCondition, Priority, DefaultTolerationSeconds, DefaultStorageClass, StorageObjectInUseProtection, PersistentVolumeClaimResize, MutatingAdmissionWebhook, ValidatingAdmissionWebhook, RuntimeClass, ResourceQuota 部分插件功能： NamespaceLifecycle 该准入控制器禁止在一个正在被终止的 Namespace 中创建新对象，并确保使用不存在的 Namespace 的请求被拒绝。该准入控制器还会禁止删除三个系统保留的命名空间，即 default、kube-system 和 kube-public。 删除 Namespace 会触发删除该命名空间中所有对象（pod、services 等）的一系列操作。为了确保这个过程的完整性，我们强烈建议启用这个准入控制器。 LimitRanger 该准入控制器会观察传入的请求，并确保它不会违反 Namespace 中 LimitRange 对象枚举的任何约束。 ServiceAccount 该准入控制器实现了 serviceAccounts 的自动化。 如果打算使用 Kubernetes 的 ServiceAccount 对象，强烈建议您使用这个准入控制器。 ResourceQuota 该准入控制器会监测传入的请求，并确保它不违反任何一个 Namespace 中的 ResourceQuota 对象中枚举出来的约束。 相关阅读1、官网：使用 RBAC 鉴权]]></content>
      <categories>
        <category>Kubernetes</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
        <tag>K8S</tag>
        <tag>CI/CD</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes K8S之固定节点nodeName和nodeSelector调度详解]]></title>
    <url>%2F2020%2F11%2F29%2Fkubernetes24%2F</url>
    <content type="text"><![CDATA[Kubernetes K8S之固定节点nodeName和nodeSelector调度详解与示例 主机配置规划 服务器名称(hostname) 系统版本 配置 内网IP 外网IP(模拟) k8s-master CentOS7.7 2C/4G/20G 172.16.1.110 10.0.0.110 k8s-node01 CentOS7.7 2C/4G/20G 172.16.1.111 10.0.0.111 k8s-node02 CentOS7.7 2C/4G/20G 172.16.1.112 10.0.0.112 nodeName调度nodeName是节点选择约束的最简单形式，但是由于其限制，通常很少使用它。nodeName是PodSpec的领域。 pod.spec.nodeName将Pod直接调度到指定的Node节点上，会【跳过Scheduler的调度策略】，该匹配规则是【强制】匹配。可以越过Taints污点进行调度。 nodeName用于选择节点的一些限制是： 如果指定的节点不存在，则容器将不会运行，并且在某些情况下可能会自动删除。 如果指定的节点没有足够的资源来容纳该Pod，则该Pod将会失败，并且其原因将被指出，例如OutOfmemory或OutOfcpu。 云环境中的节点名称并非总是可预测或稳定的。 nodeName示例获取当前的节点信息 12345[root@k8s-master scheduler]# kubectl get nodes -o wideNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIMEk8s-master Ready master 42d v1.17.4 172.16.1.110 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-1062.el7.x86_64 docker://19.3.8k8s-node01 Ready &lt;none&gt; 42d v1.17.4 172.16.1.111 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-1062.el7.x86_64 docker://19.3.8k8s-node02 Ready &lt;none&gt; 42d v1.17.4 172.16.1.112 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-1062.el7.x86_64 docker://19.3.8 当nodeName指定节点存在要运行的yaml文件 123456789101112131415161718192021222324252627[root@k8s-master scheduler]# pwd/root/k8s_practice/scheduler[root@k8s-master scheduler]# cat scheduler_nodeName.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: scheduler-nodename-deploy labels: app: nodename-deployspec: replicas: 5 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: - name: myapp-pod image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 # 指定节点运行 nodeName: k8s-master 运行yaml文件并查看信息 123456789101112131415161718[root@k8s-master scheduler]# kubectl apply -f scheduler_nodeName.yaml deployment.apps/scheduler-nodename-deploy created[root@k8s-master scheduler]# [root@k8s-master scheduler]# kubectl get deploy -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORscheduler-nodename-deploy 0/5 5 0 6s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp[root@k8s-master scheduler]# [root@k8s-master scheduler]# kubectl get rs -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTORscheduler-nodename-deploy-d5c9574bd 5 5 5 15s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp,pod-template-hash=d5c9574bd[root@k8s-master scheduler]# [root@k8s-master scheduler]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESscheduler-nodename-deploy-d5c9574bd-6l9d8 1/1 Running 0 23s 10.244.0.123 k8s-master &lt;none&gt; &lt;none&gt;scheduler-nodename-deploy-d5c9574bd-c82cc 1/1 Running 0 23s 10.244.0.119 k8s-master &lt;none&gt; &lt;none&gt;scheduler-nodename-deploy-d5c9574bd-dkkjg 1/1 Running 0 23s 10.244.0.122 k8s-master &lt;none&gt; &lt;none&gt;scheduler-nodename-deploy-d5c9574bd-hcn77 1/1 Running 0 23s 10.244.0.121 k8s-master &lt;none&gt; &lt;none&gt;scheduler-nodename-deploy-d5c9574bd-zstjx 1/1 Running 0 23s 10.244.0.120 k8s-master &lt;none&gt; &lt;none&gt; 由上可见，yaml文件中nodeName: k8s-master生效，所有pod被调度到了k8s-master节点。如果这里是nodeName: k8s-node02，那么就会直接调度到k8s-node02节点。 当nodeName指定节点不存在要运行的yaml文件 123456789101112131415161718192021222324252627[root@k8s-master scheduler]# pwd/root/k8s_practice/scheduler[root@k8s-master scheduler]# cat scheduler_nodeName_02.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: scheduler-nodename-deploy labels: app: nodename-deployspec: replicas: 5 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: - name: myapp-pod image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 # 指定节点运行，该节点不存在 nodeName: k8s-node08 运行yaml文件并查看信息 123456789101112131415161718[root@k8s-master scheduler]# kubectl apply -f scheduler_nodeName_02.yaml deployment.apps/scheduler-nodename-deploy created[root@k8s-master scheduler]# [root@k8s-master scheduler]# kubectl get deploy -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORscheduler-nodename-deploy 0/5 5 0 4s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp[root@k8s-master scheduler]# [root@k8s-master scheduler]# kubectl get rs -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTORscheduler-nodename-deploy-75944bdc5d 5 5 0 9s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp,pod-template-hash=75944bdc5d[root@k8s-master scheduler]# [root@k8s-master scheduler]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESscheduler-nodename-deploy-75944bdc5d-c8f5d 0/1 Pending 0 13s &lt;none&gt; k8s-node08 &lt;none&gt; &lt;none&gt;scheduler-nodename-deploy-75944bdc5d-hfdlv 0/1 Pending 0 13s &lt;none&gt; k8s-node08 &lt;none&gt; &lt;none&gt;scheduler-nodename-deploy-75944bdc5d-q9qgt 0/1 Pending 0 13s &lt;none&gt; k8s-node08 &lt;none&gt; &lt;none&gt;scheduler-nodename-deploy-75944bdc5d-q9zl7 0/1 Pending 0 13s &lt;none&gt; k8s-node08 &lt;none&gt; &lt;none&gt;scheduler-nodename-deploy-75944bdc5d-wxsnv 0/1 Pending 0 13s &lt;none&gt; k8s-node08 &lt;none&gt; &lt;none&gt; 由上可见，如果指定的节点不存在，则容器将不会运行，一直处于Pending 状态。 nodeSelector调度nodeSelector是节点选择约束的最简单推荐形式。nodeSelector是PodSpec的领域。它指定键值对的映射。 Pod.spec.nodeSelector是通过Kubernetes的label-selector机制选择节点，由调度器调度策略匹配label，而后调度Pod到目标节点，该匹配规则属于【强制】约束。由于是调度器调度，因此不能越过Taints污点进行调度。 nodeSelector示例获取当前的节点信息 12345[root@k8s-master ~]# kubectl get node -o wide --show-labelsNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME LABELSk8s-master Ready master 42d v1.17.4 172.16.1.110 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-1062.el7.x86_64 docker://19.3.8 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master,kubernetes.io/os=linux,node-role.kubernetes.io/master=k8s-node01 Ready &lt;none&gt; 42d v1.17.4 172.16.1.111 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-1062.el7.x86_64 docker://19.3.8 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node01,kubernetes.io/os=linuxk8s-node02 Ready &lt;none&gt; 42d v1.17.4 172.16.1.112 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-1062.el7.x86_64 docker://19.3.8 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node02,kubernetes.io/os=linux 添加label标签运行kubectl get nodes以获取群集节点的名称。然后可以对指定节点添加标签。比如：k8s-node01的磁盘为SSD，那么添加disk-type=ssd；k8s-node02的CPU核数高，那么添加cpu-type=hight；如果为Web机器，那么添加service-type=web。怎么添加标签可以根据实际规划情况而定。 12345678910### 给k8s-node01 添加指定标签[root@k8s-master ~]# kubectl label nodes k8s-node01 disk-type=ssdnode/k8s-node01 labeled#### 删除标签命令 kubectl label nodes k8s-node01 disk-type-[root@k8s-master ~]# [root@k8s-master ~]# kubectl get node --show-labelsNAME STATUS ROLES AGE VERSION LABELSk8s-master Ready master 42d v1.17.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master,kubernetes.io/os=linux,node-role.kubernetes.io/master=k8s-node01 Ready &lt;none&gt; 42d v1.17.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,disk-type=ssd,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node01,kubernetes.io/os=linuxk8s-node02 Ready &lt;none&gt; 42d v1.17.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node02,kubernetes.io/os=linux 由上可见，已经为k8s-node01节点添加了disk-type=ssd 标签。 当nodeSelector标签存在要运行的yaml文件 1234567891011121314151617181920212223242526272829[root@k8s-master scheduler]# pwd/root/k8s_practice/scheduler[root@k8s-master scheduler]# [root@k8s-master scheduler]# cat scheduler_nodeSelector.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: scheduler-nodeselector-deploy labels: app: nodeselector-deployspec: replicas: 5 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: - name: myapp-pod image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 # 指定节点标签选择，且标签存在 nodeSelector: disk-type: ssd 运行yaml文件并查看信息 123456789101112131415161718[root@k8s-master scheduler]# kubectl apply -f scheduler_nodeSelector.yaml deployment.apps/scheduler-nodeselector-deploy created[root@k8s-master scheduler]# [root@k8s-master scheduler]# kubectl get deploy -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORscheduler-nodeselector-deploy 5/5 5 5 10s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp[root@k8s-master scheduler]# [root@k8s-master scheduler]# kubectl get rs -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTORscheduler-nodeselector-deploy-79455db454 5 5 5 14s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp,pod-template-hash=79455db454[root@k8s-master scheduler]# [root@k8s-master scheduler]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESscheduler-nodeselector-deploy-79455db454-745ph 1/1 Running 0 19s 10.244.4.154 k8s-node01 &lt;none&gt; &lt;none&gt;scheduler-nodeselector-deploy-79455db454-bmjvd 1/1 Running 0 19s 10.244.4.151 k8s-node01 &lt;none&gt; &lt;none&gt;scheduler-nodeselector-deploy-79455db454-g5cg2 1/1 Running 0 19s 10.244.4.153 k8s-node01 &lt;none&gt; &lt;none&gt;scheduler-nodeselector-deploy-79455db454-hw8jv 1/1 Running 0 19s 10.244.4.152 k8s-node01 &lt;none&gt; &lt;none&gt;scheduler-nodeselector-deploy-79455db454-zrt8d 1/1 Running 0 19s 10.244.4.155 k8s-node01 &lt;none&gt; &lt;none&gt; 由上可见，所有pod都被调度到了k8s-node01节点。当然如果其他节点也有disk-type=ssd 标签，那么pod也会调度到这些节点上。 当nodeSelector标签不存在要运行的yaml文件 1234567891011121314151617181920212223242526272829[root@k8s-master scheduler]# pwd/root/k8s_practice/scheduler[root@k8s-master scheduler]# [root@k8s-master scheduler]# cat scheduler_nodeSelector_02.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: scheduler-nodeselector-deploy labels: app: nodeselector-deployspec: replicas: 5 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: - name: myapp-pod image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 # 指定节点标签选择，且标签不存在 nodeSelector: service-type: web 运行yaml文件并查看信息 123456789101112131415161718[root@k8s-master scheduler]# kubectl apply -f scheduler_nodeSelector_02.yaml deployment.apps/scheduler-nodeselector-deploy created[root@k8s-master scheduler]# [root@k8s-master scheduler]# kubectl get deploy -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORscheduler-nodeselector-deploy 0/5 5 0 26s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp[root@k8s-master scheduler]# [root@k8s-master scheduler]# kubectl get rs -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTORscheduler-nodeselector-deploy-799d748db6 5 5 0 30s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp,pod-template-hash=799d748db6[root@k8s-master scheduler]# [root@k8s-master scheduler]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESscheduler-nodeselector-deploy-799d748db6-92mqj 0/1 Pending 0 40s &lt;none&gt; &lt;none&gt; &lt;none&gt; &lt;none&gt;scheduler-nodeselector-deploy-799d748db6-c2w25 0/1 Pending 0 40s &lt;none&gt; &lt;none&gt; &lt;none&gt; &lt;none&gt;scheduler-nodeselector-deploy-799d748db6-c8tlx 0/1 Pending 0 40s &lt;none&gt; &lt;none&gt; &lt;none&gt; &lt;none&gt;scheduler-nodeselector-deploy-799d748db6-tc5n7 0/1 Pending 0 40s &lt;none&gt; &lt;none&gt; &lt;none&gt; &lt;none&gt;scheduler-nodeselector-deploy-799d748db6-z8c57 0/1 Pending 0 40s &lt;none&gt; &lt;none&gt; &lt;none&gt; &lt;none&gt; 由上可见，如果nodeSelector匹配的标签不存在，则容器将不会运行，一直处于Pending 状态。 相关阅读1、官网：Pod分配调度 2、Kubernetes K8S之调度器kube-scheduler详解 3、Kubernetes K8S之affinity亲和性与反亲和性详解与示例 4、Kubernetes K8S之Taints污点与Tolerations容忍详解 完毕！]]></content>
      <categories>
        <category>Kubernetes</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
        <tag>K8S</tag>
        <tag>CI/CD</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes K8S之Taints污点与Tolerations容忍详解]]></title>
    <url>%2F2020%2F11%2F20%2Fkubernetes23%2F</url>
    <content type="text"><![CDATA[Kubernetes K8S之Taints污点与Tolerations容忍详解与示例 主机配置规划 服务器名称(hostname) 系统版本 配置 内网IP 外网IP(模拟) k8s-master CentOS7.7 2C/4G/20G 172.16.1.110 10.0.0.110 k8s-node01 CentOS7.7 2C/4G/20G 172.16.1.111 10.0.0.111 k8s-node02 CentOS7.7 2C/4G/20G 172.16.1.112 10.0.0.112 Taints污点和Tolerations容忍概述节点和Pod亲和力，是将Pod吸引到一组节点【根据拓扑域】（作为优选或硬性要求）。污点（Taints）则相反，它们允许一个节点排斥一组Pod。 容忍（Tolerations）应用于pod，允许（但不强制要求）pod调度到具有匹配污点的节点上。 污点（Taints）和容忍（Tolerations）共同作用，确保pods不会被调度到不适当的节点。一个或多个污点应用于节点；这标志着该节点不应该接受任何不容忍污点的Pod。 说明：我们在平常使用中发现pod不会调度到k8s的master节点，就是因为master节点存在污点。 Taints污点Taints污点的组成使用kubectl taint命令可以给某个Node节点设置污点，Node被设置污点之后就和Pod之间存在一种相斥的关系，可以让Node拒绝Pod的调度执行，甚至将Node上已经存在的Pod驱逐出去。 每个污点的组成如下： 1key=value:effect 每个污点有一个key和value作为污点的标签，effect描述污点的作用。当前taint effect支持如下选项： NoSchedule：表示K8S将不会把Pod调度到具有该污点的Node节点上 PreferNoSchedule：表示K8S将尽量避免把Pod调度到具有该污点的Node节点上 NoExecute：表示K8S将不会把Pod调度到具有该污点的Node节点上，同时会将Node上已经存在的Pod驱逐出去 污点taint的NoExecute详解taint 的 effect 值 NoExecute，它会影响已经在节点上运行的 pod： 如果 pod 不能容忍 effect 值为 NoExecute 的 taint，那么 pod 将马上被驱逐 如果 pod 能够容忍 effect 值为 NoExecute 的 taint，且在 toleration 定义中没有指定 tolerationSeconds，则 pod 会一直在这个节点上运行。 如果 pod 能够容忍 effect 值为 NoExecute 的 taint，但是在toleration定义中指定了 tolerationSeconds，则表示 pod 还能在这个节点上继续运行的时间长度。 Taints污点设置污点（Taints）查看k8s master节点查看 1kubectl describe node k8s-master k8s node查看 1kubectl describe node k8s-node01 污点（Taints）添加123456789101112131415161718192021222324[root@k8s-master taint]# kubectl taint nodes k8s-node01 check=zhang:NoSchedulenode/k8s-node01 tainted[root@k8s-master taint]# [root@k8s-master taint]# kubectl describe node k8s-node01Name: k8s-node01Roles: &lt;none&gt;Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux cpu-num=12 disk-type=ssd kubernetes.io/arch=amd64 kubernetes.io/hostname=k8s-node01 kubernetes.io/os=linux mem-num=48Annotations: flannel.alpha.coreos.com/backend-data: &#123;&quot;VtepMAC&quot;:&quot;3e:15:bb:f8:85:dc&quot;&#125; flannel.alpha.coreos.com/backend-type: vxlan flannel.alpha.coreos.com/kube-subnet-manager: true flannel.alpha.coreos.com/public-ip: 10.0.0.111 kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock node.alpha.kubernetes.io/ttl: 0 volumes.kubernetes.io/controller-managed-attach-detach: trueCreationTimestamp: Tue, 12 May 2020 16:50:54 +0800Taints: check=zhang:NoSchedule ### 可见已添加污点Unschedulable: false 在k8s-node01节点添加了一个污点（taint），污点的key为check，value为zhang，污点effect为NoSchedule。这意味着没有pod可以调度到k8s-node01节点，除非具有相匹配的容忍。 污点（Taints）删除1234567891011121314151617181920212223242526[root@k8s-master taint]# kubectl taint nodes k8s-node01 check:NoExecute-##### 或者[root@k8s-master taint]# kubectl taint nodes k8s-node01 check=zhang:NoSchedule-node/k8s-node01 untainted[root@k8s-master taint]# [root@k8s-master taint]# kubectl describe node k8s-node01Name: k8s-node01Roles: &lt;none&gt;Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux cpu-num=12 disk-type=ssd kubernetes.io/arch=amd64 kubernetes.io/hostname=k8s-node01 kubernetes.io/os=linux mem-num=48Annotations: flannel.alpha.coreos.com/backend-data: &#123;&quot;VtepMAC&quot;:&quot;3e:15:bb:f8:85:dc&quot;&#125; flannel.alpha.coreos.com/backend-type: vxlan flannel.alpha.coreos.com/kube-subnet-manager: true flannel.alpha.coreos.com/public-ip: 10.0.0.111 kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock node.alpha.kubernetes.io/ttl: 0 volumes.kubernetes.io/controller-managed-attach-detach: trueCreationTimestamp: Tue, 12 May 2020 16:50:54 +0800Taints: &lt;none&gt; ### 可见已删除污点Unschedulable: false Tolerations容忍设置了污点的Node将根据taint的effect：NoSchedule、PreferNoSchedule、NoExecute和Pod之间产生互斥的关系，Pod将在一定程度上不会被调度到Node上。 但我们可以在Pod上设置容忍（Tolerations），意思是设置了容忍的Pod将可以容忍污点的存在，可以被调度到存在污点的Node上。 pod.spec.tolerations示例 1234567891011121314151617tolerations:- key: &quot;key&quot; operator: &quot;Equal&quot; value: &quot;value&quot; effect: &quot;NoSchedule&quot;---tolerations:- key: &quot;key&quot; operator: &quot;Exists&quot; effect: &quot;NoSchedule&quot;---tolerations:- key: &quot;key&quot; operator: &quot;Equal&quot; value: &quot;value&quot; effect: &quot;NoExecute&quot; tolerationSeconds: 3600 重要说明： 其中key、value、effect要与Node上设置的taint保持一致 operator的值为Exists时，将会忽略value；只要有key和effect就行 tolerationSeconds：表示pod 能够容忍 effect 值为 NoExecute 的 taint；当指定了 tolerationSeconds【容忍时间】，则表示 pod 还能在这个节点上继续运行的时间长度。 当不指定key值时当不指定key值和effect值时，且operator为Exists，表示容忍所有的污点【能匹配污点所有的keys，values和effects】 12tolerations:- operator: &quot;Exists&quot; 当不指定effect值时当不指定effect值时，则能匹配污点key对应的所有effects情况 123tolerations:- key: &quot;key&quot; operator: &quot;Exists&quot; 当有多个Master存在时当有多个Master存在时，为了防止资源浪费，可以进行如下设置： 1kubectl taint nodes Node-name node-role.kubernetes.io/master=:PreferNoSchedule 多个Taints污点和多个Tolerations容忍怎么判断可以在同一个node节点上设置多个污点（Taints），在同一个pod上设置多个容忍（Tolerations）。Kubernetes处理多个污点和容忍的方式就像一个过滤器：从节点的所有污点开始，然后忽略可以被Pod容忍匹配的污点；保留其余不可忽略的污点，污点的effect对Pod具有显示效果：特别是： 如果有至少一个不可忽略污点，effect为NoSchedule，那么Kubernetes将不调度Pod到该节点 如果没有effect为NoSchedule的不可忽视污点，但有至少一个不可忽视污点，effect为PreferNoSchedule，那么Kubernetes将尽量不调度Pod到该节点 如果有至少一个不可忽视污点，effect为NoExecute，那么Pod将被从该节点驱逐（如果Pod已经在该节点运行），并且不会被调度到该节点（如果Pod还未在该节点运行） 污点和容忍示例Node污点为NoExecute的示例记得把已有的污点清除，以免影响测验。 节点上的污点设置（Taints） 实现如下污点 123k8s-master 污点为：node-role.kubernetes.io/master:NoSchedule 【k8s自带污点，直接使用，不必另外操作添加】k8s-node01 污点为：k8s-node02 污点为： 污点添加操作如下：「无，本次无污点操作」 污点查看操作如下： 123kubectl describe node k8s-master | grep &apos;Taints&apos; -A 5kubectl describe node k8s-node01 | grep &apos;Taints&apos; -A 5kubectl describe node k8s-node02 | grep &apos;Taints&apos; -A 5 除了k8s-master默认的污点，在k8s-node01、k8s-node02无污点。 污点为NoExecute示例 yaml文件 12345678910111213141516171819202122232425262728293031323334[root@k8s-master taint]# pwd/root/k8s_practice/scheduler/taint[root@k8s-master taint]# [root@k8s-master taint]# cat noexecute_tolerations.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: noexec-tolerations-deploy labels: app: noexectolerations-deployspec: replicas: 6 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: - name: myapp-pod image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 # 有容忍并有 tolerationSeconds 时的格式# tolerations:# - key: &quot;check-mem&quot;# operator: &quot;Equal&quot;# value: &quot;memdb&quot;# effect: &quot;NoExecute&quot;# # 当Pod将被驱逐时，Pod还可以在Node节点上继续保留运行的时间# tolerationSeconds: 30 运行yaml文件 123456789101112131415[root@k8s-master taint]# kubectl apply -f noexecute_tolerations.yaml deployment.apps/noexec-tolerations-deploy created[root@k8s-master taint]# [root@k8s-master taint]# kubectl get deploy -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORnoexec-tolerations-deploy 6/6 6 6 10s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp[root@k8s-master taint]# [root@k8s-master taint]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnoexec-tolerations-deploy-85587896f9-2j848 1/1 Running 0 15s 10.244.4.101 k8s-node01 &lt;none&gt; &lt;none&gt;noexec-tolerations-deploy-85587896f9-jgqkn 1/1 Running 0 15s 10.244.2.141 k8s-node02 &lt;none&gt; &lt;none&gt;noexec-tolerations-deploy-85587896f9-jmw5w 1/1 Running 0 15s 10.244.2.142 k8s-node02 &lt;none&gt; &lt;none&gt;noexec-tolerations-deploy-85587896f9-s8x95 1/1 Running 0 15s 10.244.4.102 k8s-node01 &lt;none&gt; &lt;none&gt;noexec-tolerations-deploy-85587896f9-t82fj 1/1 Running 0 15s 10.244.4.103 k8s-node01 &lt;none&gt; &lt;none&gt;noexec-tolerations-deploy-85587896f9-wx9pz 1/1 Running 0 15s 10.244.2.143 k8s-node02 &lt;none&gt; &lt;none&gt; 由上可见，pod是在k8s-node01、k8s-node02平均分布的。 添加effect为NoExecute的污点 1kubectl taint nodes k8s-node02 check-mem=memdb:NoExecute 此时所有节点污点为 123k8s-master 污点为：node-role.kubernetes.io/master:NoSchedule 【k8s自带污点，直接使用，不必另外操作添加】k8s-node01 污点为：k8s-node02 污点为：check-mem=memdb:NoExecute 之后再次查看pod信息 12345678[root@k8s-master taint]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnoexec-tolerations-deploy-85587896f9-2j848 1/1 Running 0 2m2s 10.244.4.101 k8s-node01 &lt;none&gt; &lt;none&gt;noexec-tolerations-deploy-85587896f9-ch96j 1/1 Running 0 8s 10.244.4.106 k8s-node01 &lt;none&gt; &lt;none&gt;noexec-tolerations-deploy-85587896f9-cjrkb 1/1 Running 0 8s 10.244.4.105 k8s-node01 &lt;none&gt; &lt;none&gt;noexec-tolerations-deploy-85587896f9-qbq6d 1/1 Running 0 7s 10.244.4.104 k8s-node01 &lt;none&gt; &lt;none&gt;noexec-tolerations-deploy-85587896f9-s8x95 1/1 Running 0 2m2s 10.244.4.102 k8s-node01 &lt;none&gt; &lt;none&gt;noexec-tolerations-deploy-85587896f9-t82fj 1/1 Running 0 2m2s 10.244.4.103 k8s-node01 &lt;none&gt; &lt;none&gt; 由上可见，在k8s-node02节点上的pod已被驱逐，驱逐的pod被调度到了k8s-node01节点。 Pod没有容忍时（Tolerations）记得把已有的污点清除，以免影响测验。 节点上的污点设置（Taints） 实现如下污点 123k8s-master 污点为：node-role.kubernetes.io/master:NoSchedule 【k8s自带污点，直接使用，不必另外操作添加】k8s-node01 污点为：check-nginx=web:PreferNoSchedulek8s-node02 污点为：check-nginx=web:NoSchedule 污点添加操作如下： 12kubectl taint nodes k8s-node01 check-nginx=web:PreferNoSchedulekubectl taint nodes k8s-node02 check-nginx=web:NoSchedule 污点查看操作如下： 123kubectl describe node k8s-master | grep &apos;Taints&apos; -A 5kubectl describe node k8s-node01 | grep &apos;Taints&apos; -A 5kubectl describe node k8s-node02 | grep &apos;Taints&apos; -A 5 无容忍示例 yaml文件 1234567891011121314151617181920212223242526[root@k8s-master taint]# pwd/root/k8s_practice/scheduler/taint[root@k8s-master taint]# [root@k8s-master taint]# cat no_tolerations.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: no-tolerations-deploy labels: app: notolerations-deployspec: replicas: 5 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: - name: myapp-pod image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 运行yaml文件 1234567891011121314[root@k8s-master taint]# kubectl apply -f no_tolerations.yaml deployment.apps/no-tolerations-deploy created[root@k8s-master taint]# [root@k8s-master taint]# kubectl get deploy -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORno-tolerations-deploy 5/5 5 5 9s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp[root@k8s-master taint]# [root@k8s-master taint]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESno-tolerations-deploy-85587896f9-6bjv8 1/1 Running 0 16s 10.244.4.54 k8s-node01 &lt;none&gt; &lt;none&gt;no-tolerations-deploy-85587896f9-hbbjb 1/1 Running 0 16s 10.244.4.58 k8s-node01 &lt;none&gt; &lt;none&gt;no-tolerations-deploy-85587896f9-jlmzw 1/1 Running 0 16s 10.244.4.56 k8s-node01 &lt;none&gt; &lt;none&gt;no-tolerations-deploy-85587896f9-kfh2c 1/1 Running 0 16s 10.244.4.55 k8s-node01 &lt;none&gt; &lt;none&gt;no-tolerations-deploy-85587896f9-wmp8b 1/1 Running 0 16s 10.244.4.57 k8s-node01 &lt;none&gt; &lt;none&gt; 由上可见，因为k8s-node02节点的污点check-nginx 的effect为NoSchedule，说明pod不能被调度到该节点。此时k8s-node01节点的污点check-nginx 的effect为PreferNoSchedule【尽量不调度到该节点】；但只有该节点满足调度条件，因此都调度到了k8s-node01节点。 Pod单个容忍时（Tolerations）记得把已有的污点清除，以免影响测验。 节点上的污点设置（Taints） 实现如下污点 123k8s-master 污点为：node-role.kubernetes.io/master:NoSchedule 【k8s自带污点，直接使用，不必另外操作添加】k8s-node01 污点为：check-nginx=web:PreferNoSchedulek8s-node02 污点为：check-nginx=web:NoSchedule 污点添加操作如下： 12kubectl taint nodes k8s-node01 check-nginx=web:PreferNoSchedulekubectl taint nodes k8s-node02 check-nginx=web:NoSchedule 污点查看操作如下： 123kubectl describe node k8s-master | grep &apos;Taints&apos; -A 5kubectl describe node k8s-node01 | grep &apos;Taints&apos; -A 5kubectl describe node k8s-node02 | grep &apos;Taints&apos; -A 5 单个容忍示例 yaml文件 12345678910111213141516171819202122232425262728293031[root@k8s-master taint]# pwd/root/k8s_practice/scheduler/taint[root@k8s-master taint]# [root@k8s-master taint]# cat one_tolerations.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: one-tolerations-deploy labels: app: onetolerations-deployspec: replicas: 6 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: - name: myapp-pod image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 tolerations: - key: &quot;check-nginx&quot; operator: &quot;Equal&quot; value: &quot;web&quot; effect: &quot;NoSchedule&quot; 运行yaml文件 123456789101112131415[root@k8s-master taint]# kubectl apply -f one_tolerations.yaml deployment.apps/one-tolerations-deploy created[root@k8s-master taint]# [root@k8s-master taint]# kubectl get deploy -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORone-tolerations-deploy 6/6 6 6 3s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp[root@k8s-master taint]# [root@k8s-master taint]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESone-tolerations-deploy-5757d6b559-gbj49 1/1 Running 0 7s 10.244.2.73 k8s-node02 &lt;none&gt; &lt;none&gt;one-tolerations-deploy-5757d6b559-j9p6r 1/1 Running 0 7s 10.244.2.71 k8s-node02 &lt;none&gt; &lt;none&gt;one-tolerations-deploy-5757d6b559-kpk9q 1/1 Running 0 7s 10.244.2.72 k8s-node02 &lt;none&gt; &lt;none&gt;one-tolerations-deploy-5757d6b559-lsppn 1/1 Running 0 7s 10.244.4.65 k8s-node01 &lt;none&gt; &lt;none&gt;one-tolerations-deploy-5757d6b559-rx72g 1/1 Running 0 7s 10.244.4.66 k8s-node01 &lt;none&gt; &lt;none&gt;one-tolerations-deploy-5757d6b559-s8qr9 1/1 Running 0 7s 10.244.2.74 k8s-node02 &lt;none&gt; &lt;none&gt; 由上可见，此时pod会尽量【优先】调度到k8s-node02节点，尽量不调度到k8s-node01节点。如果我们只有一个pod，那么会一直调度到k8s-node02节点。 Pod多个容忍时（Tolerations）记得把已有的污点清除，以免影响测验。 节点上的污点设置（Taints） 实现如下污点 123k8s-master 污点为：node-role.kubernetes.io/master:NoSchedule 【k8s自带污点，直接使用，不必另外操作添加】k8s-node01 污点为：check-nginx=web:PreferNoSchedule, check-redis=memdb:NoSchedulek8s-node02 污点为：check-nginx=web:NoSchedule, check-redis=database:NoSchedule 污点添加操作如下： 1234kubectl taint nodes k8s-node01 check-nginx=web:PreferNoSchedulekubectl taint nodes k8s-node01 check-redis=memdb:NoSchedulekubectl taint nodes k8s-node02 check-nginx=web:NoSchedulekubectl taint nodes k8s-node02 check-redis=database:NoSchedule 污点查看操作如下： 123kubectl describe node k8s-master | grep &apos;Taints&apos; -A 5kubectl describe node k8s-node01 | grep &apos;Taints&apos; -A 5kubectl describe node k8s-node02 | grep &apos;Taints&apos; -A 5 多个容忍示例 yaml文件 12345678910111213141516171819202122232425262728293031323334[root@k8s-master taint]# pwd/root/k8s_practice/scheduler/taint[root@k8s-master taint]# [root@k8s-master taint]# cat multi_tolerations.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: multi-tolerations-deploy labels: app: multitolerations-deployspec: replicas: 6 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: - name: myapp-pod image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 tolerations: - key: &quot;check-nginx&quot; operator: &quot;Equal&quot; value: &quot;web&quot; effect: &quot;NoSchedule&quot; - key: &quot;check-redis&quot; operator: &quot;Exists&quot; effect: &quot;NoSchedule&quot; 运行yaml文件 123456789101112131415[root@k8s-master taint]# kubectl apply -f multi_tolerations.yaml deployment.apps/multi-tolerations-deploy created[root@k8s-master taint]# [root@k8s-master taint]# kubectl get deploy -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORmulti-tolerations-deploy 6/6 6 6 5s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp[root@k8s-master taint]# [root@k8s-master taint]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmulti-tolerations-deploy-776ff4449c-2csnk 1/1 Running 0 10s 10.244.2.171 k8s-node02 &lt;none&gt; &lt;none&gt;multi-tolerations-deploy-776ff4449c-4d9fh 1/1 Running 0 10s 10.244.4.116 k8s-node01 &lt;none&gt; &lt;none&gt;multi-tolerations-deploy-776ff4449c-c8fz5 1/1 Running 0 10s 10.244.2.173 k8s-node02 &lt;none&gt; &lt;none&gt;multi-tolerations-deploy-776ff4449c-nj29f 1/1 Running 0 10s 10.244.4.115 k8s-node01 &lt;none&gt; &lt;none&gt;multi-tolerations-deploy-776ff4449c-r7gsm 1/1 Running 0 10s 10.244.2.172 k8s-node02 &lt;none&gt; &lt;none&gt;multi-tolerations-deploy-776ff4449c-s8t2n 1/1 Running 0 10s 10.244.2.174 k8s-node02 &lt;none&gt; &lt;none&gt; 由上可见，示例中的pod容忍为：check-nginx=web:NoSchedule；check-redis=:NoSchedule。因此pod会尽量调度到k8s-node02节点，尽量不调度到k8s-node01节点。 Pod容忍指定污点key的所有effects情况记得把已有的污点清除，以免影响测验。 节点上的污点设置（Taints） 实现如下污点 123k8s-master 污点为：node-role.kubernetes.io/master:NoSchedule 【k8s自带污点，直接使用，不必另外操作添加】k8s-node01 污点为：check-redis=memdb:NoSchedulek8s-node02 污点为：check-redis=database:NoSchedule 污点添加操作如下： 12kubectl taint nodes k8s-node01 check-redis=memdb:NoSchedulekubectl taint nodes k8s-node02 check-redis=database:NoSchedule 污点查看操作如下： 123kubectl describe node k8s-master | grep &apos;Taints&apos; -A 5kubectl describe node k8s-node01 | grep &apos;Taints&apos; -A 5kubectl describe node k8s-node02 | grep &apos;Taints&apos; -A 5 多个容忍示例 yaml文件 1234567891011121314151617181920212223242526272829[root@k8s-master taint]# pwd/root/k8s_practice/scheduler/taint[root@k8s-master taint]# [root@k8s-master taint]# cat key_tolerations.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: key-tolerations-deploy labels: app: keytolerations-deployspec: replicas: 6 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: - name: myapp-pod image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 tolerations: - key: &quot;check-redis&quot; operator: &quot;Exists&quot; 运行yaml文件 123456789101112131415[root@k8s-master taint]# kubectl apply -f key_tolerations.yaml deployment.apps/key-tolerations-deploy created[root@k8s-master taint]# [root@k8s-master taint]# kubectl get deploy -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORkey-tolerations-deploy 6/6 6 6 21s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp[root@k8s-master taint]# [root@k8s-master taint]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESkey-tolerations-deploy-db5c4c4db-2zqr8 1/1 Running 0 26s 10.244.2.170 k8s-node02 &lt;none&gt; &lt;none&gt;key-tolerations-deploy-db5c4c4db-5qb5p 1/1 Running 0 26s 10.244.4.113 k8s-node01 &lt;none&gt; &lt;none&gt;key-tolerations-deploy-db5c4c4db-7xmt6 1/1 Running 0 26s 10.244.2.169 k8s-node02 &lt;none&gt; &lt;none&gt;key-tolerations-deploy-db5c4c4db-84rkj 1/1 Running 0 26s 10.244.4.114 k8s-node01 &lt;none&gt; &lt;none&gt;key-tolerations-deploy-db5c4c4db-gszxg 1/1 Running 0 26s 10.244.2.168 k8s-node02 &lt;none&gt; &lt;none&gt;key-tolerations-deploy-db5c4c4db-vlgh8 1/1 Running 0 26s 10.244.4.112 k8s-node01 &lt;none&gt; &lt;none&gt; 由上可见，示例中的pod容忍为：check-nginx=:；仅需匹配node污点的key即可，污点的value和effect不需要关心。因此可以匹配k8s-node01、k8s-node02节点。 Pod容忍所有污点记得把已有的污点清除，以免影响测验。 节点上的污点设置（Taints） 实现如下污点 123k8s-master 污点为：node-role.kubernetes.io/master:NoSchedule 【k8s自带污点，直接使用，不必另外操作添加】k8s-node01 污点为：check-nginx=web:PreferNoSchedule, check-redis=memdb:NoSchedulek8s-node02 污点为：check-nginx=web:NoSchedule, check-redis=database:NoSchedule 污点添加操作如下： 1234kubectl taint nodes k8s-node01 check-nginx=web:PreferNoSchedulekubectl taint nodes k8s-node01 check-redis=memdb:NoSchedulekubectl taint nodes k8s-node02 check-nginx=web:NoSchedulekubectl taint nodes k8s-node02 check-redis=database:NoSchedule 污点查看操作如下： 123kubectl describe node k8s-master | grep &apos;Taints&apos; -A 5kubectl describe node k8s-node01 | grep &apos;Taints&apos; -A 5kubectl describe node k8s-node02 | grep &apos;Taints&apos; -A 5 所有容忍示例 yaml文件 12345678910111213141516171819202122232425262728[root@k8s-master taint]# pwd/root/k8s_practice/scheduler/taint[root@k8s-master taint]# [root@k8s-master taint]# cat all_tolerations.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: all-tolerations-deploy labels: app: alltolerations-deployspec: replicas: 6 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: - name: myapp-pod image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 tolerations: - operator: &quot;Exists&quot; 运行yaml文件 123456789101112131415[root@k8s-master taint]# kubectl apply -f all_tolerations.yaml deployment.apps/all-tolerations-deploy created[root@k8s-master taint]# [root@k8s-master taint]# kubectl get deploy -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORall-tolerations-deploy 6/6 6 6 8s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp[root@k8s-master taint]# [root@k8s-master taint]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESall-tolerations-deploy-566cdccbcd-4klc2 1/1 Running 0 12s 10.244.0.116 k8s-master &lt;none&gt; &lt;none&gt;all-tolerations-deploy-566cdccbcd-59vvc 1/1 Running 0 12s 10.244.0.115 k8s-master &lt;none&gt; &lt;none&gt;all-tolerations-deploy-566cdccbcd-cvw4s 1/1 Running 0 12s 10.244.2.175 k8s-node02 &lt;none&gt; &lt;none&gt;all-tolerations-deploy-566cdccbcd-k8fzl 1/1 Running 0 12s 10.244.2.176 k8s-node02 &lt;none&gt; &lt;none&gt;all-tolerations-deploy-566cdccbcd-s2pw7 1/1 Running 0 12s 10.244.4.118 k8s-node01 &lt;none&gt; &lt;none&gt;all-tolerations-deploy-566cdccbcd-xzngt 1/1 Running 0 13s 10.244.4.117 k8s-node01 &lt;none&gt; &lt;none&gt; 后上可见，示例中的pod容忍所有的污点，因此pod可被调度到所有k8s节点。 相关阅读1、官网：污点与容忍 2、Kubernetes K8S调度器kube-scheduler详解 3、Kubernetes K8S之affinity亲和性与反亲和性详解与示例 完毕！]]></content>
      <categories>
        <category>Kubernetes</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
        <tag>K8S</tag>
        <tag>CI/CD</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes K8S之affinity亲和性与反亲和性详解与示例]]></title>
    <url>%2F2020%2F10%2F28%2Fkubernetes22%2F</url>
    <content type="text"><![CDATA[Kubernetes K8S之Node节点亲和性与反亲和性以及Pod亲和性与反亲和性详解与示例 主机配置规划 服务器名称(hostname) 系统版本 配置 内网IP 外网IP(模拟) k8s-master CentOS7.7 2C/4G/20G 172.16.1.110 10.0.0.110 k8s-node01 CentOS7.7 2C/4G/20G 172.16.1.111 10.0.0.111 k8s-node02 CentOS7.7 2C/4G/20G 172.16.1.112 10.0.0.112 亲和性和反亲和性nodeSelector提供了一种非常简单的方法，将pods约束到具有特定标签的节点。而亲和性/反亲和性极大地扩展了可表达的约束类型。关键的增强是： 1、亲和性/反亲和性语言更具表达性。除了使用逻辑AND操作创建的精确匹配之外，该语言还提供了更多的匹配规则； 2、可以指示规则是优选项而不是硬要求，因此如果调度器不能满足，pod仍将被调度； 3、可以针对节点（或其他拓扑域）上运行的pods的标签进行约束，而不是针对节点的自身标签，这影响哪些Pod可以或不可以共处。 亲和特性包括两种类型：node节点亲和性/反亲和性 和 pod亲和性/反亲和性。pod亲和性/反亲和性约束针对的是pod标签而不是节点标签。 拓扑域是什么：多个node节点，拥有相同的label标签【节点标签的键值相同】，那么这些节点就处于同一个拓扑域。★★★★★ node节点亲和性当前有两种类型的节点亲和性，称为requiredDuringSchedulingIgnoredDuringExecution和 preferredDuringSchedulingIgnoredDuringExecution，可以将它们分别视为“硬”【必须满足条件】和“软”【优选满足条件】要求。 前者表示Pod要调度到的节点必须满足规则条件，不满足则不会调度，pod会一直处于Pending状态；后者表示优先调度到满足规则条件的节点，如果不能满足再调度到其他节点。 名称中的 IgnoredDuringExecution 部分意味着，与nodeSelector的工作方式类似，如果节点上的标签在Pod运行时发生更改，使得pod上的亲和性规则不再满足，那么pod仍将继续在该节点上运行。 在未来，会计划提供requiredDuringSchedulingRequiredDuringExecution，类似requiredDuringSchedulingIgnoredDuringExecution。不同之处就是pod运行过程中如果节点不再满足pod的亲和性，则pod会在该节点中逐出。 节点亲和性语法支持以下运算符：In，NotIn，Exists，DoesNotExist，Gt，Lt。可以使用NotIn和DoesNotExist实现节点的反亲和行为。 运算符关系： In：label的值在某个列表中 NotIn：label的值不在某个列表中 Gt：label的值大于某个值 Lt：label的值小于某个值 Exists：某个label存在 DoesNotExist：某个label不存在 其他重要说明： 1、如果同时指定nodeSelector和nodeAffinity，则必须满足两个条件，才能将Pod调度到候选节点上。 2、如果在nodeAffinity类型下指定了多个nodeSelectorTerms对象【对象不能有多个，如果存在多个只有最后一个生效】，那么只有最后一个nodeSelectorTerms对象生效。 3、如果在nodeSelectorTerms下指定了多个matchExpressions列表，那么只要能满足其中一个matchExpressions，就可以将pod调度到某个节点上【针对节点硬亲和】。 4、如果在matchExpressions下有多个key列表，那么只有当所有key满足时，才能将pod调度到某个节点【针对硬亲和】。 5、在key下的values只要有一个满足条件，那么当前的key就满足条件 6、如果pod已经调度在该节点，当我们删除或修该节点的标签时，pod不会被移除。换句话说，亲和性选择只有在pod调度期间有效。 7、preferredDuringSchedulingIgnoredDuringExecution中的weight（权重）字段在1-100范围内。对于每个满足所有调度需求的节点(资源请求、RequiredDuringScheduling亲和表达式等)，调度器将通过迭代该字段的元素来计算一个总和，如果节点与相应的匹配表达式匹配，则向该总和添加“权重”。然后将该分数与节点的其他优先级函数的分数结合起来。总得分最高的节点是最受欢迎的。 node节点亲和性示例准备事项给node节点打label标签 123456### --overwrite覆盖已存在的标签信息kubectl label nodes k8s-node01 disk-type=ssd --overwritekubectl label nodes k8s-node01 cpu-num=12kubectl label nodes k8s-node02 disk-type=satakubectl label nodes k8s-node02 cpu-num=24 查询所有节点标签信息 12345[root@k8s-master ~]# kubectl get node -o wide --show-labelsNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME LABELSk8s-master Ready master 43d v1.17.4 172.16.1.110 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-1062.el7.x86_64 docker://19.3.8 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master,kubernetes.io/os=linux,node-role.kubernetes.io/master=k8s-node01 Ready &lt;none&gt; 43d v1.17.4 172.16.1.111 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-1062.el7.x86_64 docker://19.3.8 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,cpu-num=12,disk-type=ssd,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node01,kubernetes.io/os=linuxk8s-node02 Ready &lt;none&gt; 43d v1.17.4 172.16.1.112 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-1062.el7.x86_64 docker://19.3.8 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,cpu-num=24,disk-type=sata,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node02,kubernetes.io/os=linux 参见上面，给k8s-node01打了cpu-num=12,disk-type=ssd标签；给k8s-node02打了cpu-num=24,disk-type=sata标签。 node硬亲和性示例必须满足条件才能调度，否则不会调度 要运行的yaml文件 1234567891011121314151617181920212223242526272829303132333435363738394041[root@k8s-master nodeAffinity]# pwd/root/k8s_practice/scheduler/nodeAffinity[root@k8s-master nodeAffinity]# cat node_required_affinity.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: node-affinity-deploy labels: app: nodeaffinity-deployspec: replicas: 5 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: - name: myapp-pod image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: # 表示node标签存在 disk-type=ssd 或 disk-type=sas - key: disk-type operator: In values: - ssd - sas # 表示node标签存在 cpu-num且值大于6 - key: cpu-num operator: Gt values: - &quot;6&quot; 运行yaml文件并查看状态 123456789101112131415161718[root@k8s-master nodeAffinity]# kubectl apply -f node_required_affinity.yaml deployment.apps/node-affinity-deploy created[root@k8s-master nodeAffinity]# [root@k8s-master nodeAffinity]# kubectl get deploy -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORnode-affinity-deploy 5/5 5 5 6s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp[root@k8s-master nodeAffinity]# [root@k8s-master nodeAffinity]# kubectl get rs -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTORnode-affinity-deploy-5c88ffb8ff 5 5 5 11s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp,pod-template-hash=5c88ffb8ff[root@k8s-master nodeAffinity]# [root@k8s-master nodeAffinity]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnode-affinity-deploy-5c88ffb8ff-2mbfl 1/1 Running 0 15s 10.244.4.237 k8s-node01 &lt;none&gt; &lt;none&gt;node-affinity-deploy-5c88ffb8ff-9hjhk 1/1 Running 0 15s 10.244.4.235 k8s-node01 &lt;none&gt; &lt;none&gt;node-affinity-deploy-5c88ffb8ff-9rg75 1/1 Running 0 15s 10.244.4.239 k8s-node01 &lt;none&gt; &lt;none&gt;node-affinity-deploy-5c88ffb8ff-pqtfh 1/1 Running 0 15s 10.244.4.236 k8s-node01 &lt;none&gt; &lt;none&gt;node-affinity-deploy-5c88ffb8ff-zqpl8 1/1 Running 0 15s 10.244.4.238 k8s-node01 &lt;none&gt; &lt;none&gt; 由上可见，再根据之前打的标签，很容易推断出当前pod只能调度在k8s-node01节点。 即使我们删除原来的rs，重新生成rs后pod依旧会调度到k8s-node01节点。如下： 1234567891011121314[root@k8s-master nodeAffinity]# kubectl delete rs node-affinity-deploy-5c88ffb8ffreplicaset.apps &quot;node-affinity-deploy-5c88ffb8ff&quot; deleted[root@k8s-master nodeAffinity]# [root@k8s-master nodeAffinity]# kubectl get rs -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTORnode-affinity-deploy-5c88ffb8ff 5 5 2 4s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp,pod-template-hash=5c88ffb8ff[root@k8s-master nodeAffinity]# [root@k8s-master nodeAffinity]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnode-affinity-deploy-5c88ffb8ff-2v2tb 1/1 Running 0 11s 10.244.4.241 k8s-node01 &lt;none&gt; &lt;none&gt;node-affinity-deploy-5c88ffb8ff-gl4fm 1/1 Running 0 11s 10.244.4.240 k8s-node01 &lt;none&gt; &lt;none&gt;node-affinity-deploy-5c88ffb8ff-j26rg 1/1 Running 0 11s 10.244.4.244 k8s-node01 &lt;none&gt; &lt;none&gt;node-affinity-deploy-5c88ffb8ff-vhzmn 1/1 Running 0 11s 10.244.4.243 k8s-node01 &lt;none&gt; &lt;none&gt;node-affinity-deploy-5c88ffb8ff-xxj8m 1/1 Running 0 11s 10.244.4.242 k8s-node01 &lt;none&gt; &lt;none&gt; node软亲和性示例优先调度到满足条件的节点，如果都不满足也会调度到其他节点。 要运行的yaml文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@k8s-master nodeAffinity]# pwd/root/k8s_practice/scheduler/nodeAffinity[root@k8s-master nodeAffinity]# cat node_preferred_affinity.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: node-affinity-deploy labels: app: nodeaffinity-deployspec: replicas: 5 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: - name: myapp-pod image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: # 表示node标签存在 disk-type=ssd 或 disk-type=sas - key: disk-type operator: In values: - ssd - sas - weight: 50 preference: matchExpressions: # 表示node标签存在 cpu-num且值大于16 - key: cpu-num operator: Gt values: - &quot;16&quot; 运行yaml文件并查看状态 123456789101112131415161718[root@k8s-master nodeAffinity]# kubectl apply -f node_preferred_affinity.yaml deployment.apps/node-affinity-deploy created[root@k8s-master nodeAffinity]# [root@k8s-master nodeAffinity]# kubectl get deploy -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORnode-affinity-deploy 5/5 5 5 9s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp[root@k8s-master nodeAffinity]# [root@k8s-master nodeAffinity]# kubectl get rs -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTORnode-affinity-deploy-d5d9cbc8d 5 5 5 13s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp,pod-template-hash=d5d9cbc8d[root@k8s-master nodeAffinity]# [root@k8s-master nodeAffinity]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnode-affinity-deploy-d5d9cbc8d-bv86t 1/1 Running 0 18s 10.244.2.243 k8s-node02 &lt;none&gt; &lt;none&gt;node-affinity-deploy-d5d9cbc8d-dnbr8 1/1 Running 0 18s 10.244.2.244 k8s-node02 &lt;none&gt; &lt;none&gt;node-affinity-deploy-d5d9cbc8d-ldq82 1/1 Running 0 18s 10.244.2.246 k8s-node02 &lt;none&gt; &lt;none&gt;node-affinity-deploy-d5d9cbc8d-nt74q 1/1 Running 0 18s 10.244.4.2 k8s-node01 &lt;none&gt; &lt;none&gt;node-affinity-deploy-d5d9cbc8d-rt5nb 1/1 Running 0 18s 10.244.2.245 k8s-node02 &lt;none&gt; &lt;none&gt; 由上可见，再根据之前打的标签，很容易推断出当前pod会【优先】调度在k8s-node02节点。 node软硬亲和性联合示例硬亲和性与软亲和性一起使用 要运行的yaml文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@k8s-master nodeAffinity]# pwd/root/k8s_practice/scheduler/nodeAffinity[root@k8s-master nodeAffinity]# cat node_affinity.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: node-affinity-deploy labels: app: nodeaffinity-deployspec: replicas: 5 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: - name: myapp-pod image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: # 表示node标签存在 cpu-num且值大于10 - matchExpressions: - key: cpu-num operator: Gt values: - &quot;10&quot; preferredDuringSchedulingIgnoredDuringExecution: - weight: 50 preference: matchExpressions: # 表示node标签存在 disk-type=ssd 或 disk-type=sas - key: disk-type operator: In values: - ssd - sas 运行yaml文件并查看状态 123456789101112131415161718[root@k8s-master nodeAffinity]# kubectl apply -f node_affinity.yaml deployment.apps/node-affinity-deploy created[root@k8s-master nodeAffinity]# [root@k8s-master nodeAffinity]# kubectl get deploy -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORnode-affinity-deploy 5/5 5 5 9s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp[root@k8s-master nodeAffinity]# [root@k8s-master nodeAffinity]# kubectl get rs -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTORnode-affinity-deploy-f9cb9b99b 5 5 5 13s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp,pod-template-hash=f9cb9b99b[root@k8s-master nodeAffinity]# [root@k8s-master nodeAffinity]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnode-affinity-deploy-f9cb9b99b-8w2nc 1/1 Running 0 17s 10.244.4.10 k8s-node01 &lt;none&gt; &lt;none&gt;node-affinity-deploy-f9cb9b99b-csk2s 1/1 Running 0 17s 10.244.4.9 k8s-node01 &lt;none&gt; &lt;none&gt;node-affinity-deploy-f9cb9b99b-g42kq 1/1 Running 0 17s 10.244.4.8 k8s-node01 &lt;none&gt; &lt;none&gt;node-affinity-deploy-f9cb9b99b-m6xbv 1/1 Running 0 17s 10.244.4.7 k8s-node01 &lt;none&gt; &lt;none&gt;node-affinity-deploy-f9cb9b99b-mxbdp 1/1 Running 0 17s 10.244.2.253 k8s-node02 &lt;none&gt; &lt;none&gt; 由上可见，再根据之前打的标签，很容易推断出k8s-node01、k8s-node02都满足必要条件，但当前pod会【优先】调度在k8s-node01节点。 Pod亲和性与节点亲和性一样，当前有Pod亲和性/反亲和性都有两种类型，称为requiredDuringSchedulingIgnoredDuringExecution和 preferredDuringSchedulingIgnoredDuringExecution，分别表示“硬”与“软”要求。对于硬要求，如果不满足则pod会一直处于Pending状态。 Pod的亲和性与反亲和性是基于Node节点上已经运行pod的标签(而不是节点上的标签)决定的，从而约束哪些节点适合调度你的pod。 规则的形式是：如果X已经运行了一个或多个符合规则Y的pod，则此pod应该在X中运行(如果是反亲和的情况下，则不应该在X中运行）。当然pod必须处在同一名称空间，不然亲和性/反亲和性无作用。从概念上讲，X是一个拓扑域。我们可以使用topologyKey来表示它，topologyKey 的值是node节点标签的键以便系统用来表示这样的拓扑域。当然这里也有个隐藏条件，就是node节点标签的键值相同时，才是在同一拓扑域中；如果只是节点标签名相同，但是值不同，那么也不在同一拓扑域。★★★★★ 也就是说：Pod的亲和性/反亲和性调度是根据拓扑域来界定调度的，而不是根据node节点。★★★★★ 注意事项 1、pod之间亲和性/反亲和性需要大量的处理，这会明显降低大型集群中的调度速度。不建议在大于几百个节点的集群中使用它们。 2、Pod反亲和性要求对节点进行一致的标记。换句话说，集群中的每个节点都必须有一个匹配topologyKey的适当标签。如果某些或所有节点缺少指定的topologyKey标签，可能会导致意外行为。 requiredDuringSchedulingIgnoredDuringExecution中亲和性的一个示例是“将服务A和服务B的Pod放置在同一区域【拓扑域】中，因为它们之间有很多交流”；preferredDuringSchedulingIgnoredDuringExecution中反亲和性的示例是“将此服务的 pod 跨区域【拓扑域】分布”【此时硬性要求是说不通的，因为你可能拥有的 pod 数多于区域数】。 Pod亲和性/反亲和性语法支持以下运算符：In，NotIn，Exists，DoesNotExist。 原则上，topologyKey可以是任何合法的标签键。但是，出于性能和安全方面的原因，topologyKey有一些限制： 1、对于Pod亲和性，在requiredDuringSchedulingIgnoredDuringExecution和preferredDuringSchedulingIgnoredDuringExecution中topologyKey都不允许为空。 2、对于Pod反亲和性，在requiredDuringSchedulingIgnoredDuringExecution和preferredDuringSchedulingIgnoredDuringExecution中topologyKey也都不允许为空。 3、对于requiredDuringSchedulingIgnoredDuringExecution的pod反亲和性，引入了允许控制器LimitPodHardAntiAffinityTopology来限制topologyKey的kubernet.io/hostname。如果你想让它对自定义拓扑可用，你可以修改许可控制器，或者干脆禁用它。 4、除上述情况外，topologyKey可以是任何合法的标签键。 Pod 间亲和通过 PodSpec 中 affinity 字段下的 podAffinity 字段进行指定。而 pod 间反亲和通过 PodSpec 中 affinity 字段下的 podAntiAffinity 字段进行指定。 Pod亲和性/反亲和性的requiredDuringSchedulingIgnoredDuringExecution所关联的matchExpressions下有多个key列表，那么只有当所有key满足时，才能将pod调度到某个区域【针对Pod硬亲和】。 pod亲和性与反亲和性示例为了更好的演示Pod亲和性与反亲和性，本次示例我们会将k8s-master节点也加入进来进行演示。 准备事项给node节点打label标签 12345678910111213141516171819202122# 删除已存在标签kubectl label nodes k8s-node01 cpu-num-kubectl label nodes k8s-node01 disk-type-kubectl label nodes k8s-node02 cpu-num-kubectl label nodes k8s-node02 disk-type-### --overwrite覆盖已存在的标签信息# k8s-master 标签添加kubectl label nodes k8s-master busi-use=www --overwritekubectl label nodes k8s-master disk-type=ssd --overwritekubectl label nodes k8s-master busi-db=redis# k8s-node01 标签添加kubectl label nodes k8s-node01 busi-use=wwwkubectl label nodes k8s-node01 disk-type=sata kubectl label nodes k8s-node01 busi-db=redis# k8s-node02 标签添加kubectl label nodes k8s-node02 busi-use=wwwkubectl label nodes k8s-node02 disk-type=ssdkubectl label nodes k8s-node02 busi-db=etcd 查询所有节点标签信息 12345[root@k8s-master ~]# kubectl get node -o wide --show-labelsNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME LABELSk8s-master Ready master 28d v1.17.4 172.16.1.110 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-1062.el7.x86_64 docker://19.3.8 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,busi-db=redis,busi-use=www,disk-type=ssd,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master,kubernetes.io/os=linux,node-role.kubernetes.io/master=k8s-node01 Ready &lt;none&gt; 28d v1.17.4 172.16.1.111 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-1062.el7.x86_64 docker://19.3.8 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,busi-db=redis,busi-use=www,disk-type=sata,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node01,kubernetes.io/os=linuxk8s-node02 Ready &lt;none&gt; 28d v1.17.4 172.16.1.112 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-1062.el7.x86_64 docker://19.3.8 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,busi-db=etcd,busi-use=www,disk-type=ssd,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-node02,kubernetes.io/os=linux 如上所述：k8s-master添加了disk-type=ssd,busi-db=redis,busi-use=www标签 k8s-node01添加了disk-type=sata,busi-db=redis,busi-use=www标签 k8s-node02添加了disk-type=ssd,busi-db=etcd,busi-use=www标签 通过deployment运行一个pod，或者直接运行一个pod也可以。为后续的Pod亲和性与反亲和性测验做基础。 123456789101112131415161718192021222324252627282930313233343536### yaml文件[root@k8s-master podAffinity]# pwd/root/k8s_practice/scheduler/podAffinity[root@k8s-master podAffinity]# cat web_deploy.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: web-deploy labels: app: myweb-deployspec: replicas: 1 selector: matchLabels: app: myapp-web template: metadata: labels: app: myapp-web version: v1 spec: containers: - name: myapp-pod image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 80[root@k8s-master podAffinity]# ### 运行yaml文件[root@k8s-master podAffinity]# kubectl apply -f web_deploy.yaml deployment.apps/web-deploy created[root@k8s-master podAffinity]# ### 查看pod标签[root@k8s-master podAffinity]# kubectl get pod -o wide --show-labelsNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES LABELSweb-deploy-5ccc9d7c55-kkwst 1/1 Running 0 15m 10.244.2.4 k8s-node02 &lt;none&gt; &lt;none&gt; app=myapp-web,pod-template-hash=5ccc9d7c55,version=v1 当前pod在k8s-node02节点；其中pod的标签app=myapp-web,version=v1会在后面pod亲和性/反亲和性示例中使用。 pod硬亲和性示例要运行的yaml文件 12345678910111213141516171819202122232425262728293031323334353637383940414243[root@k8s-master podAffinity]# pwd/root/k8s_practice/scheduler/podAffinity[root@k8s-master podAffinity]# cat pod_required_affinity.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: pod-podaffinity-deploy labels: app: podaffinity-deployspec: replicas: 6 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: # 允许在master节点运行 tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: myapp-pod image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: # 由于是Pod亲和性/反亲和性；因此这里匹配规则写的是Pod的标签信息 matchExpressions: - key: app operator: In values: - myapp-web # 拓扑域 若多个node节点具有相同的标签信息【标签键值相同】，则表示这些node节点就在同一拓扑域 # 请对比如下两个不同的拓扑域，Pod的调度结果 #topologyKey: busi-use topologyKey: disk-type 运行yaml文件并查看状态 12345678910111213141516171819202122[root@k8s-master podAffinity]# kubectl apply -f pod_required_affinity.yaml deployment.apps/pod-podaffinity-deploy created[root@k8s-master podAffinity]# [root@k8s-master podAffinity]# kubectl get deploy -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORpod-podaffinity-deploy 6/6 6 6 48s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myappweb-deploy 1/1 1 1 22h myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp-web[root@k8s-master podAffinity]# [root@k8s-master podAffinity]# kubectl get rs -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTORpod-podaffinity-deploy-848559bf5b 6 6 6 52s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp,pod-template-hash=848559bf5bweb-deploy-5ccc9d7c55 1 1 1 22h myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp-web,pod-template-hash=5ccc9d7c55[root@k8s-master podAffinity]# [root@k8s-master podAffinity]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod-podaffinity-deploy-848559bf5b-8kkwm 1/1 Running 0 54s 10.244.0.80 k8s-master &lt;none&gt; &lt;none&gt;pod-podaffinity-deploy-848559bf5b-8s59f 1/1 Running 0 54s 10.244.2.252 k8s-node02 &lt;none&gt; &lt;none&gt;pod-podaffinity-deploy-848559bf5b-8z4dv 1/1 Running 0 54s 10.244.2.253 k8s-node02 &lt;none&gt; &lt;none&gt;pod-podaffinity-deploy-848559bf5b-gs7sb 1/1 Running 0 54s 10.244.0.79 k8s-master &lt;none&gt; &lt;none&gt;pod-podaffinity-deploy-848559bf5b-sm6nz 1/1 Running 0 54s 10.244.0.78 k8s-master &lt;none&gt; &lt;none&gt;pod-podaffinity-deploy-848559bf5b-zbr6v 1/1 Running 0 54s 10.244.2.251 k8s-node02 &lt;none&gt; &lt;none&gt;web-deploy-5ccc9d7c55-khhrr 1/1 Running 3 22h 10.244.2.245 k8s-node02 &lt;none&gt; &lt;none&gt; 由上可见，yaml文件中为topologyKey: disk-type；虽然k8s-master、k8s-node01、k8s-node02都有disk-type标签；但是k8s-master和k8s-node02节点的disk-type标签值为ssd；而k8s-node01节点的disk-type标签值为sata。因此k8s-master和k8s-node02节点属于同一拓扑域，Pod只会调度到这两个节点上。 pod软亲和性示例要运行的yaml文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@k8s-master podAffinity]# pwd/root/k8s_practice/scheduler/podAffinity[root@k8s-master podAffinity]# [root@k8s-master podAffinity]# cat pod_preferred_affinity.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: pod-podaffinity-deploy labels: app: podaffinity-deployspec: replicas: 6 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: # 允许在master节点运行 tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: myapp-pod image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 affinity: podAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: # 由于是Pod亲和性/反亲和性；因此这里匹配规则写的是Pod的标签信息 matchExpressions: - key: version operator: In values: - v1 - v2 # 拓扑域 若多个node节点具有相同的标签信息【标签键值相同】，则表示这些node节点就在同一拓扑域 topologyKey: disk-type 运行yaml文件并查看状态 12345678910111213141516171819202122[root@k8s-master podAffinity]# kubectl apply -f pod_preferred_affinity.yaml deployment.apps/pod-podaffinity-deploy created[root@k8s-master podAffinity]# [root@k8s-master podAffinity]# kubectl get deploy -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORpod-podaffinity-deploy 6/6 6 6 75s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myappweb-deploy 1/1 1 1 25h myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp-web[root@k8s-master podAffinity]# [root@k8s-master podAffinity]# kubectl get rs -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTORpod-podaffinity-deploy-8474b4b586 6 6 6 79s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp,pod-template-hash=8474b4b586web-deploy-5ccc9d7c55 1 1 1 25h myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp-web,pod-template-hash=5ccc9d7c55[root@k8s-master podAffinity]# [root@k8s-master podAffinity]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod-podaffinity-deploy-8474b4b586-57gxh 1/1 Running 0 83s 10.244.2.4 k8s-node02 &lt;none&gt; &lt;none&gt;pod-podaffinity-deploy-8474b4b586-kd5l4 1/1 Running 0 83s 10.244.2.3 k8s-node02 &lt;none&gt; &lt;none&gt;pod-podaffinity-deploy-8474b4b586-mlvv7 1/1 Running 0 83s 10.244.0.84 k8s-master &lt;none&gt; &lt;none&gt;pod-podaffinity-deploy-8474b4b586-mtk6r 1/1 Running 0 83s 10.244.0.86 k8s-master &lt;none&gt; &lt;none&gt;pod-podaffinity-deploy-8474b4b586-n5jpj 1/1 Running 0 83s 10.244.0.85 k8s-master &lt;none&gt; &lt;none&gt;pod-podaffinity-deploy-8474b4b586-q2xdl 1/1 Running 0 83s 10.244.3.22 k8s-node01 &lt;none&gt; &lt;none&gt;web-deploy-5ccc9d7c55-khhrr 1/1 Running 3 25h 10.244.2.245 k8s-node02 &lt;none&gt; &lt;none&gt; 由上可见，再根据k8s-master、k8s-node01、k8s-node02的标签信息；很容易推断出Pod会优先调度到k8s-master、k8s-node02节点。 pod硬反亲和性示例要运行的yaml文件 123456789101112131415161718192021222324252627282930313233343536373839404142[root@k8s-master podAffinity]# pwd/root/k8s_practice/scheduler/podAffinity[root@k8s-master podAffinity]# [root@k8s-master podAffinity]# cat pod_required_AntiAffinity.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: pod-podantiaffinity-deploy labels: app: podantiaffinity-deployspec: replicas: 6 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: # 允许在master节点运行 tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: myapp-pod image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: # 由于是Pod亲和性/反亲和性；因此这里匹配规则写的是Pod的标签信息 matchExpressions: - key: app operator: In values: - myapp-web # 拓扑域 若多个node节点具有相同的标签信息【标签键值相同】，则表示这些node节点就在同一拓扑域 topologyKey: disk-type 运行yaml文件并查看状态 12345678910111213141516171819202122[root@k8s-master podAffinity]# kubectl apply -f pod_required_AntiAffinity.yaml deployment.apps/pod-podantiaffinity-deploy created[root@k8s-master podAffinity]#[root@k8s-master podAffinity]# kubectl get deploy -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORpod-podantiaffinity-deploy 6/6 6 6 68s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myappweb-deploy 1/1 1 1 25h myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp-web[root@k8s-master podAffinity]# [root@k8s-master podAffinity]# kubectl get rs -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTORpod-podantiaffinity-deploy-5fb4764b6b 6 6 6 72s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp,pod-template-hash=5fb4764b6bweb-deploy-5ccc9d7c55 1 1 1 25h myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp-web,pod-template-hash=5ccc9d7c55[root@k8s-master podAffinity]# [root@k8s-master podAffinity]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod-podantiaffinity-deploy-5fb4764b6b-b5bzd 1/1 Running 0 75s 10.244.3.28 k8s-node01 &lt;none&gt; &lt;none&gt;pod-podantiaffinity-deploy-5fb4764b6b-b6qjg 1/1 Running 0 75s 10.244.3.23 k8s-node01 &lt;none&gt; &lt;none&gt;pod-podantiaffinity-deploy-5fb4764b6b-h262g 1/1 Running 0 75s 10.244.3.27 k8s-node01 &lt;none&gt; &lt;none&gt;pod-podantiaffinity-deploy-5fb4764b6b-q98gt 1/1 Running 0 75s 10.244.3.24 k8s-node01 &lt;none&gt; &lt;none&gt;pod-podantiaffinity-deploy-5fb4764b6b-v6kpm 1/1 Running 0 75s 10.244.3.25 k8s-node01 &lt;none&gt; &lt;none&gt;pod-podantiaffinity-deploy-5fb4764b6b-wtmm6 1/1 Running 0 75s 10.244.3.26 k8s-node01 &lt;none&gt; &lt;none&gt;web-deploy-5ccc9d7c55-khhrr 1/1 Running 3 25h 10.244.2.245 k8s-node02 &lt;none&gt; &lt;none&gt; 由上可见，由于是Pod反亲和测验，再根据k8s-master、k8s-node01、k8s-node02的标签信息；很容易推断出Pod只能调度到k8s-node01节点。 pod软反亲和性示例要运行的yaml文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445[root@k8s-master podAffinity]# pwd/root/k8s_practice/scheduler/podAffinity[root@k8s-master podAffinity]# [root@k8s-master podAffinity]# cat pod_preferred_AntiAffinity.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: pod-podantiaffinity-deploy labels: app: podantiaffinity-deployspec: replicas: 6 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: # 允许在master节点运行 tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: myapp-pod image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: # 由于是Pod亲和性/反亲和性；因此这里匹配规则写的是Pod的标签信息 matchExpressions: - key: version operator: In values: - v1 - v2 # 拓扑域 若多个node节点具有相同的标签信息【标签键值相同】，则表示这些node节点就在同一拓扑域 topologyKey: disk-type 运行yaml文件并查看状态 12345678910111213141516171819202122[root@k8s-master podAffinity]# kubectl apply -f pod_preferred_AntiAffinity.yaml deployment.apps/pod-podantiaffinity-deploy created[root@k8s-master podAffinity]# [root@k8s-master podAffinity]# kubectl get deploy -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORpod-podantiaffinity-deploy 6/6 6 6 9s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myappweb-deploy 1/1 1 1 26h myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp-web[root@k8s-master podAffinity]# [root@k8s-master podAffinity]# kubectl get rs -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTORpod-podantiaffinity-deploy-54d758ddb4 6 6 6 13s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp,pod-template-hash=54d758ddb4web-deploy-5ccc9d7c55 1 1 1 26h myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp-web,pod-template-hash=5ccc9d7c55[root@k8s-master podAffinity]# [root@k8s-master podAffinity]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod-podantiaffinity-deploy-54d758ddb4-58t9p 1/1 Running 0 17s 10.244.3.31 k8s-node01 &lt;none&gt; &lt;none&gt;pod-podantiaffinity-deploy-54d758ddb4-9ntd7 1/1 Running 0 17s 10.244.3.32 k8s-node01 &lt;none&gt; &lt;none&gt;pod-podantiaffinity-deploy-54d758ddb4-9wr6p 1/1 Running 0 17s 10.244.2.5 k8s-node02 &lt;none&gt; &lt;none&gt;pod-podantiaffinity-deploy-54d758ddb4-gnls4 1/1 Running 0 17s 10.244.3.30 k8s-node01 &lt;none&gt; &lt;none&gt;pod-podantiaffinity-deploy-54d758ddb4-jlftn 1/1 Running 0 17s 10.244.3.29 k8s-node01 &lt;none&gt; &lt;none&gt;pod-podantiaffinity-deploy-54d758ddb4-mvplv 1/1 Running 0 17s 10.244.0.87 k8s-master &lt;none&gt; &lt;none&gt;web-deploy-5ccc9d7c55-khhrr 1/1 Running 3 26h 10.244.2.245 k8s-node02 &lt;none&gt; &lt;none&gt; 由上可见，由于是Pod反亲和测验，再根据k8s-master、k8s-node01、k8s-node02的标签信息；很容易推断出Pod会优先调度到k8s-node01节点。 pod亲和性与反亲和性联合示例要运行的yaml文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354[root@k8s-master podAffinity]# pwd/root/k8s_practice/scheduler/podAffinity[root@k8s-master podAffinity]# [root@k8s-master podAffinity]# cat pod_podAffinity_all.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: pod-podaffinity-all-deploy labels: app: podaffinity-all-deployspec: replicas: 6 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: # 允许在master节点运行 tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: myapp-pod image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: # 由于是Pod亲和性/反亲和性；因此这里匹配规则写的是Pod的标签信息 matchExpressions: - key: app operator: In values: - myapp-web # 拓扑域 若多个node节点具有相同的标签信息【标签键值相同】，则表示这些node节点就在同一拓扑域 topologyKey: disk-type podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: version operator: In values: - v1 - v2 topologyKey: busi-db 运行yaml文件并查看状态 12345678910111213141516171819202122[root@k8s-master podAffinity]# kubectl apply -f pod_podAffinity_all.yaml deployment.apps/pod-podaffinity-all-deploy created[root@k8s-master podAffinity]# [root@k8s-master podAffinity]# kubectl get deploy -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORpod-podaffinity-all-deploy 6/6 6 1 5s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myappweb-deploy 1/1 1 1 28h myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp-web[root@k8s-master podAffinity]# [root@k8s-master podAffinity]# kubectl get rs -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTORpod-podaffinity-all-deploy-5ddbf9cbf8 6 6 6 10s myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp,pod-template-hash=5ddbf9cbf8web-deploy-5ccc9d7c55 1 1 1 28h myapp-pod registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp-web,pod-template-hash=5ccc9d7c55[root@k8s-master podAffinity]# [root@k8s-master podAffinity]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod-podaffinity-all-deploy-5ddbf9cbf8-5w5b7 1/1 Running 0 15s 10.244.0.91 k8s-master &lt;none&gt; &lt;none&gt;pod-podaffinity-all-deploy-5ddbf9cbf8-j57g9 1/1 Running 0 15s 10.244.0.90 k8s-master &lt;none&gt; &lt;none&gt;pod-podaffinity-all-deploy-5ddbf9cbf8-kwz6w 1/1 Running 0 15s 10.244.0.92 k8s-master &lt;none&gt; &lt;none&gt;pod-podaffinity-all-deploy-5ddbf9cbf8-l8spj 1/1 Running 0 15s 10.244.2.6 k8s-node02 &lt;none&gt; &lt;none&gt;pod-podaffinity-all-deploy-5ddbf9cbf8-lf22c 1/1 Running 0 15s 10.244.0.89 k8s-master &lt;none&gt; &lt;none&gt;pod-podaffinity-all-deploy-5ddbf9cbf8-r2fgl 1/1 Running 0 15s 10.244.0.88 k8s-master &lt;none&gt; &lt;none&gt;web-deploy-5ccc9d7c55-khhrr 1/1 Running 3 28h 10.244.2.245 k8s-node02 &lt;none&gt; &lt;none&gt; 由上可见，根据k8s-master、k8s-node01、k8s-node02的标签信息；很容易推断出Pod只能调度到k8s-master、k8s-node02节点，且会优先调度到k8s-master节点。 相关阅读1、官网：将Pods分配给节点 2、Kubernetes K8S调度器kube-scheduler详解 完毕！]]></content>
      <categories>
        <category>Kubernetes</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
        <tag>K8S</tag>
        <tag>CI/CD</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes K8S之调度器kube-scheduler详解]]></title>
    <url>%2F2020%2F10%2F25%2Fkubernetes21%2F</url>
    <content type="text"><![CDATA[Kubernetes K8S之调度器kube-scheduler概述与详解 kube-scheduler调度概述在 Kubernetes 中，调度是指将 Pod 放置到合适的 Node 节点上，然后对应 Node 上的 Kubelet 才能够运行这些 pod。 调度器通过 kubernetes 的 watch 机制来发现集群中新创建且尚未被调度到 Node 上的 Pod。调度器会将发现的每一个未调度的 Pod 调度到一个合适的 Node 上来运行。调度器会依据下文的调度原则来做出调度选择。 调度是容器编排的重要环节，需要经过严格的监控和控制，现实生产通常对调度有各类限制，譬如某些服务必须在业务独享的机器上运行，或者从灾备的角度考虑尽量把服务调度到不同机器，这些需求在Kubernetes集群依靠调度组件kube-scheduler满足。 kube-scheduler是Kubernetes中的关键模块，扮演管家的角色遵从一套机制——为Pod提供调度服务，例如基于资源的公平调度、调度Pod到指定节点、或者通信频繁的Pod调度到同一节点等。容器调度本身是一件比较复杂的事，因为要确保以下几个目标： 公平性：在调度Pod时需要公平的进行决策，每个节点都有被分配资源的机会，调度器需要对不同节点的使用作出平衡决策。 资源高效利用：最大化群集所有资源的利用率，使有限的CPU、内存等资源服务尽可能更多的Pod。 效率问题：能快速的完成对大批量Pod的调度工作，在集群规模扩增的情况下，依然保证调度过程的性能。 灵活性：在实际运作中，用户往往希望Pod的调度策略是可控的，从而处理大量复杂的实际问题。因此平台要允许多个调度器并行工作，同时支持自定义调度器。 为达到上述目标，kube-scheduler通过结合Node资源、负载情况、数据位置等各种因素进行调度判断，确保在满足场景需求的同时将Pod分配到最优节点。显然，kube-scheduler影响着Kubernetes集群的可用性与性能，Pod数量越多集群的调度能力越重要，尤其达到了数千级节点数时，优秀的调度能力将显著提升容器平台性能。 kube-scheduler调度流程kube-scheduler的根本工作任务是根据各种调度算法将Pod绑定（bind）到最合适的工作节点，整个调度流程分为两个阶段：预选策略（Predicates）和优选策略（Priorities）。 预选（Predicates）：输入是所有节点，输出是满足预选条件的节点。kube-scheduler根据预选策略过滤掉不满足策略的Nodes。例如，如果某节点的资源不足或者不满足预选策略的条件如“Node的label必须与Pod的Selector一致”时则无法通过预选。 优选（Priorities）：输入是预选阶段筛选出的节点，优选会根据优先策略为通过预选的Nodes进行打分排名，选择得分最高的Node。例如，资源越富裕、负载越小的Node可能具有越高的排名。 通俗点说，调度的过程就是在回答两个问题：1. 候选有哪些？2. 其中最适合的是哪个？ 值得一提的是，如果在预选阶段没有节点满足条件，Pod会一直处在Pending状态直到出现满足的节点，在此期间调度器会不断的进行重试。 预选策略（Predicates）官网地址：调度器预选、优选策略 过滤条件包含如下： PodFitsHostPorts：检查Pod容器所需的HostPort是否已被节点上其它容器或服务占用。如果已被占用，则禁止Pod调度到该节点。 PodFitsHost：检查Pod指定的NodeName是否匹配当前节点。 PodFitsResources：检查节点是否有足够空闲资源（例如CPU和内存）来满足Pod的要求。 PodMatchNodeSelector：检查Pod的节点选择器(nodeSelector)是否与节点(Node)的标签匹配 NoVolumeZoneConflict：对于给定的某块区域，判断如果在此区域的节点上部署Pod是否存在卷冲突。 NoDiskConflict：根据节点请求的卷和已经挂载的卷，评估Pod是否适合该节点。 MaxCSIVolumeCount：决定应该附加多少CSI卷，以及该卷是否超过配置的限制。 CheckNodeMemoryPressure：如果节点报告内存压力，并且没有配置异常，那么将不会往那里调度Pod。 CheckNodePIDPressure：如果节点报告进程id稀缺，并且没有配置异常，那么将不会往那里调度Pod。 CheckNodeDiskPressure：如果节点报告存储压力(文件系统已满或接近满)，并且没有配置异常，那么将不会往那里调度Pod。 CheckNodeCondition：节点可以报告它们有一个完全完整的文件系统，然而网络不可用，或者kubelet没有准备好运行Pods。如果为节点设置了这样的条件，并且没有配置异常，那么将不会往那里调度Pod。 PodToleratesNodeTaints：检查Pod的容忍度是否能容忍节点的污点。 CheckVolumeBinding：评估Pod是否适合它所请求的容量。这适用于约束和非约束PVC。 如果在predicates(预选)过程中没有合适的节点，那么Pod会一直在pending状态，不断重试调度，直到有节点满足条件。 经过这个步骤，如果有多个节点满足条件，就继续priorities过程，最后按照优先级大小对节点排序。 优选策略（Priorities）包含如下优选评分条件： SelectorSpreadPriority：对于属于同一服务、有状态集或副本集（Service，StatefulSet or ReplicaSet）的Pods，会将Pods尽量分散到不同主机上。 InterPodAffinityPriority：策略有podAffinity和podAntiAffinity两种配置方式。简单来说，就说根据Node上运行的Pod的Label来进行调度匹配的规则，匹配的表达式有：In, NotIn, Exists, DoesNotExist，通过该策略，可以更灵活地对Pod进行调度。 LeastRequestedPriority：偏向使用较少请求资源的节点。换句话说，放置在节点上的Pod越多，这些Pod使用的资源越多，此策略给出的排名就越低。 MostRequestedPriority：偏向具有最多请求资源的节点。这个策略将把计划的Pods放到整个工作负载集所需的最小节点上运行。 RequestedToCapacityRatioPriority：使用默认的资源评分函数模型创建基于ResourceAllocationPriority的requestedToCapacity。 BalancedResourceAllocation：偏向具有平衡资源使用的节点。 NodePreferAvoidPodsPriority：根据节点注释scheduler.alpha.kubernet .io/preferAvoidPods为节点划分优先级。可以使用它来示意两个不同的Pod不应在同一Node上运行。 NodeAffinityPriority：根据preferredduringschedulingignoredingexecution中所示的节点关联调度偏好来对节点排序。 TaintTolerationPriority：根据节点上无法忍受的污点数量，为所有节点准备优先级列表。此策略将考虑该列表调整节点的排名。 ImageLocalityPriority：偏向已经拥有本地缓存Pod容器镜像的节点。 ServiceSpreadingPriority：对于给定的服务，此策略旨在确保Service的Pods运行在不同的节点上。总的结果是，Service对单个节点故障变得更有弹性。 EqualPriority：赋予所有节点相同的权值1。 EvenPodsSpreadPriority：实现择优 pod的拓扑扩展约束 自定义调度器除了Kubernetes自带的调度器，我们也可以编写自己的调度器。通过spec.schedulername参数指定调度器名字，可以为Pod选择某个调度器进行调度。 如下Pod选择my-scheduler进行调度，而不是默认的default-scheduler 1234567891011apiVersion: v1kind: Podmetadata: name: annotation-second-scheduler labels: name: multischeduler-examplespec: schedulername: my-scheduler containers: - name: pod-with-second-annotation-container image: gcr.io/google_containers/pause:2.0 至于调度器如何编写，我们这里就不详细说了，工作中几乎不会使用到，有兴趣的同学可以自行查阅官网或其他资料。 相关阅读1、官网：调度器预选、优选策略 2、k8s调度器kube-scheduler 完毕！]]></content>
      <categories>
        <category>Kubernetes</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
        <tag>K8S</tag>
        <tag>CI/CD</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes K8S之存储PV-PVC详解]]></title>
    <url>%2F2020%2F10%2F22%2Fkubernetes20%2F</url>
    <content type="text"><![CDATA[K8S之存储PV-PVC概述与说明，并详解常用PV-PVC示例 概述与管理计算实例相比，管理存储是一个明显的问题。PersistentVolume子系统为用户和管理员提供了一个API，该API从如何使用存储中抽象出如何提供存储的详细信息。为此，我们引入了两个新的API资源：PersistentVolume和PersistentVolumeClaim。 PV概述PersistentVolume (PV)是集群中由管理员提供或使用存储类动态提供的一块存储。它是集群中的资源，就像节点是集群资源一样。 PV是与Volumes类似的卷插件，但其生命周期与使用PV的任何单个Pod无关。由此API对象捕获存储的实现细节，不管是NFS、iSCSI还是特定于云提供商的存储系统。 PVC概述PersistentVolumeClaim (PVC) 是用户对存储的请求。它类似于Pod；Pods消耗节点资源，而PVC消耗PV资源。Pods可以请求特定级别的资源(CPU和内存)。Claim可以请求特定的存储大小和访问模式(例如，它们可以挂载一次读写或多次只读)。 虽然PersistentVolumeClaims (PVC) 允许用户使用抽象的存储资源，但是用户通常需要具有不同属性(比如性能)的PersistentVolumes (PV) 来解决不同的问题。集群管理员需要能够提供各种不同的PersistentVolumes，这些卷在大小和访问模式之外还有很多不同之处，也不向用户公开这些卷是如何实现的细节。对于这些需求，有一个StorageClass资源。 volume 和 claim的生命周期PV是集群中的资源。PVC是对这些资源的请求，并且还充当对资源的声明检查。PV和PVC之间的交互遵循以下生命周期： 供应有两种方式配置PV：静态的或动态的。 静态配置 集群管理员创建一些PV。它们带有可供集群用户使用的实际存储的详细信息。存在于Kubernetes API中，可供使用。 动态配置 当管理员创建的静态PV没有一个与用户的PersistentVolumeClaim匹配时，集群可能会尝试动态地为PVC提供一个卷。此配置基于StorageClasses：PVC必须请求存储类，并且管理员必须已经创建并配置了该类，才能进行动态配置。声明该类为 &quot;&quot;，可以有效地禁用其动态配置。 要启用基于存储级别的动态存储配置，集群管理员需要启用API Server上的DefaultStorageClass[准入控制器]。例如，通过确保DefaultStorageClass位于API Server组件的 --enable-admission-plugins标志，使用逗号分隔的有序值列表中，可以完成此操作。 绑定用户创建(或者在动态配置的情况下，已经创建)具有特定存储请求量(大小)和特定访问模式的PersistentVolumeClaim。主控制器中的控制循环监视新的PV，找到匹配的PV(如果可能的话)，并将它们绑定在一起。如果PV为新的PVC动态配置，那么循环始终将该PV绑定到PVC。否则，用户始终至少得到他们所要求的，但是存储量可能会超过所要求的范围。 一旦绑定，无论是如何绑定的，PersistentVolumeClaim绑定都是互斥的。PVC到PV的绑定是一对一的映射，使用ClaimRef，它是PersistentVolume和PersistentVolumeClaim之间的双向绑定。 如果不存在匹配的卷，声明(Claims)将无限期保持未绑定。随着匹配量的增加，声明将受到约束。例如，配备有许多50Gi PV的群集将与请求100Gi的PVC不匹配。当将100Gi PV添加到群集时，可以绑定PVC。 注意：静态时PVC与PV绑定时会根据storageClassName（存储类名称）和accessModes（访问模式）判断哪些PV符合绑定需求。然后再根据存储量大小判断，首先存PV储量必须大于或等于PVC声明量；其次就是PV存储量越接近PVC声明量，那么优先级就越高（PV量越小优先级越高）。 使用Pods使用声明(claims)作为卷。集群检查声明以找到绑定卷并为Pod挂载该卷。对于支持多种访问模式的卷，用户在其声明中作为Pod中卷使用时指定所需的模式。 一旦用户拥有一个声明并且该声明被绑定，则绑定的PV就属于该用户。用户通过在Pod的卷块中包含的persistentVolumeClaim部分来调度Pods并访问其声明的PV。 持久化声明保护“使用中的存储对象保护” ：该功能的目的是确保在Pod活动时使用的PersistentVolumeClaims (PVC)和绑定到PVC的PersistentVolume (PV)不会从系统中删除，因为这可能会导致数据丢失。 如果用户删除了Pod正在使用的PVC，则不会立即删除该PVC；PVC的清除被推迟，直到任何Pod不再主动使用PVC。另外，如果管理员删除绑定到PVC的PV，则不会立即删除该PV；PV的去除被推迟，直到PV不再与PVC结合。 回收策略当用户处理完他们的卷时，他们可以从允许回收资源的API中删除PVC对象。PersistentVolume的回收策略告诉集群在释放卷的声明后该如何处理它。目前，卷可以被保留、回收或删除。 Retain (保留) 保留回收策略允许手动回收资源。当PersistentVolumeClaim被删除时，PersistentVolume仍然存在，并且该卷被认为是“释放”的。但是，由于之前声明的数据仍然存在，因此另一个声明尚无法得到。管理员可以手动回收卷。 Delete (删除) 对于支持Delete回收策略的卷插件，删除操作会同时从Kubernetes中删除PersistentVolume对象以及外部基础架构中的关联存储资产，例如AWS EBS，GCE PD，Azure Disk或Cinder卷。动态配置的卷将继承其StorageClass的回收策略，默认为Delete。管理员应根据用户的期望配置StorageClass。 Recycle (回收) 如果基础卷插件支持，Recycle回收策略将rm -rf /thevolume/*对该卷执行基本的擦除并使其可用于新的声明。 Persistent Volumes类型PersistentVolume类型作为插件实现。Kubernetes当前支持以下插件： 1234567891011121314151617181920GCEPersistentDiskAWSElasticBlockStoreAzureFileAzureDiskCSIFC (Fibre Channel)FlexVolumeFlockerNFSiSCSIRBD (Ceph Block Device)CephFSCinder (OpenStack block storage)GlusterfsVsphereVolumeQuobyte VolumesHostPath (仅用于单节点测试——本地存储不受任何方式的支持，也不能在多节点集群中工作)Portworx VolumesScaleIO VolumesStorageOS PV示例与参数说明PV示例123456789101112131415161718apiVersion: v1kind: PersistentVolumemetadata: name: pv0003spec: capacity: storage: 5Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: slow mountOptions: - hard - nfsvers=4.1 nfs: path: /tmp server: 172.17.0.2 Capacity：通常，PV将具有特定的存储容量设置。当前，存储大小是可以设置或请求的唯一资源。将来的属性可能包括IOPS，吞吐量等。 volumeMode：可选参数，为Filesystem或Block。Filesystem是volumeMode省略参数时使用的默认模式。 accessModes：PersistentVolume可以通过资源提供者支持的任何方式安装在主机上。如下文表中所示，提供商将具有不同的功能，并且每个PV的访问模式都将设置为该特定卷支持的特定模式。例如，NFS可以支持多个读/写客户端，但是特定的NFS PV可能以只读方式在服务器上导出。每个PV都有自己的一组访问模式，用于描述该特定PV的功能。 访问方式为： 123ReadWriteOnce-该卷可以被单个节点以读写方式挂载ReadOnlyMany-该卷可以被许多节点以只读方式挂载ReadWriteMany-该卷可以被多个节点以读写方式挂载 在CLI命令行中，访问模式缩写为： 123RWO-ReadWriteOnceROX-ReadOnlyManyRWX-ReadWriteMany 说明：一个卷一次只能使用一种访问模式挂载，即使它支持多种访问模式。 storageClassName：PV可以有一个类，通过将storageClassName属性设置为一个StorageClass的名称来指定这个类。特定类的PV只能绑定到请求该类的PVC。没有storageClassName的PV没有类，只能绑定到不请求特定类的PVC。 persistentVolumeReclaimPolicy：当前的回收政策是：Retain (保留)-手动回收、Recycle (回收)-基本擦除（rm -rf /thevolume/*）、Delete (删除)-删除相关的存储资产 (例如AWS EBS，GCE PD，Azure Disk或OpenStack Cinder卷)。 备注：当前，仅NFS和HostPath支持回收。AWS EBS，GCE PD，Azure Disk和Cinder卷支持删除。 PV卷状态卷将处于以下某种状态： Available：尚未绑定到声明(claim)的空闲资源 Bound：卷已被声明绑定 Released：声明已被删除，但群集尚未回收该资源 Failed：该卷自动回收失败 CLI将显示绑定到PV的PVC的名称。 PV类型与支持的访问模式 Volume Plugin ReadWriteOnce ReadOnlyMany ReadWriteMany AWSElasticBlockStore ✓ - - AzureFile ✓ ✓ ✓ AzureDisk ✓ - - CephFS ✓ ✓ ✓ Cinder ✓ - - CSI depends on the driver depends on the driver depends on the driver FC ✓ ✓ - FlexVolume ✓ ✓ depends on the driver Flocker ✓ - - GCEPersistentDisk ✓ ✓ - Glusterfs ✓ ✓ ✓ HostPath ✓ - - iSCSI ✓ ✓ - Quobyte ✓ ✓ ✓ NFS ✓ ✓ ✓ RBD ✓ ✓ - VsphereVolume ✓ - - (works when Pods are collocated) PortworxVolume ✓ - ✓ ScaleIO ✓ ✓ - StorageOS ✓ - - PV-PVC示例主机信息 服务器名称(hostname) 系统版本 配置 内网IP 外网IP(模拟) 部署模块 k8s-master CentOS7.7 2C/4G/20G 172.16.1.110 10.0.0.110 k8s-master k8s-node01 CentOS7.7 2C/4G/20G 172.16.1.111 10.0.0.111 k8s-node k8s-node02 CentOS7.7 2C/4G/20G 172.16.1.112 10.0.0.112 k8s-node k8s-node03 CentOS7.7 2C/2G/20G 172.16.1.113 10.0.0.113 NFS 存储使用NFS，在k8s-node03机器仅部署NFS服务，没有部署K8S NFS服务部署文章参考：「NFS 服务搭建与配置」 所有机器操作 12# 所需安装包yum install nfs-utils rpcbind -y NFS服务端k8s-node03机器操作 123456789101112131415161718192021222324252627282930[root@k8s-node03 ~]# mkdir -p /data/nfs1 /data/nfs2 /data/nfs3 /data/nfs4 /data/nfs5 /data/nfs6[root@k8s-node03 ~]# chown -R nfsnobody.nfsnobody /data/ [root@k8s-node03 ~]# [root@k8s-node03 ~]# ll /data/total 0drwxr-xr-x 2 nfsnobody nfsnobody 6 Jun 14 16:30 nfs1drwxr-xr-x 2 nfsnobody nfsnobody 6 Jun 14 16:30 nfs2drwxr-xr-x 2 nfsnobody nfsnobody 6 Jun 14 16:30 nfs3drwxr-xr-x 2 nfsnobody nfsnobody 6 Jun 14 16:30 nfs4drwxr-xr-x 2 nfsnobody nfsnobody 6 Jun 14 16:30 nfs5drwxr-xr-x 2 nfsnobody nfsnobody 6 Aug 22 16:25 nfs6[root@k8s-node03 ~]# vim /etc/exports/data/nfs1 172.16.1.0/24(rw,sync,root_squash,all_squash)/data/nfs2 172.16.1.0/24(rw,sync,root_squash,all_squash)/data/nfs3 172.16.1.0/24(rw,sync,root_squash,all_squash)/data/nfs4 172.16.1.0/24(rw,sync,root_squash,all_squash)/data/nfs5 172.16.1.0/24(rw,sync,root_squash,all_squash)/data/nfs6 172.16.1.0/24(rw,sync,root_squash,all_squash)### 启动NFS服务[root@k8s-node03 ~]# systemctl start rpcbind.service [root@k8s-node03 ~]# systemctl start nfs.service ### 检查NFS服务 ， 其中 172.16.1.113 为服务端IP[root@k8s-node03 ~]# showmount -e 172.16.1.113Export list for 172.16.1.113:/data/nfs6 172.16.1.0/24/data/nfs5 172.16.1.0/24/data/nfs4 172.16.1.0/24/data/nfs3 172.16.1.0/24/data/nfs2 172.16.1.0/24/data/nfs1 172.16.1.0/24 NFS客户端验证 在k8s-node02机器验证 123456789# 查看rpcbind服务，默认是启动的，如果没有启动则启动并加入开机自启动[root@k8s-node02 ~]# systemctl status rpcbind.service# 查看NFS服务信息[root@k8s-node02 ~]# showmount -e 172.16.1.113………………# 挂载，并进行读写验证[root@k8s-node02 ~]# mount -t nfs 172.16.1.113:/data/nfs1 /mnt# 验证完毕，去掉NFS挂载[root@k8s-node02 ~]# umount -lf 172.16.1.113:/data/nfs1 PV部署yaml文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192[root@k8s-master pv-pvc]# pwd/root/k8s_practice/pv-pvc[root@k8s-master pv-pvc]# cat pv.yaml apiVersion: v1kind: PersistentVolumemetadata: name: pv-nfs1spec: capacity: storage: 1Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: nfs nfs: path: /data/nfs1 server: 172.16.1.113---apiVersion: v1kind: PersistentVolumemetadata: name: pv-nfs2spec: capacity: storage: 3Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: nfs nfs: path: /data/nfs2 server: 172.16.1.113---apiVersion: v1kind: PersistentVolumemetadata: name: pv-nfs3spec: capacity: storage: 5Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: slow nfs: path: /data/nfs3 server: 172.16.1.113---apiVersion: v1kind: PersistentVolumemetadata: name: pv-nfs4spec: capacity: storage: 10Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: nfs nfs: path: /data/nfs4 server: 172.16.1.113---apiVersion: v1kind: PersistentVolumemetadata: name: pv-nfs5spec: capacity: storage: 5Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Recycle storageClassName: nfs nfs: path: /data/nfs5 server: 172.16.1.113---apiVersion: v1kind: PersistentVolumemetadata: name: pv-nfs6spec: capacity: storage: 5Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: nfs nfs: path: /data/nfs6 server: 172.16.1.113 启动PV，并查看状态 12345678910111213141516[root@k8s-master pv-pvc]# kubectl apply -f pv.yaml persistentvolume/pv-nfs1 createdpersistentvolume/pv-nfs2 createdpersistentvolume/pv-nfs3 createdpersistentvolume/pv-nfs4 createdpersistentvolume/pv-nfs5 createdpersistentvolume/pv-nfs6 created[root@k8s-master pv-pvc]# [root@k8s-master pv-pvc]# kubectl get pv -o wideNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE VOLUMEMODEpv-nfs1 1Gi RWO Recycle Available nfs 11s Filesystempv-nfs2 3Gi RWO Recycle Available nfs 11s Filesystempv-nfs3 5Gi RWO Recycle Available slow 11s Filesystempv-nfs4 10Gi RWO Recycle Available nfs 11s Filesystempv-nfs5 5Gi RWX Recycle Available nfs 11s Filesystempv-nfs6 5Gi RWO Recycle Available nfs 11s Filesystem StatefulSet创建并使用PVCStatefulSet 需要 headless 服务 来负责 Pod 的网络标识，因此需要负责创建此服务。 yaml文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051[root@k8s-master pv-pvc]# pwd/root/k8s_practice/pv-pvc[root@k8s-master pv-pvc]# cat sts-pod-pvc.yaml apiVersion: v1kind: Servicemetadata: name: nginx labels: app: nginxspec: ports: - port: 80 name: web clusterIP: None selector: app: nginx---apiVersion: apps/v1kind: StatefulSetmetadata: name: webspec: selector: matchLabels: app: nginx # has to match .spec.template.metadata.labels serviceName: &quot;nginx&quot; replicas: 3 # by default is 1 template: metadata: labels: app: nginx # has to match .spec.selector.matchLabels spec: terminationGracePeriodSeconds: 100 containers: - name: nginx image: registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: [ &quot;ReadWriteOnce&quot; ] storageClassName: &quot;nfs&quot; resources: requests: storage: 3Gi 启动pod并查看状态 12345678910111213141516171819202122232425262728[root@k8s-master pv-pvc]# kubectl apply -f sts-pod-pvc.yaml service/nginx createdstatefulset.apps/web created[root@k8s-master pv-pvc]# [root@k8s-master pv-pvc]# kubectl get svc -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 24d &lt;none&gt;nginx ClusterIP None &lt;none&gt; 80/TCP 17s app=nginx[root@k8s-master pv-pvc]# [root@k8s-master pv-pvc]# kubectl get sts -o wideNAME READY AGE CONTAINERS IMAGESweb 3/3 82m nginx registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17[root@k8s-master pv-pvc]# [root@k8s-master pv-pvc]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESweb-0 0/1 ContainerCreating 0 3s &lt;none&gt; k8s-node01 &lt;none&gt; &lt;none&gt;[root@k8s-master pv-pvc]# [root@k8s-master pv-pvc]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESweb-0 1/1 Running 0 11s 10.244.4.135 k8s-node01 &lt;none&gt; &lt;none&gt;web-1 1/1 Running 0 6s 10.244.2.171 k8s-node02 &lt;none&gt; &lt;none&gt;web-2 0/1 ContainerCreating 0 3s &lt;none&gt; k8s-node01 &lt;none&gt; &lt;none&gt;[root@k8s-master pv-pvc]# [root@k8s-master pv-pvc]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESweb-0 1/1 Running 0 8m23s 10.244.2.174 k8s-node02 &lt;none&gt; &lt;none&gt;web-1 1/1 Running 0 8m20s 10.244.4.139 k8s-node01 &lt;none&gt; &lt;none&gt;web-2 1/1 Running 0 8m17s 10.244.2.175 k8s-node02 &lt;none&gt; &lt;none&gt; PV和PVC状态信息查看123456789101112131415### 注意挂载顺序[root@k8s-master pv-pvc]# kubectl get pv -o wideNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE VOLUMEMODEpv-nfs1 1Gi RWO Recycle Available nfs 116s Filesystempv-nfs2 3Gi RWO Recycle Bound default/www-web-0 nfs 116s Filesystempv-nfs3 5Gi RWO Recycle Available slow 116s Filesystempv-nfs4 10Gi RWO Recycle Bound default/www-web-2 nfs 116s Filesystempv-nfs5 5Gi RWX Recycle Available nfs 116s Filesystempv-nfs6 5Gi RWO Recycle Bound default/www-web-1 nfs 116s Filesystem[root@k8s-master pv-pvc]# [root@k8s-master pv-pvc]# kubectl get pvc -o wideNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE VOLUMEMODEwww-web-0 Bound pv-nfs2 3Gi RWO nfs 87s Filesystemwww-web-1 Bound pv-nfs6 5Gi RWO nfs 84s Filesystemwww-web-2 Bound pv-nfs4 10Gi RWO nfs 82s Filesystem PVC与PV绑定时会根据storageClassName（存储类名称）和accessModes（访问模式）判断哪些PV符合绑定需求。然后再根据存储量大小判断，首先存PV储量必须大于或等于PVC声明量；其次就是PV存储量越接近PVC声明量，那么优先级就越高（PV量越小优先级越高）。 curl访问验证在NFS服务端k8s-node03（172.16.1.113）对应NFS共享目录创建文件 123echo &quot;pv-nfs2===&quot; &gt; /data/nfs2/index.htmlecho &quot;pv-nfs4+++&quot; &gt; /data/nfs4/index.htmlecho &quot;pv-nfs6---&quot; &gt; /data/nfs6/index.html curl访问pod 1234567891011121314[root@k8s-master pv-pvc]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESweb-0 1/1 Running 0 8m23s 10.244.2.174 k8s-node02 &lt;none&gt; &lt;none&gt;web-1 1/1 Running 0 8m20s 10.244.4.139 k8s-node01 &lt;none&gt; &lt;none&gt;web-2 1/1 Running 0 8m17s 10.244.2.175 k8s-node02 &lt;none&gt; &lt;none&gt;[root@k8s-master pv-pvc]# [root@k8s-master pv-pvc]# curl 10.244.2.174pv-nfs2===[root@k8s-master pv-pvc]# [root@k8s-master pv-pvc]# curl 10.244.4.139pv-nfs6---[root@k8s-master pv-pvc]# [root@k8s-master pv-pvc]# curl 10.244.2.175pv-nfs4+++ 即使删除其中一个pod，pod被拉起来后也能正常访问。 删除sts并回收PV删除statefulset 123456[root@k8s-master pv-pvc]# kubectl delete -f sts-pod-pvc.yaml service &quot;nginx&quot; deletedstatefulset.apps &quot;web&quot; deleted[root@k8s-master pv-pvc]# [root@k8s-master pv-pvc]# kubectl get pod -o wideNo resources found in default namespace. 查看PVC和PV，并删除PVC 12345678910111213141516171819[root@k8s-master pv-pvc]# kubectl get pvc -o wideNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE VOLUMEMODEwww-web-0 Bound pv-nfs2 3Gi RWO nfs 24m Filesystemwww-web-1 Bound pv-nfs6 5Gi RWO nfs 24m Filesystemwww-web-2 Bound pv-nfs4 10Gi RWO nfs 24m Filesystem[root@k8s-master pv-pvc]# [root@k8s-master pv-pvc]# kubectl get pv -o wideNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE VOLUMEMODEpv-nfs1 1Gi RWO Recycle Available nfs 26m Filesystempv-nfs2 3Gi RWO Recycle Bound default/www-web-0 nfs 26m Filesystempv-nfs3 5Gi RWO Recycle Available slow 26m Filesystempv-nfs4 10Gi RWO Recycle Bound default/www-web-2 nfs 26m Filesystempv-nfs5 5Gi RWX Recycle Available nfs 26m Filesystempv-nfs6 5Gi RWO Recycle Bound default/www-web-1 nfs 26m Filesystem[root@k8s-master pv-pvc]# [root@k8s-master pv-pvc]# kubectl delete pvc www-web-0 www-web-1 www-web-2persistentvolumeclaim &quot;www-web-0&quot; deletedpersistentvolumeclaim &quot;www-web-1&quot; deletedpersistentvolumeclaim &quot;www-web-2&quot; deleted 回收PV 1234567891011121314151617181920212223242526272829303132333435363738### 由下可见，还有一个pv虽然声明被删除，但资源尚未回收；我们只需等一会儿即可[root@k8s-master pv-pvc]# kubectl get pv -o wideNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE VOLUMEMODEpv-nfs1 1Gi RWO Recycle Available nfs 90m Filesystempv-nfs2 3Gi RWO Recycle Available nfs 90m Filesystempv-nfs3 5Gi RWO Recycle Available slow 90m Filesystempv-nfs4 10Gi RWO Recycle Available nfs 90m Filesystempv-nfs5 5Gi RWX Recycle Available nfs 90m Filesystempv-nfs6 5Gi RWO Recycle Released default/www-web-1 nfs 90m Filesystem[root@k8s-master pv-pvc]# ### 可见该pv还有引用[root@k8s-master pv-pvc]# kubectl get pv pv-nfs6 -o yamlapiVersion: v1kind: PersistentVolumemetadata:………………spec: accessModes: - ReadWriteOnce capacity: storage: 5Gi ################### 可见仍然在被使用 claimRef: apiVersion: v1 kind: PersistentVolumeClaim name: www-web-1 namespace: default resourceVersion: &quot;1179810&quot; uid: d4d8943c-6b16-45a5-8ffc-691fcefc4f88 ################### nfs: path: /data/nfs6 server: 172.16.1.113 persistentVolumeReclaimPolicy: Recycle storageClassName: nfs volumeMode: Filesystemstatus: phase: Released 在NFS服务端查看结果如下，可见/data/nfs6资源尚未回收，而/data/nfs2/、/data/nfs4/资源已经被回收。 1234567891011[root@k8s-node03 ~]# tree /data//data/├── nfs1├── nfs2├── nfs3├── nfs4├── nfs5└── nfs6 └── index.html5 directories, 2 files 针对这种情况有两种处理方式： 1、我们什么也不用做，等一会儿集群就能回收该资源 2、我们进行手动回收，操作如下 手动回收资源 1234567891011[root@k8s-master pv-pvc]# kubectl edit pv pv-nfs6### 去掉claimRef: 部分### 再次查看pv信息[root@k8s-master pv-pvc]# kubectl get pv -o wideNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE VOLUMEMODEpv-nfs1 1Gi RWO Recycle Available nfs 108m Filesystempv-nfs2 3Gi RWO Recycle Available nfs 108m Filesystempv-nfs3 5Gi RWO Recycle Available slow 108m Filesystempv-nfs4 10Gi RWO Recycle Available nfs 108m Filesystempv-nfs5 5Gi RWX Recycle Available nfs 108m Filesystempv-nfs6 5Gi RWO Recycle Available nfs 108m Filesystem 之后到NFS服务端操作，清除该pv下的数据 1[root@k8s-node03 ~]# rm -fr /data/nfs6/* 到此，手动回收资源操作成功！ StatefulSet网络标识与PVC1、匹配StatefulSet的Pod name(网络标识)的模式为：$(statefulset名称)-$(序号)，比如StatefulSet名称为web，副本数为3。则为：web-0、web-1、web-2 2、StatefulSet为每个Pod副本创建了一个DNS域名，这个域名的格式为：$(podname).(headless service name)，也就意味着服务之间是通过Pod域名来通信而非Pod IP。当Pod所在Node发生故障时，Pod会被漂移到其他Node上，Pod IP会发生改变，但Pod域名不会变化 3、StatefulSet使用Headless服务来控制Pod的域名，这个Headless服务域名的为：$(service name).$(namespace).svc.cluster.local，其中 cluster.local 指定的集群的域名 4、根据volumeClaimTemplates，为每个Pod创建一个PVC，PVC的命令规则为：$(volumeClaimTemplates name)-$(pod name)，比如volumeClaimTemplates为www，pod name为web-0、web-1、web-2；那么创建出来的PVC为：www-web-0、www-web-1、www-web-2 5、删除Pod不会删除对应的PVC，手动删除PVC将自动释放PV。 相关阅读1、NFS 服务搭建与配置 2、Kubernetes K8S 资源控制器StatefulSets详解 完毕！]]></content>
      <categories>
        <category>Kubernetes</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
        <tag>K8S</tag>
        <tag>CI/CD</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes K8S之存储Volume详解]]></title>
    <url>%2F2020%2F10%2F19%2Fkubernetes19%2F</url>
    <content type="text"><![CDATA[K8S之存储Volume概述与说明，并详解常用Volume示例 主机配置规划 服务器名称(hostname) 系统版本 配置 内网IP 外网IP(模拟) k8s-master CentOS7.7 2C/4G/20G 172.16.1.110 10.0.0.110 k8s-node01 CentOS7.7 2C/4G/20G 172.16.1.111 10.0.0.111 k8s-node02 CentOS7.7 2C/4G/20G 172.16.1.112 10.0.0.112 Volume概述在容器中的文件在磁盘上是临时存放的，当容器关闭时这些临时文件也会被一并清除。这给容器中运行的特殊应用程序带来一些问题。 首先，当容器崩溃时，kubelet 将重新启动容器，容器中的文件将会丢失——因为容器会以干净的状态重建。 其次，当在一个 Pod 中同时运行多个容器时，常常需要在这些容器之间共享文件。 Kubernetes 抽象出 Volume 对象来解决这两个问题。 Kubernetes Volume卷具有明确的生命周期——与包裹它的 Pod 相同。 因此，Volume比 Pod 中运行的任何容器的存活期都长，在容器重新启动时数据也会得到保留。 当然，当一个 Pod 不再存在时，Volume也将不再存在。更重要的是，Kubernetes 可以支持许多类型的Volume卷，Pod 也能同时使用任意数量的Volume卷。 使用卷时，Pod 声明中需要提供卷的类型 (.spec.volumes 字段)和卷挂载的位置 (.spec.containers.volumeMounts 字段). Volume类型Kubernetes 支持下列类型的卷： 12345678910111213141516171819202122232425262728awsElasticBlockStoreazureDiskazureFilecephfscinderconfigMapcsidownwardAPIemptyDirfc (fibre channel)flexVolumeflockergcePersistentDiskgitRepo (deprecated)glusterfshostPathiscsilocalnfspersistentVolumeClaimprojectedportworxVolumequobyterbdscaleIOsecretstorageosvsphereVolume 这里我们只介绍常用的存储，包括：Secret、ConfigMap、emptyDir、hostPath。 其中Secret参考文章：「Kubernetes K8S之存储Secret详解」 ConfigMap参考文章：「Kubernetes K8S之存储ConfigMap详解」 本文只说emptyDir和hostPath存储。 emptyDir卷当 Pod 指定到某个节点上时，首先创建的是一个 emptyDir 卷，并且只要 Pod 在该节点上运行，卷就一直存在。就像它的名称表示的那样，卷最初是空的。 尽管 Pod 中每个容器挂载 emptyDir 卷的路径可能相同也可能不同，但是这些容器都可以读写 emptyDir 卷中相同的文件。 如果Pod中有多个容器，其中某个容器重启，不会影响emptyDir 卷中的数据。当 Pod 因为某些原因被删除时，emptyDir 卷中的数据也会永久删除。 注意：容器崩溃并不会导致 Pod 被从节点上移除，因此容器崩溃时 emptyDir 卷中的数据是安全的。 emptyDir的一些用途： 缓存空间，例如基于磁盘的归并排序 为耗时较长的计算任务提供检查点，以便任务能方便地从崩溃前状态恢复执行 在 Web 服务器容器服务数据时，保存内容管理器容器获取的文件 emptyDir示例yaml文件 1234567891011121314151617181920212223242526[root@k8s-master emptydir]# pwd/root/k8s_practice/emptydir[root@k8s-master emptydir]# cat pod_emptydir.yaml apiVersion: v1kind: Podmetadata: name: pod-emptydir namespace: defaultspec: containers: - name: myapp-pod image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 imagePullPolicy: IfNotPresent volumeMounts: - mountPath: /cache name: cache-volume - name: busybox-pod image: registry.cn-beijing.aliyuncs.com/google_registry/busybox:1.24 imagePullPolicy: IfNotPresent command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;sleep 3600&quot;] volumeMounts: - mountPath: /test/cache name: cache-volume volumes: - name: cache-volume emptyDir: &#123;&#125; 启动pod，并查看状态 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182[root@k8s-master emptydir]# kubectl apply -f pod_emptydir.yaml pod/pod-emptydir created[root@k8s-master emptydir]# [root@k8s-master emptydir]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod-emptydir 2/2 Running 0 10s 10.244.2.166 k8s-node02 &lt;none&gt; &lt;none&gt;[root@k8s-master emptydir]# [root@k8s-master emptydir]# kubectl describe pod pod-emptydirName: pod-emptydirNamespace: defaultPriority: 0Node: k8s-node02/172.16.1.112Start Time: Fri, 12 Jun 2020 22:49:11 +0800Labels: &lt;none&gt;Annotations: kubectl.kubernetes.io/last-applied-configuration: &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Pod&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;pod-emptydir&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;,&quot;spec&quot;:&#123;&quot;containers&quot;:[&#123;&quot;image&quot;:&quot;...Status: RunningIP: 10.244.2.166IPs: IP: 10.244.2.166Containers: myapp-pod: Container ID: docker://d45663776b40a24e7cfc3cf46cb08cf3ed6b98b023a5d2cb5f42bee2234c7338 Image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 Image ID: docker-pullable://10.0.0.110:5000/k8s-secret/myapp@sha256:9eeca44ba2d410e54fccc54cbe9c021802aa8b9836a0bcf3d3229354e4c8870e Port: &lt;none&gt; Host Port: &lt;none&gt; State: Running Started: Fri, 12 Jun 2020 22:49:12 +0800 Ready: True Restart Count: 0 Environment: &lt;none&gt; Mounts: /cache from cache-volume (rw) ##### 挂载信息 /var/run/secrets/kubernetes.io/serviceaccount from default-token-v48g4 (ro) busybox-pod: Container ID: docker://c2917ba30c3322fb0caead5d97476b341e691f9fb1990091264364b8cd340512 Image: registry.cn-beijing.aliyuncs.com/google_registry/busybox:1.24 Image ID: docker-pullable://registry.cn-beijing.aliyuncs.com/ducafe/busybox@sha256:f73ae051fae52945d92ee20d62c315306c593c59a429ccbbdcba4a488ee12269 Port: &lt;none&gt; Host Port: &lt;none&gt; Command: /bin/sh -c sleep 3600 State: Running Started: Fri, 12 Jun 2020 22:49:12 +0800 Ready: True Restart Count: 0 Environment: &lt;none&gt; Mounts: /test/cache from cache-volume (rw) ##### 挂载信息 /var/run/secrets/kubernetes.io/serviceaccount from default-token-v48g4 (ro)Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: cache-volume: Type: EmptyDir (a temporary directory that shares a pod&apos;s lifetime) Medium: SizeLimit: &lt;unset&gt; default-token-v48g4: Type: Secret (a volume populated by a Secret) SecretName: default-token-v48g4 Optional: falseQoS Class: BestEffortNode-Selectors: &lt;none&gt;Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300sEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 3s default-scheduler Successfully assigned default/pod-emptydir to k8s-node02 Normal Pulled 2s kubelet, k8s-node02 Container image &quot;registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1&quot; already present on machine Normal Created 2s kubelet, k8s-node02 Created container myapp-pod Normal Started 2s kubelet, k8s-node02 Started container myapp-pod Normal Pulled 2s kubelet, k8s-node02 Container image &quot;registry.cn-beijing.aliyuncs.com/google_registry/busybox:1.24&quot; already present on machine Normal Created 2s kubelet, k8s-node02 Created container busybox-pod Normal Started 2s kubelet, k8s-node02 Started container busybox-pod emptyDir验证在pod中的myapp-pod容器内操作 123456789101112[root@k8s-master emptydir]# kubectl exec -it pod-emptydir -c myapp-pod -- sh/ # cd /cache/cache # /cache # pwd/cache/cache # /cache # date &gt;&gt; data.info/cache # ls -ltotal 4-rw-r--r-- 1 root root 29 Jun 12 14:53 data.info/cache # cat data.info Fri Jun 12 14:53:27 UTC 2020 在pod中的busybox-pod容器内操作 1234567891011121314[root@k8s-master emptydir]# kubectl exec -it pod-emptydir -c busybox-pod -- sh/ # cd /test/cache/test/cache # ls -ltotal 4-rw-r--r-- 1 root root 29 Jun 12 14:53 data.info/test/cache # cat data.info Fri Jun 12 14:53:27 UTC 2020/test/cache # /test/cache # echo &quot;===&quot; &gt;&gt; data.info /test/cache # date &gt;&gt; data.info /test/cache # cat data.info Fri Jun 12 14:53:27 UTC 2020===Fri Jun 12 14:56:05 UTC 2020 由上可见，一个Pod中多个容器可共享同一个emptyDir卷。 hostPath卷hostPath 卷能将主机node节点文件系统上的文件或目录挂载到你的 Pod 中。 虽然这不是大多数 Pod 需要的，但是它为一些应用程序提供了强大的逃生舱。 hostPath 的一些用法有 运行一个需要访问 Docker 引擎内部机制的容器；请使用 hostPath 挂载 /var/lib/docker 路径。 在容器中运行 cAdvisor 时，以 hostPath 方式挂载 /sys。 允许 Pod 指定给定的 hostPath 在运行 Pod 之前是否应该存在，是否应该创建以及应该以什么方式存在。 支持类型除了必需的 path 属性之外，用户可以选择性地为 hostPath 卷指定 type。支持的 type 值如下： 取值 行为 空字符串（默认）用于向后兼容，这意味着在安装 hostPath 卷之前不会执行任何检查 DirectoryOrCreate 如果指定的路径不存在，那么将根据需要创建空目录，权限设置为 0755，具有与 Kubelet 相同的组和所有权 Directory 给定的路径必须存在 FileOrCreate 如果给定路径的文件不存在，那么将在那里根据需要创建空文件，权限设置为 0644，具有与 Kubelet 相同的组和所有权【前提：文件所在目录必须存在；目录不存在则不能创建文件】 File 给定路径上的文件必须存在 Socket 在给定路径上必须存在的 UNIX 套接字 CharDevice 在给定路径上必须存在的字符设备 BlockDevice 在给定路径上必须存在的块设备 注意事项当使用这种类型的卷时要小心，因为： 具有相同配置（例如从 podTemplate 创建）的多个 Pod 会由于节点上文件的不同而在不同节点上有不同的行为。 当 Kubernetes 按照计划添加资源感知的调度时，这类调度机制将无法考虑由 hostPath 卷使用的资源。 基础主机上创建的文件或目录只能由 root 用户写入。需要在 特权容器 中以 root 身份运行进程，或者修改主机上的文件权限以便容器能够写入 hostPath 卷。 hostPath示例yaml文件 123456789101112131415161718192021222324252627282930[root@k8s-master hostpath]# pwd/root/k8s_practice/hostpath[root@k8s-master hostpath]# cat pod_hostpath.yaml apiVersion: v1kind: Podmetadata: name: pod-hostpath namespace: defaultspec: containers: - name: myapp-pod image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 imagePullPolicy: IfNotPresent volumeMounts: - name: hostpath-dir-volume mountPath: /test-k8s/hostpath-dir - name: hostpath-file-volume mountPath: /test/hostpath-file/test.conf volumes: - name: hostpath-dir-volume hostPath: # 宿主机目录 path: /k8s/hostpath-dir # hostPath 卷指定 type，如果目录不存在则创建(可创建多层目录) type: DirectoryOrCreate - name: hostpath-file-volume hostPath: path: /k8s2/hostpath-file/test.conf # 如果文件不存在则创建。 前提：文件所在目录必须存在 目录不存在则不能创建文件 type: FileOrCreate 启动pod，并查看状态 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566[root@k8s-master hostpath]# kubectl apply -f pod_hostpath.yaml pod/pod-hostpath created[root@k8s-master hostpath]# [root@k8s-master hostpath]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod-hostpath 1/1 Running 0 17s 10.244.4.133 k8s-node01 &lt;none&gt; &lt;none&gt;[root@k8s-master hostpath]# [root@k8s-master hostpath]# kubectl describe pod pod-hostpathName: pod-hostpathNamespace: defaultPriority: 0Node: k8s-node01/172.16.1.111Start Time: Sat, 13 Jun 2020 16:12:15 +0800Labels: &lt;none&gt;Annotations: kubectl.kubernetes.io/last-applied-configuration: &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Pod&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;pod-hostpath&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;,&quot;spec&quot;:&#123;&quot;containers&quot;:[&#123;&quot;image&quot;:&quot;...Status: RunningIP: 10.244.4.133IPs: IP: 10.244.4.133Containers: myapp-pod: Container ID: docker://8cc87217fb483288067fb6d227c46aa890d02f75cae85c6d110646839435ab96 Image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 Image ID: docker-pullable://registry.cn-beijing.aliyuncs.com/google_registry/myapp@sha256:9eeca44ba2d410e54fccc54cbe9c021802aa8b9836a0bcf3d3229354e4c8870e Port: &lt;none&gt; Host Port: &lt;none&gt; State: Running Started: Sat, 13 Jun 2020 16:12:17 +0800 Ready: True Restart Count: 0 Environment: &lt;none&gt; Mounts: /test-k8s/hostpath-dir from hostpath-dir-volume (rw) /test/hostpath-file/test.conf from hostpath-file-volume (rw) /var/run/secrets/kubernetes.io/serviceaccount from default-token-v48g4 (ro)Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: hostpath-dir-volume: Type: HostPath (bare host directory volume) Path: /k8s/hostpath-dir HostPathType: DirectoryOrCreate hostpath-file-volume: Type: HostPath (bare host directory volume) Path: /k8s2/hostpath-file/test.conf HostPathType: FileOrCreate default-token-v48g4: Type: Secret (a volume populated by a Secret) SecretName: default-token-v48g4 Optional: falseQoS Class: BestEffortNode-Selectors: &lt;none&gt;Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300sEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled &lt;unknown&gt; default-scheduler Successfully assigned default/pod-hostpath to k8s-node01 Normal Pulled 12m kubelet, k8s-node01 Container image &quot;registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1&quot; already present on machine Normal Created 12m kubelet, k8s-node01 Created container myapp-pod Normal Started 12m kubelet, k8s-node01 Started container myapp-pod hostPath验证宿主机操作 根据pod，在k8s-node01节点宿主机操作【因为Pod分配到了该节点】 1234567891011121314151617# 对挂载的目录操作[root@k8s-node01 hostpath-dir]# pwd/k8s/hostpath-dir[root@k8s-node01 hostpath-dir]# echo &quot;dir&quot; &gt;&gt; info[root@k8s-node01 hostpath-dir]# date &gt;&gt; info[root@k8s-node01 hostpath-dir]# cat info dirSat Jun 13 16:22:37 CST 2020# 对挂载的文件操作[root@k8s-node01 hostpath-file]# pwd/k8s2/hostpath-file[root@k8s-node01 hostpath-file]# echo &quot;file&quot; &gt;&gt; test.conf [root@k8s-node01 hostpath-file]# date &gt;&gt; test.conf [root@k8s-node01 hostpath-file]# [root@k8s-node01 hostpath-file]# cat test.conf fileSat Jun 13 16:23:05 CST 2020 在Pod 容器中操作 1234567891011121314151617181920212223242526# 进入pod 中的指定容器【如果只有一个容器，那么可以不指定容器】[root@k8s-master hostpath]# kubectl exec -it pod-hostpath -c myapp-pod -- /bin/sh##### 对挂载的目录操作/ # cd /test-k8s/hostpath-dir/test-k8s/hostpath-dir # ls -ltotal 4-rw-r--r-- 1 root root 33 Jun 13 08:22 info/test-k8s/hostpath-dir # cat infodirSat Jun 13 16:22:37 CST 2020/test-k8s/hostpath-dir # /test-k8s/hostpath-dir # date &gt;&gt; info /test-k8s/hostpath-dir # cat infodirSat Jun 13 16:22:37 CST 2020Sat Jun 13 08:26:10 UTC 2020##### 对挂载的文件操作# cd /test/hostpath-file//test/hostpath-file # cat test.conf fileSat Jun 13 16:23:05 CST 2020/test/hostpath-file # echo &quot;file====&quot; &gt;&gt; test.conf /test/hostpath-file # cat test.conf fileSat Jun 13 16:23:05 CST 2020file==== 相关阅读1、Kubernetes K8S之存储Secret详解 2、Kubernetes K8S之存储ConfigMap详解 3、官网Volume详解]]></content>
      <categories>
        <category>Kubernetes</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
        <tag>K8S</tag>
        <tag>CI/CD</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes K8S之存储ConfigMap详解]]></title>
    <url>%2F2020%2F10%2F14%2Fkubernetes18%2F</url>
    <content type="text"><![CDATA[K8S之存储ConfigMap概述与说明，并详解常用ConfigMap示例 主机配置规划 服务器名称(hostname) 系统版本 配置 内网IP 外网IP(模拟) k8s-master CentOS7.7 2C/4G/20G 172.16.1.110 10.0.0.110 k8s-node01 CentOS7.7 2C/4G/20G 172.16.1.111 10.0.0.111 k8s-node02 CentOS7.7 2C/4G/20G 172.16.1.112 10.0.0.112 ConfigMap概述ConfigMap 是一种 API 对象，用来将非机密性的数据保存到健值对中。使用时可以用作环境变量、命令行参数或者存储卷中的配置文件。 ConfigMap 将环境配置信息和容器镜像解耦，便于应用配置的修改。当你需要储存机密信息时可以使用 Secret 对象。 备注：ConfigMap 并不提供保密或者加密功能。如果你想存储的数据是机密的，请使用 Secret；或者使用其他第三方工具来保证数据的私密性，而不是用 ConfigMap。 ConfigMap创建方式通过目录创建配置文件目录 12345678910111213141516171819202122[root@k8s-master storage]# pwd/root/k8s_practice/storage[root@k8s-master storage]# ll /root/k8s_practice/storage/configmap # 配置文件存在哪个目录下total 8-rw-r--r-- 1 root root 159 Jun 7 14:52 game.properties-rw-r--r-- 1 root root 83 Jun 7 14:53 ui.properties[root@k8s-master storage]# [root@k8s-master storage]# cat configmap/game.properties # 涉及文件1enemies=alienslives=3enemies.cheat=trueenemies.cheat.level=noGoodRottensecret.code.passphrase=UUDDLRLRBABAssecret.code.allowed=truesecret.code.lives=30[root@k8s-master storage]# [root@k8s-master storage]# cat configmap/ui.properties # 涉及文件2color.good=purplecolor.bad=yellowallow.textmode=truehow.nice.to.look=fairlyNice 创建ConfigMap并查看状态 123456[root@k8s-master storage]# kubectl create configmap game-config --from-file=/root/k8s_practice/storage/configmapconfigmap/game-config created[root@k8s-master storage]# [root@k8s-master storage]# kubectl get configmap NAME DATA AGEgame-config 2 14s 查看ConfigMap有哪些数据 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[root@k8s-master storage]# kubectl get configmap -o yaml ##### 查看方式1apiVersion: v1items:- apiVersion: v1 data: game.properties: |+ ##### 本段最后有一行空格，+ 表示保留字符串行末尾的换行 enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAs secret.code.allowed=true secret.code.lives=30 ui.properties: | color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNice kind: ConfigMap metadata: creationTimestamp: &quot;2020-06-07T06:57:28Z&quot; name: game-config namespace: default resourceVersion: &quot;889177&quot; selfLink: /api/v1/namespaces/default/configmaps/game-config uid: 6952ac85-ded0-4c5e-89fd-b0c6f0546ecfkind: Listmetadata: resourceVersion: &quot;&quot; selfLink: &quot;&quot;[root@k8s-master storage]# [root@k8s-master storage]# kubectl describe configmap game-config ##### 查看方式2Name: game-configNamespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Data====game.properties:----enemies=alienslives=3enemies.cheat=trueenemies.cheat.level=noGoodRottensecret.code.passphrase=UUDDLRLRBABAssecret.code.allowed=truesecret.code.lives=30ui.properties:----color.good=purplecolor.bad=yellowallow.textmode=truehow.nice.to.look=fairlyNiceEvents: &lt;none&gt; 通过文件创建配置文件位置 12345678910[root@k8s-master storage]# pwd/root/k8s_practice/storage[root@k8s-master storage]# cat /root/k8s_practice/storage/configmap/game.propertiesenemies=alienslives=3enemies.cheat=trueenemies.cheat.level=noGoodRottensecret.code.passphrase=UUDDLRLRBABAssecret.code.allowed=truesecret.code.lives=30 创建ConfigMap并查看状态 123456[root@k8s-master storage]# kubectl create configmap game-config-2 --from-file=/root/k8s_practice/storage/configmap/game.propertiesconfigmap/game-config-2 created[root@k8s-master storage]# [root@k8s-master storage]# kubectl get configmap game-config-2NAME DATA AGEgame-config-2 1 29s 查看ConfigMap有哪些数据 1234567891011121314151617181920212223242526272829303132333435363738394041[root@k8s-master storage]# kubectl get configmap game-config-2 -o yaml ##### 查看方式1apiVersion: v1data: game.properties: |+ ##### 本段最后有一行空格，+ 表示保留字符串行末尾的换行 enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAs secret.code.allowed=true secret.code.lives=30kind: ConfigMapmetadata: creationTimestamp: &quot;2020-06-07T07:05:47Z&quot; name: game-config-2 namespace: default resourceVersion: &quot;890437&quot; selfLink: /api/v1/namespaces/default/configmaps/game-config-2 uid: 02d99802-c23f-45ad-b4e1-dea9bcb166d8[root@k8s-master storage]# [root@k8s-master storage]# kubectl describe configmap game-config-2 ##### 查看方式2Name: game-config-2Namespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Data====game.properties:----enemies=alienslives=3enemies.cheat=trueenemies.cheat.level=noGoodRottensecret.code.passphrase=UUDDLRLRBABAssecret.code.allowed=truesecret.code.lives=30Events: &lt;none&gt; 通过命令行创建创建ConfigMap并查看状态 12345678[root@k8s-master storage]# pwd/root/k8s_practice/storage[root@k8s-master storage]# kubectl create configmap special-config --from-literal=special.how=very --from-literal=&quot;special.type=charm&quot;configmap/special-config created[root@k8s-master storage]# [root@k8s-master storage]# kubectl get configmap special-configNAME DATA AGEspecial-config 2 23s 查看ConfigMap有哪些数据 1234567891011121314151617181920212223242526272829[root@k8s-master storage]# kubectl get configmap special-config -o yaml ##### 查看方式1apiVersion: v1data: special.how: very special.type: charmkind: ConfigMapmetadata: creationTimestamp: &quot;2020-06-07T09:32:04Z&quot; name: special-config namespace: default resourceVersion: &quot;912702&quot; selfLink: /api/v1/namespaces/default/configmaps/special-config uid: 76698e78-1380-4826-b5ac-d9c81f746eac[root@k8s-master storage]# [root@k8s-master storage]# kubectl describe configmap special-config ##### 查看方式2Name: special-configNamespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Data====special.how:----veryspecial.type:----charmEvents: &lt;none&gt; 通过yaml文件创建yaml文件 1234567891011121314151617181920[root@k8s-master storage]# pwd/root/k8s_practice/storage[root@k8s-master storage]# cat configmap.yaml apiVersion: v1kind: ConfigMapmetadata: name: configmap-demodata: # 类属性键；每一个键都映射到一个简单的值 player_initial_lives: &quot;3&quot; ui_properties_file_name: &apos;user-interface.properties&apos; # # 类文件键 game.properties: | enemy.types=aliens,monsters player.maximum-lives=5 user-interface.properties: | color.good=purple color.bad=yellow allow.textmode=true 创建ConfigMap并查看状态 12345[root@k8s-master storage]# kubectl apply -f configmap.yamlconfigmap/configmap-demo created[root@k8s-master storage]# kubectl get configmap configmap-demoNAME DATA AGEconfigmap-demo 4 2m59s 查看ConfigMap有哪些数据 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152[root@k8s-master storage]# kubectl get configmap configmap-demo -o yaml ##### 查看方式1apiVersion: v1data: game.properties: | enemy.types=aliens,monsters player.maximum-lives=5 player_initial_lives: &quot;3&quot; ui_properties_file_name: user-interface.properties user-interface.properties: | color.good=purple color.bad=yellow allow.textmode=truekind: ConfigMapmetadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;data&quot;:&#123;&quot;game.properties&quot;:&quot;enemy.types=aliens,monsters\nplayer.maximum-lives=5\n&quot;,&quot;player_initial_lives&quot;:&quot;3&quot;,&quot;ui_properties_file_name&quot;:&quot;user-interface.properties&quot;,&quot;user-interface.properties&quot;:&quot;color.good=purple\ncolor.bad=yellow\nallow.textmode=true\n&quot;&#125;,&quot;kind&quot;:&quot;ConfigMap&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;configmap-demo&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;&#125; creationTimestamp: &quot;2020-06-07T11:36:46Z&quot; name: configmap-demo namespace: default resourceVersion: &quot;931685&quot; selfLink: /api/v1/namespaces/default/configmaps/configmap-demo uid: fdad7000-87bd-4b72-be98-40dd8fe6400a[root@k8s-master storage]# [root@k8s-master storage]# [root@k8s-master storage]# kubectl describe configmap configmap-demo ##### 查看方式2Name: configmap-demoNamespace: defaultLabels: &lt;none&gt;Annotations: kubectl.kubernetes.io/last-applied-configuration: &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;data&quot;:&#123;&quot;game.properties&quot;:&quot;enemy.types=aliens,monsters\nplayer.maximum-lives=5\n&quot;,&quot;player_initial_lives&quot;:&quot;3&quot;,&quot;ui_proper...Data====game.properties:----enemy.types=aliens,monstersplayer.maximum-lives=5player_initial_lives:----3ui_properties_file_name:----user-interface.propertiesuser-interface.properties:----color.good=purplecolor.bad=yellowallow.textmode=trueEvents: &lt;none&gt; Pod中使用ConfigMap如何在Pod中使用上述的ConfigMap信息。 当前存在的ConfigMap123456[root@k8s-master storage]# kubectl get configmapNAME DATA AGEconfigmap-demo 4 30mgame-config 2 5h9mgame-config-2 1 5h1mspecial-config 2 5m48s 使用ConfigMap来替代环境变量yaml文件 1234567891011121314151617181920212223242526272829[root@k8s-master storage]# pwd/root/k8s_practice/storage[root@k8s-master storage]# cat pod_configmap_env.yaml apiVersion: v1kind: Podmetadata: name: pod-configmap-envspec: containers: - name: myapp image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env&quot;] ### 引用方式1 env: - name: SPECAIL_HOW_KEY valueFrom: configMapKeyRef: name: special-config ### 这个name的值来自 ConfigMap key: special.how ### 这个key的值为需要取值的键 - name: SPECAIL_TPYE_KEY valueFrom: configMapKeyRef: name: special-config key: special.type ### 引用方式2 envFrom: - configMapRef: name: game-config-2 ### 这个name的值来自 ConfigMap restartPolicy: Never 启动pod并查看状态 123456[root@k8s-master storage]# kubectl apply -f pod_configmap_env.yaml pod/pod-configmap-env created[root@k8s-master storage]# [root@k8s-master storage]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod-configmap-env 0/1 Completed 0 6s 10.244.2.147 k8s-node02 &lt;none&gt; &lt;none&gt; 查看打印日志 123456789101112131415161718192021222324252627282930313233[root@k8s-master storage]# kubectl logs pod-configmap-env MYAPP_SVC_PORT_80_TCP_ADDR=10.98.57.156KUBERNETES_SERVICE_PORT=443KUBERNETES_PORT=tcp://10.96.0.1:443MYAPP_SVC_PORT_80_TCP_PORT=80HOSTNAME=pod-configmap-envSHLVL=1MYAPP_SVC_PORT_80_TCP_PROTO=tcpHOME=/rootSPECAIL_HOW_KEY=very ### 来自ConfigMapgame.properties=enemies=aliens ### 来自ConfigMaplives=3 ### 来自ConfigMapenemies.cheat=true ### 来自ConfigMapenemies.cheat.level=noGoodRotten ### 来自ConfigMapsecret.code.passphrase=UUDDLRLRBABAs ### 来自ConfigMapsecret.code.allowed=true ### 来自ConfigMapsecret.code.lives=30 ### 来自ConfigMapSPECAIL_TPYE_KEY=charm ### 来自ConfigMapMYAPP_SVC_PORT_80_TCP=tcp://10.98.57.156:80NGINX_VERSION=1.12.2KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binKUBERNETES_PORT_443_TCP_PORT=443KUBERNETES_PORT_443_TCP_PROTO=tcpMYAPP_SVC_SERVICE_HOST=10.98.57.156KUBERNETES_SERVICE_PORT_HTTPS=443KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443PWD=/KUBERNETES_SERVICE_HOST=10.96.0.1MYAPP_SVC_SERVICE_PORT=80MYAPP_SVC_PORT=tcp://10.98.57.156:80 使用ConfigMap设置命令行参数yaml文件 123456789101112131415161718192021222324[root@k8s-master storage]# pwd/root/k8s_practice/storage[root@k8s-master storage]# cat pod_configmap_cmd.yaml apiVersion: v1kind: Podmetadata: name: pod-configmap-cmdspec: containers: - name: myapp image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo \&quot;===$(SPECAIL_HOW_KEY)===$(SPECAIL_TPYE_KEY)===\&quot;&quot;] env: - name: SPECAIL_HOW_KEY valueFrom: configMapKeyRef: name: special-config key: special.how - name: SPECAIL_TPYE_KEY valueFrom: configMapKeyRef: name: special-config key: special.type restartPolicy: Never 启动pod并查看状态 123456[root@k8s-master storage]# kubectl apply -f pod_configmap_cmd.yaml pod/pod-configmap-cmd created[root@k8s-master storage]# [root@k8s-master storage]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod-configmap-cmd 0/1 Completed 0 5s 10.244.4.125 k8s-node01 &lt;none&gt; &lt;none&gt; 查看打印日志 12[root@k8s-master storage]# kubectl logs pod-configmap-cmd===very===charm=== 通过数据卷插件使用ConfigMap【推荐】在数据卷里面使用ConfigMap，最基本的就是将文件填入数据卷，在这个文件中，键就是文件名【第一层级的键】，键值就是文件内容。 yaml文件 123456789101112131415161718192021[root@k8s-master storage]# pwd/root/k8s_practice/storage[root@k8s-master storage]# cat pod_configmap_volume.yaml apiVersion: v1kind: Podmetadata: name: pod-configmap-volumespec: containers: - name: myapp image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 #command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;ls -l /etc/config/&quot;] command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;sleep 600&quot;] volumeMounts: - name: config-volume mountPath: /etc/config volumes: - name: config-volume configMap: name: configmap-demo restartPolicy: Never 启动pod并查看状态 123456[root@k8s-master storage]# kubectl apply -f pod_configmap_volume.yaml pod/pod-configmap-volume created[root@k8s-master storage]#[root@k8s-master storage]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod-configmap-volume 1/1 Running 0 5s 10.244.2.153 k8s-node02 &lt;none&gt; &lt;none&gt; 进入pod并查看 1234567891011121314151617181920212223[root@k8s-master storage]# kubectl exec -it pod-configmap-volume sh/ # ls /etc/configgame.properties player_initial_lives ui_properties_file_name user-interface.properties/ # / # / # / # cat /etc/config/player_initial_lives 3/ # / # / # / # cat /etc/config/ui_properties_file_name user-interface.properties/ # / # / # / # cat /etc/config/game.properties enemy.types=aliens,monstersplayer.maximum-lives=5/ # / # / # cat /etc/config/user-interface.properties color.good=purplecolor.bad=yellowallow.textmode=true ConfigMap热更新准备工作yaml文件 123456789101112131415161718192021222324252627282930313233343536373839404142[root@k8s-master storage]# pwd/root/k8s_practice/storage[root@k8s-master storage]# cat pod_configmap_hot.yaml apiVersion: v1kind: ConfigMapmetadata: name: log-config namespace: defaultdata: log_level: INFO---apiVersion: apps/v1kind: Deploymentmetadata: name: myapp-deploy namespace: defaultspec: replicas: 2 selector: matchLabels: app: myapp release: v1 template: metadata: labels: app: myapp release: v1 env: test spec: containers: - name: myapp image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 imagePullPolicy: IfNotPresent ports: - containerPort: 80 volumeMounts: - name: config-volume mountPath: /etc/config volumes: - name: config-volume configMap: name: log-config 应用yaml文件并查看状态 123456789101112[root@k8s-master storage]# kubectl apply -f pod_configmap_hot.yaml configmap/log-config createddeployment.apps/myapp-deploy created[root@k8s-master storage]# [root@k8s-master storage]# kubectl get configmap log-configNAME DATA AGElog-config 1 21s[root@k8s-master storage]# [root@k8s-master storage]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmyapp-deploy-58ff9c997-drhwk 1/1 Running 0 30s 10.244.2.154 k8s-node02 &lt;none&gt; &lt;none&gt;myapp-deploy-58ff9c997-n68j2 1/1 Running 0 30s 10.244.4.126 k8s-node01 &lt;none&gt; &lt;none&gt; 查看ConfigMap信息 123456789101112131415[root@k8s-master storage]# kubectl get configmap log-config -o yamlapiVersion: v1data: log_level: INFOkind: ConfigMapmetadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;data&quot;:&#123;&quot;log_level&quot;:&quot;INFO&quot;&#125;,&quot;kind&quot;:&quot;ConfigMap&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;log-config&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;&#125; creationTimestamp: &quot;2020-06-07T16:08:11Z&quot; name: log-config namespace: default resourceVersion: &quot;971348&quot; selfLink: /api/v1/namespaces/default/configmaps/log-config uid: 7e78e1d7-12de-4601-9915-cefbc96ca305 查看pod中的ConfigMap信息 12[root@k8s-master storage]# kubectl exec -it myapp-deploy-58ff9c997-drhwk -- cat /etc/config/log_levelINFO 热更新修改ConfigMap 12345678910111213141516171819[root@k8s-master storage]# kubectl edit configmap log-config ### 将 INFO 改为了 DEBUG# Please edit the object below. Lines beginning with a &apos;#&apos; will be ignored,# and an empty file will abort the edit. If an error occurs while saving this file will be# reopened with the relevant failures.#apiVersion: v1data: log_level: DEBUGkind: ConfigMapmetadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;data&quot;:&#123;&quot;log_level&quot;:&quot;DEBUG&quot;&#125;,&quot;kind&quot;:&quot;ConfigMap&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;log-config&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;&#125; creationTimestamp: &quot;2020-06-07T16:08:11Z&quot; name: log-config namespace: default resourceVersion: &quot;971348&quot; selfLink: /api/v1/namespaces/default/configmaps/log-config uid: 7e78e1d7-12de-4601-9915-cefbc96ca305 查看ConfigMap信息 123456789101112131415[root@k8s-master storage]# kubectl get configmap log-config -o yamlapiVersion: v1data: log_level: DEBUGkind: ConfigMapmetadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;data&quot;:&#123;&quot;log_level&quot;:&quot;DEBUG&quot;&#125;,&quot;kind&quot;:&quot;ConfigMap&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;log-config&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;&#125; creationTimestamp: &quot;2020-06-07T16:08:11Z&quot; name: log-config namespace: default resourceVersion: &quot;972893&quot; selfLink: /api/v1/namespaces/default/configmaps/log-config uid: 7e78e1d7-12de-4601-9915-cefbc96ca305 稍后10秒左右，再次查看pod中的ConfigMap信息 12[root@k8s-master storage]# kubectl exec -it myapp-deploy-58ff9c997-drhwk -- cat /etc/config/log_levelDEBUG 由此可见，完成了一次热更新 相关阅读1、YAML 语言教程与使用案例 2、Kubernetes K8S之通过yaml创建pod与pod文件常用字段详解 3、Kubernetes K8S之存储Secret详解]]></content>
      <categories>
        <category>Kubernetes</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
        <tag>K8S</tag>
        <tag>CI/CD</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Linux下如何根据域名自签发OpenSSL证书与常用证书转换]]></title>
    <url>%2F2020%2F10%2F11%2Fopenssl%2F</url>
    <content type="text"><![CDATA[在Linux下如何根据域名自签发各种SSL证书，这里我们以Apache、Tomcat、Nginx为例。 openssl自签发泛域名（通配符）证书首先要有openssl工具，如果没有那么使用如下命令安装： 1yum install -y openssl openssl-devel 修改openssl.cnf配置文件具体修改如下 12345678910111213141516171819[root@docker02 ~]# vim /etc/pki/tls/openssl.cnf[ req ]………………# 将如下配置的注释放开req_extensions = v3_req # The extensions to add to a certificate request………………[ v3_req ]# Extensions to add to a certificate requestbasicConstraints = CA:FALSEkeyUsage = nonRepudiation, digitalSignature, keyEncipherment# 添加如下行subjectAltName = @SubjectAlternativeName# 同时增加如下信息[SubjectAlternativeName]DNS.1 = zhangbook.comDNS.2 = *.zhangbook.com 说明：本次我们以 *.zhangbook.com 泛域名为例。 创建根证书123456789101112131415[root@docker02 ssl]# pwd/root/software/ssl[root@docker02 ssl]# ## 创建CA私钥[root@docker02 ssl]# openssl genrsa -out CA.key 2048## 制作CA公钥[root@docker02 ssl]# openssl req -sha256 -new -x509 -days 36500 -key CA.key -out CA.crt -config /etc/pki/tls/openssl.cnf ………………Country Name (2 letter code) [XX]:CNState or Province Name (full name) []:BJLocality Name (eg, city) [Default City]:BeiJingOrganization Name (eg, company) [Default Company Ltd]:BTCOrganizational Unit Name (eg, section) []:MOSTCommon Name (eg, your name or your server&apos;s hostname) []:Light Zhang # 这里就是证书上的：颁发者Email Address []:ca@test.com 当然上述的公钥制作方式需要交互式输入信息，如果不想频繁输入，那么可以使用如下命令： 12## 免交互式制作CA公钥openssl req -sha256 -new -x509 -days 36500 -key CA.key -out CA.crt -config /etc/pki/tls/openssl.cnf -subj &quot;/C=CN/ST=BJ/L=BeiJing/O=BTC/OU=MOST/CN=Light Zhang/emailAddress=ca@test.com&quot; subj内容详解： 1234567C = Country Name (2 letter code)ST = State or Province Name (full name)L = Locality Name (eg, city) [Default City]O = Organization Name (eg, company) [Default Company Ltd]OU = Organizational Unit Name (eg, section)CN = Common Name (eg, your name or your server&apos;s hostname)emailAddress = Email Address 此时的的文件有： 1234[root@docker02 ssl]# lltotal 32-rw-r--r-- 1 root root 1387 Oct 2 10:25 CA.crt-rw-r--r-- 1 root root 1679 Oct 2 10:04 CA.key 自签发泛域名证书操作步骤为： 生成域名私钥 生成证书签发请求文件 使用自签署的CA，生成域名公钥 具体如下： 12345678910111213141516171819202122### 当前目录 /root/software/ssl# 生成 zhangbook.com.key 密钥openssl genrsa -out zhangbook.com.key 2048# 生成 zhangbook.com.csr 证书签发请求 交互式openssl req -new -sha256 -key zhangbook.com.key -out zhangbook.com.csr -config /etc/pki/tls/openssl.cnf………………##### 产生的交互式内容与填写如下Country Name (2 letter code) [XX]:CNState or Province Name (full name) []:BJLocality Name (eg, city) [Default City]:BeiJingOrganization Name (eg, company) [Default Company Ltd]:BTCOrganizational Unit Name (eg, section) []:MOSTCommon Name (eg, your name or your server&apos;s hostname) []:*.zhangbook.com # 这里就是证书上的：颁发给Email Address []:ca@test.comPlease enter the following &apos;extra&apos; attributesto be sent with your certificate requestA challenge password []:123456An optional company name []:BTC………………# 生成 zhangbook.com.csr 证书签发请求 非交互式openssl req -new -sha256 -key zhangbook.com.key -out zhangbook.com.csr -config /etc/pki/tls/openssl.cnf -subj &quot;/C=CN/ST=BJ/L=BeiJing/O=BTC/OU=MOST/CN=*.zhangbook.com/emailAddress=ca@test.com&quot; PS1：上面的Common Name 就是在这步填写 *.zhangbook.com ，表示的就是该证书支持泛域名，common name一定要在SubjectAlternativeName中包含 PS2：进行CA签名获取证书时，需要注意国家、省、单位需要与CA证书相同，否则会报异常 查看签名请求文件信息 1openssl req -in zhangbook.com.csr -text 使用自签署的CA，签署zhangbook.com.crt 1openssl ca -in zhangbook.com.csr -md sha256 -days 36500 -out zhangbook.com.crt -cert CA.crt -keyfile CA.key -extensions v3_req -config /etc/pki/tls/openssl.cnf 这里证书有效时间为100年。 PS1：即便是你前面是sha256的根证书和sha256的请求文件，如果这里不加 -md sha256，那么默认是按照sha1进行签名的 PS2：在执行时，可能出现如下错误 异常问题1： 12345Using configuration from /etc/pki/tls/openssl.cnf/etc/pki/CA/index.txt: No such file or directoryunable to open &apos;/etc/pki/CA/index.txt&apos;140652962035600:error:02001002:system library:fopen:No such file or directory:bss_file.c:402:fopen(&apos;/etc/pki/CA/index.txt&apos;,&apos;r&apos;)140652962035600:error:20074002:BIO routines:FILE_CTRL:system lib:bss_file.c:404: 处理：这时我们创建该文件即可 1touch /etc/pki/CA/index.txt 异常问题2： 然后我们继续使用 【自签署的CA，签署zhangbook.com.crt】；结果又出现新问题 12345Using configuration from /etc/pki/tls/openssl.cnf/etc/pki/CA/serial: No such file or directoryerror while loading serial number140087163742096:error:02001002:system library:fopen:No such file or directory:bss_file.c:402:fopen(&apos;/etc/pki/CA/serial&apos;,&apos;r&apos;)140087163742096:error:20074002:BIO routines:FILE_CTRL:system lib:bss_file.c:404: 处理：使用如下命令即可。表示：用来跟踪最后一次颁发证书的序列号。 1echo &quot;01&quot; &gt; /etc/pki/CA/serial 之后我们再次执行 【自签署的CA，签署zhangbook.com.crt 】 就正常了。详情如下： 123456789101112131415161718192021222324252627282930[root@docker02 ssl]# openssl ca -in zhangbook.com.csr -md sha256 -days 36500 -out zhangbook.com.crt -cert CA.crt -keyfile CA.key -extensions v3_req -config /etc/pki/tls/openssl.cnfUsing configuration from /etc/pki/tls/openssl.cnfCheck that the request matches the signatureSignature okCertificate Details: Serial Number: 1 (0x1) Validity Not Before: Oct 2 03:42:39 2020 GMT Not After : Sep 8 03:42:39 2120 GMT Subject: countryName = CN stateOrProvinceName = BJ organizationName = BTC organizationalUnitName = MOST commonName = *.zhangbook.com emailAddress = ca@test.com X509v3 extensions: X509v3 Basic Constraints: CA:FALSE X509v3 Key Usage: Digital Signature, Non Repudiation, Key Encipherment X509v3 Subject Alternative Name: DNS:zhangbook.com, DNS:*.zhangbook.comCertificate is to be certified until Sep 8 03:42:39 2120 GMT (36500 days)Sign the certificate? [y/n]:y &lt;== 需要输入的1 out of 1 certificate requests certified, commit? [y/n]y &lt;== 需要输入的Write out database with 1 new entriesData Base Updated 说明：此时我们再看，/etc/pki/CA/index.txt 和 /etc/pki/CA/serial 文件信息。如下： 12345[root@docker02 ~]# cat /etc/pki/CA/index.txtV 21200908034239Z 01 unknown /C=CN/ST=BJ/O=BTC/OU=MOST/CN=*.zhangbook.com/emailAddress=ca@test.com[root@docker02 ~]# [root@docker02 ~]# cat /etc/pki/CA/serial02 由上可知：域名签署信息已经保存到index.txt文件；并且证书序列serial文件已经更新【从01变为了02】。 PS： 同一个域名不能签署多次；由于签署了*.zhangbook.com，且已经被记录，因此不能再次被签署。除非删除该记录。 注意index.txt文件和serial文件的关系。serial文件内容为index.txt文件内容行数加1。 查看证书信息 1openssl x509 -in zhangbook.com.crt -text 验证签发证书是否有效 12[root@docker02 ssl]# openssl verify -CAfile CA.crt zhangbook.com.crtzhangbook.com.crt: OK 此时的文件有： 1234567[root@docker02 ssl]# lltotal 32-rw-r--r-- 1 root root 1387 Oct 2 10:25 CA.crt-rw-r--r-- 1 root root 1679 Oct 2 10:04 CA.key-rw-r--r-- 1 root root 4364 Oct 2 11:42 zhangbook.com.crt-rw-r--r-- 1 root root 1151 Oct 2 10:48 zhangbook.com.csr-rw-r--r-- 1 root root 1679 Oct 2 10:44 zhangbook.com.key 此时我们已经完成自签发证书。 证书格式转换实际工作和生产环境中，可能需要各种各样的证书格式。下面我们将证书转换为常用的其他证书格式。 将crt转pem格式命令如下： 1openssl x509 -in zhangbook.com.crt -out zhangbook.com.pem -outform PEM 生成 p12 格式的证书利用生成的CA根证书和服务证书的crt 和 key 文件生成 p12 文件 1openssl pkcs12 -export -in zhangbook.com.crt -inkey zhangbook.com.key -passin pass:CS2i1QkR -name *.zhangbook.com -chain -CAfile CA.crt -password pass:CS2i1QkR -caname *.zhangbook.com -out zhangbook.com.p12 PS：p12证书的password为CS2i1QkR 查看p12证书信息【keytool命令依赖于Java，因此需要先安装Java】 1234567[root@docker02 ssl]# keytool -rfc -list -keystore zhangbook.com.p12 -storetype pkcs12 Enter keystore password: &lt;== 输入：CS2i1QkRKeystore type: PKCS12Keystore provider: SunJSSEYour keystore contains 1 entry……………… 转换 p12 证书为 jks 证书文件使用jdk keytool工具进而生成tomcat/jboss端使用的证书文件【需要安装 Java】。 具体如下： 12345678910[root@docker02 ssl]# keytool -importkeystore -srckeystore zhangbook.com.p12 -srcstoretype PKCS12 -deststoretype JKS -destkeystore zhangbook.com.jksImporting keystore zhangbook.com.p12 to zhangbook.com.jks...Enter destination keystore password: &lt;== 输入 jks 证书的密码，如：CS2i1QkRRe-enter new password: &lt;== 重复输入 jks 证书的密码，如：CS2i1QkREnter source keystore password: &lt;== 输入 p12 证书的密码，这里是：CS2i1QkREntry for alias *.zhangbook.com successfully imported.Import command completed: 1 entries successfully imported, 0 entries failed or cancelledWarning:The JKS keystore uses a proprietary format. It is recommended to migrate to PKCS12 which is an industry standard format using &quot;keytool -importkeystore -srckeystore zhangbook.com.jks -destkeystore zhangbook.com.jks -deststoretype pkcs12&quot;. PS：p12证书和jks证书的密码相同，防止出现各种异常情况。 利用 jks 证书生成 cer 证书具体如下 1keytool -export -alias *.zhangbook.com -keystore zhangbook.com.jks -storepass CS2i1QkR -file zhangbook.com.cer -storepass CS2i1QkR 为jks证书密码 利用 cer 证书文件生成 jdk 所使用的文件具体如下 1keytool -import -alias *.zhangbook.com -keystore cacerts -file zhangbook.com.cer 目前存在文件说明123456789101112131415[root@docker02 ssl]# pwd/root/software/ssl[root@docker02 ssl]# [root@docker02 ssl]# lltotal 52-rw-r--r-- 1 root root 1018 Oct 2 14:24 cacerts ## jdk 所使用的文件-rw-r--r-- 1 root root 1387 Oct 2 10:25 CA.crt ## CA公钥-rw-r--r-- 1 root root 1679 Oct 2 10:04 CA.key ## CA私钥-rw-r--r-- 1 root root 946 Oct 2 14:21 zhangbook.com.cer ## cer证书-rw-r--r-- 1 root root 4364 Oct 2 11:42 zhangbook.com.crt ## zhangbook.com域名 CA签发公钥-rw-r--r-- 1 root root 1151 Oct 2 10:48 zhangbook.com.csr ## zhangbook.com域名 证书签发请求-rw-r--r-- 1 root root 3303 Oct 2 14:13 zhangbook.com.jks ## jks证书-rw-r--r-- 1 root root 1679 Oct 2 10:44 zhangbook.com.key ## zhangbook.com域名 私钥-rw-r--r-- 1 root root 3716 Oct 2 14:02 zhangbook.com.p12 ## p12格式证书-rw-r--r-- 1 root root 1338 Oct 2 13:56 zhangbook.com.pem ## zhangbook.com域名 PEM文件 SSL证书使用修改本地Windows的hosts文件，用于域名解析 123文件位置：C:\WINDOWS\System32\drivers\etc\hosts 追加如下信息# zhangbook.com172.16.1.32 www.zhangbook.com blog.zhangbook.com auth.zhangbook.com 其中172.16.1.32为测试使用的Linux机器，后面会部署WEB服务。由于自签发的是泛域名证书，因此可以有多个二级域名。 后面访问的时候，既可以使用域名访问，也可以使用IP访问。【推荐】使用域名访问。 Apache服务的SSL证书使用1、将Apache httpd用到的证书拷贝到指定目录 1234[root@docker02 ssl]# pwd/root/software/ssl[root@docker02 ssl]# [root@docker02 ssl]# cp -a zhangbook.com.crt zhangbook.com.key zhangbook.com.pem /etc/pki/tls/certs 2、在Linux机器安装httpd服务并添加ssl插件 12yum install -y httpdyum install -y mod_ssl openssl # 执行后，会增加 /etc/httpd/conf.d/ssl.conf 文件 3、在httpd添加SSL配置 123456789101112131415[root@docker02 conf.d]# pwd/etc/httpd/conf.d[root@docker02 conf.d]# [root@docker02 conf.d]# vim ssl.conf&lt;VirtualHost _default_:443&gt;# General setup for the virtual host, inherited from global configuration#DocumentRoot &quot;/var/www/html&quot;………………# 修改如下3行SSLCertificateFile /etc/pki/tls/certs/zhangbook.com.crtSSLCertificateKeyFile /etc/pki/tls/certs/zhangbook.com.key# 如下行可以注释掉，也可以取消注释#SSLCertificateChainFile /etc/pki/tls/certs/zhangbook.com.pem………………&lt;/VirtualHost&gt; 4、向VirtualHost的默认目录添加文件 1echo &quot;Apache web&quot; &gt; /var/www/html/index.html 5、启动httpd服务 1systemctl start httpd 6、浏览器访问 1234https://172.16.1.32/https://www.zhangbook.com/https://blog.zhangbook.com https://auth.zhangbook.com 7、验证完毕，停止httpd服务 1systemctl stop httpd Nginx服务的SSL证书使用1、在Linux机器安装nginx服务 1yum install -y nginx 通过 nginx -V 可见，--with-http_ssl_module 已安装。 2、将nginx用到的证书拷贝到指定目录 1234[root@docker02 ssl]# pwd/root/software/ssl[root@docker02 ssl]# [root@docker02 ssl]# cp -a zhangbook.com.key zhangbook.com.crt /etc/nginx/cert 3、在nginx添加SSL配置 12345678910111213141516171819202122232425262728293031323334[root@docker02 nginx]# pwd/etc/nginx[root@docker02 nginx]# [root@docker02 nginx]# vim nginx.conf……………… server &#123; listen 80; listen [::]:80; server_name www.zhangbook.com blog.zhangbook.com auth.zhangbook.com; return 301 https://$server_name$request_uri; &#125;# Settings for a TLS enabled server. server &#123; listen 443 ssl http2 default_server; listen [::]:443 ssl http2 default_server; server_name www.zhangbook.com blog.zhangbook.com auth.zhangbook.com; root /usr/share/nginx/html; ssl_certificate &quot;/etc/nginx/cert/zhangbook.com.crt&quot;; ssl_certificate_key &quot;/etc/nginx/cert/zhangbook.com.key&quot;; ssl_session_cache shared:SSL:1m; ssl_session_timeout 10m; ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; # Load configuration files for the default server block. include /etc/nginx/default.d/*.conf; location / &#123; index index.html index.htm; &#125; &#125;……………… 4、向WEB站点添加html文件 1echo &quot;Nginx web&quot; &gt; /usr/share/nginx/html/index.html 5、启动nginx服务 1systemctl start nginx.service 6、浏览器访问 123https://www.zhangbook.com/https://blog.zhangbook.com https://auth.zhangbook.com 7、验证完毕，停止nginx服务 1systemctl stop nginx Tomcat服务的SSL证书使用1、下载Tomcat。 1234567891011121314151617[root@docker02 App]# pwd/root/App[root@docker02 App]# [root@docker02 App]# wget https://mirrors.bfsu.edu.cn/apache/tomcat/tomcat-8/v8.5.58/bin/apache-tomcat-8.5.58.tar.gz[root@docker02 App]# [root@docker02 App]# tar xf apache-tomcat-8.5.58.tar.gz### 查看Java版本信息[root@docker02 App]# java -versionjava version &quot;1.8.0_231&quot;Java(TM) SE Runtime Environment (build 1.8.0_231-b11)Java HotSpot(TM) 64-Bit Server VM (build 25.231-b11, mixed mode)### 查看Tomcat版本信息[root@docker02 bin]# pwd/root/App/apache-tomcat-8.5.58/bin[root@docker02 bin]# [root@docker02 bin]# ./version.sh……………… 2、将Tomcat用到的证书拷贝到指定目录 1234[root@docker02 ssl]# pwd/root/software/ssl[root@docker02 ssl]# [root@docker02 ssl]# cp -a zhangbook.com.jks /root/App/apache-tomcat-8.5.58/conf/cert/ 3、在Tomcat添加SSL配置 12345678910111213141516[root@docker02 conf]# pwd/root/App/apache-tomcat-8.5.58/conf[root@docker02 conf]# [root@docker02 conf]# vim server.xml……………… &lt;Connector port=&quot;80&quot; protocol=&quot;HTTP/1.1&quot; connectionTimeout=&quot;20000&quot; redirectPort=&quot;443&quot; /&gt;………………### 其中Connector标签中的子标签 SSLHostConfig 已去掉 &lt;Connector port=&quot;443&quot; protocol=&quot;org.apache.coyote.http11.Http11NioProtocol&quot; maxThreads=&quot;600&quot; SSLEnabled=&quot;true&quot; clientAuth=&quot;false&quot; keystoreFile=&quot;/root/App/apache-tomcat-8.5.58/conf/cert/zhangbook.com.jks&quot; keystorePass=&quot;CS2i1QkR&quot; keystoreType=&quot;JKS&quot; scheme=&quot;https&quot; secure=&quot;true&quot; sslProtocol=&quot;TLS&quot; compression=&quot;on&quot; acceptorThreadCount=&quot;2&quot; connectionTimeout=&quot;20000&quot;&gt; &lt;/Connector&gt;……………… 4、向WEB站点添加html文件 1234[root@docker02 ROOT]# pwd/root/App/apache-tomcat-8.5.58/webapps/ROOT[root@docker02 ROOT]# [root@docker02 ROOT]# echo &apos;Tomcat web&apos; &gt; index.html PS：原ROOT目录下的文件已移走。 5、启动Tomcat服务 1234[root@docker02 bin]# pwd/root/App/apache-tomcat-8.5.58/bin[root@docker02 bin]# [root@docker02 bin]# sh startup.sh 6、浏览器访问 1234https://172.16.1.32/https://www.zhangbook.com/https://blog.zhangbook.com https://auth.zhangbook.com 7、验证完毕，停止Tomcat服务 1234[root@docker02 bin]# pwd/root/App/apache-tomcat-8.5.58/bin[root@docker02 bin]# [root@docker02 bin]# sh shutdown.sh 相关阅读1、openssl生成自签名泛域名（通配符）证书]]></content>
      <categories>
        <category>linux</category>
        <category>openssl</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>openssl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux系统如何在离线环境或内网环境安装部署Docker服务和其他服务]]></title>
    <url>%2F2020%2F10%2F09%2Flinux-yum%2F</url>
    <content type="text"><![CDATA[如何在离线环境或纯内网环境的Linux机器上安装部署Docker服务或其他服务。本次我们以Docker服务和Ansible服务为例。 获取指定服务的所有rpm包保证要获取rpm包的机器能够上网。 本次我们以Docker服务和Ansible服务为例。 修改配置实现：yum安装后保留rpm包在linux上，使用yum安装，默认安装完成之后会删除下载的rpm包；想要yum安装软件后，还保留安装包，那么需要修改 /etc/yum.conf 配置文件中的keepcache参数。 123456789101112131415[root@docker02 ~]# vim /etc/yum.conf [main]# 安装包保存位置cachedir=/var/cache/yum/$basearch/$releasever # 默认0，是不保存安装包；改为1，保留安装包keepcache=1debuglevel=2logfile=/var/log/yum.logexactarch=1obsoletes=1gpgcheck=1plugins=1installonly_limit=5bugtracker_url=http://bugs.centos.org/set_project.php?project_id=23&amp;ref=http://bugs.centos.org/bug_report_page.php?category=yumdistroverpkg=centos-release 此时我们进入安装包保存位置：/var/cache/yum/，可见是没有任何rpm包的 123[root@docker02 yum]# pwd/var/cache/yum[root@docker02 yum]# find . -type f | grep &apos;rpm&apos; 获取安装Docker服务所需的rpm包安装docker服务步骤如下： 12345678### 安装必要依赖yum install -y yum-utils device-mapper-persistent-data lvm2### 添加软件源信息yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo### 安装最新版本的docker服务yum -y install docker-ce### 查看docker版本信息docker -v 此时我们在 /var/cache/yum/目录通过命令过滤，可见安装docker服务的rpm包，并将这些包移到指定的 /root/software/docker_rpm目录中。 12345[root@docker02 yum]# pwd/var/cache/yum[root@docker02 yum]# find . -type f | grep &apos;rpm&apos;[root@docker02 yum]# [root@docker02 yum]# mv $(find . -type f | grep &apos;rpm&apos;) /root/software/docker_rpm 这时在 /root/software/docker_rpm目录中的包就是我们安装docker服务所需的rpm包。我们可以打包下载，然后上传到离线或内网环境的Linux机器，之后进行离线Docker服务安装部署。 获取安装Ansible服务所需的rpm包安装ansible服务步骤如下： 123yum install -y ansible### 查看ansible版本信息ansible --version 此时我们在 /var/cache/yum/目录通过命令过滤，可见安装ansible服务的rpm包，并将这些包移到指定的 /root/software/ansible_rpm目录中。 12345[root@docker02 yum]# pwd/var/cache/yum[root@docker02 yum]# find . -type f | grep &apos;rpm&apos;[root@docker02 yum]# [root@docker02 yum]# mv $(find . -type f | grep &apos;rpm&apos;) /root/software/ansible_rpm/ 这时在 /root/software/ansible_rpm目录中的包就是我们安装ansible服务所需的rpm包。我们可以打包下载，然后上传到离线或内网环境的Linux机器，之后进行离线Ansible服务安装部署。 离线或内网环境部署指定服务将上面Docker服务的rpm安装包和Ansible服务的rpm安装包，上传到离线或内网环境的Linux机器。 内网安装Docker服务安装步骤如下： 1234567891011121314151617181920212223242526272829[root@docker02 docker_rpm]# pwd/root/service_install/docker_rpm[root@docker02 docker_rpm]# [root@docker02 docker_rpm]# ll total 100904-rw-r--r-- 1 root root 78256 Aug 23 2019 audit-libs-python-2.8.5-4.el7.x86_64.rpm-rw-r--r-- 1 root root 302068 Nov 12 2018 checkpolicy-2.5-8.el7.x86_64.rpm-rw-r--r-- 1 root root 30374084 Sep 18 01:54 containerd.io-1.3.7-3.1.el7.x86_64.rpm-rw-r--r-- 1 root root 40816 Jul 6 22:33 container-selinux-2.119.2-1.911c772.el7_8.noarch.rpm-rw-r--r-- 1 root root 302564 May 14 01:09 device-mapper-1.02.164-7.el7_8.2.x86_64.rpm-rw-r--r-- 1 root root 195448 May 14 01:11 device-mapper-event-1.02.164-7.el7_8.2.x86_64.rpm-rw-r--r-- 1 root root 195004 May 14 01:12 device-mapper-event-libs-1.02.164-7.el7_8.2.x86_64.rpm-rw-r--r-- 1 root root 331908 May 14 01:13 device-mapper-libs-1.02.164-7.el7_8.2.x86_64.rpm-rw-r--r-- 1 root root 432624 Apr 4 04:50 device-mapper-persistent-data-0.8.5-2.el7.x86_64.rpm-rw-r--r-- 1 root root 25268380 Sep 18 03:06 docker-ce-19.03.13-3.el7.x86_64.rpm-rw-r--r-- 1 root root 40247476 Sep 18 03:06 docker-ce-cli-19.03.13-3.el7.x86_64.rpm-rw-r--r-- 1 root root 24744 Nov 25 2015 libaio-0.3.109-13.el7.x86_64.rpm-rw-r--r-- 1 root root 67720 Aug 23 2019 libcgroup-0.41-21.el7.x86_64.rpm-rw-r--r-- 1 root root 57460 Apr 4 04:59 libseccomp-2.3.1-4.el7.x86_64.rpm-rw-r--r-- 1 root root 115284 Nov 12 2018 libsemanage-python-2.5-14.el7.x86_64.rpm-rw-r--r-- 1 root root 1384208 May 14 01:19 lvm2-2.02.186-7.el7_8.2.x86_64.rpm-rw-r--r-- 1 root root 1143916 May 14 01:19 lvm2-libs-2.02.186-7.el7_8.2.x86_64.rpm-rw-r--r-- 1 root root 938736 Apr 4 05:05 policycoreutils-2.5-34.el7.x86_64.rpm-rw-r--r-- 1 root root 468316 Apr 4 05:05 policycoreutils-python-2.5-34.el7.x86_64.rpm-rw-r--r-- 1 root root 232448 Aug 23 2019 python-chardet-2.2.1-3.el7.noarch.rpm-rw-r--r-- 1 root root 32880 Jul 4 2014 python-IPy-0.75-6.el7.noarch.rpm-rw-r--r-- 1 root root 273012 Jul 4 2014 python-kitchen-1.1.1-5.el7.noarch.rpm-rw-r--r-- 1 root root 635184 Nov 12 2018 setools-libs-3.3.8-4.el7.x86_64.rpm-rw-r--r-- 1 root root 124852 May 14 03:58 yum-utils-1.1.31-54.el7_8.noarch.rpm 由上可见rpm包比较多，由于包之间会存在相互依赖，我们不可能手动安装rpm包。因此我们使用yum安装，并且安装时会自动处理rpm包相互依赖的问题，具体如下： 1234567[root@docker02 docker_rpm]# pwd/root/service_install/docker_rpm[root@docker02 docker_rpm]# [root@docker02 docker_rpm]# yum install -y *.rpm[root@docker02 docker_rpm]# [root@docker02 docker_rpm]# docker -vDocker version 19.03.13, build 4484c46d9d 如此，docker服务安装完毕！ 内网安装Ansible服务安装步骤如下： 1234567891011121314151617181920212223242526[root@docker02 ansible_rpm]# pwd/root/service_install/ansible_rpm[root@docker02 ansible_rpm]# [root@docker02 ansible_rpm]# ll -htotal 22M-rw-r--r-- 1 root root 18M Jun 19 13:08 ansible-2.9.10-1.el7.noarch.rpm-rw-r--r-- 1 root root 55K Jan 30 2015 libyaml-0.1.4-11.el7_0.x86_64.rpm-rw-r--r-- 1 root root 503K Apr 25 2018 python2-cryptography-1.7.2-2.el7.x86_64.rpm-rw-r--r-- 1 root root 126K Jun 20 05:14 python2-httplib2-0.18.1-3.el7.noarch.rpm-rw-r--r-- 1 root root 42K Apr 23 23:36 python2-jmespath-0.9.4-2.el7.noarch.rpm-rw-r--r-- 1 root root 100K Nov 21 2016 python2-pyasn1-0.1.9-7.el7.noarch.rpm-rw-r--r-- 1 root root 1.4M Jul 4 2014 python-babel-0.9.6-8.el7.noarch.rpm-rw-r--r-- 1 root root 5.8K Mar 14 2015 python-backports-1.0-8.el7.x86_64.rpm-rw-r--r-- 1 root root 13K Apr 25 2018 python-backports-ssl_match_hostname-3.5.0.1-1.el7.noarch.rpm-rw-r--r-- 1 root root 218K Nov 21 2016 python-cffi-1.6.0-5.el7.x86_64.rpm-rw-r--r-- 1 root root 53K Nov 25 2015 python-enum34-1.0.4-1.el7.noarch.rpm-rw-r--r-- 1 root root 94K Aug 11 2017 python-idna-2.4-1.el7.noarch.rpm-rw-r--r-- 1 root root 35K Nov 21 2016 python-ipaddress-1.0.16-2.el7.noarch.rpm-rw-r--r-- 1 root root 519K Aug 23 2019 python-jinja2-2.7.2-4.el7.noarch.rpm-rw-r--r-- 1 root root 26K Jul 4 2014 python-markupsafe-0.11-10.el7.x86_64.rpm-rw-r--r-- 1 root root 269K Nov 21 2018 python-paramiko-2.1.1-9.el7.noarch.rpm-rw-r--r-- 1 root root 123K Aug 11 2017 python-ply-3.4-11.el7.noarch.rpm-rw-r--r-- 1 root root 105K Nov 25 2015 python-pycparser-2.14-1.el7.noarch.rpm-rw-r--r-- 1 root root 397K Aug 11 2017 python-setuptools-0.9.8-7.el7.noarch.rpm-rw-r--r-- 1 root root 154K Jul 4 2014 PyYAML-3.10-11.el7.x86_64.rpm-rw-r--r-- 1 root root 22K Sep 8 2017 sshpass-1.06-2.el7.x86_64.rpm 由上可见rpm包比较多，由于包之间会存在相互依赖，我们不可能手动安装rpm包。因此我们使用yum安装，并且安装时会自动处理rpm包相互依赖的问题，具体如下： 123456789101112[root@docker02 ansible_rpm]# pwd/root/service_install/ansible_rpm[root@docker02 ansible_rpm]# [root@docker02 ansible_rpm]# yum install -y *.rpm[root@docker02 ansible_rpm]# [root@docker02 ansible_rpm]# ansible --versionansible 2.9.10 config file = /etc/ansible/ansible.cfg configured module search path = [u&apos;/root/.ansible/plugins/modules&apos;, u&apos;/usr/share/ansible/plugins/modules&apos;] ansible python module location = /usr/lib/python2.7/site-packages/ansible executable location = /bin/ansible python version = 2.7.5 (default, Aug 7 2019, 00:51:29) [GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] 如此，ansible服务安装完毕！ 相关阅读1、安装指定版本的docker服务]]></content>
      <categories>
        <category>Linux</category>
        <category>Yum</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Yum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes K8S之存储Secret详解]]></title>
    <url>%2F2020%2F09%2F28%2Fkubernetes17%2F</url>
    <content type="text"><![CDATA[K8S之存储Secret概述与类型说明，并详解常用Secret示例 主机配置规划 服务器名称(hostname) 系统版本 配置 内网IP 外网IP(模拟) k8s-master CentOS7.7 2C/4G/20G 172.16.1.110 10.0.0.110 k8s-node01 CentOS7.7 2C/4G/20G 172.16.1.111 10.0.0.111 k8s-node02 CentOS7.7 2C/4G/20G 172.16.1.112 10.0.0.112 Secret概述Secret解决了密码、token、秘钥等敏感数据的配置问题，而不需要把这些敏感数据暴露到镜像或者Pod Spec中。Secret可以以Volume或者环境变量的方式使用。 用户可以创建 secret，同时系统也创建了一些 secret。 要使用 secret，pod 需要引用 secret。Pod 可以用两种方式使用 secret：作为 volume 中的文件被挂载到 pod 中的一个或者多个容器里，或者当 kubelet 为 pod 拉取镜像时使用。 Secret类型 Service Account：用来访问Kubernetes API，由Kubernetes自动创建，并且会自动挂载到Pod的 /run/secrets/kubernetes.io/serviceaccount 目录中。 Opaque：base64编码格式的Secret，用来存储密码、秘钥等。 kubernetes.io/dockerconfigjson：用来存储私有docker registry的认证信息。 Service Account通过kube-proxy查看 1234567891011[root@k8s-master ~]# kubectl get pod -A | grep &apos;kube-proxy&apos; kube-system kube-proxy-6bfh7 1/1 Running 12 7d3hkube-system kube-proxy-6vfkf 1/1 Running 11 7d3hkube-system kube-proxy-bvl9n 1/1 Running 11 7d3h[root@k8s-master ~]# [root@k8s-master ~]# kubectl exec -it -n kube-system kube-proxy-6bfh7 -- /bin/sh # ls -l /run/secrets/kubernetes.io/serviceaccounttotal 0lrwxrwxrwx 1 root root 13 Jun 8 13:39 ca.crt -&gt; ..data/ca.crtlrwxrwxrwx 1 root root 16 Jun 8 13:39 namespace -&gt; ..data/namespacelrwxrwxrwx 1 root root 12 Jun 8 13:39 token -&gt; ..data/token Opaque Secret创建secret手动加密，基于base64加密 1234[root@k8s-master ~]# echo -n &apos;admin&apos; | base64YWRtaW4=[root@k8s-master ~]# echo -n &apos;1f2d1e2e67df&apos; | base64MWYyZDFlMmU2N2Rm yaml文件 1234567891011[root@k8s-master secret]# pwd/root/k8s_practice/secret[root@k8s-master secret]# cat secret.yaml apiVersion: v1kind: Secretmetadata: name: mysecrettype: Opaquedata: username: YWRtaW4= password: MWYyZDFlMmU2N2Rm 或者通过如下命令行创建【secret名称故意设置不一样，以方便查看对比】，生成secret后会自动加密，而非明文存储。 1kubectl create secret generic db-user-pass --from-literal=username=admin --from-literal=password=1f2d1e2e67df 生成secret，并查看状态 123456789101112131415161718192021222324252627282930313233343536373839[root@k8s-master secret]# kubectl apply -f secret.yaml secret/mysecret created[root@k8s-master secret]# [root@k8s-master secret]# kubectl get secret ### 查看默认名称空间的secret简要信息NAME TYPE DATA AGEbasic-auth Opaque 1 2d12hdefault-token-v48g4 kubernetes.io/service-account-token 3 27dmysecret Opaque 2 23s ### 可见已创建tls-secret kubernetes.io/tls 2 3d2h[root@k8s-master secret]# [root@k8s-master secret]# kubectl get secret mysecret -o yaml ### 查看mysecret详细信息apiVersion: v1data: password: MWYyZDFlMmU2N2Rm username: YWRtaW4=kind: Secretmetadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;data&quot;:&#123;&quot;password&quot;:&quot;MWYyZDFlMmU2N2Rm&quot;,&quot;username&quot;:&quot;YWRtaW4=&quot;&#125;,&quot;kind&quot;:&quot;Secret&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;mysecret&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;,&quot;type&quot;:&quot;Opaque&quot;&#125; creationTimestamp: &quot;2020-06-08T14:08:59Z&quot; name: mysecret namespace: default resourceVersion: &quot;987419&quot; selfLink: /api/v1/namespaces/default/secrets/mysecret uid: 27b58929-71c4-495b-99a5-0d411910a529type: Opaque[root@k8s-master secret]# [root@k8s-master secret]# kubectl describe secret mysecret ### 查看描述信息Name: mysecretNamespace: defaultLabels: &lt;none&gt;Annotations: Type: OpaqueData====password: 12 bytesusername: 5 bytes 将Secret挂载到Volume中yaml文件 12345678910111213141516171819[root@k8s-master secret]# pwd/root/k8s_practice/secret[root@k8s-master secret]# cat pod_secret_volume.yaml apiVersion: v1kind: Podmetadata: name: pod-secret-volumespec: containers: - name: myapp image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 volumeMounts: - name: secret-volume mountPath: /etc/secret readOnly: true volumes: - name: secret-volume secret: secretName: mysecret 启动pod并查看状态 123456[root@k8s-master secret]# kubectl apply -f pod_secret_volume.yaml pod/pod-secret-volume created[root@k8s-master secret]# [root@k8s-master secret]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod-secret-volume 1/1 Running 0 16s 10.244.2.159 k8s-node02 &lt;none&gt; &lt;none&gt; 查看secret信息 123456789[root@k8s-master secret]# kubectl exec -it pod-secret-volume -- /bin/sh/ # ls /etc/secretpassword username/ # / # cat /etc/secret/usernameadmin/ # / # / # cat /etc/secret/password1f2d1e2e67df/ # 由上可见，在pod中的secret信息实际已经被解密。 将Secret导入到环境变量中yaml文件 1234567891011121314151617181920212223[root@k8s-master secret]# pwd/root/k8s_practice/secret[root@k8s-master secret]# cat pod_secret_env.yaml apiVersion: v1kind: Podmetadata: name: pod-secret-envspec: containers: - name: myapp image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 env: - name: SECRET_USERNAME valueFrom: secretKeyRef: name: mysecret key: username - name: SECRET_PASSWORD valueFrom: secretKeyRef: name: mysecret key: password restartPolicy: Never 启动pod并查看状态 123456[root@k8s-master secret]# kubectl apply -f pod_secret_env.yaml pod/pod-secret-env created[root@k8s-master secret]# [root@k8s-master secret]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod-secret-env 1/1 Running 0 6s 10.244.2.160 k8s-node02 &lt;none&gt; &lt;none&gt; 查看secret信息 12345678910111213141516[root@k8s-master secret]# kubectl exec -it pod-secret-env -- /bin/sh/ # env………………HOME=/rootSECRET_PASSWORD=1f2d1e2e67df ### secret信息MYAPP_SVC_PORT_80_TCP=tcp://10.98.57.156:80TERM=xtermNGINX_VERSION=1.12.2KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binKUBERNETES_PORT_443_TCP_PORT=443KUBERNETES_PORT_443_TCP_PROTO=tcpMYAPP_SVC_SERVICE_HOST=10.98.57.156SECRET_USERNAME=admin ### secret信息KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443……………… 由上可见，在pod中的secret信息实际已经被解密。 docker-registry Secretharbor镜像仓库首先使用harbor搭建镜像仓库，搭建部署过程参考：「Harbor企业级私有Docker镜像仓库部署」 harbor部分配置文件信息 12345678910111213141516171819[root@k8s-master harbor]# pwd/root/App/harbor[root@k8s-master harbor]# vim harbor.yml # Configuration file of Harborhostname: 172.16.1.110# http related confighttp: # port for http, default is 80. If https enabled, this port will redirect to https port port: 5000# https related confighttps: # https port for harbor, default is 443 port: 443 # The path of cert and key files for nginx certificate: /etc/harbor/cert/httpd.crt private_key: /etc/harbor/cert/httpd.keyharbor_admin_password: Harbor12345 启动harbor后客户端http设置 集群所有机器都要操作 1234567891011[root@k8s-master ~]# vim /etc/docker/daemon.json&#123; &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;], &quot;log-driver&quot;: &quot;json-file&quot;, &quot;log-opts&quot;: &#123; &quot;max-size&quot;: &quot;100m&quot; &#125;, &quot;insecure-registries&quot;: [&quot;172.16.1.110:5000&quot;]&#125;[root@k8s-master ~]# [root@k8s-master ~]# systemctl restart docker # 重启docker服务 添加了 “insecure-registries”: [“172.16.1.110:5000”] 这行，其中172.16.1.110为内网IP地址。该文件必须符合 json 规范，否则 Docker 将不能启动。 如果在Harbor所在的机器重启了docker服务，记得要重新启动Harbor。 创建「私有」仓库 镜像上传 123456docker pull registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1docker tag registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 172.16.1.110:5000/k8s-secret/myapp:v1# 登录docker login 172.16.1.110:5000 -u admin -p Harbor12345# 上传docker push 172.16.1.110:5000/k8s-secret/myapp:v1 退出登录 之后在操作机上退出harbor登录，便于后面演示 123456### 退出harbor登录[root@k8s-node02 ~]# docker logout 172.16.1.110:5000 Removing login credentials for 172.16.1.110:5000### 拉取失败，需要先登录。表明完成准备工作[root@k8s-master secret]# docker pull 172.16.1.110:5000/k8s-secret/myapp:v1Error response from daemon: pull access denied for 172.16.1.110:5000/k8s-secret/myapp, repository does not exist or may require &apos;docker login&apos;: denied: requested access to the resource is denied pod直接下载镜像在yaml文件中指定image后，直接启动pod 1234567891011[root@k8s-master secret]# pwd/root/k8s_practice/secret[root@k8s-master secret]# cat pod_secret_registry.yaml apiVersion: v1kind: Podmetadata: name: pod-secret-registryspec: containers: - name: myapp image: 172.16.1.110:5000/k8s-secret/myapp:v1 启动pod并查看状态 12345678910111213141516171819202122232425262728293031323334353637[root@k8s-master secret]# kubectl apply -f pod_secret_registry.yaml pod/pod-secret-registry created[root@k8s-master secret]# [root@k8s-master secret]# kubectl get pod -o wide ### 可见镜像下载失败NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod-secret-registry 0/1 ImagePullBackOff 0 7s 10.244.2.161 k8s-node02 &lt;none&gt; &lt;none&gt;[root@k8s-master secret]# [root@k8s-master secret]# kubectl describe pod pod-secret-registry ### 查看pod详情Name: pod-secret-registryNamespace: defaultPriority: 0Node: k8s-node02/172.16.1.112Start Time: Mon, 08 Jun 2020 23:59:07 +0800Labels: &lt;none&gt;Annotations: kubectl.kubernetes.io/last-applied-configuration: &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Pod&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;pod-secret-registry&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;,&quot;spec&quot;:&#123;&quot;containers&quot;:[&#123;&quot;i...Status: PendingIP: 10.244.2.161IPs: IP: 10.244.2.161Containers: myapp: Container ID: Image: 172.16.1.110:5000/k8s-secret/myapp:v1 Image ID: ………………Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 23s default-scheduler Successfully assigned default/pod-secret-registry to k8s-node02 Normal BackOff 19s (x2 over 20s) kubelet, k8s-node02 Back-off pulling image &quot;172.16.1.110:5000/k8s-secret/myapp:v1&quot; Warning Failed 19s (x2 over 20s) kubelet, k8s-node02 Error: ImagePullBackOff Normal Pulling 9s (x2 over 21s) kubelet, k8s-node02 Pulling image &quot;172.16.1.110:5000/k8s-secret/myapp:v1&quot; Warning Failed 9s (x2 over 21s) kubelet, k8s-node02 Failed to pull image &quot;172.16.1.110:5000/k8s-secret/myapp:v1&quot;: rpc error: code = Unknown desc = Error response from daemon: pull access denied for 172.16.1.110:5000/k8s-secret/myapp, repository does not exist or may require &apos;docker login&apos;: denied: requested access to the resource is denied Warning Failed 9s (x2 over 21s) kubelet, k8s-node02 Error: ErrImagePull[root@k8s-master secret]# [root@k8s-master secret]# kubectl delete -f pod_secret_registry.yaml 可见拉取私有镜像失败。 pod通过Secret下载镜像通过命令行创建Secret，并查看其描述信息 123456789101112131415161718192021222324252627282930313233343536[root@k8s-master secret]# kubectl create secret docker-registry myregistrysecret --docker-server=&apos;172.16.1.110:5000&apos; --docker-username=&apos;admin&apos; --docker-password=&apos;Harbor12345&apos; secret/myregistrysecret created[root@k8s-master secret]#[root@k8s-master secret]# kubectl get secret NAME TYPE DATA AGEbasic-auth Opaque 1 2d14hdefault-token-v48g4 kubernetes.io/service-account-token 3 27dmyregistrysecret kubernetes.io/dockerconfigjson 1 8s # 刚刚创建的mysecret Opaque 2 118mtls-secret kubernetes.io/tls 2 3d4h[root@k8s-master secret]# [root@k8s-master secret]# kubectl get secret myregistrysecret -o yaml ### 查看详细信息apiVersion: v1data: .dockerconfigjson: eyJhdXRocyI6eyIxMC4wLjAuMTEwOjUwMDAiOnsidXNlcm5hbWUiOiJhZG1pbiIsInBhc3N3b3JkIjoiSGFyYm9yMTIzNDUiLCJhdXRoIjoiWVdSdGFXNDZTR0Z5WW05eU1USXpORFU9In19fQ==kind: Secretmetadata: creationTimestamp: &quot;2020-06-08T16:07:32Z&quot; name: myregistrysecret namespace: default resourceVersion: &quot;1004582&quot; selfLink: /api/v1/namespaces/default/secrets/myregistrysecret uid: b95f4386-64bc-4ba3-b43a-08afb1c1eb9dtype: kubernetes.io/dockerconfigjson[root@k8s-master secret]# [root@k8s-master secret]# kubectl describe secret myregistrysecret ### 查看描述信息Name: myregistrysecretNamespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Type: kubernetes.io/dockerconfigjsonData====.dockerconfigjson: 109 bytes 修改之前的yaml文件 1234567891011[root@k8s-master secret]# cat pod_secret_registry.yaml apiVersion: v1kind: Podmetadata: name: pod-secret-registryspec: containers: - name: myapp image: 172.16.1.110:5000/k8s-secret/myapp:v1 imagePullSecrets: - name: myregistrysecret 启动pod并查看状态 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657[root@k8s-master secret]# kubectl apply -f pod_secret_registry.yaml pod/pod-secret-registry created[root@k8s-master secret]# [root@k8s-master secret]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod-secret-registry 1/1 Running 0 8s 10.244.2.162 k8s-node02 &lt;none&gt; &lt;none&gt;[root@k8s-master secret]# [root@k8s-master secret]# kubectl describe pod pod-secret-registryName: pod-secret-registryNamespace: defaultPriority: 0Node: k8s-node02/172.16.1.112Start Time: Tue, 09 Jun 2020 00:22:40 +0800Labels: &lt;none&gt;Annotations: kubectl.kubernetes.io/last-applied-configuration: &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Pod&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;pod-secret-registry&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;,&quot;spec&quot;:&#123;&quot;containers&quot;:[&#123;&quot;i...Status: RunningIP: 10.244.2.162IPs: IP: 10.244.2.162Containers: myapp: Container ID: docker://ef4d42f1f1616a44c2a6c0a5a71333b27f46dfe76eb392962813a28d69150c00 Image: 172.16.1.110:5000/k8s-secret/myapp:v1 Image ID: docker-pullable://172.16.1.110:5000/k8s-secret/myapp@sha256:9eeca44ba2d410e54fccc54cbe9c021802aa8b9836a0bcf3d3229354e4c8870e Port: &lt;none&gt; Host Port: &lt;none&gt; State: Running Started: Tue, 09 Jun 2020 00:22:41 +0800 Ready: True Restart Count: 0 Environment: &lt;none&gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-v48g4 (ro)Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: default-token-v48g4: Type: Secret (a volume populated by a Secret) SecretName: default-token-v48g4 Optional: falseQoS Class: BestEffortNode-Selectors: &lt;none&gt;Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300sEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 22s default-scheduler Successfully assigned default/pod-secret-registry to k8s-node02 Normal Pulling 22s kubelet, k8s-node02 Pulling image &quot;172.16.1.110:5000/k8s-secret/myapp:v1&quot; Normal Pulled 22s kubelet, k8s-node02 Successfully pulled image &quot;172.16.1.110:5000/k8s-secret/myapp:v1&quot; Normal Created 22s kubelet, k8s-node02 Created container myapp Normal Started 21s kubelet, k8s-node02 Started container myapp 由上可见，通过secret认证后pod拉取私有镜像是可以的。 相关阅读1、Harbor企业级私有Docker镜像仓库部署]]></content>
      <categories>
        <category>Kubernetes</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
        <tag>K8S</tag>
        <tag>CI/CD</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes K8S之Ingress详解与示例]]></title>
    <url>%2F2020%2F09%2F23%2Fkubernetes16%2F</url>
    <content type="text"><![CDATA[K8S之Ingress概述与说明，并详解Ingress常用示例 主机配置规划 服务器名称(hostname) 系统版本 配置 内网IP 外网IP(模拟) k8s-master CentOS7.7 2C/4G/20G 172.16.1.110 10.0.0.110 k8s-node01 CentOS7.7 2C/4G/20G 172.16.1.111 10.0.0.111 k8s-node02 CentOS7.7 2C/4G/20G 172.16.1.112 10.0.0.112 Ingress概述Ingress 是对集群中服务的外部访问进行管理的 API 对象，典型的访问方式是 HTTP和HTTPS。 Ingress 可以提供负载均衡、SSL 和基于名称的虚拟托管。 必须具有 ingress 控制器【例如 ingress-nginx】才能满足 Ingress 的要求。仅创建 Ingress 资源无效。 Ingress 是什么Ingress 公开了从集群外部到集群内 services 的 HTTP 和 HTTPS 路由。 流量路由由 Ingress 资源上定义的规则控制。 12345 internet |[ Ingress ]--|-----|--[ Services ] 可以将 Ingress 配置为提供服务外部可访问的 URL、负载均衡流量、 SSL / TLS，以及提供基于名称的虚拟主机。Ingress 控制器 通常负责通过负载均衡器来实现 Ingress，尽管它也可以配置边缘路由器或其他前端来帮助处理流量。 Ingress 不会公开任意端口或协议。若将 HTTP 和 HTTPS 以外的服务公开到 Internet 时，通常使用 Service.Type=NodePort 或者 Service.Type=LoadBalancer 类型的服务。 以Nginx Ingress为例，图如下 Ingress示例架构图 部署Ingress-Nginx该Nginx是经过改造的，而不是传统的Nginx。 Ingress-Nginx官网地址 1https://kubernetes.github.io/ingress-nginx/ Ingress-Nginx GitHub地址 1https://github.com/kubernetes/ingress-nginx 本次下载版本：nginx-0.30.0 镜像下载与重命名 123docker pull registry.cn-beijing.aliyuncs.com/google_registry/nginx-ingress-controller:0.30.0docker tag 89ccad40ce8e quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0docker rmi registry.cn-beijing.aliyuncs.com/google_registry/nginx-ingress-controller:0.30.0 ingress-nginx的yaml文件修改后并启动 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# 当前目录[root@k8s-master ingress]# pwd/root/k8s_practice/ingress# 获取NGINX: 0.30.0[root@k8s-master ingress]# wget https://github.com/kubernetes/ingress-nginx/archive/nginx-0.30.0.tar.gz[root@k8s-master ingress]# tar xf nginx-0.30.0.tar.gz# yaml文件在下载包中的位置：ingress-nginx-nginx-0.30.0/deploy/static/mandatory.yaml[root@k8s-master ingress]# cp -a ingress-nginx-nginx-0.30.0/deploy/static/mandatory.yaml ./[root@k8s-master ingress]# # yaml文件配置修改[root@k8s-master ingress]# vim mandatory.yaml………………apiVersion: apps/v1kind: DaemonSet # 从Deployment改为DaemonSetmetadata: name: nginx-ingress-controller namespace: ingress-nginx labels: app.kubernetes.io/name: ingress-nginx app.kubernetes.io/part-of: ingress-nginxspec: #replicas: 1 # 注释掉……………… nodeSelector: kubernetes.io/hostname: k8s-master # 修改处 # 如下几行为新加行 作用【允许在master节点运行】 tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule……………… ports: - name: http containerPort: 80 hostPort: 80 # 添加处【可在宿主机通过该端口访问Pod】 protocol: TCP - name: https containerPort: 443 hostPort: 443 # 添加处【可在宿主机通过该端口访问Pod】 protocol: TCP………………[root@k8s-master ingress]# [root@k8s-master ingress]# kubectl apply -f mandatory.yamlnamespace/ingress-nginx createdconfigmap/nginx-configuration createdconfigmap/tcp-services createdconfigmap/udp-services createdserviceaccount/nginx-ingress-serviceaccount createdclusterrole.rbac.authorization.k8s.io/nginx-ingress-clusterrole createdrole.rbac.authorization.k8s.io/nginx-ingress-role createdrolebinding.rbac.authorization.k8s.io/nginx-ingress-role-nisa-binding createdclusterrolebinding.rbac.authorization.k8s.io/nginx-ingress-clusterrole-nisa-binding createddaemonset.apps/nginx-ingress-controller createdlimitrange/ingress-nginx created[root@k8s-master ingress]# [root@k8s-master ingress]# kubectl get ds -n ingress-nginx -o wideNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE CONTAINERS IMAGES SELECTORnginx-ingress-controller 1 1 1 1 1 kubernetes.io/hostname=k8s-master 9m47s nginx-ingress-controller quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0 app.kubernetes.io/name=ingress-nginx,app.kubernetes.io/part-of=ingress-nginx[root@k8s-master ingress]# [root@k8s-master ingress]# kubectl get pod -n ingress-nginx -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnginx-ingress-controller-rrbh9 1/1 Running 0 9m55s 10.244.0.46 k8s-master &lt;none&gt; &lt;none&gt; deply_service1的yaml信息yaml文件 12345678910111213141516171819202122232425262728293031323334353637383940414243[root@k8s-master ingress]# pwd/root/k8s_practice/ingress[root@k8s-master ingress]# cat deply_service1.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: myapp-deploy1 namespace: defaultspec: replicas: 3 selector: matchLabels: app: myapp release: v1 template: metadata: labels: app: myapp release: v1 env: test spec: containers: - name: myapp image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80---apiVersion: v1kind: Servicemetadata: name: myapp-clusterip1 namespace: defaultspec: type: ClusterIP # 默认类型 selector: app: myapp release: v1 ports: - name: http port: 80 targetPort: 80 启动Deployment和Service 123[root@k8s-master ingress]# kubectl apply -f deply_service1.yaml deployment.apps/myapp-deploy1 createdservice/myapp-clusterip1 created 查看Deploy状态和信息 12345678910111213[root@k8s-master ingress]# kubectl get deploy -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORmyapp-deploy1 3/3 3 3 28s myapp registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp,release=v1[root@k8s-master ingress]# [root@k8s-master ingress]# kubectl get rs -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTORmyapp-deploy1-5695bb5658 3 3 3 30s myapp registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp,pod-template-hash=5695bb5658,release=v1[root@k8s-master ingress]# [root@k8s-master ingress]# kubectl get pod -o wide --show-labelsNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES LABELSmyapp-deploy1-5695bb5658-n6548 1/1 Running 0 36s 10.244.2.144 k8s-node02 &lt;none&gt; &lt;none&gt; app=myapp,env=test,pod-template-hash=5695bb5658,release=v1myapp-deploy1-5695bb5658-rqcpb 1/1 Running 0 36s 10.244.2.143 k8s-node02 &lt;none&gt; &lt;none&gt; app=myapp,env=test,pod-template-hash=5695bb5658,release=v1myapp-deploy1-5695bb5658-vv6gm 1/1 Running 0 36s 10.244.3.200 k8s-node01 &lt;none&gt; &lt;none&gt; app=myapp,env=test,pod-template-hash=5695bb5658,release=v1 curl访问pod 1234567891011[root@k8s-master ingress]# curl 10.244.2.144Hello MyApp | Version: v1 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;[root@k8s-master ingress]# [root@k8s-master ingress]# curl 10.244.2.144/hostname.htmlmyapp-deploy1-5695bb5658-n6548[root@k8s-master ingress]# [root@k8s-master ingress]# curl 10.244.2.143/hostname.htmlmyapp-deploy1-5695bb5658-rqcpb[root@k8s-master ingress]# [root@k8s-master ingress]# curl 10.244.3.200/hostname.htmlmyapp-deploy1-5695bb5658-vv6gm 查看Service状态和信息 1234[root@k8s-master ingress]# kubectl get svc -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 19d &lt;none&gt;myapp-clusterip1 ClusterIP 10.104.146.14 &lt;none&gt; 80/TCP 5m38s app=myapp,release=v1 curl访问svc 1234567891011[root@k8s-master ingress]# curl 10.104.146.14Hello MyApp | Version: v1 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;[root@k8s-master ingress]# [root@k8s-master ingress]# curl 10.104.146.14/hostname.htmlmyapp-deploy1-5695bb5658-n6548[root@k8s-master ingress]# [root@k8s-master ingress]# curl 10.104.146.14/hostname.htmlmyapp-deploy1-5695bb5658-vv6gm[root@k8s-master ingress]# [root@k8s-master ingress]# curl 10.104.146.14/hostname.htmlmyapp-deploy1-5695bb5658-rqcpb deply_service2的yaml信息yaml文件 12345678910111213141516171819202122232425262728293031323334353637383940414243[root@k8s-master ingress]# pwd/root/k8s_practice/ingress[root@k8s-master ingress]# cat deply_service2.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: myapp-deploy2 namespace: defaultspec: replicas: 3 selector: matchLabels: app: myapp release: v2 template: metadata: labels: app: myapp release: v2 env: test spec: containers: - name: myapp image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v2 imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80---apiVersion: v1kind: Servicemetadata: name: myapp-clusterip2 namespace: defaultspec: type: ClusterIP # 默认类型 selector: app: myapp release: v2 ports: - name: http port: 80 targetPort: 80 启动Deployment和Service 123[root@k8s-master ingress]# kubectl apply -f deply_service2.yaml deployment.apps/myapp-deploy2 createdservice/myapp-clusterip2 created 查看Deploy状态和信息 1234567891011121314[root@k8s-master ingress]# kubectl get deploy myapp-deploy2 -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORmyapp-deploy2 3/3 3 3 9s myapp registry.cn-beijing.aliyuncs.com/google_registry/myapp:v2 app=myapp,release=v2[root@k8s-master ingress]# [root@k8s-master ingress]# kubectl get rs -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTORmyapp-deploy1-5695bb5658 3 3 3 7m23s myapp registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp,pod-template-hash=5695bb5658,release=v1 # 之前创建的myapp-deploy2-54f48f879b 3 3 3 15s myapp registry.cn-beijing.aliyuncs.com/google_registry/myapp:v2 app=myapp,pod-template-hash=54f48f879b,release=v2 # 当前deploy创建的[root@k8s-master ingress]# [root@k8s-master ingress]# kubectl get pod -o wide --show-labels -l &quot;release=v2&quot;NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES LABELSmyapp-deploy2-54f48f879b-7pxwp 1/1 Running 0 25s 10.244.3.201 k8s-node01 &lt;none&gt; &lt;none&gt; app=myapp,env=test,pod-template-hash=54f48f879b,release=v2myapp-deploy2-54f48f879b-lqlh2 1/1 Running 0 25s 10.244.2.146 k8s-node02 &lt;none&gt; &lt;none&gt; app=myapp,env=test,pod-template-hash=54f48f879b,release=v2myapp-deploy2-54f48f879b-pfvnn 1/1 Running 0 25s 10.244.2.145 k8s-node02 &lt;none&gt; &lt;none&gt; app=myapp,env=test,pod-template-hash=54f48f879b,release=v2 查看Service状态和信息 12345[root@k8s-master ingress]# kubectl get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 19d &lt;none&gt;myapp-clusterip1 ClusterIP 10.104.146.14 &lt;none&gt; 80/TCP 8m9s app=myapp,release=v1myapp-clusterip2 ClusterIP 10.110.181.62 &lt;none&gt; 80/TCP 61s app=myapp,release=v2 curl访问svc 1234567891011[root@k8s-master ingress]# curl 10.110.181.62Hello MyApp | Version: v2 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;[root@k8s-master ingress]# [root@k8s-master ingress]# curl 10.110.181.62/hostname.htmlmyapp-deploy2-54f48f879b-lqlh2[root@k8s-master ingress]# [root@k8s-master ingress]# curl 10.110.181.62/hostname.htmlmyapp-deploy2-54f48f879b-7pxwp[root@k8s-master ingress]# [root@k8s-master ingress]# curl 10.110.181.62/hostname.htmlmyapp-deploy2-54f48f879b-pfvnn Ingress HTTP代理访问yaml文件【由于自建的service在默认default名称空间，因此这里也是default名称空间】 123456789101112131415161718192021222324[root@k8s-master ingress]# pwd/root/k8s_practice/ingress[root@k8s-master ingress]# cat ingress-http.yamlapiVersion: networking.k8s.io/v1beta1kind: Ingressmetadata: name: nginx-http namespace: defaultspec: rules: - host: www.zhangtest.com http: paths: - path: / backend: serviceName: myapp-clusterip1 servicePort: 80 - host: blog.zhangtest.com http: paths: - path: / backend: serviceName: myapp-clusterip2 servicePort: 80 启动ingress http并查看状态 123456[root@k8s-master ingress]# kubectl apply -f ingress-http.yaml ingress.networking.k8s.io/nginx-http created[root@k8s-master ingress]# [root@k8s-master ingress]# kubectl get ingress -o wideNAME HOSTS ADDRESS PORTS AGEnginx-http www.zhangtest.com,blog.zhangtest.com 80 9s 查看nginx配置文件 1234567[root@k8s-master ~]# kubectl get pod -A | grep &apos;ingre&apos;ingress-nginx nginx-ingress-controller-rrbh9 1/1 Running 0 27m[root@k8s-master ~]# [root@k8s-master ~]# kubectl exec -it -n ingress-nginx nginx-ingress-controller-rrbh9 bashbash-5.0$ cat /etc/nginx/nginx.conf…………##### 可见server www.zhangtest.com 和 server blog.zhangtest.com的配置 浏览器访问hosts文件修改，添加如下信息 1234文件位置：C:\WINDOWS\System32\drivers\etc\hosts添加信息如下：# K8S ingress学习10.0.0.110 www.zhangtest.com blog.zhangtest.com 浏览器访问www.zhangtest.com 12http://www.zhangtest.com/http://www.zhangtest.com/hostname.html 浏览器访问blog.zhangtest.com 12http://blog.zhangtest.com/http://blog.zhangtest.com/hostname.html 当然：除了用浏览器访问外，也可以在Linux使用curl访问。前提是修改/etc/hosts文件，对上面的两个域名进行解析。 Ingress HTTPS代理访问SSL证书创建12345678910[root@k8s-master cert]# pwd/root/k8s_practice/ingress/cert[root@k8s-master cert]# openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj &quot;/C=CN/ST=BJ/L=BeiJing/O=BTC/OU=MOST/CN=zhang/emailAddress=ca@test.com&quot;Generating a 2048 bit RSA private key......................................................+++........................+++writing new private key to &apos;tls.key&apos;-----[root@k8s-master cert]# kubectl create secret tls tls-secret --key tls.key --cert tls.crtsecret/tls-secret created 创建ingress httpsyaml文件【由于自建的service在默认default名称空间，因此这里也是default名称空间】 1234567891011121314151617181920212223242526272829[root@k8s-master ingress]# pwd/root/k8s_practice/ingress[root@k8s-master ingress]# cat ingress-https.yaml apiVersion: networking.k8s.io/v1beta1kind: Ingressmetadata: name: nginx-https namespace: defaultspec: tls: - hosts: - www.zhangtest.com - blog.zhangtest.com secretName: tls-secret rules: - host: www.zhangtest.com http: paths: - path: / backend: serviceName: myapp-clusterip1 servicePort: 80 - host: blog.zhangtest.com http: paths: - path: / backend: serviceName: myapp-clusterip2 servicePort: 80 启动ingress https并查看状态 123456[root@k8s-master ingress]# kubectl apply -f ingress-https.yaml ingress.networking.k8s.io/nginx-https created[root@k8s-master ingress]# [root@k8s-master ingress]# kubectl get ingress -o wideNAME HOSTS ADDRESS PORTS AGEnginx-https www.zhangtest.com,blog.zhangtest.com 80, 443 8s 浏览器访问hosts文件修改，添加如下信息 1234文件位置：C:\WINDOWS\System32\drivers\etc\hosts添加信息如下：# K8S ingress学习10.0.0.110 www.zhangtest.com blog.zhangtest.com 浏览器访问www.zhangtest.com 12https://www.zhangtest.com/https://www.zhangtest.com/hostname.html 浏览器访问blog.zhangtest.com 12https://blog.zhangtest.com/https://blog.zhangtest.com/hostname.html Ingress-Nginx实现BasicAuth认证官网地址： 1https://kubernetes.github.io/ingress-nginx/examples/auth/basic/ 准备工作 1234567891011121314151617181920212223[root@k8s-master ingress]# pwd/root/k8s_practice/ingress[root@k8s-master ingress]# yum install -y httpd[root@k8s-master ingress]# htpasswd -c auth fooNew password: #输入密码Re-type new password: #重复输入的密码Adding password for user foo ##### 此时会生成一个 auth文件[root@k8s-master ingress]# kubectl create secret generic basic-auth --from-file=authsecret/basic-auth created[root@k8s-master ingress]# [root@k8s-master ingress]# kubectl get secret basic-auth -o yamlapiVersion: v1data: auth: Zm9vOiRhcHIxJFpaSUJUMDZOJDVNZ3hxdkpFNWVRTi9NdnZCcVpHaC4Kkind: Secretmetadata: creationTimestamp: &quot;2020-08-17T09:42:04Z&quot; name: basic-auth namespace: default resourceVersion: &quot;775573&quot; selfLink: /api/v1/namespaces/default/secrets/basic-auth uid: eef0853b-a52b-4684-922a-817e4cd9e9catype: Opaque ingress yaml文件 1234567891011121314151617181920212223[root@k8s-master ingress]# pwd/root/k8s_practice/ingress[root@k8s-master ingress]# cat nginx_basicauth.yaml apiVersion: networking.k8s.io/v1beta1kind: Ingressmetadata: name: ingress-with-auth annotations: # type of authentication nginx.ingress.kubernetes.io/auth-type: basic # name of the secret that contains the user/password definitions nginx.ingress.kubernetes.io/auth-secret: basic-auth # message to display with an appropriate context why the authentication is required nginx.ingress.kubernetes.io/auth-realm: &apos;Authentication Required - foo&apos;spec: rules: - host: auth.zhangtest.com http: paths: - path: / backend: serviceName: myapp-clusterip1 servicePort: 80 启动ingress并查看状态 123456[root@k8s-master ingress]# kubectl apply -f nginx_basicauth.yaml ingress.networking.k8s.io/ingress-with-auth created[root@k8s-master ingress]# [root@k8s-master ingress]# kubectl get ingress -o wideNAME HOSTS ADDRESS PORTS AGEingress-with-auth auth.zhangtest.com 80 6s 浏览器访问hosts文件修改，添加如下信息 1234文件位置：C:\WINDOWS\System32\drivers\etc\hosts添加信息如下：# K8S ingress学习10.0.0.110 www.zhangtest.com blog.zhangtest.com auth.zhangtest.com 浏览器访问auth.zhangtest.com 1http://auth.zhangtest.com/ Ingress-Nginx实现Rewrite重写官网地址： 1https://kubernetes.github.io/ingress-nginx/examples/rewrite/ 重写可以使用以下注解控制： 名称 描述 值 nginx.ingress.kubernetes.io/rewrite-target 必须重定向的目标URL String nginx.ingress.kubernetes.io/ssl-redirect 指示位置部分是否只能由SSL访问(当Ingress包含证书时，默认为True) Bool nginx.ingress.kubernetes.io/force-ssl-redirect 即使Ingress没有启用TLS，也强制重定向到HTTPS Bool nginx.ingress.kubernetes.io/app-root 定义应用程序根目录，Controller在“/”上下文中必须重定向该根目录 String nginx.ingress.kubernetes.io/use-regex 指示Ingress上定义的路径是否使用正则表达式 Bool ingress yaml文件 123456789101112131415161718[root@k8s-master ingress]# pwd/root/k8s_practice/ingress[root@k8s-master ingress]# cat nginx_rewrite.yaml apiVersion: networking.k8s.io/v1beta1kind: Ingressmetadata: annotations: nginx.ingress.kubernetes.io/rewrite-target: https://www.baidu.com name: rewrite namespace: defaultspec: rules: - host: rewrite.zhangtest.com http: paths: - backend: serviceName: myapp-clusterip1 servicePort: 80 启动ingress并查看状态 123456[root@k8s-master ingress]# kubectl apply -f nginx_rewrite.yaml ingress.networking.k8s.io/rewrite created[root@k8s-master ingress]# [root@k8s-master ingress]# kubectl get ingress -o wideNAME HOSTS ADDRESS PORTS AGErewrite rewrite.zhangtest.com 80 13s 浏览器访问hosts文件修改，添加如下信息 1234文件位置：C:\WINDOWS\System32\drivers\etc\hosts添加信息如下：# K8S ingress学习10.0.0.110 www.zhangtest.com blog.zhangtest.com auth.zhangtest.com rewrite.zhangtest.com 浏览器访问rewrite.zhangtest.com 1http://rewrite.zhangtest.com/ 之后，可见重定向到了https://www.baidu.com百度页面 相关阅读1、k8s 官方 Ingress 2、Ingress-Nginx官网地址 3、Ingress-Nginx GitHub地址 4、Ingress-Nginx实现BasicAuth认证 5、Ingress-Nginx实现Rewrite重写]]></content>
      <categories>
        <category>Kubernetes</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
        <tag>K8S</tag>
        <tag>CI/CD</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes K8S之Service服务详解与示例]]></title>
    <url>%2F2020%2F09%2F16%2Fkubernetes15%2F</url>
    <content type="text"><![CDATA[K8S之Service概述与代理说明，并详解所有的service服务类型与示例 主机配置规划 服务器名称(hostname) 系统版本 配置 内网IP 外网IP(模拟) k8s-master CentOS7.7 2C/4G/20G 172.16.1.110 10.0.0.110 k8s-node01 CentOS7.7 2C/4G/20G 172.16.1.111 10.0.0.111 k8s-node02 CentOS7.7 2C/4G/20G 172.16.1.112 10.0.0.112 Service概述Kubernetes Service定义了这样一种抽象：逻辑上的一组 Pod，一种可以访问它们的策略 —— 通常被称为微服务。这一组 Pod 能够被 Service 访问到，通常是通过 selector实现的。 举例：考虑一个图片处理 backend，它运行了3个副本。这些副本是可互换的 —— frontend 不需要关心它们调用了哪个 backend 副本。 然而组成这一组 backend 程序的 Pod 实际上可能会发生变化，frontend 客户端不应该也没必要知道，而且也不需要跟踪这一组 backend 的状态。Service 定义的抽象能够解耦这种关联。 Service可以提供负载均衡的能力，但是使用上存在如下限制： 只能提供4层负载均衡能力，而没有7层功能。有时我们可能需要更多的匹配规则来转发请求，这点上4层负载均衡是不支持的、 如web访问的service服务示例图： VIP和Service代理在 Kubernetes 集群中，每个 Node 运行一个 kube-proxy 进程。kube-proxy 负责为 Service 实现了一种 VIP（虚拟 IP）的形式，而不是 ExternalName 的形式。 从Kubernetes v1.0开始，已经可以使用 userspace代理模式。Kubernetes v1.1添加了 iptables 代理模式，在 Kubernetes v1.2 中kube-proxy 的 iptables 模式成为默认设置。Kubernetes v1.8添加了 ipvs 代理模式。 为什么不使用 DNS 轮询？ 原因如下： DNS 实现的历史由来已久，它不遵守记录 TTL，并且在名称查找到结果后会对其进行缓存。 有些应用程序仅执行一次 DNS 查找，并无限期地缓存结果。 即使应用和库进行了适当的重新解析，DNS 记录上的 TTL 值低或为零也可能会给 DNS 带来高负载，从而使管理变得困难。 总之就是因为有缓存，因此不合适。 userspace代理模式这种模式，kube-proxy 会监视 Kubernetes master 对 Service 对象和 Endpoints 对象的添加和移除。 对每个 Service，它会在本地 Node 上打开一个端口（随机选择）。 任何连接到“代理端口”的请求，都会被代理到 Service 的backend Pods 中的某个上面（如 Endpoints 所报告的一样）。 使用哪个 backend Pod，是 kube-proxy 基于 SessionAffinity 来确定的。 最后，它配置 iptables 规则，捕获到达该 Service 的 clusterIP（是虚拟 IP）和 Port 的请求，并重定向到代理端口，代理端口再代理请求到 backend Pod。 默认情况下，userspace模式下的kube-proxy通过循环算法选择后端。 默认的策略是，通过 round-robin 算法来选择 backend Pod。 iptables 代理模式这种模式，kube-proxy 会监视 Kubernetes 控制节点对 Service 对象和 Endpoints 对象的添加和移除。 对每个 Service，它会配置 iptables 规则，从而捕获到达该 Service 的 clusterIP 和端口的请求，进而将请求重定向到 Service 的一组 backend 中的某个上面。对于每个 Endpoints 对象，它也会配置 iptables 规则，这个规则会选择一个 backend 组合。 默认的策略是，kube-proxy 在 iptables 模式下随机选择一个 backend。 使用 iptables 处理流量具有较低的系统开销，因为流量由 Linux netfilter 处理，而无需在用户空间和内核空间之间切换。 这种方法也可能更可靠。 如果 kube-proxy 在 iptables模式下运行，并且所选的第一个 Pod 没有响应，则连接失败。 这与userspace模式不同：在这种情况下，kube-proxy 将检测到与第一个 Pod 的连接已失败，并会自动使用其他后端 Pod 重试。 我们可以使用 Pod readiness 探测器 验证后端 Pod 是否可以正常工作，以便 iptables 模式下的 kube-proxy 仅看到测试正常的后端。这样做意味着可以避免将流量通过 kube-proxy 发送到已知已失败的Pod。 IPVS 代理模式在 ipvs 模式下，kube-proxy监视Kubernetes服务(Service)和端点(Endpoints)，调用 netlink 接口相应地创建 IPVS 规则， 并定期将 IPVS 规则与 Kubernetes服务(Service)和端点(Endpoints)同步。该控制循环可确保 IPVS 状态与所需状态匹配。访问服务(Service)时，IPVS 将流量定向到后端Pod之一。 IPVS代理模式基于类似于 iptables 模式的 netfilter 挂钩函数，但是使用哈希表作为基础数据结构，并且在内核空间中工作。 这意味着，与 iptables 模式下的 kube-proxy 相比，IPVS 模式下的 kube-proxy 重定向通信的延迟要短，并且在同步代理规则时具有更好的性能。与其他代理模式相比，IPVS 模式还支持更高的网络流量吞吐量。 IPVS提供了更多选项来平衡后端Pod的流量。这些是： rr: round-robin lc: least connection (smallest number of open connections) dh: destination hashing sh: source hashing sed: shortest expected delay nq: never queue 注意：要在 IPVS 模式下运行 kube-proxy，必须在启动 kube-proxy 之前使 IPVS Linux 在节点上可用。 当 kube-proxy 以 IPVS 代理模式启动时，它将验证 IPVS 内核模块是否可用。 如果未检测到 IPVS 内核模块，则 kube-proxy 将退回到以 iptables 代理模式运行。 Service服务类型Kubernetes 中Service有以下4中类型： ClusterIP：默认类型，自动分配一个仅Cluster内部可以访问的虚拟IP NodePort：通过每个 Node 上的 IP 和静态端口（NodePort）暴露服务。以ClusterIP为基础，NodePort 服务会路由到 ClusterIP 服务。通过请求 &lt;NodeIP&gt;:&lt;NodePort&gt;，可以从集群的外部访问一个集群内部的 NodePort 服务。 LoadBalancer：使用云提供商的负载均衡器，可以向外部暴露服务。外部的负载均衡器可以路由到 NodePort 服务和 ClusterIP 服务。 ExternalName：通过返回 CNAME 和它的值，可以将服务映射到 externalName 字段的内容（例如，foo.bar.example.com）。没有任何类型代理被创建。 需要注意的是：Service 能够将一个接收 port 映射到任意的 targetPort。默认情况下，targetPort 将被设置为与 port 字段相同的值。 Service域名格式：$(service name).$(namespace).svc.cluster.local，其中 cluster.local 为指定的集群的域名 Deployment的yaml信息yaml文件 12345678910111213141516171819202122232425262728[root@k8s-master service]# pwd/root/k8s_practice/service[root@k8s-master service]# cat myapp-deploy.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: myapp-deploy namespace: defaultspec: replicas: 3 selector: matchLabels: app: myapp release: v1 template: metadata: labels: app: myapp release: v1 env: test spec: containers: - name: myapp image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 启动Deployment并查看状态 12345678910111213141516[root@k8s-master service]# kubectl apply -f myapp-deploy.yaml deployment.apps/myapp-deploy created[root@k8s-master service]# [root@k8s-master service]# kubectl get deploy -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORmyapp-deploy 3/3 3 3 31h myapp registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp,release=v1[root@k8s-master service]# [root@k8s-master service]# kubectl get rs -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTORmyapp-deploy-5695bb5658 3 3 3 31h myapp registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp,pod-template-hash=5695bb5658,release=v1[root@k8s-master service]# [root@k8s-master service]# kubectl get pod -o wide --show-labelsNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES LABELSmyapp-deploy-5695bb5658-2866m 1/1 Running 2 31h 10.244.2.116 k8s-node02 &lt;none&gt; &lt;none&gt; app=myapp,env=test,pod-template-hash=5695bb5658,release=v1myapp-deploy-5695bb5658-dcfw7 1/1 Running 2 31h 10.244.4.105 k8s-node01 &lt;none&gt; &lt;none&gt; app=myapp,env=test,pod-template-hash=5695bb5658,release=v1myapp-deploy-5695bb5658-n2b5w 1/1 Running 2 31h 10.244.2.115 k8s-node02 &lt;none&gt; &lt;none&gt; app=myapp,env=test,pod-template-hash=5695bb5658,release=v1 curl访问 1234567891011121314151617[root@k8s-master service]# curl 10.244.2.116Hello MyApp | Version: v1 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;[root@k8s-master service]# [root@k8s-master service]# curl 10.244.2.116/hostname.htmlmyapp-deploy-5695bb5658-2866m[root@k8s-master service]# [root@k8s-master service]# curl 10.244.4.105Hello MyApp | Version: v1 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;[root@k8s-master service]# [root@k8s-master service]# curl 10.244.4.105/hostname.htmlmyapp-deploy-5695bb5658-dcfw7[root@k8s-master service]# [root@k8s-master service]# curl 10.244.2.115Hello MyApp | Version: v1 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;[root@k8s-master service]# [root@k8s-master service]# curl 10.244.2.115/hostname.htmlmyapp-deploy-5695bb5658-n2b5w ClusterIP类型示例yaml文件 1234567891011121314151617[root@k8s-master service]# pwd/root/k8s_practice/service[root@k8s-master service]# cat myapp-svc-ClusterIP.yaml apiVersion: v1kind: Servicemetadata: name: myapp-clusterip namespace: defaultspec: type: ClusterIP # 可以不写，为默认类型 selector: app: myapp release: v1 ports: - name: http port: 80 targetPort: 80 启动Service并查看状态 1234567[root@k8s-master service]# kubectl apply -f myapp-svc-ClusterIP.yaml service/myapp-clusterip created[root@k8s-master service]# [root@k8s-master service]# kubectl get svc -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 22d &lt;none&gt;myapp-clusterip ClusterIP 10.106.66.120 &lt;none&gt; 80/TCP 15s app=myapp,release=v1 查看pod信息 12345[root@k8s-master service]# kubectl get pod -o wide --show-labelsNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES LABELSmyapp-deploy-5695bb5658-2866m 1/1 Running 2 31h 10.244.2.116 k8s-node02 &lt;none&gt; &lt;none&gt; app=myapp,env=test,pod-template-hash=5695bb5658,release=v1myapp-deploy-5695bb5658-dcfw7 1/1 Running 2 31h 10.244.4.105 k8s-node01 &lt;none&gt; &lt;none&gt; app=myapp,env=test,pod-template-hash=5695bb5658,release=v1myapp-deploy-5695bb5658-n2b5w 1/1 Running 2 31h 10.244.2.115 k8s-node02 &lt;none&gt; &lt;none&gt; app=myapp,env=test,pod-template-hash=5695bb5658,release=v1 查看ipvs信息 123456789[root@k8s-master service]# ipvsadm -LnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConn……………… TCP 10.106.66.120:80 rr -&gt; 10.244.2.115:80 Masq 1 0 0 -&gt; 10.244.2.116:80 Masq 1 0 0 -&gt; 10.244.4.105:80 Masq 1 0 0 curl访问 1234567891011121314[root@k8s-master service]# curl 10.106.66.120Hello MyApp | Version: v1 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;[root@k8s-master service]# [root@k8s-master service]# curl 10.106.66.120/hostname.htmlmyapp-deploy-5695bb5658-2866m[root@k8s-master service]# [root@k8s-master service]# curl 10.106.66.120/hostname.htmlmyapp-deploy-5695bb5658-n2b5w[root@k8s-master service]# [root@k8s-master service]# curl 10.106.66.120/hostname.htmlmyapp-deploy-5695bb5658-dcfw7[root@k8s-master service]# [root@k8s-master service]# curl 10.106.66.120/hostname.htmlmyapp-deploy-5695bb5658-2866m 备注：如果访问失败，请参考如下文章： 「Kubernetes K8S在IPVS代理模式下svc服务的ClusterIP类型访问失败处理」 Headless Services有时不需要或不想要负载均衡，以及单独的 Service IP。遇到这种情况，可以通过指定 Cluster IP（spec.clusterIP）的值为 “None” 来创建 Headless Service。 这对headless Service 并不会分配 Cluster IP，kube-proxy 不会处理它们，而且平台也不会为它们进行负载均衡和路由。 使用场景 第一种：自主选择权，有时候client想自己来决定使用哪个Real Server，可以通过查询DNS来获取Real Server的信息。 第二种：Headless Services还有一个用处（PS：也就是我们需要的那个特性）。Headless Service对应的每一个Endpoints，即每一个Pod，都会有对应的DNS域名；这样Pod之间就可以互相访问。【结合statefulset有状态服务使用，如Web、MySQL集群】 示例 yaml文件 12345678910111213141516[root@k8s-master service]# pwd/root/k8s_practice/service[root@k8s-master service]# cat myapp-svc-headless.yaml apiVersion: v1kind: Servicemetadata: name: myapp-headless namespace: defaultspec: selector: app: myapp release: v1 clusterIP: &quot;None&quot; ports: - port: 80 targetPort: 80 启动Service并查看状态和详情 12345678910111213141516171819202122[root@k8s-master service]# kubectl apply -f myapp-svc-headless.yaml service/myapp-headless created[root@k8s-master service]# [root@k8s-master service]# kubectl get svc -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 22d &lt;none&gt;myapp-headless ClusterIP None &lt;none&gt; 80/TCP 6s app=myapp,release=v1[root@k8s-master service]# [root@k8s-master service]# kubectl describe svc/myapp-headlessName: myapp-headlessNamespace: defaultLabels: &lt;none&gt;Annotations: kubectl.kubernetes.io/last-applied-configuration: &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Service&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;myapp-headless&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;,&quot;spec&quot;:&#123;&quot;clusterIP&quot;:&quot;None&quot;...Selector: app=myapp,release=v1Type: ClusterIPIP: NonePort: &lt;unset&gt; 80/TCPTargetPort: 80/TCPEndpoints: 10.244.2.115:80,10.244.2.116:80,10.244.4.105:80 # 后端的Pod信息Session Affinity: NoneEvents: &lt;none&gt; service只要创建成功就会写入到coredns。我们得到coredns IP的命令如下： 123[root@k8s-master service]# kubectl get pod -o wide -A | grep &apos;coredns&apos;kube-system coredns-6955765f44-c9zfh 1/1 Running 29 22d 10.244.0.62 k8s-master &lt;none&gt; &lt;none&gt;kube-system coredns-6955765f44-lrz5q 1/1 Running 29 22d 10.244.0.61 k8s-master &lt;none&gt; &lt;none&gt; 在宿主机安装nslookup、dig命令安装 1yum install -y bind-utils coredns记录信息如下 12345678910111213141516171819202122232425262728293031323334353637383940# 其中 10.244.0.61 为 coredns IP# myapp-headless.default.svc.cluster.local 为Headless Service域名。格式为:$(service name).$(namespace).svc.cluster.local，其中 cluster.local 指定的集群的域名[root@k8s-master service]# nslookup myapp-headless.default.svc.cluster.local 10.244.0.61Server: 10.244.0.61Address: 10.244.0.61#53Name: myapp-headless.default.svc.cluster.localAddress: 10.244.2.116Name: myapp-headless.default.svc.cluster.localAddress: 10.244.4.105Name: myapp-headless.default.svc.cluster.localAddress: 10.244.2.115[root@k8s-master service]#### 或使用如下命令[root@k8s-master service]# dig -t A myapp-headless.default.svc.cluster.local. @10.244.0.61; &lt;&lt;&gt;&gt; DiG 9.11.4-P2-RedHat-9.11.4-16.P2.el7_8.6 &lt;&lt;&gt;&gt; -t A myapp-headless.default.svc.cluster.local. @10.244.0.61;; global options: +cmd;; Got answer:;; WARNING: .local is reserved for Multicast DNS;; You are currently testing what happens when an mDNS query is leaked to DNS;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 7089;; flags: qr aa rd; QUERY: 1, ANSWER: 3, AUTHORITY: 0, ADDITIONAL: 1;; WARNING: recursion requested but not available;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096;; QUESTION SECTION:;myapp-headless.default.svc.cluster.local. IN A;; ANSWER SECTION:myapp-headless.default.svc.cluster.local. 14 IN A 10.244.2.116myapp-headless.default.svc.cluster.local. 14 IN A 10.244.4.105myapp-headless.default.svc.cluster.local. 14 IN A 10.244.2.115;; Query time: 0 msec;; SERVER: 10.244.0.61#53(10.244.0.61);; WHEN: Wed Jun 03 22:34:46 CST 2020;; MSG SIZE rcvd: 237 NodePort类型示例如果将 type 字段设置为 NodePort，则 Kubernetes 控制层面将在 --service-node-port-range 标志指定的范围内分配端口（默认值：30000-32767）。 yaml文件 12345678910111213141516171819[root@k8s-master service]# pwd/root/k8s_practice/service[root@k8s-master service]# cat myapp-svc-NodePort.yaml apiVersion: v1kind: Servicemetadata: name: myapp-nodeport namespace: defaultspec: type: NodePort selector: app: myapp release: v1 ports: - name: http # 默认情况下，为了方便起见，`targetPort` 被设置为与 `port` 字段相同的值。 port: 80 # Service对外提供服务端口 targetPort: 80 # 请求转发后端Pod使用的端口 nodePort: 31682 # 可选字段，默认情况下，为了方便起见，Kubernetes 控制层面会从某个范围内分配一个端口号（默认：30000-32767） 启动Service并查看状态 1234567[root@k8s-master service]# kubectl apply -f myapp-svc-NodePort.yaml service/myapp-nodeport created[root@k8s-master service]# [root@k8s-master service]# kubectl get svc -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 22d &lt;none&gt;myapp-nodeport NodePort 10.99.50.81 &lt;none&gt; 80:31682/TCP 6s app=myapp,release=v1 由上可见，类型变为了NodePort 查看ipvs信息 123456789[root@k8s-master service]# ipvsadm -LnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConn………………TCP 10.99.50.81:80 rr -&gt; 10.244.2.115:80 Masq 1 0 0 -&gt; 10.244.2.116:80 Masq 1 0 0 -&gt; 10.244.4.105:80 Masq 1 0 0 端口查看，可见在本地宿主机监听了相应的端口（备注：集群所有机器都监听了该端口） 123# 集群所有机器都可以执行查看[root@k8s-master service]# netstat -lntp | grep &apos;31682&apos;tcp6 0 0 :::31682 :::* LISTEN 3961/kube-proxy curl通过ClusterIP访问 123456789101112# 通过ClusterIP访问[root@k8s-master service]# curl 10.99.50.81Hello MyApp | Version: v1 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;[root@k8s-master service]# [root@k8s-master service]# curl 10.99.50.81/hostname.htmlmyapp-deploy-5695bb5658-2866m[root@k8s-master service]# [root@k8s-master service]# curl 10.99.50.81/hostname.htmlmyapp-deploy-5695bb5658-n2b5w[root@k8s-master service]# [root@k8s-master service]# curl 10.99.50.81/hostname.htmlmyapp-deploy-5695bb5658-dcfw7 curl通过节点IP访问 123456789101112131415161718# 通过集群节点IP访问[root@k8s-master service]# curl 172.16.1.110:31682Hello MyApp | Version: v1 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;[root@k8s-master service]# [root@k8s-master service]# curl 172.16.1.110:31682/hostname.htmlmyapp-deploy-5695bb5658-2866m[root@k8s-master service]# [root@k8s-master service]# curl 172.16.1.110:31682/hostname.htmlmyapp-deploy-5695bb5658-n2b5w[root@k8s-master service]# [root@k8s-master service]# curl 172.16.1.110:31682/hostname.htmlmyapp-deploy-5695bb5658-dcfw7# 访问集群其他节点。每台机器都有LVS，和相关调度[root@k8s-master service]# curl 172.16.1.111:31682/hostname.htmlmyapp-deploy-5695bb5658-dcfw7[root@k8s-master service]# [root@k8s-master service]# curl 172.16.1.112:31682/hostname.htmlmyapp-deploy-5695bb5658-dcfw7 访问日志查看 1kubectl logs -f svc/myapp-nodeport LoadBalancer类型示例需要相关云厂商服务支持，这里就不表述了。 ExternalName类型示例这种类型的Service通过返回CNAME和它的值，可以将服务映射到externalName字段的内容（例如：my.k8s.example.com；可以实现跨namespace名称空间访问）。ExternalName Service是Service的特例，它没有selector，也没有定义任何的端口和Endpoint。相反的，对于运行在集群外部的服务，它通过返回该外部服务的别名这种方式提供服务。 具体使用参见：「Kubernetes K8S之Pod跨namespace名称空间访问Service服务」 yaml文件 1234567891011[root@k8s-master service]# pwd/root/k8s_practice/service[root@k8s-master service]# cat myapp-svc-ExternalName.yaml apiVersion: v1kind: Servicemetadata: name: myapp-externalname namespace: defaultspec: type: ExternalName externalName: my.k8s.example.com 启动Service并查看状态 1234567[root@k8s-master service]# kubectl apply -f myapp-svc-ExternalName.yaml service/myapp-externalname created[root@k8s-master service]# [root@k8s-master service]# kubectl get svc -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 21d &lt;none&gt;myapp-externalname ExternalName &lt;none&gt; my.k8s.example.com &lt;none&gt; 21s &lt;none&gt; 由上可见，类型变为了ExternalName 宿主机dig命令安装 1yum install -y bind-utils coredns记录信息如下 1234567891011121314151617181920212223242526272829303132333435# 其中 10.244.0.61 为 coredns IP# myapp-externalname.default.svc.cluster.local 为Service域名。格式为:$(service name).$(namespace).svc.cluster.local，其中 cluster.local 指定的集群的域名##### 通过 nslookup 访问[root@k8s-master service]# nslookup myapp-externalname.default.svc.cluster.local 10.244.0.61Server: 10.244.0.61Address: 10.244.0.61#53myapp-externalname.default.svc.cluster.local canonical name = my.k8s.example.com.** server can&apos;t find my.k8s.example.com: NXDOMAIN[root@k8s-master service]###### 通过 dig 访问[root@k8s-master service]# dig -t A myapp-externalname.default.svc.cluster.local. @10.244.0.61; &lt;&lt;&gt;&gt; DiG 9.11.4-P2-RedHat-9.11.4-16.P2.el7_8.6 &lt;&lt;&gt;&gt; -t A myapp-externalname.default.svc.cluster.local. @10.244.0.61;; global options: +cmd;; Got answer:;; WARNING: .local is reserved for Multicast DNS;; You are currently testing what happens when an mDNS query is leaked to DNS;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 39541;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1;; WARNING: recursion requested but not available;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096;; QUESTION SECTION:;myapp-externalname.default.svc.cluster.local. IN A;; ANSWER SECTION:myapp-externalname.default.svc.cluster.local. 30 IN CNAME my.k8s.example.com.;; Query time: 2072 msec;; SERVER: 10.244.0.61#53(10.244.0.61);; WHEN: Wed Jun 03 23:15:47 CST 2020;; MSG SIZE rcvd: 149 ExternalIP示例如果外部的 IP 路由到集群中一个或多个 Node 上，Kubernetes Service 会被暴露给这些 externalIPs。通过外部 IP（作为目的 IP 地址）进入到集群，打到 Service 端口上的流量，将会被路由到 Service 的 Endpoint 上。 externalIPs 不会被 Kubernetes 管理，它属于集群管理员的职责范畴。 根据 Service 的规定，externalIPs 可以同任意的 ServiceType 来一起指定。在下面的例子中，my-service 可以在【模拟外网IP】“10.0.0.240”(externalIP:port) 上被客户端访问。 yaml文件 123456789101112131415161718[root@k8s-master service]# pwd/root/k8s_practice/service[root@k8s-master service]# cat myapp-svc-externalIP.yamlapiVersion: v1kind: Servicemetadata: name: myapp-externalip namespace: defaultspec: selector: app: myapp release: v1 ports: - name: http port: 80 targetPort: 80 externalIPs: - 10.0.0.240 启动Service并查看状态 1234567[root@k8s-master service]# kubectl apply -f myapp-svc-externalIP.yaml service/myapp-externalip created[root@k8s-master service]# [root@k8s-master service]# kubectl get svc -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 22d &lt;none&gt;myapp-externalip ClusterIP 10.107.186.167 10.0.0.240 80/TCP 8s app=myapp,release=v1 查看ipvs信息 1234567891011121314[root@k8s-master service]# ipvsadm -LnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConn………………TCP 10.107.186.167:80 rr -&gt; 10.244.2.115:80 Masq 1 0 0 -&gt; 10.244.2.116:80 Masq 1 0 0 -&gt; 10.244.4.105:80 Masq 1 0 0 ………………TCP 10.0.0.240:80 rr -&gt; 10.244.2.115:80 Masq 1 0 0 -&gt; 10.244.2.116:80 Masq 1 0 0 -&gt; 10.244.4.105:80 Masq 1 0 0 curl访问，通过ClusterIP 1234567891011[root@k8s-master service]# curl 10.107.186.167Hello MyApp | Version: v1 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;[root@k8s-master service]# [root@k8s-master service]# curl 10.107.186.167/hostname.htmlmyapp-deploy-5695bb5658-n2b5w[root@k8s-master service]# [root@k8s-master service]# curl 10.107.186.167/hostname.htmlmyapp-deploy-5695bb5658-2866m[root@k8s-master service]# [root@k8s-master service]# curl 10.107.186.167/hostname.htmlmyapp-deploy-5695bb5658-dcfw7 curl访问，通过ExternalIP 1234567891011[root@k8s-master service]# curl 10.0.0.240Hello MyApp | Version: v1 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;[root@k8s-master service]# [root@k8s-master service]# curl 10.0.0.240/hostname.htmlmyapp-deploy-5695bb5658-2866m[root@k8s-master service]# [root@k8s-master service]# curl 10.0.0.240/hostname.htmlmyapp-deploy-5695bb5658-dcfw7[root@k8s-master service]# [root@k8s-master service]# curl 10.0.0.240/hostname.htmlmyapp-deploy-5695bb5658-n2b5w 相关阅读1、Kubernetes K8S在IPVS代理模式下svc服务的ClusterIP类型访问失败处理 2、Kubernetes K8S之Pod跨namespace名称空间访问Service服务 3、kubernetes学习Service之headless和statefulSet结合 4、linuxea:kubernetes Headless Service无头服务]]></content>
      <categories>
        <category>Kubernetes</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
        <tag>K8S</tag>
        <tag>CI/CD</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes K8S之Pod跨namespace名称空间访问Service服务]]></title>
    <url>%2F2020%2F09%2F13%2Fkubernetes14%2F</url>
    <content type="text"><![CDATA[Kubernetes的两个Service（ServiceA、ServiceB）和对应的Pod（PodA、PodB）分别属于不同的namespace名称空间，现需要PodA和PodB跨namespace名称空间并通过Service实现互访。应该如何实现？ 场景需求Kubernetes的两个Service（ServiceA、ServiceB）和对应的Pod（PodA、PodB）分别属于不同的namespace名称空间，现需要PodA和PodB跨namespace名称空间并通过Service实现互访。如何实现？ 说明：这里是指通过Service的Name进行通信访问，而不是通过Service的IP【因因为每次重启Service，NAME不会改变，而IP是会改变的】。 主机配置规划 服务器名称(hostname) 系统版本 配置 内网IP 外网IP(模拟) k8s-master CentOS7.7 2C/4G/20G 172.16.1.110 10.0.0.110 k8s-node01 CentOS7.7 2C/4G/20G 172.16.1.111 10.0.0.111 k8s-node02 CentOS7.7 2C/4G/20G 172.16.1.112 10.0.0.112 创建Service和Pod相关yaml文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495[root@k8s-master cross_ns]# pwd/root/k8s_practice/cross_ns[root@k8s-master cross_ns]# [root@k8s-master cross_ns]# cat deply_service_myns.yaml apiVersion: v1kind: Namespacemetadata: name: myns---apiVersion: apps/v1kind: Deploymentmetadata: name: myapp-deploy1 namespace: mynsspec: replicas: 2 selector: matchLabels: app: myapp release: v1 template: metadata: labels: app: myapp release: v1 spec: containers: - name: myapp image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80---apiVersion: v1kind: Servicemetadata: name: myapp-clusterip1 namespace: mynsspec: type: ClusterIP # 默认类型 selector: app: myapp release: v1 ports: - name: http port: 80 targetPort: 80[root@k8s-master cross_ns]# [root@k8s-master cross_ns]# cat deply_service_mytest.yaml apiVersion: v1kind: Namespacemetadata: name: mytest---apiVersion: apps/v1kind: Deploymentmetadata: name: myapp-deploy2 namespace: mytestspec: replicas: 2 selector: matchLabels: app: myapp release: v2 template: metadata: labels: app: myapp release: v2 spec: containers: - name: myapp image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v2 imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80---apiVersion: v1kind: Servicemetadata: name: myapp-clusterip2 namespace: mytestspec: type: ClusterIP # 默认类型 selector: app: myapp release: v2 ports: - name: http port: 80 targetPort: 80 运行yaml文件 12kubectl apply -f deply_service_myns.yaml kubectl apply -f deply_service_mytest.yaml 查看myns名称空间信息 12345678910111213141516[root@k8s-master cross_ns]# kubectl get svc -n myns -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORmyapp-clusterip1 ClusterIP 10.100.61.11 &lt;none&gt; 80/TCP 3m app=myapp,release=v1[root@k8s-master cross_ns]# [root@k8s-master cross_ns]# kubectl get deploy -n myns -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORmyapp-deploy1 2/2 2 2 3m7s myapp registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp,release=v1[root@k8s-master cross_ns]# [root@k8s-master cross_ns]# kubectl get rs -n myns -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTORmyapp-deploy1-5b9d78576c 2 2 2 3m15s myapp registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp,pod-template-hash=5b9d78576c,release=v1[root@k8s-master cross_ns]# [root@k8s-master cross_ns]# kubectl get pod -n myns -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmyapp-deploy1-5b9d78576c-wfw4n 1/1 Running 0 3m20s 10.244.2.136 k8s-node02 &lt;none&gt; &lt;none&gt;myapp-deploy1-5b9d78576c-zsfjl 1/1 Running 0 3m20s 10.244.3.193 k8s-node01 &lt;none&gt; &lt;none&gt; 查看mytest名称空间信息 12345678910111213141516[root@k8s-master cross_ns]# kubectl get svc -n mytest -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORmyapp-clusterip2 ClusterIP 10.100.201.103 &lt;none&gt; 80/TCP 4m9s app=myapp,release=v2[root@k8s-master cross_ns]# [root@k8s-master cross_ns]# kubectl get deploy -n mytest -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORmyapp-deploy2 2/2 2 2 4m15s myapp registry.cn-beijing.aliyuncs.com/google_registry/myapp:v2 app=myapp,release=v2[root@k8s-master cross_ns]# [root@k8s-master cross_ns]# kubectl get rs -n mytest -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTORmyapp-deploy2-dc8f96497 2 2 2 4m22s myapp registry.cn-beijing.aliyuncs.com/google_registry/myapp:v2 app=myapp,pod-template-hash=dc8f96497,release=v2[root@k8s-master cross_ns]# [root@k8s-master cross_ns]# kubectl get pod -n mytest -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmyapp-deploy2-dc8f96497-nnkqn 1/1 Running 0 4m27s 10.244.3.194 k8s-node01 &lt;none&gt; &lt;none&gt;myapp-deploy2-dc8f96497-w47dt 1/1 Running 0 4m27s 10.244.2.137 k8s-node02 &lt;none&gt; &lt;none&gt; 只看Service和Pod 123456789101112[root@k8s-master cross_ns]# kubectl get pod -A -o wide | grep -E &apos;(my)|(NAME)&apos;NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmyns myapp-deploy1-5b9d78576c-wfw4n 1/1 Running 0 41m 10.244.2.136 k8s-node02 &lt;none&gt; &lt;none&gt;myns myapp-deploy1-5b9d78576c-zsfjl 1/1 Running 0 41m 10.244.3.193 k8s-node01 &lt;none&gt; &lt;none&gt;mytest myapp-deploy2-dc8f96497-nnkqn 1/1 Running 0 41m 10.244.3.194 k8s-node01 &lt;none&gt; &lt;none&gt;mytest myapp-deploy2-dc8f96497-w47dt 1/1 Running 0 41m 10.244.2.137 k8s-node02 &lt;none&gt; &lt;none&gt;[root@k8s-master cross_ns]# [root@k8s-master cross_ns]# [root@k8s-master cross_ns]# kubectl get svc -A -o wide | grep -E &apos;(my)|(NAME)&apos;NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORmyns myapp-clusterip1 ClusterIP 10.100.61.11 &lt;none&gt; 80/TCP 41m app=myapp,release=v1mytest myapp-clusterip2 ClusterIP 10.100.201.103 &lt;none&gt; 80/TCP 41m app=myapp,release=v2 pod跨名称空间namespace与Service通信说明：是通过Service的NAME进行通信，而不是Service的IP【因为每次重启Service，NAME不会改变，而IP是会改变的】。 12345678910111213141516171819202122# 进入ns名称空间下的一个Pod容器[root@k8s-master cross_ns]# kubectl exec -it -n myns myapp-deploy1-5b9d78576c-wfw4n sh/ # cd /root/### 如下说明在同一名称空间下，通信无问题~ # ping myapp-clusterip1 PING myapp-clusterip1 (10.100.61.11): 56 data bytes64 bytes from 10.100.61.11: seq=0 ttl=64 time=0.046 ms64 bytes from 10.100.61.11: seq=1 ttl=64 time=0.081 ms~ # ~ # wget myapp-clusterip1 -O myns.htmlConnecting to myapp-clusterip1 (10.100.61.11:80)myns.html 100%~ # ~ # cat myns.html Hello MyApp | Version: v1 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;### 如下说明在不同的名称空间下，通过Service的NAME进行通信存在问题~ # ping myapp-clusterip2ping: bad address &apos;myapp-clusterip2&apos;~ # ~ # wget myapp-clusterip2 -O mytest.htmlwget: bad address &apos;myapp-clusterip2&apos; 实现跨namespace与Service通信通过Service的ExternalName类型即可实现跨namespace名称空间与Service通信。 Service域名格式：$(service name).$(namespace).svc.cluster.local，其中 cluster.local 为指定的集群的域名 相关yaml文件 12345678910111213141516171819202122232425262728293031[root@k8s-master cross_ns]# pwd/root/k8s_practice/cross_ns[root@k8s-master cross_ns]# [root@k8s-master cross_ns]# cat svc_ExternalName_visit.yaml # 实现 myns 名称空间的pod，访问 mytest 名称空间的Service：myapp-clusterip2apiVersion: v1kind: Servicemetadata: name: myapp-clusterip1-externalname namespace: mynsspec: type: ExternalName externalName: myapp-clusterip2.mytest.svc.cluster.local ports: - name: http port: 80 targetPort: 80---# 实现 mytest 名称空间的Pod，访问 myns 名称空间的Service：myapp-clusterip1apiVersion: v1kind: Servicemetadata: name: myapp-clusterip2-externalname namespace: mytestspec: type: ExternalName externalName: myapp-clusterip1.myns.svc.cluster.local ports: - name: http port: 80 targetPort: 80 运行yaml文件 123456[root@k8s-master cross_ns]# kubectl apply -f svc_ExternalName_visit.yaml[root@k8s-master cross_ns]# [root@k8s-master cross_ns]# kubectl get svc -A -o wide | grep -E &apos;(ExternalName)|(NAME)&apos; NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORmyns myapp-clusterip1-externalname ExternalName &lt;none&gt; myapp-clusterip2.mytest.svc.cluster.local 80/TCP 28s &lt;none&gt;mytest myapp-clusterip2-externalname ExternalName &lt;none&gt; myapp-clusterip1.myns.svc.cluster.local 80/TCP 28s &lt;none&gt; pod跨名称空间namespace与Service通信到目前所有service和pod信息查看 12345678910111213[root@k8s-master cross_ns]# kubectl get svc -A -o wide | grep -E &apos;(my)|(NAME)&apos;NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORmyns myapp-clusterip1 ClusterIP 10.100.61.11 &lt;none&gt; 80/TCP 62m app=myapp,release=v1myns myapp-clusterip1-externalname ExternalName &lt;none&gt; myapp-clusterip2.mytest.svc.cluster.local 80/TCP 84s &lt;none&gt;mytest myapp-clusterip2 ClusterIP 10.100.201.103 &lt;none&gt; 80/TCP 62m app=myapp,release=v2mytest myapp-clusterip2-externalname ExternalName &lt;none&gt; myapp-clusterip1.myns.svc.cluster.local 80/TCP 84s &lt;none&gt;[root@k8s-master cross_ns]# [root@k8s-master cross_ns]# kubectl get pod -A -o wide | grep -E &apos;(my)|(NAME)&apos;NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmyns myapp-deploy1-5b9d78576c-wfw4n 1/1 Running 0 62m 10.244.2.136 k8s-node02 &lt;none&gt; &lt;none&gt;myns myapp-deploy1-5b9d78576c-zsfjl 1/1 Running 0 62m 10.244.3.193 k8s-node01 &lt;none&gt; &lt;none&gt;mytest myapp-deploy2-dc8f96497-nnkqn 1/1 Running 0 62m 10.244.3.194 k8s-node01 &lt;none&gt; &lt;none&gt;mytest myapp-deploy2-dc8f96497-w47dt 1/1 Running 0 62m 10.244.2.137 k8s-node02 &lt;none&gt; &lt;none&gt; myns 名称空间的pod，访问 mytest 名称空间的Service：myapp-clusterip2 1234567891011121314151617181920212223242526272829[root@k8s-master cross_ns]# kubectl exec -it -n myns myapp-deploy1-5b9d78576c-wfw4n sh/ # cd /root/### 如下说明在同一名称空间下，通信无问题~ # ping myapp-clusterip1 PING myapp-clusterip1 (10.100.61.11): 56 data bytes64 bytes from 10.100.61.11: seq=0 ttl=64 time=0.057 ms64 bytes from 10.100.61.11: seq=1 ttl=64 time=0.071 ms………………~ # ~ # wget myapp-clusterip1 -O myns.htmlConnecting to myapp-clusterip1 (10.100.61.11:80)myns.html 100%~ # ~ # cat myns.html Hello MyApp | Version: v1 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;### 如下说明通过Service externalname类型，实现了Pod跨namespace名称空间与Service访问~ # ping myapp-clusterip1-externalnamePING myapp-clusterip1-externalname (10.100.201.103): 56 data bytes64 bytes from 10.100.201.103: seq=0 ttl=64 time=0.050 ms64 bytes from 10.100.201.103: seq=1 ttl=64 time=0.311 ms………………~ # ~ # wget myapp-clusterip1-externalname -O mytest.htmlConnecting to myapp-clusterip1-externalname (10.100.201.103:80)mytest.html 100%~ # ~ # cat mytest.html Hello MyApp | Version: v2 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt; mytest 名称空间的Pod，访问 myns 名称空间的Service：myapp-clusterip1 1234567891011121314151617181920212223242526272829[root@k8s-master cross_ns]# kubectl exec -it -n mytest myapp-deploy2-dc8f96497-w47dt sh/ # cd /root/### 如下说明在同一名称空间下，通信无问题~ # ping myapp-clusterip2 PING myapp-clusterip2 (10.100.201.103): 56 data bytes64 bytes from 10.100.201.103: seq=0 ttl=64 time=0.087 ms64 bytes from 10.100.201.103: seq=1 ttl=64 time=0.073 ms………………~ # ~ # wget myapp-clusterip2 -O mytest.htmlConnecting to myapp-clusterip2 (10.100.201.103:80)mytest.html 100%~ # ~ # cat mytest.html Hello MyApp | Version: v2 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;### 如下说明通过Service externalname类型，实现了Pod跨namespace名称空间与Service访问~ # ping myapp-clusterip2-externalnamePING myapp-clusterip2-externalname (10.100.61.11): 56 data bytes64 bytes from 10.100.61.11: seq=0 ttl=64 time=0.089 ms64 bytes from 10.100.61.11: seq=1 ttl=64 time=0.071 ms………………~ # ~ # wget myapp-clusterip2-externalname -O myns.htmlConnecting to myapp-clusterip2-externalname (10.100.61.11:80)myns.html 100%~ # ~ # cat myns.html Hello MyApp | Version: v1 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt; 由上可见，实现了Pod跨namespace名称空间与Service访问。 完毕！]]></content>
      <categories>
        <category>Kubernetes</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
        <tag>K8S</tag>
        <tag>CI/CD</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes K8S在IPVS代理模式下Service服务的ClusterIP类型访问失败处理]]></title>
    <url>%2F2020%2F09%2F08%2Fkubernetes13%2F</url>
    <content type="text"><![CDATA[Kubernetes K8S使用IPVS代理模式，当Service的类型为ClusterIP时，如何处理访问service却不能访问后端pod的情况。 背景现象Kubernetes K8S使用IPVS代理模式，当Service的类型为ClusterIP时，出现访问service却不能访问后端pod的情况。 主机配置规划 服务器名称(hostname) 系统版本 配置 内网IP 外网IP(模拟) k8s-master CentOS7.7 2C/4G/20G 172.16.1.110 10.0.0.110 k8s-node01 CentOS7.7 2C/4G/20G 172.16.1.111 10.0.0.111 k8s-node02 CentOS7.7 2C/4G/20G 172.16.1.112 10.0.0.112 场景复现Deployment的yaml信息yaml文件 12345678910111213141516171819202122232425262728[root@k8s-master service]# pwd/root/k8s_practice/service[root@k8s-master service]# cat myapp-deploy.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: myapp-deploy namespace: defaultspec: replicas: 3 selector: matchLabels: app: myapp release: v1 template: metadata: labels: app: myapp release: v1 env: test spec: containers: - name: myapp image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 启动Deployment并查看状态 123456789101112131415[root@k8s-master service]# kubectl apply -f myapp-deploy.yaml deployment.apps/myapp-deploy created[root@k8s-master service]# [root@k8s-master service]# kubectl get deploy -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORmyapp-deploy 3/3 3 3 14s myapp registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp,release=v1[root@k8s-master service]# kubectl get rs -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTORmyapp-deploy-5695bb5658 3 3 3 21s myapp registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 app=myapp,pod-template-hash=5695bb5658,release=v1[root@k8s-master service]#[root@k8s-master service]# kubectl get pod -o wide --show-labelsNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES LABELSmyapp-deploy-5695bb5658-7tgfx 1/1 Running 0 39s 10.244.2.111 k8s-node02 &lt;none&gt; &lt;none&gt; app=myapp,env=test,pod-template-hash=5695bb5658,release=v1myapp-deploy-5695bb5658-95zxm 1/1 Running 0 39s 10.244.3.165 k8s-node01 &lt;none&gt; &lt;none&gt; app=myapp,env=test,pod-template-hash=5695bb5658,release=v1myapp-deploy-5695bb5658-xtxbp 1/1 Running 0 39s 10.244.3.164 k8s-node01 &lt;none&gt; &lt;none&gt; app=myapp,env=test,pod-template-hash=5695bb5658,release=v1 curl访问 12345678[root@k8s-master service]# curl 10.244.2.111/hostname.htmlmyapp-deploy-5695bb5658-7tgfx[root@k8s-master service]# [root@k8s-master service]# curl 10.244.3.165/hostname.htmlmyapp-deploy-5695bb5658-95zxm[root@k8s-master service]# [root@k8s-master service]# curl 10.244.3.164/hostname.htmlmyapp-deploy-5695bb5658-xtxbp Service的ClusterIP类型信息yaml文件 1234567891011121314151617[root@k8s-master service]# pwd/root/k8s_practice/service[root@k8s-master service]# cat myapp-svc-ClusterIP.yaml apiVersion: v1kind: Servicemetadata: name: myapp-clusterip namespace: defaultspec: type: ClusterIP # 可以不写，为默认类型 selector: app: myapp release: v1 ports: - name: http port: 8080 # 对外暴露端口 targetPort: 80 # 转发到后端端口 启动Service并查看状态 1234567[root@k8s-master service]# kubectl apply -f myapp-svc-ClusterIP.yaml service/myapp-clusterip created[root@k8s-master service]# [root@k8s-master service]# kubectl get svc -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 16d &lt;none&gt;myapp-clusterip ClusterIP 10.102.246.104 &lt;none&gt; 8080/TCP 6s app=myapp,release=v1 查看ipvs信息123456789[root@k8s-master service]# ipvsadm -LnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConn………………TCP 10.102.246.104:8080 rr -&gt; 10.244.2.111:80 Masq 1 0 0 -&gt; 10.244.3.164:80 Masq 1 0 0 -&gt; 10.244.3.165:80 Masq 1 0 0 由此可见，正常情况下：当我们访问Service时，访问链路是能够传递到后端的Pod并返回信息。 Curl访问结果直接访问Pod，如下所示是能够正常访问的。 12345678[root@k8s-master service]# curl 10.244.2.111/hostname.htmlmyapp-deploy-5695bb5658-7tgfx[root@k8s-master service]# [root@k8s-master service]# curl 10.244.3.165/hostname.htmlmyapp-deploy-5695bb5658-95zxm[root@k8s-master service]# [root@k8s-master service]# curl 10.244.3.164/hostname.htmlmyapp-deploy-5695bb5658-xtxbp 但通过Service访问结果异常，信息如下。 12[root@k8s-master service]# curl 10.102.246.104:8080curl: (7) Failed connect to 10.102.246.104:8080; Connection timed out 处理过程抓包核实使用如下命令进行抓包，并通过Wireshark工具进行分析。 1tcpdump -i any -n -nn port 80 -w ./$(date +%Y%m%d%H%M%S).pcap 结果如下图： 可见，已经向Pod发了请求，但是没有得到回复。结果TCP又重传了【TCP Retransmission】。 查看kube-proxy日志12345678910111213141516171819202122[root@k8s-master service]# kubectl get pod -A | grep &apos;kube-proxy&apos;kube-system kube-proxy-6bfh7 1/1 Running 1 3h52mkube-system kube-proxy-6vfkf 1/1 Running 1 3h52mkube-system kube-proxy-bvl9n 1/1 Running 1 3h52m[root@k8s-master service]# [root@k8s-master service]# kubectl logs -n kube-system kube-proxy-6bfh7W0601 13:01:13.170506 1 feature_gate.go:235] Setting GA feature gate SupportIPVSProxyMode=true. It will be removed in a future release.I0601 13:01:13.338922 1 node.go:135] Successfully retrieved node IP: 172.16.1.112I0601 13:01:13.338960 1 server_others.go:172] Using ipvs Proxier. ##### 可见使用的是ipvs模式W0601 13:01:13.339400 1 proxier.go:420] IPVS scheduler not specified, use rr by defaultI0601 13:01:13.339638 1 server.go:571] Version: v1.17.4I0601 13:01:13.340126 1 conntrack.go:100] Set sysctl &apos;net/netfilter/nf_conntrack_max&apos; to 131072I0601 13:01:13.340159 1 conntrack.go:52] Setting nf_conntrack_max to 131072I0601 13:01:13.340500 1 conntrack.go:83] Setting conntrack hashsize to 32768I0601 13:01:13.346991 1 conntrack.go:100] Set sysctl &apos;net/netfilter/nf_conntrack_tcp_timeout_established&apos; to 86400I0601 13:01:13.347035 1 conntrack.go:100] Set sysctl &apos;net/netfilter/nf_conntrack_tcp_timeout_close_wait&apos; to 3600I0601 13:01:13.347703 1 config.go:313] Starting service config controllerI0601 13:01:13.347718 1 shared_informer.go:197] Waiting for caches to sync for service configI0601 13:01:13.347736 1 config.go:131] Starting endpoints config controllerI0601 13:01:13.347743 1 shared_informer.go:197] Waiting for caches to sync for endpoints configI0601 13:01:13.448223 1 shared_informer.go:204] Caches are synced for endpoints config I0601 13:01:13.448236 1 shared_informer.go:204] Caches are synced for service config 可见kube-proxy日志无异常 网卡设置并修改备注：在k8s-master节点操作的 之后进一步搜索表明，这可能是由于“Checksum offloading” 造成的。信息如下： 12345678[root@k8s-master service]# ethtool -k flannel.1 | grep checksumrx-checksumming: ontx-checksumming: on ##### 当前为 on tx-checksum-ipv4: off [fixed] tx-checksum-ip-generic: on ##### 当前为 on tx-checksum-ipv6: off [fixed] tx-checksum-fcoe-crc: off [fixed] tx-checksum-sctp: off [fixed] flannel的网络设置将发送端的checksum打开了，而实际应该关闭，从而让物理网卡校验。操作如下： 123456789101112131415161718192021# 临时关闭操作[root@k8s-master service]# ethtool -K flannel.1 tx-checksum-ip-generic off Actual changes:tx-checksumming: off tx-checksum-ip-generic: offtcp-segmentation-offload: off tx-tcp-segmentation: off [requested on] tx-tcp-ecn-segmentation: off [requested on] tx-tcp6-segmentation: off [requested on] tx-tcp-mangleid-segmentation: off [requested on]udp-fragmentation-offload: off [requested on][root@k8s-master service]# # 再次查询结果[root@k8s-master service]# ethtool -k flannel.1 | grep checksumrx-checksumming: ontx-checksumming: off ##### 当前为 off tx-checksum-ipv4: off [fixed] tx-checksum-ip-generic: off ##### 当前为 off tx-checksum-ipv6: off [fixed] tx-checksum-fcoe-crc: off [fixed] tx-checksum-sctp: off [fixed] 当然上述操作只能临时生效。机器重启后flannel虚拟网卡还会开启Checksum校验。 之后我们再次curl尝试 1234567891011121314[root@k8s-master ~]# curl 10.102.246.104:8080Hello MyApp | Version: v1 | &lt;a href=&quot;hostname.html&quot;&gt;Pod Name&lt;/a&gt;[root@k8s-master ~]# [root@k8s-master ~]# curl 10.102.246.104:8080/hostname.htmlmyapp-deploy-5695bb5658-7tgfx[root@k8s-master ~]# [root@k8s-master ~]# curl 10.102.246.104:8080/hostname.htmlmyapp-deploy-5695bb5658-95zxm[root@k8s-master ~]# [root@k8s-master ~]# curl 10.102.246.104:8080/hostname.htmlmyapp-deploy-5695bb5658-xtxbp[root@k8s-master ~]# [root@k8s-master ~]# curl 10.102.246.104:8080/hostname.htmlmyapp-deploy-5695bb5658-7tgfx 由上可见，能够正常访问了。 永久关闭flannel网卡发送校验备注：所有机器都操作 使用以下代码创建服务 1234567891011[root@k8s-node02 ~]# cat /etc/systemd/system/k8s-flannel-tx-checksum-off.service [Unit]Description=Turn off checksum offload on flannel.1After=sys-devices-virtual-net-flannel.1.device[Install]WantedBy=sys-devices-virtual-net-flannel.1.device[Service]Type=oneshotExecStart=/sbin/ethtool -K flannel.1 tx-checksum-ip-generic off 开机自启动，并启动服务 12systemctl enable k8s-flannel-tx-checksum-offsystemctl start k8s-flannel-tx-checksum-off 相关阅读1、关于k8s的ipvs转发svc服务访问慢的问题分析(一) 2、Kubernetes + Flannel: UDP packets dropped for wrong checksum – Workaround]]></content>
      <categories>
        <category>Kubernetes</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
        <tag>K8S</tag>
        <tag>CI/CD</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes K8S之资源控制器Job和CronJob详解]]></title>
    <url>%2F2020%2F09%2F06%2Fkubernetes12%2F</url>
    <content type="text"><![CDATA[Kubernetes的资源控制器Job和CronJob详解与示例 主机配置规划 服务器名称(hostname) 系统版本 配置 内网IP 外网IP(模拟) k8s-master CentOS7.7 2C/4G/20G 172.16.1.110 10.0.0.110 k8s-node01 CentOS7.7 2C/4G/20G 172.16.1.111 10.0.0.111 k8s-node02 CentOS7.7 2C/4G/20G 172.16.1.112 10.0.0.112 什么是控制器kubernetes中内建了很多controller（控制器），这些相当于一个状态机，用来控制pod的具体状态和行为。 部分控制器类型如下： ReplicationController 和 ReplicaSet Deployment DaemonSet StatefulSet Job/CronJob HorizontalPodAutoscaler Job负责批处理任务 Job创建一个或多个Pod，并确保指定数量的Pod成功终止。Pod成功完成后，Job将跟踪成功完成的情况。当达到指定的成功完成次数时，任务（即Job）就完成了。删除Job将清除其创建的Pod。 一个简单的情况是创建一个Job对象，以便可靠地运行一个Pod来完成。如果第一个Pod发生故障或被删除（例如，由于节点硬件故障或节点重启），则Job对象将启动一个新的Pod。 当然还可以使用Job并行运行多个Pod。 Job终止和清理Job完成后，不会再创建其他Pod，但是Pod也不会被删除。这样使我们仍然可以查看已完成容器的日志，以检查是否有错误、警告或其他诊断输出。Job对象在完成后也将保留下来，以便您查看其状态。 当我们删除Job对象时，对应的pod也会被删除。 特殊说明 单个Pod时，默认Pod成功运行后Job即结束 restartPolicy 仅支持Never和OnFailure .spec.completions 标识Job结束所需要成功运行的Pod个数，默认为1 .spec.parallelism 标识并行运行的Pod个数，默认为1 .spec.activeDeadlineSeconds 为Job的持续时间，不管有多少Pod创建。一旦工作到指定时间，所有的运行pod都会终止且工作状态将成为type: Failed与reason: DeadlineExceeded。 CronJobCron Job 创建是基于时间调度的 Jobs 一个 CronJob 对象就像 crontab (cron table) 文件中的一行。它用 Cron 格式进行编写，并周期性地在给定的调度时间执行 Job。 CronJob 限制CronJob 创建 Job 对象，每个 Job 的执行次数大约为一次。 之所以说 “大约” ，是因为在某些情况下，可能会创建两个 Job，或者不会创建任何 Job。虽然试图使这些情况尽量少发生，但不能完全杜绝。因此，Job 应该是幂等的。 CronJob 仅负责创建与其调度时间相匹配的 Job，而 Job 又负责管理其代表的 Pod。 使用案例： 1、在给定时间点调度Job 2、创建周期性运行的Job。如：数据备份、数仓导数、执行任务、邮件发送、数据拉取、数据推送 特殊说明.spec.schedule 必选，任务被创建和执行的调度时间。同Cron格式串，例如 0 * * * *。 .spec.jobTemplate 必选，任务模版。它和 Job的语法完全一样 .spec.startingDeadlineSeconds 可选的。默认未设置。它表示任务如果由于某种原因错过了调度时间，开始该任务的截止时间的秒数。过了截止时间，CronJob 就不会开始任务。不满足这种最后期限的任务会被统计为失败任务。如果没有该声明，那任务就没有最后期限。 .spec.concurrencyPolicy 可选的。它声明了 CronJob 创建的任务执行时发生重叠如何处理。spec 仅能声明下列规则中的一种： Allow (默认)：CronJob 允许并发任务执行。 Forbid：CronJob 不允许并发任务执行；如果新任务的执行时间到了而老任务没有执行完，CronJob 会忽略新任务的执行。 Replace：如果新任务的执行时间到了而老任务没有执行完，CronJob 会用新任务替换当前正在运行的任务。 请注意，并发性规则仅适用于相同 CronJob 创建的任务。如果有多个 CronJob，它们相应的任务总是允许并发执行的。 .spec.suspend 可选的。如果设置为 true ，后续发生的执行都会挂起。这个设置对已经开始执行的Job不起作用。默认是关闭的false。备注：在调度时间内挂起的执行都会被统计为错过的任务。当 .spec.suspend 从 true 改为 false 时，且没有开始的最后期限，错过的任务会被立即调度。 .spec.successfulJobsHistoryLimit 和 .spec.failedJobsHistoryLimit 可选的。 这两个声明了有多少执行完成和失败的任务会被保留。默认设置为3和1。限制设置为0代表相应类型的任务完成后不会保留。 说明：如果 startingDeadlineSeconds 设置为很大的数值或未设置（默认），并且 concurrencyPolicy 设置为 Allow，则作业将始终至少运行一次。 Job示例yaml文件 1234567891011121314151617[root@k8s-master controller]# pwd/root/k8s_practice/controller[root@k8s-master controller]# cat job.yaml apiVersion: batch/v1kind: Jobmetadata: name: pispec: #completions: 3 # 标识Job结束所需要成功运行的Pod个数，默认为1 template: spec: containers: - name: pi image: registry.cn-beijing.aliyuncs.com/google_registry/perl:5.26 command: [&quot;perl&quot;, &quot;-Mbignum=bpi&quot;, &quot;-wle&quot;, &quot;print bpi(2000)&quot;] restartPolicy: Never backoffLimit: 4 创建job，与状态查看 12345678[root@k8s-master controller]# kubectl apply -f job.yaml job.batch/pi created[root@k8s-master controller]# kubectl get job -o wideNAME COMPLETIONS DURATION AGE CONTAINERS IMAGES SELECTORpi 0/1 16s 16s pi registry.cn-beijing.aliyuncs.com/google_registry/perl:5.26 controller-uid=77004357-fd5e-4395-9bbb-cd0698e19cb9[root@k8s-master controller]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpi-6zvm5 0/1 ContainerCreating 0 85s &lt;none&gt; k8s-node01 &lt;none&gt; &lt;none&gt; 之后再次查看 1234567891011121314151617181920212223242526272829303132333435363738394041[root@k8s-master controller]# kubectl get job -o wideNAME COMPLETIONS DURATION AGE CONTAINERS IMAGES SELECTORpi 1/1 14m 44m pi registry.cn-beijing.aliyuncs.com/google_registry/perl:5.26 controller-uid=77004357-fd5e-4395-9bbb-cd0698e19cb9[root@k8s-master controller]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpi-6zvm5 0/1 Completed 0 44m 10.244.4.63 k8s-node01 &lt;none&gt; &lt;none&gt;[root@k8s-master controller]# [root@k8s-master controller]# kubectl describe job piName: piNamespace: defaultSelector: controller-uid=76680f6f-442c-4a09-91dc-c3d4c18465b0Labels: controller-uid=76680f6f-442c-4a09-91dc-c3d4c18465b0 job-name=piAnnotations: kubectl.kubernetes.io/last-applied-configuration: &#123;&quot;apiVersion&quot;:&quot;batch/v1&quot;,&quot;kind&quot;:&quot;Job&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;pi&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;,&quot;spec&quot;:&#123;&quot;backoffLimit&quot;:4,&quot;Parallelism: 1Completions: 1Start Time: Tue, 11 Aug 2020 23:34:44 +0800Completed At: Tue, 11 Aug 2020 23:35:02 +0800Duration: 18sPods Statuses: 0 Running / 1 Succeeded / 0 FailedPod Template: Labels: controller-uid=76680f6f-442c-4a09-91dc-c3d4c18465b0 job-name=pi Containers: pi: Image: registry.cn-beijing.aliyuncs.com/google_registry/perl:5.26 Port: &lt;none&gt; Host Port: &lt;none&gt; Command: perl -Mbignum=bpi -wle print bpi(2000) Environment: &lt;none&gt; Mounts: &lt;none&gt; Volumes: &lt;none&gt;Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 2m33s job-controller Created pod: pi-6zvm5 并查看 Pod 的标准输出 12[root@k8s-master controller]# kubectl logs --tail 500 pi-6zvm53.141592653589793238462643383279502884197169399375105820974944592307816406……………… CronJob示例yaml文件 123456789101112131415161718192021[root@k8s-master controller]# pwd/root/k8s_practice/controller[root@k8s-master controller]# cat cronjob.yaml apiVersion: batch/v1beta1kind: CronJobmetadata: name: hellospec: schedule: &quot;*/1 * * * *&quot; jobTemplate: spec: template: spec: containers: - name: hello image: registry.cn-beijing.aliyuncs.com/google_registry/busybox:1.24 args: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure 启动cronjob并查看状态 12345678910111213[root@k8s-master controller]# kubectl apply -f cronjob.yaml cronjob.batch/hello created[root@k8s-master controller]# kubectl get cronjob -o wideNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE CONTAINERS IMAGES SELECTORhello */1 * * * * False 1 8s 27s hello registry.cn-beijing.aliyuncs.com/google_registry/busybox:1.24 &lt;none&gt;[root@k8s-master controller]# [root@k8s-master controller]# kubectl get job -o wideNAME COMPLETIONS DURATION AGE CONTAINERS IMAGES SELECTORhello-1590721020 1/1 2s 21s hello registry.cn-beijing.aliyuncs.com/google_registry/busybox:1.24 controller-uid=9e0180e8-8362-4a58-8b93-089b92774b5e[root@k8s-master controller]# [root@k8s-master controller]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEShello-1590721020-m4fr8 0/1 Completed 0 36s 10.244.4.66 k8s-node01 &lt;none&gt; &lt;none&gt; 几分钟之后的状态信息 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273[root@k8s-master controller]# kubectl get cronjob -o wideNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE CONTAINERS IMAGES SELECTORhello */1 * * * * False 0 55s 7m14s hello registry.cn-beijing.aliyuncs.com/google_registry/busybox:1.24 &lt;none&gt;[root@k8s-master controller]# [root@k8s-master controller]# [root@k8s-master controller]# kubectl get job -o wideNAME COMPLETIONS DURATION AGE CONTAINERS IMAGES SELECTORhello-1590721260 1/1 1s 3m1s hello registry.cn-beijing.aliyuncs.com/google_registry/busybox:1.24 controller-uid=0676bd6d-861b-440b-945b-4b2704872728hello-1590721320 1/1 2s 2m1s hello registry.cn-beijing.aliyuncs.com/google_registry/busybox:1.24 controller-uid=09c1902e-76ef-4731-b3b4-3188961c13e9hello-1590721380 1/1 2s 61s hello registry.cn-beijing.aliyuncs.com/google_registry/busybox:1.24 controller-uid=f30dc159-8905-4cfc-b06b-f950c8dcfc28[root@k8s-master controller]# [root@k8s-master controller]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEShello-1590721320-m4pxf 0/1 Completed 0 2m6s 10.244.4.70 k8s-node01 &lt;none&gt; &lt;none&gt;hello-1590721380-wk7jh 0/1 Completed 0 66s 10.244.2.77 k8s-node02 &lt;none&gt; &lt;none&gt;hello-1590721440-rcx7v 0/1 Completed 0 6s 10.244.4.72 k8s-node01 &lt;none&gt; &lt;none&gt;[root@k8s-master controller]# [root@k8s-master controller]# kubectl describe cronjob helloName: helloNamespace: defaultLabels: &lt;none&gt;Annotations: kubectl.kubernetes.io/last-applied-configuration: &#123;&quot;apiVersion&quot;:&quot;batch/v1beta1&quot;,&quot;kind&quot;:&quot;CronJob&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;hello&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;,&quot;spec&quot;:&#123;&quot;jobTemplate&quot;:&#123;&quot;...Schedule: */1 * * * *Concurrency Policy: AllowSuspend: FalseSuccessful Job History Limit: 3Failed Job History Limit: 1Starting Deadline Seconds: &lt;unset&gt;Selector: &lt;unset&gt;Parallelism: &lt;unset&gt;Completions: &lt;unset&gt;Pod Template: Labels: &lt;none&gt; Containers: hello: Image: registry.cn-beijing.aliyuncs.com/google_registry/busybox:1.24 Port: &lt;none&gt; Host Port: &lt;none&gt; Args: /bin/sh -c date; echo Hello from the Kubernetes cluster Environment: &lt;none&gt; Mounts: &lt;none&gt; Volumes: &lt;none&gt;Last Schedule Time: Wed, 12 Aug 2020 00:01:00 +0800Active Jobs: &lt;none&gt;Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 19m cronjob-controller Created job hello-1597160520 Normal SawCompletedJob 19m cronjob-controller Saw completed job: hello-1597160520, status: Complete Normal SuccessfulCreate 18m cronjob-controller Created job hello-1597160580 Normal SawCompletedJob 18m cronjob-controller Saw completed job: hello-1597160580, status: Complete Normal SuccessfulCreate 17m cronjob-controller Created job hello-1597160640 Normal SawCompletedJob 17m cronjob-controller Saw completed job: hello-1597160640, status: Complete Normal SuccessfulCreate 16m cronjob-controller Created job hello-1597160700 Normal SuccessfulDelete 16m cronjob-controller Deleted job hello-1597160520 Normal SawCompletedJob 16m cronjob-controller Saw completed job: hello-1597160700, status: Complete Normal SuccessfulCreate 15m cronjob-controller Created job hello-1597160760 Normal SawCompletedJob 15m cronjob-controller Saw completed job: hello-1597160760, status: Complete Normal SuccessfulDelete 15m cronjob-controller Deleted job hello-1597160580 Normal SuccessfulCreate 14m cronjob-controller Created job hello-1597160820 Normal SuccessfulDelete 14m cronjob-controller Deleted job hello-1597160640 Normal SawCompletedJob 14m cronjob-controller Saw completed job: hello-1597160820, status: Complete Normal SuccessfulCreate 13m cronjob-controller Created job hello-1597160880 Normal SawCompletedJob 13m cronjob-controller Saw completed job: hello-1597160880, status: Complete……………… Normal SawCompletedJob 11m cronjob-controller Saw completed job: hello-1597161000, status: Complete Normal SuccessfulDelete 11m cronjob-controller Deleted job hello-1597160820 Normal SawCompletedJob 10m cronjob-controller (combined from similar events): Saw completed job: hello-1597161060, status: Complete Normal SuccessfulCreate 4m13s (x7 over 10m) cronjob-controller (combined from similar events): Created job hello-1597161420 找到最后一次调度任务创建的 Pod， 并查看 Pod 的标准输出。请注意任务名称和 Pod 名称是不同的。 123[root@k8s-master controller]# kubectl logs pod/hello-1590721740-rcx7v # 或者 kubectl logs hello-1590721740-rcx7vFri May 29 03:09:04 UTC 2020Hello from the Kubernetes cluster 删除 CronJob 12345678[root@k8s-master controller]# kubectl get cronjobNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGEhello */1 * * * * False 0 32s 19m[root@k8s-master controller]# [root@k8s-master controller]# kubectl delete cronjob hello # 或者 kubectl delete -f cronjob.yamlcronjob.batch &quot;hello&quot; deleted[root@k8s-master controller]# kubectl get cronjob # 可见已删除No resources found in default namespace. 相关阅读1、Kubernetes K8S之资源控制器RC、RS、Deployment详解 2、Kubernetes K8S之资源控制器StatefulSets详解 3、Kubernetes K8S之资源控制器Daemonset详解 4、官网：Jobs 5、官网：CronJob]]></content>
      <categories>
        <category>Kubernetes</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
        <tag>K8S</tag>
        <tag>CI/CD</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes K8S之资源控制器Daemonset详解]]></title>
    <url>%2F2020%2F09%2F02%2Fkubernetes11%2F</url>
    <content type="text"><![CDATA[Kubernetes的资源控制器Daemonset详解与示例 主机配置规划 服务器名称(hostname) 系统版本 配置 内网IP 外网IP(模拟) k8s-master CentOS7.7 2C/4G/20G 172.16.1.110 10.0.0.110 k8s-node01 CentOS7.7 2C/4G/20G 172.16.1.111 10.0.0.111 k8s-node02 CentOS7.7 2C/4G/20G 172.16.1.112 10.0.0.112 什么是控制器kubernetes中内建了很多controller（控制器），这些相当于一个状态机，用来控制pod的具体状态和行为。 部分控制器类型如下： ReplicationController 和 ReplicaSet Deployment DaemonSet StatefulSet Job/CronJob HorizontalPodAutoscaler DaemonSetDaemonSet 确保全部（或者某些）节点上运行一个 Pod 的副本。当有节点加入集群时，会为他们新增一个 Pod。当有节点从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。 DaemonSet 的一些典型用法： 在每个节点上运行集群存储 DaemonSet，例如 glusterd、ceph。 在每个节点上运行日志收集 DaemonSet，例如 fluentd、logstash。 在每个节点上运行监控 DaemonSet，例如 Prometheus Node Exporter、Flowmill、Sysdig 代理、collectd、Dynatrace OneAgent、AppDynamics 代理、Datadog 代理、New Relic 代理、Ganglia gmond 或者 Instana 代理。 一个简单的用法是在所有的节点上都启动一个 DaemonSet，并作为每种类型的 daemon 使用。 一个稍微复杂的用法是单独对每种 daemon 类型使用一种DaemonSet。这样有多个 DaemonSet，但具有不同的标识，并且对不同硬件类型具有不同的内存、CPU 要求。 备注：DaemonSet 中的 Pod 可以使用 hostPort，从而可以通过节点 IP 访问到 Pod；因为DaemonSet模式下Pod不会被调度到其他节点。使用示例如下： 1234567ports:- name: httpd containerPort: 80 #除非绝对必要，否则不要为 Pod 指定 hostPort。 将 Pod 绑定到hostPort时，它会限制 Pod 可以调度的位置数；DaemonSet除外 #一般情况下 containerPort与hostPort值相同 hostPort: 8090 #可以通过宿主机+hostPort的方式访问该Pod。例如：pod在/调度到了k8s-node02【172.16.1.112】，那么该Pod可以通过172.16.1.112:8090方式进行访问。 protocol: TCP 可参见：「Kubernetes K8S之通过yaml创建pod与pod文件常用字段详解」 DaemonSet示例yaml文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748[root@k8s-master controller]# pwd/root/k8s_practice/controller[root@k8s-master controller]# cat daemonset.yaml apiVersion: apps/v1kind: DaemonSetmetadata: name: fluentd-elasticsearch namespace: default labels: k8s-app: fluentd-loggingspec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: tolerations: # 允许在master节点运行 - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: fluentd-elasticsearch image: registry.cn-beijing.aliyuncs.com/google_registry/fluentd:v2.5.2 resources: limits: cpu: 1 memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true # 优雅关闭应用，时间设置。超过该时间会强制关闭【可选项】，默认30秒 terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers 运行daemonset，并查看状态 123456789101112[root@k8s-master controller]# kubectl apply -f daemonset.yaml daemonset.apps/fluentd-elasticsearch created[root@k8s-master controller]# [root@k8s-master controller]# kubectl get daemonset -o wideNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE CONTAINERS IMAGES SELECTORfluentd-elasticsearch 3 3 3 3 3 &lt;none&gt; 92s fluentd-elasticsearch registry.cn-beijing.aliyuncs.com/google_registry/fluentd:v2.5.2 name=fluentd-elasticsearch[root@k8s-master controller]# [root@k8s-master controller]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESfluentd-elasticsearch-52b8z 1/1 Running 0 95s 10.244.2.92 k8s-node02 &lt;none&gt; &lt;none&gt;fluentd-elasticsearch-fps95 1/1 Running 0 95s 10.244.0.46 k8s-master &lt;none&gt; &lt;none&gt;fluentd-elasticsearch-pz8j7 1/1 Running 0 95s 10.244.4.83 k8s-node01 &lt;none&gt; &lt;none&gt; 由上可见，在k8s集群所有节点包括master节点都运行了daemonset的pod。 相关阅读1、Kubernetes K8S之通过yaml创建pod与pod文件常用字段详解 2、Kubernetes K8S之资源控制器RC、RS、Deployment详解 3、Kubernetes K8S之资源控制器StatefulSets详解]]></content>
      <categories>
        <category>Kubernetes</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
        <tag>K8S</tag>
        <tag>CI/CD</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes K8S之资源控制器StatefulSets详解]]></title>
    <url>%2F2020%2F08%2F26%2Fkubernetes10%2F</url>
    <content type="text"><![CDATA[Kubernetes的资源控制器StatefulSet详解与示例 主机配置规划 服务器名称(hostname) 系统版本 配置 内网IP 外网IP(模拟) k8s-master CentOS7.7 2C/4G/20G 172.16.1.110 10.0.0.110 k8s-node01 CentOS7.7 2C/4G/20G 172.16.1.111 10.0.0.111 k8s-node02 CentOS7.7 2C/4G/20G 172.16.1.112 10.0.0.112 什么是控制器kubernetes中内建了很多controller（控制器），这些相当于一个状态机，用来控制pod的具体状态和行为。 部分控制器类型如下： ReplicationController 和 ReplicaSet Deployment DaemonSet StatefulSet Job/CronJob HorizontalPodAutoscaler StatefulSetStatefulSet 是用来管理有状态应用的工作负载 API 对象。 StatefulSet 中的 Pod 拥有一个具有黏性的、独一无二的身份标识。这个标识基于 StatefulSet 控制器分配给每个 Pod 的唯一顺序索引。Pod 的名称的形式为&lt;statefulset name&gt;-&lt;ordinal index&gt; 。例如：web的StatefulSet 拥有两个副本，所以它创建了两个 Pod：web-0和web-1。 和 Deployment 相同的是，StatefulSet 管理了基于相同容器定义的一组 Pod。但和 Deployment 不同的是，StatefulSet 为它们的每个 Pod 维护了一个固定的 ID。这些 Pod 是基于相同的声明来创建的，但是不能相互替换：无论怎么调度，每个 Pod 都有一个永久不变的 ID。 【使用场景】StatefulSets 对于需要满足以下一个或多个需求的应用程序很有价值： 稳定的、唯一的网络标识符，即Pod重新调度后其PodName和HostName不变【当然IP是会变的】 稳定的、持久的存储，即Pod重新调度后还是能访问到相同的持久化数据，基于PVC实现 有序的、优雅的部署和缩放 有序的、自动的滚动更新 如上面，稳定意味着 Pod 调度或重调度的整个过程是有持久性的。 如果应用程序不需要任何稳定的标识符或有序的部署、删除或伸缩，则应该使用由一组无状态的副本控制器提供的工作负载来部署应用程序，比如使用 Deployment 或者 ReplicaSet 可能更适用于无状态应用部署需要。 限制 给定 Pod 的存储必须由 PersistentVolume 驱动 基于所请求的 storage class 来提供，或者由管理员预先提供。 删除或者收缩 StatefulSet 并不会删除它关联的存储卷。这样做是为了保证数据安全，它通常比自动清除 StatefulSet 所有相关的资源更有价值。 StatefulSet 当前需要 headless 服务 来负责 Pod 的网络标识。你需要负责创建此服务。 当删除 StatefulSets 时，StatefulSet 不提供任何终止 Pod 的保证。为了实现 StatefulSet 中的 Pod 可以有序和优雅的终止，可以在删除之前将 StatefulSet 缩放为 0。 在默认 Pod 管理策略(OrderedReady) 时使用滚动更新，可能进入需要人工干预才能修复的损坏状态。 有序索引对于具有 N 个副本的 StatefulSet，StatefulSet 中的每个 Pod 将被分配一个整数序号，从 0 到 N-1，该序号在 StatefulSet 上是唯一的。 StatefulSet 中的每个 Pod 根据 StatefulSet 中的名称和 Pod 的序号来派生出它的主机名。组合主机名的格式为$(StatefulSet 名称)-$(序号)。 部署和扩缩保证 对于包含 N 个 副本的 StatefulSet，当部署 Pod 时，它们是依次创建的，顺序为 0~(N-1)。 当删除 Pod 时，它们是逆序终止的，顺序为 (N-1)~0。 在将缩放操作应用到 Pod 之前，它前面的所有 Pod 必须是 Running 和 Ready 状态。 在 Pod 终止之前，所有的继任者必须完全关闭。 StatefulSet 不应将 pod.Spec.TerminationGracePeriodSeconds 设置为 0。这种做法是不安全的，要强烈阻止。 部署顺序 在下面的 nginx 示例被创建后，会按照 web-0、web-1、web-2 的顺序部署三个 Pod。在 web-0 进入 Running 和 Ready 状态前不会部署 web-1。在 web-1 进入 Running 和 Ready 状态前不会部署 web-2。 如果 web-1 已经处于 Running 和 Ready 状态，而 web-2 尚未部署，在此期间发生了 web-0 运行失败，那么 web-2 将不会被部署，要等到 web-0 部署完成并进入 Running 和 Ready 状态后，才会部署 web-2。 收缩顺序 如果想将示例中的 StatefulSet 收缩为 replicas=1，首先被终止的是 web-2。在 web-2 没有被完全停止和删除前，web-1 不会被终止。当 web-2 已被终止和删除；但web-1 尚未被终止，如果在此期间发生 web-0 运行失败，那么就不会终止 web-1，必须等到 web-0 进入 Running 和 Ready 状态后才会终止 web-1。 StatefulSet示例说明：本次示例不涉及存储，StatefulSet的存储示例会在「Kubernetes K8S之存储PV-PVC详解」文章中演示。 yaml文件 123456789101112131415161718192021222324252627282930313233343536373839[root@k8s-master controller]# pwd/root/k8s_practice/controller[root@k8s-master controller]# cat statefulset.yaml apiVersion: v1kind: Servicemetadata: name: nginx labels: app: nginxspec: ports: - port: 80 name: http clusterIP: None selector: app: nginx---apiVersion: apps/v1kind: StatefulSetmetadata: name: webspec: selector: matchLabels: app: nginx # has to match .spec.template.metadata.labels serviceName: &quot;nginx&quot; replicas: 3 # by default is 1 template: metadata: labels: app: nginx # has to match .spec.selector.matchLabels spec: terminationGracePeriodSeconds: 10 # 默认30秒 containers: - name: nginx image: registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17 ports: - containerPort: 80 name: http 启动StatefulSet和Service，并查看状态 1234567891011121314151617[root@k8s-master controller]# kubectl apply -f statefulset.yaml service/nginx createdstatefulset.apps/web created[root@k8s-master controller]# kubectl get service -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORkubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 17d &lt;none&gt;nginx ClusterIP None &lt;none&gt; 80/TCP 87s app=nginx[root@k8s-master controller]# [root@k8s-master controller]# kubectl get statefulset -o wideNAME READY AGE CONTAINERS IMAGESweb 3/3 15m nginx registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17[root@k8s-master controller]# [root@k8s-master controller]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESweb-0 1/1 Running 0 16m 10.244.2.95 k8s-node02 &lt;none&gt; &lt;none&gt;web-1 1/1 Running 0 16m 10.244.3.103 k8s-node01 &lt;none&gt; &lt;none&gt;web-2 1/1 Running 0 16m 10.244.3.104 k8s-node01 &lt;none&gt; &lt;none&gt; 由上可见，StatefulSet 中的pod是有序的。有N个副本，那么序列号为0~(N-1)。 查看StatefulSet相关的域名信息启动一个pod 12345678910111213141516171819202122232425[root@k8s-master test]# pwd/root/k8s_practice/test[root@k8s-master test]# cat myapp_demo.yaml apiVersion: v1kind: Podmetadata: name: myapp-demo namespace: default labels: k8s-app: myappspec: containers: - name: myapp image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 imagePullPolicy: IfNotPresent ports: - name: httpd containerPort: 80 protocol: TCP[root@k8s-master test]# [root@k8s-master test]# kubectl apply -f myapp_demo.yamlpod/myapp-demo created[root@k8s-master test]# [root@k8s-master test]# kubectl get pod -o wide | grep &apos;myapp&apos;myapp-demo 1/1 Running 0 3m24s 10.244.2.101 k8s-node02 &lt;none&gt; &lt;none&gt; 进入pod并查看StatefulSet域名信息 12345678910111213141516171819202122232425262728293031# 进入一个k8s管理的myapp镜像容器。[root@k8s-master test]# kubectl exec -it myapp-demo sh/ # nslookup 10.244.2.95nslookup: can&apos;t resolve &apos;(null)&apos;: Name does not resolveName: 10.244.2.95Address 1: 10.244.2.95 web-0.nginx.default.svc.cluster.local/ # / # / # nslookup 10.244.3.103nslookup: can&apos;t resolve &apos;(null)&apos;: Name does not resolveName: 10.244.3.103Address 1: 10.244.3.103 web-1.nginx.default.svc.cluster.local/ # / # / # nslookup 10.244.3.104nslookup: can&apos;t resolve &apos;(null)&apos;: Name does not resolveName: 10.244.3.104Address 1: 10.244.3.104 web-2.nginx.default.svc.cluster.local/ # / # ##### nginx.default.svc.cluster.local 为service的域名信息/ # nslookup nginx.default.svc.cluster.localnslookup: can&apos;t resolve &apos;(null)&apos;: Name does not resolveName: nginx.default.svc.cluster.localAddress 1: 10.244.3.104 web-2.nginx.default.svc.cluster.localAddress 2: 10.244.3.103 web-1.nginx.default.svc.cluster.localAddress 3: 10.244.2.95 web-0.nginx.default.svc.cluster.local StatefulSet网络标识与PVC有上文可得如下信息： 1、匹配StatefulSet的Pod name(网络标识)的模式为：$(statefulset名称)-$(序号)，比如StatefulSet名称为web，副本数为3。则为：web-0、web-1、web-2 2、StatefulSet为每个Pod副本创建了一个DNS域名，这个域名的格式为：$(podname).(headless service name)，也就意味着服务之间是通过Pod域名来通信而非Pod IP。当Pod所在Node发生故障时，Pod会被漂移到其他Node上，Pod IP会发生改变，但Pod域名不会变化 3、StatefulSet使用Headless服务来控制Pod的域名，这个Headless服务域名的为：$(service name).$(namespace).svc.cluster.local，其中 cluster.local 指定的集群的域名 4、根据volumeClaimTemplates，为每个Pod创建一个PVC，PVC的命令规则为：$(volumeClaimTemplates name)-$(pod name)，比如volumeClaimTemplates为www，pod name为web-0、web-1、web-2；那么创建出来的PVC为：www-web-0、www-web-1、www-web-2 5、删除Pod不会删除对应的PVC，手动删除PVC将自动释放PV。 相关阅读1、Kubernetes K8S之资源控制器RC、RS、Deployment详解 完毕！]]></content>
      <categories>
        <category>Kubernetes</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
        <tag>K8S</tag>
        <tag>CI/CD</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes K8S之资源控制器RC、RS、Deployment详解]]></title>
    <url>%2F2020%2F08%2F23%2Fkubernetes09%2F</url>
    <content type="text"><![CDATA[Kubernetes的资源控制器ReplicationController（RC）、ReplicaSet（RS）、Deployment（Deploy）详解与示例 主机配置规划 服务器名称(hostname) 系统版本 配置 内网IP 外网IP(模拟) k8s-master CentOS7.7 2C/4G/20G 172.16.1.110 10.0.0.110 k8s-node01 CentOS7.7 2C/4G/20G 172.16.1.111 10.0.0.111 k8s-node02 CentOS7.7 2C/4G/20G 172.16.1.112 10.0.0.112 什么是控制器kubernetes中内建了很多controller（控制器），这些相当于一个状态机，用来控制pod的具体状态和行为。 部分控制器类型如下： ReplicationController 和 ReplicaSet Deployment DaemonSet StatefulSet Job/CronJob HorizontalPodAutoscaler ReplicationController 和 ReplicaSetReplicationController (RC)用来确保容器应用的副本数始终保持在用户定义的副本数，即如果有容器异常退出，会自动创建新的pod来替代；而异常多出来的容器也会自动回收。 在新版的Kubernetes中建议使用ReplicaSet (RS)来取代ReplicationController。ReplicaSet跟ReplicationController没有本质的不同，只是名字不一样，但ReplicaSet支持集合式selector。 虽然 ReplicaSets 可以独立使用，但如今它主要被Deployments 用作协调 Pod 的创建、删除和更新的机制。当使用 Deployment 时，你不必担心还要管理它们创建的 ReplicaSet，Deployment 会拥有并管理它们的 ReplicaSet。 ReplicaSet 是下一代的 Replication Controller。 ReplicaSet 和 Replication Controller 的唯一区别是选择器的支持。ReplicaSet 支持新的基于集合的选择器需求，这在标签用户指南中有描述。而 Replication Controller 仅支持基于相等选择器的需求。 DeploymentsDeployment 控制器为 Pods和 ReplicaSets提供描述性的更新方式。用来替代以前的ReplicationController以方便管理应用。 典型的应用场景包括： 定义Deployment来创建Pod和ReplicaSet 滚动升级和回滚应用 扩容和缩容 暂停和继续Deployment ReplicaSet示例yaml文件 123456789101112131415161718192021222324[root@k8s-master controller]# pwd/root/k8s_practice/controller[root@k8s-master controller]# cat ReplicaSet-01.yaml apiVersion: apps/v1kind: ReplicaSetmetadata: name: frontendspec: replicas: 3 selector: matchLabels: tier: frontend template: metadata: labels: tier: frontend spec: containers: - name: nginx image: registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17 imagePullPolicy: IfNotPresent ports: - name: httpd containerPort: 80 创建ReplicaSet，并查看rs状态与详情 12345678910111213141516171819202122232425262728293031[root@k8s-master controller]# kubectl apply -f ReplicaSet-01.yaml replicaset.apps/frontend created[root@k8s-master controller]# kubectl get rs -o wide # 查看状态NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTORfrontend 3 3 3 2m12s nginx registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17 tier=frontend[root@k8s-master controller]# [root@k8s-master controller]# kubectl describe rs frontend # 查看详情Name: frontendNamespace: defaultSelector: tier=frontendLabels: &lt;none&gt;Annotations: kubectl.kubernetes.io/last-applied-configuration: &#123;&quot;apiVersion&quot;:&quot;apps/v1&quot;,&quot;kind&quot;:&quot;ReplicaSet&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;name&quot;:&quot;frontend&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;,&quot;spec&quot;:&#123;&quot;replicas&quot;:3,&quot;se...Replicas: 3 current / 3 desiredPods Status: 3 Running / 0 Waiting / 0 Succeeded / 0 FailedPod Template: Labels: tier=frontend Containers: nginx: Image: registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17 Port: 80/TCP Host Port: 0/TCP Environment: &lt;none&gt; Mounts: &lt;none&gt; Volumes: &lt;none&gt;Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 10m replicaset-controller Created pod: frontend-kltwp Normal SuccessfulCreate 10m replicaset-controller Created pod: frontend-76dbn Normal SuccessfulCreate 10m replicaset-controller Created pod: frontend-jk8td 查看pod状态信息 12345[root@k8s-master controller]# kubectl get pod -o wide --show-labelsNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES LABELSfrontend-76dbn 1/1 Running 0 5m15s 10.244.4.31 k8s-node01 &lt;none&gt; &lt;none&gt; tier=frontendfrontend-jk8td 1/1 Running 0 5m15s 10.244.2.35 k8s-node02 &lt;none&gt; &lt;none&gt; tier=frontendfrontend-kltwp 1/1 Running 0 5m15s 10.244.2.34 k8s-node02 &lt;none&gt; &lt;none&gt; tier=frontend 删除一个pod，然后再次查看 12345678[root@k8s-master controller]# kubectl delete pod frontend-kltwppod &quot;frontend-kltwp&quot; deleted[root@k8s-master controller]# [root@k8s-master controller]# kubectl get pod -o wide --show-labels # 可见重新创建了一个podNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES LABELSfrontend-76dbn 1/1 Running 0 7m27s 10.244.4.31 k8s-node01 &lt;none&gt; &lt;none&gt; tier=frontendfrontend-jk8td 1/1 Running 0 7m27s 10.244.2.35 k8s-node02 &lt;none&gt; &lt;none&gt; tier=frontendfrontend-mf79k 1/1 Running 0 16s 10.244.4.32 k8s-node01 &lt;none&gt; &lt;none&gt; tier=frontend 由上可见，rs又新建了一个pod，保证了pod数总是为3. Deployment示例创建 Deploymentyaml文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[root@k8s-master controller]# pwd/root/k8s_practice/controller[root@k8s-master controller]# cat nginx-deployment-1.17.1.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment labels: app: nginxspec: replicas: 3 # 重点关注该字段 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17.1 ports: - containerPort: 80[root@k8s-master controller]# [root@k8s-master controller]# cat nginx-deployment-1.17.5.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment labels: app: nginxspec: replicas: 3 # 重点关注该字段 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17.5 ports: - containerPort: 80 selector 字段定义 Deployment 如何查找要管理的 Pods。 在这种情况下，会选择在 template（Pod）模板中定义的标签labels（app: nginx）。但更复杂的选择规则是可能的，只要 template （Pod） 模板本身满足规则。 刚启动时状态说明 启动deployment，并查看状态 12345678[root@k8s-master controller]# kubectl apply -f nginx-deployment-1.17.1.yaml --recorddeployment.apps/nginx-deployment created[root@k8s-master controller]# [root@k8s-master controller]# kubectl get deployment -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORnginx-deployment 2/3 3 2 10s nginx registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17.1 app=nginx# --record 参数可以记录命令，通过 kubectl rollout history deployment/nginx-deployment 可查询 参数说明： NAME：列出集群中 Deployments 的名称 READY：已就绪副本数/期望副本数 UP-TO-DATE：显示已更新和正在更新中的副本数 AVAILABLE：显示应用程序可供用户使用的副本数 AGE：显示运行的时间 查看ReplicaSet状态 123[root@k8s-master controller]# kubectl get rs -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTORnginx-deployment-76b9d6bcf5 3 3 2 17s nginx registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17.1 app=nginx,pod-template-hash=76b9d6bcf5 参数说明： NAME：列出集群中 ReplicaSet的名称 DESIRED：期望副本数 CURRENT：当前副本数 READY：已就绪副本数 AGE：运行时间 查看pod状态 12345[root@k8s-master controller]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnginx-deployment-76b9d6bcf5-ngpg5 1/1 Running 0 26s 10.244.2.43 k8s-node02 &lt;none&gt; &lt;none&gt;nginx-deployment-76b9d6bcf5-rw827 1/1 Running 0 26s 10.244.2.44 k8s-node02 &lt;none&gt; &lt;none&gt;nginx-deployment-76b9d6bcf5-ttf4j 0/1 ContainerCreating 0 26s &lt;none&gt; k8s-node01 &lt;none&gt; &lt;none&gt; 过一会儿状态说明 12345678910111213[root@k8s-master controller]# kubectl get deployment -o wide --show-labelsNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR LABELSnginx-deployment 3/3 3 3 23m nginx registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17.1 app=nginx app=nginx[root@k8s-master controller]# [root@k8s-master controller]# kubectl get rs -o wide --show-labelsNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR LABELSnginx-deployment-76b9d6bcf5 3 3 3 23m nginx registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17.1 app=nginx,pod-template-hash=76b9d6bcf5 app=nginx,pod-template-hash=76b9d6bcf5[root@k8s-master controller]# [root@k8s-master controller]# kubectl get pod -o wide --show-labelsNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES LABELSnginx-deployment-76b9d6bcf5-ngpg5 1/1 Running 0 23m 10.244.2.43 k8s-node02 &lt;none&gt; &lt;none&gt; app=nginx,pod-template-hash=76b9d6bcf5nginx-deployment-76b9d6bcf5-rw827 1/1 Running 0 23m 10.244.2.44 k8s-node02 &lt;none&gt; &lt;none&gt; app=nginx,pod-template-hash=76b9d6bcf5nginx-deployment-76b9d6bcf5-ttf4j 1/1 Running 0 23m 10.244.4.37 k8s-node01 &lt;none&gt; &lt;none&gt; app=nginx,pod-template-hash=76b9d6bcf5 重点说明 1、ReplicaSet 的名称始终被格式化为[DEPLOYMENT-NAME]-[RANDOM-STRING]。随机字符串是随机生成，并使用 pod-template-hash 作为选择器和标签。 2、Deployment 控制器将 pod-template-hash 标签添加到 Deployment 创建或使用的每个 ReplicaSet 。此标签可确保 Deployment 的子 ReplicaSets 不重叠。因此不可修改。 3、注意Deployment、ReplicaSet和Pod三者的名称关系 更新 DeploymentDeployment 可确保在更新时仅关闭一定数量的 Pods。默认情况下，它确保至少 75%所需 Pods 运行（25%最大不可用）。 Deployment 更新过程中还确保仅创建一定数量的 Pods 且高于期望的 Pods 数。默认情况下，它可确保最多增加 25% 期望 Pods 数（25%最大增量）。 备注：实际操作中如果更新Deployment，那么最好通过yaml文件更新，这样回滚到任何版本都非常便捷，而且更容易追述；而不是通过命令行。 如下Deployment示例，由于只有3个副本。因此更新时不会先删除旧的pod，而是先新建一个pod。新pod运行时，才会删除对应老的pod。一切的前提都是为了满足上述的条件。 需求：更新 nginx Pods，从当前的1.17.1版本改为1.17.5版本。 1234567891011# 方式一kubectl edit deployment/nginx-deployment # 然后修改 image 镜像信息 【不推荐】# 上述方法不会记录命令，通过kubectl rollout history deployment/nginx-deployment 无法查询# 方式二如下【可使用】：kubectl set image deployment/nginx-deployment nginx=registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17.5 --record # 方式三如下【推荐★★★★★】kubectl apply -f nginx-deployment-1.17.5.yaml --record# --record 参数可以记录命令，通过 kubectl rollout history deployment/nginx-deployment 可查询 要查看更新状态 123456789101112[root@k8s-master controller]# kubectl rollout status deployment/nginx-deployment# 如没有更新完成，则显示更新过程直到更新成功Waiting for deployment &quot;nginx-deployment&quot; rollout to finish: 1 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-deployment&quot; rollout to finish: 1 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-deployment&quot; rollout to finish: 2 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-deployment&quot; rollout to finish: 2 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-deployment&quot; rollout to finish: 2 out of 3 new replicas have been updated...Waiting for deployment &quot;nginx-deployment&quot; rollout to finish: 1 old replicas are pending termination...Waiting for deployment &quot;nginx-deployment&quot; rollout to finish: 1 old replicas are pending termination...deployment &quot;nginx-deployment&quot; successfully rolled out# 如已更新完毕，直接显示更新成功deployment &quot;nginx-deployment&quot; successfully rolled out 更新中的Deployment、ReplicaSet、Pod信息 123456789101112131415[root@k8s-master controller]# kubectl get deployment -o wide --show-labelsNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR LABELSnginx-deployment 3/3 1 3 12m nginx registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17.5 app=nginx app=nginx[root@k8s-master controller]# [root@k8s-master controller]# kubectl get rs -o wide --show-labelsNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR LABELSnginx-deployment-56d78686f5 1 1 0 23s nginx registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17.5 app=nginx,pod-template-hash=56d78686f5 app=nginx,pod-template-hash=56d78686f5nginx-deployment-76b9d6bcf5 3 3 3 12m nginx registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17.1 app=nginx,pod-template-hash=76b9d6bcf5 app=nginx,pod-template-hash=76b9d6bcf5[root@k8s-master controller]# [root@k8s-master controller]# kubectl get pod -o wide --show-labelsNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES LABELSnginx-deployment-56d78686f5-4kn4c 0/1 ContainerCreating 0 30s &lt;none&gt; k8s-node02 &lt;none&gt; &lt;none&gt; app=nginx,pod-template-hash=56d78686f5nginx-deployment-76b9d6bcf5-7lcr9 1/1 Running 0 12m 10.244.4.41 k8s-node01 &lt;none&gt; &lt;none&gt; app=nginx,pod-template-hash=76b9d6bcf5nginx-deployment-76b9d6bcf5-jbb5h 1/1 Running 0 12m 10.244.2.48 k8s-node02 &lt;none&gt; &lt;none&gt; app=nginx,pod-template-hash=76b9d6bcf5nginx-deployment-76b9d6bcf5-rt4m7 1/1 Running 0 12m 10.244.4.42 k8s-node01 &lt;none&gt; &lt;none&gt; app=nginx,pod-template-hash=76b9d6bcf5 更新成功后的Deployment、ReplicaSet、Pod信息 1234567891011121314[root@k8s-master controller]# kubectl get deployment -o wide --show-labelsNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR LABELSnginx-deployment 3/3 3 3 15m nginx registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17.5 app=nginx app=nginx[root@k8s-master controller]# [root@k8s-master controller]# kubectl get rs -o wide --show-labelsNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR LABELSnginx-deployment-56d78686f5 3 3 3 3m23s nginx registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17.5 app=nginx,pod-template-hash=56d78686f5 app=nginx,pod-template-hash=56d78686f5nginx-deployment-76b9d6bcf5 0 0 0 15m nginx registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17.1 app=nginx,pod-template-hash=76b9d6bcf5 app=nginx,pod-template-hash=76b9d6bcf5[root@k8s-master controller]# [root@k8s-master controller]# kubectl get pod -o wide --show-labelsNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES LABELSnginx-deployment-56d78686f5-4kn4c 1/1 Running 0 3m25s 10.244.2.49 k8s-node02 &lt;none&gt; &lt;none&gt; app=nginx,pod-template-hash=56d78686f5nginx-deployment-56d78686f5-khsnm 1/1 Running 0 100s 10.244.2.50 k8s-node02 &lt;none&gt; &lt;none&gt; app=nginx,pod-template-hash=56d78686f5nginx-deployment-56d78686f5-t24qw 1/1 Running 0 2m44s 10.244.4.43 k8s-node01 &lt;none&gt; &lt;none&gt; app=nginx,pod-template-hash=56d78686f5 通过查询Deployment详情，知晓pod替换过程 123456789101112131415161718192021222324252627282930313233343536373839404142[root@k8s-master controller]# kubectl describe deploy nginx-deploymentName: nginx-deploymentNamespace: defaultCreationTimestamp: Thu, 28 May 2020 00:04:09 +0800Labels: app=nginxAnnotations: deployment.kubernetes.io/revision: 2 kubectl.kubernetes.io/last-applied-configuration: &#123;&quot;apiVersion&quot;:&quot;apps/v1&quot;,&quot;kind&quot;:&quot;Deployment&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&quot;kubernetes.io/change-cause&quot;:&quot;kubectl apply --filename=nginx-deploy... kubernetes.io/change-cause: kubectl set image deployment/nginx-deployment nginx=registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17.5 --record=trueSelector: app=nginxReplicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailableStrategyType: RollingUpdateMinReadySeconds: 0RollingUpdateStrategy: 25% max unavailable, 25% max surgePod Template: Labels: app=nginx Containers: nginx: Image: registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17.5 Port: 80/TCP Host Port: 0/TCP Environment: &lt;none&gt; Mounts: &lt;none&gt; Volumes: &lt;none&gt;Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailableOldReplicaSets: &lt;none&gt;NewReplicaSet: nginx-deployment-56d78686f5 (3/3 replicas created)Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 93s deployment-controller Scaled up replica set nginx-deployment-76b9d6bcf5 to 3 Normal ScalingReplicaSet 38s deployment-controller Scaled up replica set nginx-deployment-56d78686f5 to 1 Normal ScalingReplicaSet 37s deployment-controller Scaled down replica set nginx-deployment-76b9d6bcf5 to 2 Normal ScalingReplicaSet 37s deployment-controller Scaled up replica set nginx-deployment-56d78686f5 to 2 Normal ScalingReplicaSet 35s deployment-controller Scaled down replica set nginx-deployment-76b9d6bcf5 to 1 Normal ScalingReplicaSet 35s deployment-controller Scaled up replica set nginx-deployment-56d78686f5 to 3 Normal ScalingReplicaSet 34s deployment-controller Scaled down replica set nginx-deployment-76b9d6bcf5 to 0 多 Deployment 动态更新当 Deployment 正在展开进行更新时，Deployment 会为每个更新创建一个新的 ReplicaSet 并开始向上扩展，之前的 ReplicaSet 会被添加到旧 ReplicaSets 队列并开始向下扩展。 例如，假设创建一个 Deployment 以创建 nginx:1.7.9 的 5 个副本，然后更新 Deployment 以创建 5 个 nginx:1.9.1 的副本，而此时只有 3 个nginx:1.7.9 的副本已创建。在这种情况下， Deployment 会立即开始杀死3个 nginx:1.7.9 Pods，并开始创建 nginx:1.9.1 Pods。它不等待 nginx:1.7.9 的 5 个副本完成后再更新为nginx:1.9.1。 回滚 Deploymentyaml文件方式针对应用的每个镜像版本，都有对应deploy的yaml文件。不管是升级还是回滚都已轻松应对。如下 1234nginx-deployment-1.15.6.yamlnginx-deployment-1.17.yamlnginx-deployment-1.17.1.yamlnginx-deployment-1.17.5.yaml yaml文件中的信息，参考上文即可。 命令行方式问题产生 假设在更新 Deployment 时犯了一个拼写错误，将镜像名称命名为了 nginx:1.1710 而不是 nginx:1.17.10 12[root@k8s-master controller]# kubectl set image deployment/nginx-deployment nginx=registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.1710 --recorddeployment.apps/nginx-deployment image updated 查看Deployment、ReplicaSet、Pod信息 1234567891011121314151617[root@k8s-master controller]# kubectl get deploy -o wide --show-labelsNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR LABELSnginx-deployment 3/3 1 3 14m nginx registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.1710 app=nginx app=nginx[root@k8s-master controller]#[root@k8s-master controller]# kubectl get rs -o wide --show-labelsNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR LABELSnginx-deployment-55c7bdfb86 3 3 3 9m19s nginx registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17 app=nginx,pod-template-hash=55c7bdfb86 app=nginx,pod-template-hash=55c7bdfb86nginx-deployment-56d78686f5 0 0 0 12m nginx registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17.5 app=nginx,pod-template-hash=56d78686f5 app=nginx,pod-template-hash=56d78686f5nginx-deployment-76b9d6bcf5 0 0 0 13m nginx registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17.1 app=nginx,pod-template-hash=76b9d6bcf5 app=nginx,pod-template-hash=76b9d6bcf5nginx-deployment-844d7bbb7f 1 1 0 64s nginx registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.1710 app=nginx,pod-template-hash=844d7bbb7f app=nginx,pod-template-hash=844d7bbb7f[root@k8s-master controller]#[root@k8s-master controller]# kubectl get pod -o wide --show-labelsNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES LABELSnginx-deployment-55c7bdfb86-bwzk9 1/1 Running 0 10m 10.244.4.49 k8s-node01 &lt;none&gt; &lt;none&gt; app=nginx,pod-template-hash=55c7bdfb86nginx-deployment-55c7bdfb86-cmvzg 1/1 Running 0 10m 10.244.2.55 k8s-node02 &lt;none&gt; &lt;none&gt; app=nginx,pod-template-hash=55c7bdfb86nginx-deployment-55c7bdfb86-kjrrw 1/1 Running 0 10m 10.244.2.56 k8s-node02 &lt;none&gt; &lt;none&gt; app=nginx,pod-template-hash=55c7bdfb86nginx-deployment-844d7bbb7f-pctwr 0/1 ImagePullBackOff 0 2m3s 10.244.4.51 k8s-node01 &lt;none&gt; &lt;none&gt; app=nginx,pod-template-hash=844d7bbb7f 需求：回滚到以前稳定的 Deployment 版本。 操作步骤如下： 检查 Deployment 修改历史 1234567[root@k8s-master controller]# kubectl rollout history deployment/nginx-deploymentdeployment.apps/nginx-deployment REVISION CHANGE-CAUSE1 kubectl apply --filename=nginx-deployment.yaml --record=true2 kubectl set image deployment/nginx-deployment nginx=registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17.5 --record=true3 kubectl set image deployment/nginx-deployment nginx=registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17 --record=true4 kubectl set image deployment/nginx-deployment nginx=registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.1710 --record=true 查看修改历史的详细信息，运行 123456789101112131415[root@k8s-master controller]# kubectl rollout history deployment/nginx-deployment --revision=3deployment.apps/nginx-deployment with revision #3Pod Template: Labels: app=nginx pod-template-hash=55c7bdfb86 Annotations: kubernetes.io/change-cause: kubectl set image deployment/nginx-deployment nginx=registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17 --record=true Containers: nginx: Image: registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17 Port: 80/TCP Host Port: 0/TCP Environment: &lt;none&gt; Mounts: &lt;none&gt; Volumes: &lt;none&gt; 回滚到上一次修改（即版本 3）或指定版本 现在已决定撤消当前更新并回滚到以前的版本 123456# 回滚到上一版本[root@k8s-master controller]# kubectl rollout undo deployment/nginx-deploymentdeployment.apps/nginx-deployment rolled back# 回滚到指定历史版本[root@k8s-master controller]# kubectl rollout undo deployment/nginx-deployment --to-revision=2deployment.apps/nginx-deployment rolled back 检查回滚是否成功、 Deployment 是否正在运行 123[root@k8s-master controller]# kubectl get deploy -o wide --show-labelsNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR LABELSnginx-deployment 3/3 3 3 17h nginx registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17.5 app=nginx app=nginx 获取 Deployment 描述信息 123456789101112131415161718192021222324252627282930313233343536373839404142[root@k8s-master controller]# kubectl describe deploymentName: nginx-deploymentNamespace: defaultCreationTimestamp: Thu, 28 May 2020 00:04:09 +0800Labels: app=nginxAnnotations: deployment.kubernetes.io/revision: 7 kubectl.kubernetes.io/last-applied-configuration: &#123;&quot;apiVersion&quot;:&quot;apps/v1&quot;,&quot;kind&quot;:&quot;Deployment&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&quot;kubernetes.io/change-cause&quot;:&quot;kubectl apply --filename=nginx-deploy... kubernetes.io/change-cause: kubectl set image deployment/nginx-deployment nginx=registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17.5 --record=trueSelector: app=nginxReplicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailableStrategyType: RollingUpdateMinReadySeconds: 0RollingUpdateStrategy: 25% max unavailable, 25% max surgePod Template: Labels: app=nginx Containers: nginx: Image: registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17.5 Port: 80/TCP Host Port: 0/TCP Environment: &lt;none&gt; Mounts: &lt;none&gt; Volumes: &lt;none&gt;Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailableOldReplicaSets: &lt;none&gt;NewReplicaSet: nginx-deployment-56d78686f5 (3/3 replicas created)Events: Type Reason Age From Message ---- ------ ---- ---- -------……………… Normal ScalingReplicaSet 107s deployment-controller Scaled up replica set nginx-deployment-56d78686f5 to 1 Normal ScalingReplicaSet 104s deployment-controller Scaled down replica set nginx-deployment-55c7bdfb86 to 2 Normal ScalingReplicaSet 104s deployment-controller Scaled up replica set nginx-deployment-56d78686f5 to 2 Normal ScalingReplicaSet 103s deployment-controller Scaled down replica set nginx-deployment-55c7bdfb86 to 1 Normal ScalingReplicaSet 103s deployment-controller Scaled up replica set nginx-deployment-56d78686f5 to 3 Normal ScalingReplicaSet 102s deployment-controller Scaled down replica set nginx-deployment-55c7bdfb86 to 0 扩容/缩容Deployment操作过程如下 12345678910111213141516171819202122232425[root@k8s-master controller]# kubectl scale deployment/nginx-deployment --replicas=10deployment.apps/nginx-deployment scaled[root@k8s-master controller]# kubectl get deploy -o wide --show-labelsNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR LABELSnginx-deployment 10/10 10 10 17h nginx registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17.5 app=nginx app=nginx[root@k8s-master controller]# [root@k8s-master controller]# kubectl get rs -o wide --show-labelsNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR LABELSnginx-deployment-55c7bdfb86 0 0 0 17h nginx registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17 app=nginx,pod-template-hash=55c7bdfb86 app=nginx,pod-template-hash=55c7bdfb86nginx-deployment-56d78686f5 10 10 10 17h nginx registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17.5 app=nginx,pod-template-hash=56d78686f5 app=nginx,pod-template-hash=56d78686f5nginx-deployment-76b9d6bcf5 0 0 0 17h nginx registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17.1 app=nginx,pod-template-hash=76b9d6bcf5 app=nginx,pod-template-hash=76b9d6bcf5nginx-deployment-844d7bbb7f 0 0 0 17h nginx registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.1710 app=nginx,pod-template-hash=844d7bbb7f app=nginx,pod-template-hash=844d7bbb7f[root@k8s-master controller]# [root@k8s-master controller]# kubectl get pod -o wide --show-labelsNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES LABELSnginx-deployment-56d78686f5-4v5mj 1/1 Running 0 44s 10.244.2.64 k8s-node02 &lt;none&gt; &lt;none&gt; app=nginx,pod-template-hash=56d78686f5nginx-deployment-56d78686f5-8m7mx 1/1 Running 0 44s 10.244.4.60 k8s-node01 &lt;none&gt; &lt;none&gt; app=nginx,pod-template-hash=56d78686f5nginx-deployment-56d78686f5-c7wlb 1/1 Running 0 44s 10.244.4.59 k8s-node01 &lt;none&gt; &lt;none&gt; app=nginx,pod-template-hash=56d78686f5nginx-deployment-56d78686f5-jg5lt 1/1 Running 0 44s 10.244.2.63 k8s-node02 &lt;none&gt; &lt;none&gt; app=nginx,pod-template-hash=56d78686f5nginx-deployment-56d78686f5-jj58d 1/1 Running 0 11m 10.244.4.56 k8s-node01 &lt;none&gt; &lt;none&gt; app=nginx,pod-template-hash=56d78686f5nginx-deployment-56d78686f5-k2kts 1/1 Running 0 11m 10.244.4.57 k8s-node01 &lt;none&gt; &lt;none&gt; app=nginx,pod-template-hash=56d78686f5nginx-deployment-56d78686f5-qltkv 1/1 Running 0 44s 10.244.2.61 k8s-node02 &lt;none&gt; &lt;none&gt; app=nginx,pod-template-hash=56d78686f5nginx-deployment-56d78686f5-r7vmm 1/1 Running 0 11m 10.244.2.60 k8s-node02 &lt;none&gt; &lt;none&gt; app=nginx,pod-template-hash=56d78686f5nginx-deployment-56d78686f5-rxlpm 1/1 Running 0 44s 10.244.2.62 k8s-node02 &lt;none&gt; &lt;none&gt; app=nginx,pod-template-hash=56d78686f5nginx-deployment-56d78686f5-vlzrf 1/1 Running 0 44s 10.244.4.58 k8s-node01 &lt;none&gt; &lt;none&gt; app=nginx,pod-template-hash=56d78686f5 清理策略Policy可以在 Deployment 中设置 .spec.revisionHistoryLimit，以指定保留多少该 Deployment 的 ReplicaSets数量。其余的将在后台进行垃圾回收。默认情况下，是10。 注意：此字段设置为 0 将导致清理 Deployment 的所有历史记录，因此 Deployment 将无法通过命令行回滚。 相关阅读1、Kubernetes K8S之kubectl命令详解及常用示例 完毕！]]></content>
      <categories>
        <category>Kubernetes</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
        <tag>K8S</tag>
        <tag>CI/CD</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes K8S之Pod 生命周期与postStart、preStop事件]]></title>
    <url>%2F2020%2F08%2F18%2Fkubernetes08%2F</url>
    <content type="text"><![CDATA[Kubernetes 支持 postStart 和 preStop 事件。当一个主容器启动后，Kubernetes 将立即发送 postStart 事件；在主容器被终结之前，Kubernetes 将发送一个 preStop 事件。 主机配置规划 服务器名称(hostname) 系统版本 配置 内网IP 外网IP(模拟) k8s-master CentOS7.7 2C/4G/20G 172.16.1.110 10.0.0.110 k8s-node01 CentOS7.7 2C/4G/20G 172.16.1.111 10.0.0.111 k8s-node02 CentOS7.7 2C/4G/20G 172.16.1.112 10.0.0.112 Pod容器生命周期 Pause容器说明每个Pod里运行着一个特殊的被称之为Pause的容器，其他容器则为业务容器，这些业务容器共享Pause容器的网络栈和Volume挂载卷，因此他们之间通信和数据交换更为高效。在设计时可以充分利用这一特性，将一组密切相关的服务进程放入同一个Pod中；同一个Pod里的容器之间仅需通过localhost就能互相通信。 kubernetes中的pause容器主要为每个业务容器提供以下功能： PID命名空间：Pod中的不同应用程序可以看到其他应用程序的进程ID。 网络命名空间：Pod中的多个容器能够访问同一个IP和端口范围。 IPC命名空间：Pod中的多个容器能够使用SystemV IPC或POSIX消息队列进行通信。 UTS命名空间：Pod中的多个容器共享一个主机名；Volumes（共享存储卷）。 Pod中的各个容器可以访问在Pod级别定义的Volumes。 主容器生命周期事件的处理函数Kubernetes 支持 postStart 和 preStop 事件。当一个主容器启动后，Kubernetes 将立即发送 postStart 事件；在主容器被终结之前，Kubernetes 将发送一个 preStop 事件。 postStart 和 preStop 处理函数示例pod yaml文件 123456789101112131415161718192021222324252627282930313233343536373839[root@k8s-master lifecycle]# pwd/root/k8s_practice/lifecycle[root@k8s-master lifecycle]# cat lifecycle-events.yaml apiVersion: v1kind: Podmetadata: name: lifecycle-demo-pod namespace: default labels: test: lifecyclespec: containers: - name: lifecycle-demo image: registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17 imagePullPolicy: IfNotPresent lifecycle: postStart: exec: command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo &apos;Hello from the postStart handler&apos; &gt;&gt; /var/log/nginx/message&quot;] preStop: exec: command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo &apos;Hello from the preStop handler&apos; &gt;&gt; /var/log/nginx/message&quot;] volumeMounts: #定义容器挂载内容 - name: message-log #使用的存储卷名称，如果跟下面volume字段name值相同，则表示使用volume的nginx-site这个存储卷 mountPath: /var/log/nginx/ #挂载至容器中哪个目录 readOnly: false #读写挂载方式，默认为读写模式false initContainers: - name: init-myservice image: registry.cn-beijing.aliyuncs.com/google_registry/busybox:1.24 command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo &apos;Hello initContainers&apos; &gt;&gt; /var/log/nginx/message&quot;] volumeMounts: #定义容器挂载内容 - name: message-log #使用的存储卷名称，如果跟下面volume字段name值相同，则表示使用volume的nginx-site这个存储卷 mountPath: /var/log/nginx/ #挂载至容器中哪个目录 readOnly: false #读写挂载方式，默认为读写模式false volumes: #volumes字段定义了paues容器关联的宿主机或分布式文件系统存储卷 - name: message-log #存储卷名称 hostPath: #路径，为宿主机存储路径 path: /data/volumes/nginx/log/ #在宿主机上目录的路径 type: DirectoryOrCreate #定义类型，这表示如果宿主机没有此目录则会自动创建 启动pod，查看pod状态 12345[root@k8s-master lifecycle]# kubectl apply -f lifecycle-events.yaml pod/lifecycle-demo-pod created[root@k8s-master lifecycle]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESlifecycle-demo-pod 1/1 Running 0 5s 10.244.2.30 k8s-node02 &lt;none&gt; &lt;none&gt; 查看pod详情 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374[root@k8s-master lifecycle]# kubectl describe pod lifecycle-demo-podName: lifecycle-demo-podNamespace: defaultPriority: 0Node: k8s-node02/172.16.1.112Start Time: Sat, 23 May 2020 22:08:04 +0800Labels: test=lifecycle………………Init Containers: init-myservice: Container ID: docker://1cfabcb60b817efd5c7283ad9552dafada95dbe932f92822b814aaa9c38f8ba5 Image: registry.cn-beijing.aliyuncs.com/google_registry/busybox:1.24 Image ID: docker-pullable://registry.cn-beijing.aliyuncs.com/ducafe/busybox@sha256:f73ae051fae52945d92ee20d62c315306c593c59a429ccbbdcba4a488ee12269 Port: &lt;none&gt; Host Port: &lt;none&gt; Command: /bin/sh -c echo &apos;Hello initContainers&apos; &gt;&gt; /var/log/nginx/message State: Terminated Reason: Completed Exit Code: 0 Started: Sat, 23 May 2020 22:08:06 +0800 Finished: Sat, 23 May 2020 22:08:06 +0800 Ready: True Restart Count: 0 Environment: &lt;none&gt; Mounts: /var/log/nginx/ from message-log (rw) /var/run/secrets/kubernetes.io/serviceaccount from default-token-v48g4 (ro)Containers: lifecycle-demo: Container ID: docker://c07f7f3d838206878ad0bfeaec9b4222ac7d6b13fb758cc1b340ac43e7212a3a Image: registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17 Image ID: docker-pullable://registry.cn-beijing.aliyuncs.com/google_registry/nginx@sha256:7ac7819e1523911399b798309025935a9968b277d86d50e5255465d6592c0266 Port: &lt;none&gt; Host Port: &lt;none&gt; State: Running Started: Sat, 23 May 2020 22:08:07 +0800 Ready: True Restart Count: 0 Environment: &lt;none&gt; Mounts: /var/log/nginx/ from message-log (rw) /var/run/secrets/kubernetes.io/serviceaccount from default-token-v48g4 (ro)Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: message-log: Type: HostPath (bare host directory volume) Path: /data/volumes/nginx/log/ HostPathType: DirectoryOrCreate default-token-v48g4: Type: Secret (a volume populated by a Secret) SecretName: default-token-v48g4 Optional: falseQoS Class: BestEffortNode-Selectors: &lt;none&gt;Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300sEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled &lt;unknown&gt; default-scheduler Successfully assigned default/lifecycle-demo-pod to k8s-node02 Normal Pulled 87s kubelet, k8s-node02 Container image &quot;registry.cn-beijing.aliyuncs.com/google_registry/busybox:1.24&quot; already present on machine Normal Created 87s kubelet, k8s-node02 Created container init-myservice Normal Started 87s kubelet, k8s-node02 Started container init-myservice Normal Pulled 86s kubelet, k8s-node02 Container image &quot;registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17&quot; already present on machine Normal Created 86s kubelet, k8s-node02 Created container lifecycle-demo Normal Started 86s kubelet, k8s-node02 Started container lifecycle-demo 此时在k8s-node02查看输出信息如下： 12345[root@k8s-node02 log]# pwd/data/volumes/nginx/log[root@k8s-node02 log]# cat message Hello initContainersHello from the postStart handler 由上可知，init Container先执行，然后当一个主容器启动后，Kubernetes 将立即发送 postStart 事件。 停止该pod 12[root@k8s-master lifecycle]# kubectl delete pod lifecycle-demo-podpod &quot;lifecycle-demo-pod&quot; deleted 此时在k8s-node02查看输出信息如下： 123456[root@k8s-node02 log]# pwd/data/volumes/nginx/log[root@k8s-node02 log]# cat message Hello initContainersHello from the postStart handlerHello from the preStop handler 由上可知，当在容器被终结之前， Kubernetes 将发送一个 preStop 事件。 完毕！]]></content>
      <categories>
        <category>Kubernetes</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
        <tag>K8S</tag>
        <tag>CI/CD</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes K8S之Pod生命周期与探针检测]]></title>
    <url>%2F2020%2F08%2F16%2Fkubernetes07%2F</url>
    <content type="text"><![CDATA[K8S中Pod的生命周期与ExecAction、TCPSocketAction和HTTPGetAction探针检测 主机配置规划 服务器名称(hostname) 系统版本 配置 内网IP 外网IP(模拟) k8s-master CentOS7.7 2C/4G/20G 172.16.1.110 10.0.0.110 k8s-node01 CentOS7.7 2C/4G/20G 172.16.1.111 10.0.0.111 k8s-node02 CentOS7.7 2C/4G/20G 172.16.1.112 10.0.0.112 Pod容器生命周期 Pause容器说明每个Pod里运行着一个特殊的被称之为Pause的容器，其他容器则为业务容器，这些业务容器共享Pause容器的网络栈和Volume挂载卷，因此他们之间通信和数据交换更为高效。在设计时可以充分利用这一特性，将一组密切相关的服务进程放入同一个Pod中；同一个Pod里的容器之间仅需通过localhost就能互相通信。 kubernetes中的pause容器主要为每个业务容器提供以下功能： PID命名空间：Pod中的不同应用程序可以看到其他应用程序的进程ID。 网络命名空间：Pod中的多个容器能够访问同一个IP和端口范围。 IPC命名空间：Pod中的多个容器能够使用SystemV IPC或POSIX消息队列进行通信。 UTS命名空间：Pod中的多个容器共享一个主机名；Volumes（共享存储卷）。 Pod中的各个容器可以访问在Pod级别定义的Volumes。 容器探针探针是由 kubelet 对容器执行的定期诊断。要执行诊断，则需kubelet 调用由容器实现的 Handler。探针有三种类型的处理程序： ExecAction：在容器内执行指定命令。如果命令退出时返回码为 0 则认为诊断成功。 CPSocketAction：对指定端口上的容器的 IP 地址进行 TCP 检查。如果端口打开，则诊断被认为是成功的。 HTTPGetAction：对指定的端口和路径上的容器的 IP 地址执行 HTTP Get 请求。如果响应的状态码大于等于200 且小于 400，则诊断被认为是成功的。 每次探测都将获得以下三种结果之一： 成功：容器通过了诊断。 失败：容器未通过诊断。 未知：诊断失败，因此不会采取任何行动。 Kubelet 可以选择是否在容器上运行三种探针执行和做出反应： livenessProbe：指示容器是否正在运行。如果存活探测失败，则 kubelet 会杀死容器，并且容器将受到其重启策略的影响。如果容器不提供存活探针，则默认状态为 Success。 readinessProbe：指示容器是否准备好服务请求【对外接受请求访问】。如果就绪探测失败，端点控制器将从与 Pod 匹配的所有 Service 的端点中删除该 Pod 的 IP 地址。初始延迟之前的就绪状态默认为 Failure。如果容器不提供就绪探针，则默认状态为 Success。 startupProbe: 指示容器中的应用是否已经启动。如果提供了启动探测(startup probe)，则禁用所有其他探测，直到它成功为止。如果启动探测失败，kubelet 将杀死容器，容器服从其重启策略进行重启。如果容器没有提供启动探测，则默认状态为成功Success。 备注：可以以Tomcat web服务为例。 容器重启策略PodSpec 中有一个 restartPolicy 字段，可能的值为 Always、OnFailure 和 Never。默认为 Always。 Always表示一旦不管以何种方式终止运行，kubelet都将重启；OnFailure表示只有Pod以非0退出码退出才重启；Nerver表示不再重启该Pod。 restartPolicy 适用于 Pod 中的所有容器。restartPolicy 仅指通过同一节点上的 kubelet 重新启动容器。失败的容器由 kubelet 以五分钟为上限的指数退避延迟（10秒，20秒，40秒…）重新启动，并在成功执行十分钟后重置。如 Pod 文档中所述，一旦pod绑定到一个节点，Pod 将永远不会重新绑定到另一个节点。 存活（liveness）和就绪（readiness）探针的使用场景如果容器中的进程能够在遇到问题或不健康的情况下自行崩溃，则不一定需要存活探针；kubelet 将根据 Pod 的restartPolicy 自动执行正确的操作。 如果你希望容器在探测失败时被杀死并重新启动，那么请指定一个存活探针，并指定restartPolicy 为 Always 或 OnFailure。 如果要仅在探测成功时才开始向 Pod 发送流量，请指定就绪探针。在这种情况下，就绪探针可能与存活探针相同，但是 spec 中的就绪探针的存在意味着 Pod 将在没有接收到任何流量的情况下启动，并且只有在探针探测成功后才开始接收流量。 Pod phase（阶段）Pod 的 status 定义在 PodStatus 对象中，其中有一个 phase 字段。 Pod 的运行阶段（phase）是 Pod 在其生命周期中的简单宏观概述。该阶段并不是对容器或 Pod 的综合汇总，也不是为了做为综合状态机。 Pod 相位的数量和含义是严格指定的。除了本文档中列举的内容外，不应该再假定 Pod 有其他的 phase 值。 下面是 phase 可能的值： 挂起（Pending）：Pod 已被 Kubernetes 系统接受，但有一个或者多个容器镜像尚未创建。等待时间包括调度 Pod 的时间和通过网络下载镜像的时间，这可能需要花点时间。 运行中（Running）：该 Pod 已经绑定到了一个节点上，Pod 中所有的容器都已被创建。至少有一个容器正在运行，或者正处于启动或重启状态。 成功（Succeeded）：Pod 中的所有容器都被成功终止，并且不会再重启。 失败（Failed）：Pod 中的所有容器都已终止了，并且至少有一个容器是因为失败终止。也就是说，容器以非0状态退出或者被系统终止。 未知（Unknown）：因为某些原因无法取得 Pod 的状态，通常是因为与 Pod 所在主机通信失败。 检测探针-就绪检测pod yaml脚本 123456789101112131415161718192021[root@k8s-master lifecycle]# pwd/root/k8s_practice/lifecycle[root@k8s-master lifecycle]# cat readinessProbe-httpget.yaml apiVersion: v1kind: Podmetadata: name: readiness-httpdget-pod namespace: default labels: test: readiness-httpdgetspec: containers: - name: readiness-httpget image: registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17 imagePullPolicy: IfNotPresent readinessProbe: httpGet: path: /index1.html port: 80 initialDelaySeconds: 5 #容器启动完成后，kubelet在执行第一次探测前应该等待 5 秒。默认是 0 秒，最小值是 0。 periodSeconds: 3 #指定 kubelet 每隔 3 秒执行一次存活探测。默认是 10 秒。最小值是 1 创建 Pod，并查看pod状态 12345[root@k8s-master lifecycle]# kubectl apply -f readinessProbe-httpget.yaml pod/readiness-httpdget-pod created[root@k8s-master lifecycle]# kubectl get pod -n default -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESreadiness-httpdget-pod 0/1 Running 0 5s 10.244.2.25 k8s-node02 &lt;none&gt; &lt;none&gt; 查看pod详情 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[root@k8s-master lifecycle]# kubectl describe pod readiness-httpdget-podName: readiness-httpdget-podNamespace: defaultPriority: 0Node: k8s-node02/172.16.1.112Start Time: Sat, 23 May 2020 16:10:04 +0800Labels: test=readiness-httpdgetAnnotations: kubectl.kubernetes.io/last-applied-configuration: &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Pod&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;labels&quot;:&#123;&quot;test&quot;:&quot;readiness-httpdget&quot;&#125;,&quot;name&quot;:&quot;readiness-httpdget-pod&quot;,&quot;names...Status: RunningIP: 10.244.2.25IPs: IP: 10.244.2.25Containers: readiness-httpget: Container ID: docker://066d66aaef191b1db08e1b3efba6a9be75378d2fe70e99400fc513b91242089c……………… Port: &lt;none&gt; Host Port: &lt;none&gt; State: Running Started: Sat, 23 May 2020 16:10:05 +0800 Ready: False ##### 状态为False Restart Count: 0 Readiness: http-get http://:80/index1.html delay=5s timeout=1s period=3s #success=1 #failure=3 Environment: &lt;none&gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-v48g4 (ro)Conditions: Type Status Initialized True Ready False ##### 为False ContainersReady False ##### 为False PodScheduled True Volumes: default-token-v48g4: Type: Secret (a volume populated by a Secret) SecretName: default-token-v48g4 Optional: falseQoS Class: BestEffortNode-Selectors: &lt;none&gt;Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300sEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled &lt;unknown&gt; default-scheduler Successfully assigned default/readiness-httpdget-pod to k8s-node02 Normal Pulled 49s kubelet, k8s-node02 Container image &quot;registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17&quot; already present on machine Normal Created 49s kubelet, k8s-node02 Created container readiness-httpget Normal Started 49s kubelet, k8s-node02 Started container readiness-httpget Warning Unhealthy 2s (x15 over 44s) kubelet, k8s-node02 Readiness probe failed: HTTP probe failed with statuscode: 404 由上可见，容器未就绪。 我们进入pod的第一个容器，然后创建对应的文件 1234567[root@k8s-master lifecycle]# kubectl exec -it readiness-httpdget-pod -c readiness-httpget bashroot@readiness-httpdget-pod:/# cd /usr/share/nginx/htmlroot@readiness-httpdget-pod:/usr/share/nginx/html# ls50x.html index.htmlroot@readiness-httpdget-pod:/usr/share/nginx/html# echo &quot;readiness-httpdget info&quot; &gt; index1.htmlroot@readiness-httpdget-pod:/usr/share/nginx/html# ls50x.html index.html index1.html 之后看pod状态与详情 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253[root@k8s-master lifecycle]# kubectl get pod -n default -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESreadiness-httpdget-pod 1/1 Running 0 2m30s 10.244.2.25 k8s-node02 &lt;none&gt; &lt;none&gt;[root@k8s-master lifecycle]# kubectl describe pod readiness-httpdget-podName: readiness-httpdget-podNamespace: defaultPriority: 0Node: k8s-node02/172.16.1.112Start Time: Sat, 23 May 2020 16:10:04 +0800Labels: test=readiness-httpdgetAnnotations: kubectl.kubernetes.io/last-applied-configuration: &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Pod&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;labels&quot;:&#123;&quot;test&quot;:&quot;readiness-httpdget&quot;&#125;,&quot;name&quot;:&quot;readiness-httpdget-pod&quot;,&quot;names...Status: RunningIP: 10.244.2.25IPs: IP: 10.244.2.25Containers: readiness-httpget: Container ID: docker://066d66aaef191b1db08e1b3efba6a9be75378d2fe70e99400fc513b91242089c……………… Port: &lt;none&gt; Host Port: &lt;none&gt; State: Running Started: Sat, 23 May 2020 16:10:05 +0800 Ready: True ##### 状态为True Restart Count: 0 Readiness: http-get http://:80/index1.html delay=5s timeout=1s period=3s #success=1 #failure=3 Environment: &lt;none&gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-v48g4 (ro)Conditions: Type Status Initialized True Ready True ##### 为True ContainersReady True ##### 为True PodScheduled True Volumes: default-token-v48g4: Type: Secret (a volume populated by a Secret) SecretName: default-token-v48g4 Optional: falseQoS Class: BestEffortNode-Selectors: &lt;none&gt;Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300sEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled &lt;unknown&gt; default-scheduler Successfully assigned default/readiness-httpdget-pod to k8s-node02 Normal Pulled 2m33s kubelet, k8s-node02 Container image &quot;registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17&quot; already present on machine Normal Created 2m33s kubelet, k8s-node02 Created container readiness-httpget Normal Started 2m33s kubelet, k8s-node02 Started container readiness-httpget Warning Unhealthy 85s (x22 over 2m28s) kubelet, k8s-node02 Readiness probe failed: HTTP probe failed with statuscode: 404 由上可见，容器已就绪。 检测探针-存活检测存活检测-执行命令pod yaml脚本 12345678910111213141516171819202122232425[root@k8s-master lifecycle]# pwd/root/k8s_practice/lifecycle[root@k8s-master lifecycle]# cat livenessProbe-exec.yaml apiVersion: v1kind: Podmetadata: name: liveness-exec-pod labels: test: livenessspec: containers: - name: liveness-exec image: registry.cn-beijing.aliyuncs.com/google_registry/busybox:1.24 imagePullPolicy: IfNotPresent args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 # 第一次检测前等待5秒 periodSeconds: 3 # 检测周期3秒一次 这个容器生命的前 30 秒，/tmp/healthy 文件是存在的。所以在这最开始的 30 秒内，执行命令 cat /tmp/healthy 会返回成功码。30 秒之后，执行命令 cat /tmp/healthy 就会返回失败状态码。 创建 Pod 12[root@k8s-master lifecycle]# kubectl apply -f livenessProbe-exec.yaml pod/liveness-exec-pod created 在 30 秒内，查看 Pod 的描述： 12345678910111213141516[root@k8s-master lifecycle]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESliveness-exec-pod 1/1 Running 0 17s 10.244.2.21 k8s-node02 &lt;none&gt; &lt;none&gt;[root@k8s-master lifecycle]# kubectl describe pod liveness-exec-podName: liveness-exec-podNamespace: defaultPriority: 0Node: k8s-node02/172.16.1.112………………Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 25s default-scheduler Successfully assigned default/liveness-exec-pod to k8s-node02 Normal Pulled 24s kubelet, k8s-node02 Container image &quot;registry.cn-beijing.aliyuncs.com/google_registry/busybox:1.24&quot; already present on machine Normal Created 24s kubelet, k8s-node02 Created container liveness-exec Normal Started 24s kubelet, k8s-node02 Started container liveness-exec 输出结果显示：存活探测器成功。 35 秒之后，再来看 Pod 的描述： 1234567891011121314[root@k8s-master lifecycle]# kubectl get pod -o wide # 显示 RESTARTS 的值增加了 1NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESliveness-exec-pod 1/1 Running 1 89s 10.244.2.22 k8s-node02 &lt;none&gt; &lt;none&gt;[root@k8s-master lifecycle]# kubectl describe pod liveness-exec-pod………………Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 42s default-scheduler Successfully assigned default/liveness-exec-pod to k8s-node02 Normal Pulled 41s kubelet, k8s-node02 Container image &quot;registry.cn-beijing.aliyuncs.com/google_registry/busybox:1.24&quot; already present on machine Normal Created 41s kubelet, k8s-node02 Created container liveness-exec Normal Started 41s kubelet, k8s-node02 Started container liveness-exec Warning Unhealthy 2s (x3 over 8s) kubelet, k8s-node02 Liveness probe failed: cat: can&apos;t open &apos;/tmp/healthy&apos;: No such file or directory Normal Killing 2s kubelet, k8s-node02 Container liveness-exec failed liveness probe, will be restarted 由上可见，在输出结果的最下面，有信息显示存活探测器失败了，因此这个容器被杀死并且被重建了。 存活检测-HTTP请求pod yaml脚本 1234567891011121314151617181920212223242526[root@k8s-master lifecycle]# pwd/root/k8s_practice/lifecycle[root@k8s-master lifecycle]# cat livenessProbe-httpget.yaml apiVersion: v1kind: Podmetadata: name: liveness-httpget-pod labels: test: livenessspec: containers: - name: liveness-httpget image: registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17 imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 livenessProbe: httpGet: # 任何大于或等于 200 并且小于 400 的返回码表示成功，其它返回码都表示失败。 path: /index.html port: 80 httpHeaders: #请求中自定义的 HTTP 头。HTTP 头字段允许重复。 - name: Custom-Header value: Awesome initialDelaySeconds: 5 periodSeconds: 3 创建 Pod，查看pod状态 12345[root@k8s-master lifecycle]# kubectl apply -f livenessProbe-httpget.yaml pod/liveness-httpget-pod created[root@k8s-master lifecycle]# kubectl get pod -n default -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESliveness-httpget-pod 1/1 Running 0 3s 10.244.2.27 k8s-node02 &lt;none&gt; &lt;none&gt; 查看pod详情 1234567891011121314151617181920212223242526272829303132333435363738394041[root@k8s-master lifecycle]# kubectl describe pod liveness-httpget-podName: liveness-httpget-podNamespace: defaultPriority: 0Node: k8s-node02/172.16.1.112Start Time: Sat, 23 May 2020 16:45:25 +0800Labels: test=livenessAnnotations: kubectl.kubernetes.io/last-applied-configuration: &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Pod&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;labels&quot;:&#123;&quot;test&quot;:&quot;liveness&quot;&#125;,&quot;name&quot;:&quot;liveness-httpget-pod&quot;,&quot;namespace&quot;:&quot;defau...Status: RunningIP: 10.244.2.27IPs: IP: 10.244.2.27Containers: liveness-httpget: Container ID: docker://4b42a351414667000fe94d4f3166d75e72a3401e549fed723126d2297124ea1a……………… Port: 80/TCP Host Port: 8080/TCP State: Running Started: Sat, 23 May 2020 16:45:26 +0800 Ready: True Restart Count: 0 Liveness: http-get http://:80/index.html delay=5s timeout=1s period=3s #success=1 #failure=3 Environment: &lt;none&gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-v48g4 (ro)Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True ………………Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled &lt;unknown&gt; default-scheduler Successfully assigned default/liveness-httpget-pod to k8s-node02 Normal Pulled 5m52s kubelet, k8s-node02 Container image &quot;registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17&quot; already present on machine Normal Created 5m52s kubelet, k8s-node02 Created container liveness-httpget Normal Started 5m52s kubelet, k8s-node02 Started container liveness-httpget 由上可见，pod存活检测正常 我们进入pod的第一个容器，然后删除对应的文件 1234567[root@k8s-master lifecycle]# kubectl exec -it liveness-httpget-pod -c liveness-httpget bashroot@liveness-httpget-pod:/# cd /usr/share/nginx/html/root@liveness-httpget-pod:/usr/share/nginx/html# ls50x.html index.htmlroot@liveness-httpget-pod:/usr/share/nginx/html# rm -f index.html root@liveness-httpget-pod:/usr/share/nginx/html# ls50x.html 再次看pod状态和详情，可见Pod的RESTARTS从0变为了1。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[root@k8s-master lifecycle]# kubectl get pod -n default -o wide # RESTARTS 从0变为了1NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESliveness-httpget-pod 1/1 Running 1 8m16s 10.244.2.27 k8s-node02 &lt;none&gt; &lt;none&gt;[root@k8s-master lifecycle]# kubectl describe pod liveness-httpget-podName: liveness-httpget-podNamespace: defaultPriority: 0Node: k8s-node02/172.16.1.112Start Time: Sat, 23 May 2020 16:45:25 +0800Labels: test=livenessAnnotations: kubectl.kubernetes.io/last-applied-configuration: &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Pod&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;labels&quot;:&#123;&quot;test&quot;:&quot;liveness&quot;&#125;,&quot;name&quot;:&quot;liveness-httpget-pod&quot;,&quot;namespace&quot;:&quot;defau...Status: RunningIP: 10.244.2.27IPs: IP: 10.244.2.27Containers: liveness-httpget: Container ID: docker://5d0962d383b1df5e59cd3d1100b259ff0415ac37c8293b17944034f530fb51c8……………… Port: 80/TCP Host Port: 8080/TCP State: Running Started: Sat, 23 May 2020 16:53:38 +0800 Last State: Terminated Reason: Completed Exit Code: 0 Started: Sat, 23 May 2020 16:45:26 +0800 Finished: Sat, 23 May 2020 16:53:38 +0800 Ready: True Restart Count: 1 Liveness: http-get http://:80/index.html delay=5s timeout=1s period=3s #success=1 #failure=3 Environment: &lt;none&gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-v48g4 (ro)Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: default-token-v48g4: Type: Secret (a volume populated by a Secret) SecretName: default-token-v48g4 Optional: falseQoS Class: BestEffortNode-Selectors: &lt;none&gt;Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300sEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled &lt;unknown&gt; default-scheduler Successfully assigned default/liveness-httpget-pod to k8s-node02 Normal Pulled 7s (x2 over 8m19s) kubelet, k8s-node02 Container image &quot;registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17&quot; already present on machine Normal Created 7s (x2 over 8m19s) kubelet, k8s-node02 Created container liveness-httpget Normal Started 7s (x2 over 8m19s) kubelet, k8s-node02 Started container liveness-httpget Warning Unhealthy 7s (x3 over 13s) kubelet, k8s-node02 Liveness probe failed: HTTP probe failed with statuscode: 404 Normal Killing 7s kubelet, k8s-node02 Container liveness-httpget failed liveness probe, will be restarted 由上可见，当liveness-httpget检测失败，重建了Pod容器 存活检测-TCP端口pod yaml脚本 12345678910111213141516171819202122[root@k8s-master lifecycle]# pwd/root/k8s_practice/lifecycle[root@k8s-master lifecycle]# cat livenessProbe-tcp.yaml apiVersion: v1kind: Podmetadata: name: liveness-tcp-pod labels: test: livenessspec: containers: - name: liveness-tcp image: registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17 imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 livenessProbe: tcpSocket: port: 80 initialDelaySeconds: 5 periodSeconds: 3 TCP探测正常情况创建 Pod，查看pod状态 12345[root@k8s-master lifecycle]# kubectl apply -f livenessProbe-tcp.yamlpod/liveness-tcp-pod created[root@k8s-master lifecycle]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESliveness-tcp-pod 1/1 Running 0 50s 10.244.4.23 k8s-node01 &lt;none&gt; &lt;none&gt; 查看pod详情 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[root@k8s-master lifecycle]# kubectl describe pod liveness-tcp-podName: liveness-tcp-podNamespace: defaultPriority: 0Node: k8s-node01/172.16.1.111Start Time: Sat, 23 May 2020 18:02:46 +0800Labels: test=livenessAnnotations: kubectl.kubernetes.io/last-applied-configuration: &#123;&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Pod&quot;,&quot;metadata&quot;:&#123;&quot;annotations&quot;:&#123;&#125;,&quot;labels&quot;:&#123;&quot;test&quot;:&quot;liveness&quot;&#125;,&quot;name&quot;:&quot;liveness-tcp-pod&quot;,&quot;namespace&quot;:&quot;default&quot;&#125;...Status: RunningIP: 10.244.4.23IPs: IP: 10.244.4.23Containers: liveness-tcp: Container ID: docker://4de13e7c2e36c028b2094bf9dcf8e2824bfd15b8c45a0b963e301b91ee1a926d……………… Port: 80/TCP Host Port: 8080/TCP State: Running Started: Sat, 23 May 2020 18:03:04 +0800 Ready: True Restart Count: 0 Liveness: tcp-socket :80 delay=5s timeout=1s period=3s #success=1 #failure=3 Environment: &lt;none&gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-v48g4 (ro)Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: default-token-v48g4: Type: Secret (a volume populated by a Secret) SecretName: default-token-v48g4 Optional: falseQoS Class: BestEffortNode-Selectors: &lt;none&gt;Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300sEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled &lt;unknown&gt; default-scheduler Successfully assigned default/liveness-tcp-pod to k8s-node01 Normal Pulling 74s kubelet, k8s-node01 Pulling image &quot;registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17&quot; Normal Pulled 58s kubelet, k8s-node01 Successfully pulled image &quot;registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17&quot; Normal Created 57s kubelet, k8s-node01 Created container liveness-tcp Normal Started 57s kubelet, k8s-node01 Started container liveness-tcp 以上是正常情况，可见存活探测成功。 模拟TCP探测失败情况将上面yaml文件中的探测TCP端口进行如下修改： 123livenessProbe: tcpSocket: port: 8090 # 之前是80 删除之前的pod并重新创建，并过一会儿看pod状态 12345[root@k8s-master lifecycle]# kubectl apply -f livenessProbe-tcp.yaml pod/liveness-tcp-pod created[root@k8s-master lifecycle]# kubectl get pod -o wide # 可见RESTARTS变为了1，再过一会儿会变为2，之后依次叠加NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESliveness-tcp-pod 1/1 Running 1 25s 10.244.2.28 k8s-node02 &lt;none&gt; &lt;none&gt; pod详情 1234567891011121314151617[root@k8s-master lifecycle]# kubectl describe pod liveness-tcp-podName: liveness-tcp-podNamespace: defaultPriority: 0Node: k8s-node02/172.16.1.112Start Time: Sat, 23 May 2020 18:08:32 +0800Labels: test=liveness………………Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled &lt;unknown&gt; default-scheduler Successfully assigned default/liveness-tcp-pod to k8s-node02 Normal Pulled 12s (x2 over 29s) kubelet, k8s-node02 Container image &quot;registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17&quot; already present on machine Normal Created 12s (x2 over 29s) kubelet, k8s-node02 Created container liveness-tcp Normal Started 12s (x2 over 28s) kubelet, k8s-node02 Started container liveness-tcp Normal Killing 12s kubelet, k8s-node02 Container liveness-tcp failed liveness probe, will be restarted Warning Unhealthy 0s (x4 over 18s) kubelet, k8s-node02 Liveness probe failed: dial tcp 10.244.2.28:8090: connect: connection refused 由上可见，liveness-tcp检测失败，重建了Pod容器。 检测探针-启动检测有时候，会有一些现有的应用程序在启动时需要较多的初始化时间【如：Tomcat服务】。这种情况下，在不影响对触发这种探测的死锁的快速响应的情况下，设置存活探测参数是要有技巧的。 技巧就是使用一个命令来设置启动探测。针对HTTP 或者 TCP 检测，可以通过设置 failureThreshold * periodSeconds 参数来保证有足够长的时间应对糟糕情况下的启动时间。 示例如下： pod yaml文件 12345678910111213141516171819202122232425262728293031[root@k8s-master lifecycle]# pwd/root/k8s_practice/lifecycle[root@k8s-master lifecycle]# cat startupProbe-httpget.yamlapiVersion: v1kind: Podmetadata: name: startup-pod labels: test: startupspec: containers: - name: startup image: registry.cn-beijing.aliyuncs.com/google_registry/tomcat:7.0.94-jdk8-openjdk imagePullPolicy: IfNotPresent ports: - name: web-port containerPort: 8080 hostPort: 8080 livenessProbe: httpGet: path: /index.jsp port: web-port initialDelaySeconds: 5 periodSeconds: 10 failureThreshold: 1 startupProbe: httpGet: path: /index.jsp port: web-port periodSeconds: 10 #指定 kubelet 每隔 10 秒执行一次存活探测。默认是 10 秒。最小值是 1 failureThreshold: 30 #最大的失败次数 启动pod，并查看状态 12345[root@k8s-master lifecycle]# kubectl apply -f startupProbe-httpget.yaml pod/startup-pod created[root@k8s-master lifecycle]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESstartup-pod 1/1 Running 0 8m46s 10.244.4.26 k8s-node01 &lt;none&gt; &lt;none&gt; 查看pod详情 1[root@k8s-master ~]# kubectl describe pod startup-pod 有启动探测，应用程序将会有最多 5 分钟(30 * 10 = 300s) 的时间来完成它的启动。一旦启动探测成功一次，存活探测任务就会接管对容器的探测，对容器死锁可以快速响应。 如果启动探测一直没有成功，容器会在 300 秒后被杀死，并且根据 restartPolicy 来设置 Pod 状态。 探测器配置详解使用如下这些字段可以精确的控制存活和就绪检测行为： initialDelaySeconds：容器启动后要等待多少秒后存活和就绪探测器才被初始化，默认是 0 秒，最小值是 0。 periodSeconds：执行探测的时间间隔（单位是秒）。默认是 10 秒。最小值是 1。 timeoutSeconds：探测的超时时间。默认值是 1 秒。最小值是 1。 successThreshold：探测器在失败后，被视为成功的最小连续成功数。默认值是 1。存活探测的这个值必须是 1。最小值是 1。 failureThreshold：当探测失败时，Kubernetes 的重试次数。存活探测情况下的放弃就意味着重新启动容器。就绪探测情况下的放弃 Pod 会被打上未就绪的标签。默认值是 3。最小值是 1。 HTTP 探测器可以在 httpGet 上配置额外的字段： host：连接使用的主机名，默认是 Pod 的 IP。也可以在 HTTP 头中设置 “Host” 来代替。 scheme ：用于设置连接主机的方式（HTTP 还是 HTTPS）。默认是 HTTP。 path：访问 HTTP 服务的路径。 httpHeaders：请求中自定义的 HTTP 头。HTTP 头字段允许重复。 port：访问容器的端口号或者端口名。如果数字必须在 1 ～ 65535 之间。 相关阅读1、官网：配置存活、就绪和启动探测器 2、Kubernetes K8S之Pod 生命周期与init container初始化容器 完毕！]]></content>
      <categories>
        <category>Kubernetes</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
        <tag>K8S</tag>
        <tag>CI/CD</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes K8S之Pod 生命周期与init container初始化容器详解]]></title>
    <url>%2F2020%2F08%2F12%2Fkubernetes06%2F</url>
    <content type="text"><![CDATA[K8S中Pod的生命周期与init container初始化容器详解 主机配置规划 服务器名称(hostname) 系统版本 配置 内网IP 外网IP(模拟) k8s-master CentOS7.7 2C/4G/20G 172.16.1.110 10.0.0.110 k8s-node01 CentOS7.7 2C/4G/20G 172.16.1.111 10.0.0.111 k8s-node02 CentOS7.7 2C/4G/20G 172.16.1.112 10.0.0.112 Pod容器生命周期 Pause容器说明每个Pod里运行着一个特殊的被称之为Pause的容器，其他容器则为业务容器，这些业务容器共享Pause容器的网络栈和Volume挂载卷，因此他们之间通信和数据交换更为高效。在设计时可以充分利用这一特性，将一组密切相关的服务进程放入同一个Pod中；同一个Pod里的容器之间仅需通过localhost就能互相通信。 kubernetes中的pause容器主要为每个业务容器提供以下功能： PID命名空间：Pod中的不同应用程序可以看到其他应用程序的进程ID。 网络命名空间：Pod中的多个容器能够访问同一个IP和端口范围。 IPC命名空间：Pod中的多个容器能够使用System V IPC或POSIX消息队列进行通信。 UTS命名空间：Pod中的多个容器共享一个主机名；Volumes（共享存储卷）。 Pod中的各个容器可以访问在Pod级别定义的Volumes。 Init Container容器Pod可以包含多个容器，应用运行在这些容器里面，同时 Pod 也可以有一个或多个先于应用容器启动的 Init 容器。 如果为一个 Pod 指定了多个 Init 容器，这些Init容器会按顺序逐个运行。每个 Init 容器都必须运行成功，下一个才能够运行。当所有的 Init 容器运行完成时，Kubernetes 才会为 Pod 初始化应用容器并像平常一样运行。 Init容器与普通的容器非常像，除了以下两点： 1、Init容器总是运行到成功完成且正常退出为止 2、只有前一个Init容器成功完成并正常退出，才能运行下一个Init容器。 如果Pod的Init容器失败，Kubernetes会不断地重启Pod，直到Init容器成功为止。但如果Pod对应的restartPolicy为Never，则不会重新启动。 在所有的 Init 容器没有成功之前，Pod 将不会变成 Ready 状态。 Init 容器的端口将不会在 Service 中进行聚集。 正在初始化中的 Pod 处于 Pending 状态，但会将条件 Initializing 设置为 true。 如果 Pod 重启，所有 Init 容器必须重新执行。 在 Pod 中的每个应用容器和 Init 容器的名称必须唯一；与任何其它容器共享同一个名称，会在校验时抛出错误。 Init 容器能做什么？因为 Init 容器是与应用容器分离的单独镜像，其启动相关代码具有如下优势： 1、Init 容器可以包含一些安装过程中应用容器不存在的实用工具或个性化代码。例如，在安装过程中要使用类似 sed、 awk、 python 或 dig 这样的工具，那么放到Init容器去安装这些工具；再例如，应用容器需要一些必要的目录或者配置文件甚至涉及敏感信息，那么放到Init容器去执行。而不是在主容器执行。 2、Init 容器可以安全地运行这些工具，避免这些工具导致应用镜像的安全性降低。 3、应用镜像的创建者和部署者可以各自独立工作，而没有必要联合构建一个单独的应用镜像。 4、Init 容器能以不同于Pod内应用容器的文件系统视图运行。因此，Init容器可具有访问 Secrets 的权限，而应用容器不能够访问。 5、由于 Init 容器必须在应用容器启动之前运行完成，因此 Init 容器提供了一种机制来阻塞或延迟应用容器的启动，直到满足了一组先决条件。一旦前置条件满足，Pod内的所有的应用容器会并行启动。 Init 容器示例下面的例子定义了一个具有 2 个 Init 容器的简单 Pod。 第一个等待 myservice 启动，第二个等待 mydb 启动。 一旦这两个 Init容器都启动完成，Pod 将启动spec区域中的应用容器。 Pod yaml文件123456789101112131415161718192021[root@k8s-master lifecycle]# pwd/root/k8s_practice/lifecycle[root@k8s-master lifecycle]# cat init_C_pod.yamlapiVersion: v1kind: Podmetadata: name: myapp-busybox-pod labels: app: myappspec: containers: - name: myapp-container image: registry.cn-beijing.aliyuncs.com/google_registry/busybox:1.24 command: [&apos;sh&apos;, &apos;-c&apos;, &apos;echo The app is running! &amp;&amp; sleep 3600&apos;] initContainers: - name: init-myservice image: registry.cn-beijing.aliyuncs.com/google_registry/busybox:1.24 command: [&apos;sh&apos;, &apos;-c&apos;, &quot;until nslookup myservice; do echo waiting for myservice; sleep 60; done&quot;] - name: init-mydb image: registry.cn-beijing.aliyuncs.com/google_registry/busybox:1.24 command: [&apos;sh&apos;, &apos;-c&apos;, &quot;until nslookup mydb; do echo waiting for mydb; sleep 60; done&quot;] 启动这个 Pod，并检查其状态，可以执行如下命令： 12345[root@k8s-master lifecycle]# kubectl apply -f init_C_pod.yaml pod/myapp-busybox-pod created [root@k8s-master lifecycle]# kubectl get -f init_C_pod.yaml -o wide # 或者kubectl get pod myapp-busybox-pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmyapp-busybox-pod 0/1 Init:0/2 0 55s 10.244.4.16 k8s-node01 &lt;none&gt; &lt;none&gt; 如需更详细的信息： 123456789101112131415[root@k8s-master lifecycle]# kubectl describe pod myapp-busybox-pod Name: myapp-busybox-podNamespace: defaultPriority: 0…………Node-Selectors: &lt;none&gt;Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300sEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 2m18s default-scheduler Successfully assigned default/myapp-busybox-pod to k8s-node01 Normal Pulled 2m17s kubelet, k8s-node01 Container image &quot;registry.cn-beijing.aliyuncs.com/google_registry/busybox:1.24&quot; already present on machine Normal Created 2m17s kubelet, k8s-node01 Created container init-myservice Normal Started 2m17s kubelet, k8s-node01 Started container init-myservice 如需查看Pod内 Init 容器的日志，请执行： 1234567891011[root@k8s-master lifecycle]# kubectl logs -f --tail 500 myapp-busybox-pod -c init-myservice # 第一个 init container 详情Server: 10.96.0.10Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.localwaiting for myservicenslookup: can&apos;t resolve &apos;myservice&apos;Server: 10.96.0.10Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local………………[root@k8s-master lifecycle]# kubectl logs myapp-busybox-pod -c init-mydb # 第二个 init container 详情Error from server (BadRequest): container &quot;init-mydb&quot; in pod &quot;myapp-busybox-pod&quot; is waiting to start: PodInitializing 此时Init 容器将会等待直至发现名称为mydb和myservice的 Service。 Service yaml文件1234567891011121314151617181920212223[root@k8s-master lifecycle]# pwd/root/k8s_practice/lifecycle[root@k8s-master lifecycle]# cat init_C_service.yaml ---kind: ServiceapiVersion: v1metadata: name: myservicespec: ports: - protocol: TCP port: 80 targetPort: 9376---kind: ServiceapiVersion: v1metadata: name: mydbspec: ports: - protocol: TCP port: 80 targetPort: 9377 创建mydb和myservice的 service 命令： 123[root@k8s-master lifecycle]# kubectl create -f init_C_service.yaml service/myservice createdservice/mydb created 之后查看pod状态和service状态，能看到这些 Init容器执行完毕后，随后myapp-busybox-pod的Pod转移进入 Running 状态： 12345678[root@k8s-master lifecycle]# kubectl get svc -o wide mydb myserviceNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORmydb ClusterIP 10.108.24.84 &lt;none&gt; 80/TCP 72s &lt;none&gt;myservice ClusterIP 10.105.252.196 &lt;none&gt; 80/TCP 72s &lt;none&gt;[root@k8s-master lifecycle]# [root@k8s-master lifecycle]# kubectl get pod myapp-busybox-pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESmyapp-busybox-pod 1/1 Running 0 7m33s 10.244.4.17 k8s-node01 &lt;none&gt; &lt;none&gt; 由上可知：一旦我们启动了 mydb 和 myservice 这两个 Service，我们就能够看到 Init 容器完成，并且 myapp-busybox-pod 被创建。 进入myapp-busybox-pod容器，并通过nslookup查看这两个Service的DNS记录。 12345678910111213141516[root@k8s-master lifecycle]# kubectl exec -it myapp-busybox-pod sh/ # nslookup mydb Server: 10.96.0.10Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.localName: mydbAddress 1: 10.108.24.84 mydb.default.svc.cluster.local/ # / # / # / # nslookup myserviceServer: 10.96.0.10Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.localName: myserviceAddress 1: 10.105.252.196 myservice.default.svc.cluster.local 完毕！]]></content>
      <categories>
        <category>Kubernetes</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
        <tag>K8S</tag>
        <tag>CI/CD</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes K8S之kubectl命令详解及常用示例]]></title>
    <url>%2F2020%2F08%2F09%2Fkubernetes05%2F</url>
    <content type="text"><![CDATA[Kubernetes kubectl命令详解与常用示例，基于k8s v1.17.4版本 kubectl常用示例查看类命令12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970# 获取节点和服务版本信息kubectl get nodes# 获取节点和服务版本信息，并查看附加信息kubectl get nodes -o wide# 获取pod信息，默认是default名称空间kubectl get pod# 获取pod信息，默认是default名称空间，并查看附加信息【如：pod的IP及在哪个节点运行】kubectl get pod -o wide# 获取指定名称空间的podkubectl get pod -n kube-system# 获取指定名称空间中的指定podkubectl get pod -n kube-system podName# 获取所有名称空间的podkubectl get pod -A # 查看pod的详细信息，以yaml格式或json格式显示kubectl get pods -o yamlkubectl get pods -o json# 查看pod的标签信息kubectl get pod -A --show-labels # 根据Selector（label query）来查询podkubectl get pod -A --selector=&quot;k8s-app=kube-dns&quot;# 查看运行pod的环境变量kubectl exec podName env# 查看指定pod的日志kubectl logs -f --tail 500 -n kube-system kube-apiserver-k8s-master# 查看所有名称空间的service信息kubectl get svc -A# 查看指定名称空间的service信息kubectl get svc -n kube-system# 查看componentstatuses信息kubectl get cs# 查看所有configmaps信息kubectl get cm -A# 查看所有serviceaccounts信息kubectl get sa -A# 查看所有daemonsets信息kubectl get ds -A# 查看所有deployments信息kubectl get deploy -A# 查看所有replicasets信息kubectl get rs -A# 查看所有statefulsets信息kubectl get sts -A# 查看所有jobs信息kubectl get jobs -A# 查看所有ingresses信息kubectl get ing -A# 查看有哪些名称空间kubectl get ns# 查看pod的描述信息kubectl describe pod podNamekubectl describe pod -n kube-system kube-apiserver-k8s-master # 查看指定名称空间中指定deploy的描述信息kubectl describe deploy -n kube-system coredns# 查看node或pod的资源使用情况# 需要heapster 或metrics-server支持kubectl top nodekubectl top pod # 查看集群信息kubectl cluster-info 或 kubectl cluster-info dump# 查看各组件信息【172.16.1.110为master机器】kubectl -s https://172.16.1.110:6443 get componentstatuses 操作类命令123456789101112131415161718192021222324252627# 创建资源kubectl create -f xxx.yaml# 应用资源kubectl apply -f xxx.yaml# 应用资源，该目录下的所有 .yaml, .yml, 或 .json 文件都会被使用kubectl apply -f &lt;directory&gt;# 创建test名称空间kubectl create namespace test# 删除资源kubectl delete -f xxx.yamlkubectl delete -f &lt;directory&gt;# 删除指定的podkubectl delete pod podName# 删除指定名称空间的指定podkubectl delete pod -n test podName# 删除其他资源kubectl delete svc svcNamekubectl delete deploy deployNamekubectl delete ns nsName# 强制删除kubectl delete pod podName -n nsName --grace-period=0 --forcekubectl delete pod podName -n nsName --grace-period=1kubectl delete pod podName -n nsName --now# 编辑资源kubectl edit pod podName 进阶命令操作1234567891011121314151617181920# kubectl exec：进入pod启动的容器kubectl exec -it podName -n nsName /bin/sh #进入容器kubectl exec -it podName -n nsName /bin/bash #进入容器# kubectl label：添加label值kubectl label nodes k8s-node01 zone=north #为指定节点添加标签 kubectl label nodes k8s-node01 zone- #为指定节点删除标签kubectl label pod podName -n nsName role-name=test #为指定pod添加标签kubectl label pod podName -n nsName role-name=dev --overwrite #修改lable标签值kubectl label pod podName -n nsName role-name- #删除lable标签# kubectl滚动升级； 通过 kubectl apply -f myapp-deployment-v1.yaml 启动deploykubectl apply -f myapp-deployment-v2.yaml #通过配置文件滚动升级kubectl set image deploy/myapp-deployment myapp=&quot;registry.cn-beijing.aliyuncs.com/google_registry/myapp:v3&quot; #通过命令滚动升级kubectl rollout undo deploy/myapp-deployment 或者 kubectl rollout undo deploy myapp-deployment #pod回滚到前一个版本kubectl rollout undo deploy/myapp-deployment --to-revision=2 #回滚到指定历史版本# kubectl scale：动态伸缩kubectl scale deploy myapp-deployment --replicas=5 # 动态伸缩kubectl scale --replicas=8 -f myapp-deployment-v2.yaml #动态伸缩【根据资源类型和名称伸缩，其他配置「如：镜像版本不同」不生效】 上面滚动更新和动态伸缩涉及的deploy的yaml文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748[root@k8s-master deploy]# cat myapp-deployment-v1.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: myapp-deployment labels: app: myappspec: replicas: 10 # 重点关注该字段 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: - name: myapp image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v1 ports: - containerPort: 80[root@k8s-master deploy]# [root@k8s-master deploy]# cat myapp-deployment-v2.yaml apiVersion: apps/v1kind: Deploymentmetadata: name: myapp-deployment labels: app: myappspec: replicas: 10 # 重点关注该字段 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: containers: - name: myapp image: registry.cn-beijing.aliyuncs.com/google_registry/myapp:v2 ports: - containerPort: 80 kubectl语法1kubectl [command] [TYPE] [NAME] [flags] 12官网地址：https://kubernetes.io/docs/reference/kubectl/overview/ kubectl flags：https://kubernetes.io/docs/reference/kubectl/kubectl/ 说明： 1、command：指定在一个或多个资源上要执行的操作。例如：create、get、describe、delete、apply等 2、TYPE：指定资源类型(如：pod、node、services、deployments等)。资源类型大小写敏感，可以指定单数、复数或缩写形式。例如，以下命令生成相同的输出： 123kubectl get pod -n kubernetes-dashboardkubectl get pods -n kubernetes-dashboardkubectl get po -n kubernetes-dashboard 3、NAME：指定资源的名称。名称大小写敏感。如果省略名称空间，则显示默认名称空间资源的详细信息或者提示：No resources found in default namespace.。 12345678910111213141516171819202122232425# 示例：[root@k8s-master ~]# kubectl get podsNo resources found in default namespace.[root@k8s-master ~]# kubectl get pods --all-namespaces # 或者 kubectl get pods --ANAMESPACE NAME READY STATUS RESTARTS AGEkube-system coredns-6955765f44-c9zfh 1/1 Running 8 6d7hkube-system coredns-6955765f44-lrz5q 1/1 Running 8 6d7hkube-system etcd-k8s-master 1/1 Running 9 6d7hkube-system kube-apiserver-k8s-master 1/1 Running 9 6d7hkube-system kube-controller-manager-k8s-master 1/1 Running 8 6d7hkube-system kube-flannel-ds-amd64-dngrk 1/1 Running 13 6d7hkube-system kube-flannel-ds-amd64-h4sn6 1/1 Running 13 6d6hkube-system kube-flannel-ds-amd64-m92wp 1/1 Running 11 6d6hkube-system kube-proxy-28dwj 1/1 Running 9 6d6hkube-system kube-proxy-c875m 1/1 Running 8 6d7hkube-system kube-proxy-stg6w 1/1 Running 10 6d6hkube-system kube-scheduler-k8s-master 1/1 Running 9 6d7hkubernetes-dashboard dashboard-metrics-scraper-7b8b58dc8b-nr5fz 1/1 Running 7 6d1hkubernetes-dashboard kubernetes-dashboard-755dcb9575-9kg7p 1/1 Running 9 6d1h[root@k8s-master ~]# kubectl get service --all-namespaces # 或者 kubectl get service -ANAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdefault kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 6d7hkube-system kube-dns ClusterIP 10.96.0.10 &lt;none&gt; 53/UDP,53/TCP,9153/TCP 6d7hkubernetes-dashboard dashboard-metrics-scraper ClusterIP 10.104.12.221 &lt;none&gt; 8000/TCP 6d1hkubernetes-dashboard kubernetes-dashboard NodePort 10.110.157.29 &lt;none&gt; 443:30001/TCP 6d1h 3、flags：指定可选的标记。例如，可以使用 -s 或 --server标识来指定Kubernetes API服务器的地址和端口；-n指定名称空间；等等。 注意：你从命令行指定的flags将覆盖默认值和任何相应的环境变量。优先级最高。 4、在多个资源上执行操作时，可以通过类型 [TYPE] 和名称 [NAME] 指定每个资源，也可以指定一个或多个文件。 按类型和名称指定资源： 123456789101112131415# 查看一个资源类型中的多个资源[root@k8s-master ~]# kubectl get pod -n kube-system coredns-6955765f44-c9zfh kube-proxy-28dwjNAME READY STATUS RESTARTS AGEcoredns-6955765f44-c9zfh 1/1 Running 8 6d7hkube-proxy-28dwj 1/1 Running 9 6d6h[root@k8s-master ~]# # 查看多个资源类型[root@k8s-master ~]# kubectl get svc,nodeNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 45hNAME STATUS ROLES AGE VERSIONnode/k8s-master Ready master 45h v1.17.4node/k8s-node01 Ready &lt;none&gt; 45h v1.17.4node/k8s-node02 Ready &lt;none&gt; 45h v1.17.4 使用一个或多个文件指定资源：-f file1 -f file2 -f file&lt;#&gt; 12# 使用YAML而不是JSON，因为YAML更容易使用，特别是对于配置文件。kubectl get pod -f pod.yaml kubectl语法中的command操作下表包括常见kubectl操作的简短描述和通用语法： 也可在命令行可通过kubectl -h 命令获取部分信息或者通过以下地址查看更多详情： 12https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commandshttps://kubernetes.io/docs/reference/kubectl/overview/#operations Operation Syntax Description create kubectl create -f FILENAME [flags] 从文件或标准输入创建一个或多个资源★★★ expose kubectl expose (-f FILENAME TYPE NAME run kubectl run NAME –image=image [–env=”key=value”] [–port=port] [–replicas=replicas] [–dry-run=bool] [–overrides=inline-json] [flags] 在集群上运行指定的镜像★★★ explain kubectl explain [–recursive=false] [flags] 获取各种资源的文档。例如pods、nodes、services等。★★★★★ get kubectl get (-f FILENAME TYPE [NAME edit kubectl edit (-f FILENAME TYPE NAME delete kubectl delete (-f FILENAME TYPE [NAME rollout kubectl rollout SUBCOMMAND [options] 对资源进行管理。有效的资源类型包括：deployments，daemonsets 和statefulsets scale kubectl scale (-f FILENAME TYPE NAME autoscale kubectl autoscale (-f FILENAME TYPE NAME cluster-info kubectl cluster-info [flags] 显示集群信息，显示关于集群中的主机和服务的端点信息。★★★ top kubectl top node、kubectl top pod 需要heapster 或metrics-server支持 显示资源(CPU/内存/存储)使用情况★★★ cordon kubectl cordon NODE [options] 将node标记为不可调度 uncordon kubectl uncordon NODE [options] 将node标记为可调度 drain kubectl drain NODE [options] 排除指定node节点，为维护做准备 taint kubectl taint NODE NAME KEY_1=VAL_1:TAINT_EFFECT_1 … KEY_N=VAL_N:TAINT_EFFECT_N [options] 更新一个或多个节点上的污点★★★ describe kubectl describe (-f FILENAME TYPE [NAME_PREFIX logs kubectl logs POD [-c CONTAINER] [–follow] [flags] 打印pod中一个容器的日志★★★★★ exec kubectl exec POD [-c CONTAINER] [-i] [-t] [flags] [– COMMAND [args…]] 对pod中的容器执行命令或进入Pod容器★★★★★ proxy kubectl proxy [–port=PORT] [–www=static-dir] [–www-prefix=prefix] [–api-prefix=prefix] [flags] 运行Kubernetes API服务的代理 cp kubectl cp [options] 从宿主机复制文件和目录到一个容器；或则从容器中复制文件和目录到宿主机★★★ auth kubectl auth [flags] [options] 检查授权 apply kubectl apply -f FILENAME [flags] 通过文件名中的内容或stdin将配置应用于资源★★★★★ patch kubectl patch (-f FILENAME TYPE NAME replace kubectl replace -f FILENAME 通过文件或stdin替换资源 rolling-update kubectl rolling-update OLD_CONTROLLER_NAME ([NEW_CONTROLLER_NAME] –image=NEW_CONTAINER_IMAGE -f NEW_CONTROLLER_SPEC) [flags] label kubectl label (-f FILENAME TYPE NAME annotate kubectl annotate (-f FILENAME TYPE NAME api-resources kubectl api-resources [flags] [options] 打印支持的API资源★★★ api-versions kubectl api-versions [flags] 列出可用的API版本★★★ config kubectl config SUBCOMMAND [flags] 修改kubeconfig文件。有关详细信息，请参见各个子命令 plugin kubectl plugin [flags] [options] 提供与插件交互的实用工具 version kubectl version [–client] [flags] 显示在客户端和服务器上运行的Kubernetes版本★★★ kubectl语法中的TYPE资源下表包含常用的资源类型及其缩写别名的列表。 也可以在命令行通过kubectl api-resources得到。 Resource Name Short Names Namespaced Resource Kind bindings TRUE Binding componentstatuses cs FALSE ComponentStatus configmaps cm TRUE ConfigMap endpoints ep TRUE Endpoints events ev TRUE Event limitranges limits TRUE LimitRange namespaces ns FALSE Namespace nodes no FALSE Node persistentvolumeclaims pvc TRUE PersistentVolumeClaim persistentvolumes pv FALSE PersistentVolume pods po TRUE Pod podtemplates TRUE PodTemplate replicationcontrollers rc TRUE ReplicationController resourcequotas quota TRUE ResourceQuota secrets TRUE Secret serviceaccounts sa TRUE ServiceAccount services svc TRUE Service mutatingwebhookconfigurations FALSE MutatingWebhookConfiguration validatingwebhookconfigurations FALSE ValidatingWebhookConfiguration customresourcedefinitions crd, crds FALSE CustomResourceDefinition apiservices FALSE APIService controllerrevisions TRUE ControllerRevision daemonsets ds TRUE DaemonSet deployments deploy TRUE Deployment replicasets rs TRUE ReplicaSet statefulsets sts TRUE StatefulSet tokenreviews FALSE TokenReview localsubjectaccessreviews TRUE LocalSubjectAccessReview selfsubjectaccessreviews FALSE SelfSubjectAccessReview selfsubjectrulesreviews FALSE SelfSubjectRulesReview subjectaccessreviews FALSE SubjectAccessReview horizontalpodautoscalers hpa TRUE HorizontalPodAutoscaler cronjobs cj TRUE CronJob jobs TRUE Job certificatesigningrequests csr FALSE CertificateSigningRequest leases TRUE Lease endpointslices TRUE EndpointSlice events ev TRUE Event ingresses ing TRUE Ingress networkpolicies netpol TRUE NetworkPolicy runtimeclasses FALSE RuntimeClass poddisruptionbudgets pdb TRUE PodDisruptionBudget podsecuritypolicies psp FALSE PodSecurityPolicy clusterrolebindings FALSE ClusterRoleBinding clusterroles FALSE ClusterRole rolebindings TRUE RoleBinding roles TRUE Role priorityclasses pc FALSE PriorityClass csidrivers FALSE CSIDriver csinodes FALSE CSINode storageclasses sc FALSE StorageClass volumeattachments FALSE VolumeAttachment kubectl 输出选项格式化输出所有kubectl命令的默认输出格式是人类可读的纯文本格式。 要将详细信息以特定的格式输出到终端窗口，可以将 -o 或 --output标识添加到受支持的kubectl命令中。 语法1kubectl [command] [TYPE] [NAME] -o &lt;output_format&gt; 根据kubectl操作，支持以下输出格式： Output format Description -o custom-columns= 使用逗号分隔的自定义列列表打印表 -o custom-columns-file= 使用文件中的自定义列模板打印表 -o json 输出一个JSON格式的API对象 -o jsonpath= 打印jsonpath表达式中定义的字段 -o jsonpath-file= 通过文件打印jsonpath表达式定义的字段 -o name 只打印资源名，不打印其他任何内容 -o wide 以纯文本格式输出，包含附加信息。对于pods，包含节点名 -o yaml 输出一个YAML格式的API对象 示例wide示例 1234567[root@k8s-master ~]# kubectl get pod NAME READY STATUS RESTARTS AGEnginx-demo 1/1 Running 1 28h[root@k8s-master ~]# [root@k8s-master ~]# kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnginx-demo 1/1 Running 1 28h 10.244.3.9 k8s-node01 &lt;none&gt; &lt;none&gt; yaml示例 123456789101112[root@k8s-master ~]# kubectl get pod NAME READY STATUS RESTARTS AGEnginx-demo 1/1 Running 1 28h[root@k8s-master ~]# [root@k8s-master ~]# kubectl get pod -o yamlapiVersion: v1items:- apiVersion: v1 kind: Pod metadata: annotations:……………… json示例 1234567891011121314[root@k8s-master ~]# kubectl get pod NAME READY STATUS RESTARTS AGEnginx-demo 1/1 Running 1 28h[root@k8s-master ~]# [root@k8s-master ~]# kubectl get pod -o json&#123; &quot;apiVersion&quot;: &quot;v1&quot;, &quot;items&quot;: [ &#123; &quot;apiVersion&quot;: &quot;v1&quot;, &quot;kind&quot;: &quot;Pod&quot;, &quot;metadata&quot;: &#123; &quot;annotations&quot;: &#123;……………… name示例 123456[root@k8s-master ~]# kubectl get pod NAME READY STATUS RESTARTS AGEnginx-demo 1/1 Running 1 28h[root@k8s-master ~]# [root@k8s-master ~]# kubectl get pod -o namepod/nginx-demo custom-columns示例 1234567[root@k8s-master ~]# kubectl get podNAME READY STATUS RESTARTS AGEnginx-demo 1/1 Running 1 29h[root@k8s-master ~]# [root@k8s-master ~]# kubectl get pods -o custom-columns=NAME:.metadata.name,UID:.metadata.uid,imageName:.spec.containers[0].imageNAME UID imageNamenginx-demo 08121fc6-969b-4b4e-9aa4-b990a5d02148 registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17 说明：custom-columns=key:value；其中key表示列明；value表示要显示信息，这个value信息可以通过-o json或-o yaml获取。 custom-columns-file示例 123456789101112[root@k8s-master test]# kubectl get pod NAME READY STATUS RESTARTS AGEnginx-demo 1/1 Running 0 80s[root@k8s-master test]# # 要显示的列明和数据来源[root@k8s-master test]# cat custom-col.conf NAME UID imageName containerPortmetadata.name metadata.uid spec.containers[0].image spec.containers[0].ports[0].containerPort[root@k8s-master test]# [root@k8s-master test]# kubectl get pod -o custom-columns-file=custom-col.confNAME UID imageName containerPortnginx-demo 769dc3f4-2ffc-407c-a351-56b74ddaba4c registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17 80 jsonpath示例 123456[root@k8s-master test]# kubectl get podsNAME READY STATUS RESTARTS AGEnginx-demo 1/1 Running 0 13m[root@k8s-master test]# [root@k8s-master test]# kubectl get pods -o jsonpath=&apos;&#123;.items[0].metadata.name&#125;,&#123;.items[0].spec.containers[0].image&#125;&apos;nginx-demo,registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17 jsonpath-file示例 12345678910[root@k8s-master test]# kubectl get pod NAME READY STATUS RESTARTS AGEnginx-demo 1/1 Running 0 16m[root@k8s-master test]# # 要显示的数据来源[root@k8s-master test]# cat custom-json.conf&#123;.items[0].metadata.name&#125;,&#123;.items[0].spec.containers[0].image&#125;,&#123;.items[0].spec.containers[0].ports[0].containerPort&#125;[root@k8s-master test]# [root@k8s-master test]# kubectl get pod -o jsonpath-file=custom-json.confnginx-demo,registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17,80 相关阅读1、官网kubectl概述 2、官网kubectl [flags] 3、官网kubectl-commands详情 完毕！]]></content>
      <categories>
        <category>Kubernetes</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
        <tag>K8S</tag>
        <tag>CI/CD</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes K8S之通过yaml文件创建Pod与Pod常用字段详解]]></title>
    <url>%2F2020%2F08%2F05%2Fkubernetes04%2F</url>
    <content type="text"><![CDATA[YAML语法规范；在kubernetes k8s中如何通过yaml文件创建pod，以及pod常用字段详解 YAML 语法规范K8S 里所有的资源或者配置都可以用 yaml 或 Json 定义。YAML 是一个 JSON 的超集，任何有效的 JSON 文件也都是一个有效的YAML文件。 具体参见：「YAML 语言教程与使用案例」 通过yaml创建nginx pod对象yaml文件在Kubernetes的 yaml文件中，最好不要出现下划线，可以有中横线。 123456789101112131415161718192021222324252627282930313233343536373839404142[root@k8s-master test]# pwd/root/k8s_practice/test[root@k8s-master test]# cat nginx_demo.yamlapiVersion: v1kind: Podmetadata: name: nginx-demo namespace: default labels: k8s-app: nginx environment: dev annotations: name: nginx-demospec: containers: - name: nginx image: registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17 imagePullPolicy: IfNotPresent ports: - name: httpd containerPort: 80 #除非绝对必要，否则不要为 Pod 指定 hostPort。 将 Pod 绑定到hostPort时，它会限制 Pod 可以调度的位置数 #DaemonSet 中的 Pod 可以使用 hostPort，从而可以通过节点 IP 访问到 Pod；因为DaemonSet模式下Pod不会被调度到其他节点。 #一般情况下 containerPort与hostPort值相同 hostPort: 8090 #可以通过宿主机+hostPort的方式访问该Pod。例如：pod在/调度到了k8s-node02【172.16.1.112】，那么该Pod可以通过172.16.1.112:8090方式进行访问。 protocol: TCP volumeMounts: #定义容器挂载内容 - name: nginx-site #使用的存储卷名称，跟下面volume字段的某个name值相同，这里表示使用volume的nginx-site这个存储卷 mountPath: /usr/share/nginx/html #挂载至容器中哪个目录 readOnly: false #读写挂载方式，默认为读写模式false - name: nginx-log mountPath: /var/log/nginx/ readOnly: false volumes: #volumes字段定义了paues容器关联的宿主机或分布式文件系统存储卷 - name: nginx-site #存储卷名称 hostPath: #路径，为宿主机存储路径 path: /data/volumes/nginx/html/ #在宿主机上目录的路径 type: DirectoryOrCreate #定义类型，这表示如果宿主机没有此目录，则会自动创建 - name: nginx-log hostPath: path: /data/volumes/nginx/log/ type: DirectoryOrCreate Pod常见操作12345678910111213141516171819202122[root@k8s-master test]# pwd/root/k8s_practice/test[root@k8s-master test]# lltotal 4-rw-r--r-- 1 root root 1317 Jul 29 16:42 nginx_demo.yaml# 创建pod[root@k8s-master test]# kubectl apply -f nginx_demo.yaml pod/nginx-demo created# 查看pod。根据结果可见被调度到了 k8s-node02 节点[root@k8s-master test]# kubectl get pod -o wide # 或者 kubectl get pod -n default -o wide 因为名称空间为defaultNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnginx-demo 1/1 Running 0 10s 10.244.2.16 k8s-node02 &lt;none&gt; &lt;none&gt;# 查看pod描述[root@k8s-master test]# kubectl describe pod -n default nginx-demo # 由于是默认名称空间，因此可以省略 -n default…………# 查看指定pod的基本信息，并显示标签信息[root@k8s-master test]# kubectl get pod nginx-demo -o wide --show-labelsNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES LABELSnginx-demo 1/1 Running 0 61s 10.244.2.16 k8s-node02 &lt;none&gt; &lt;none&gt; environment=dev,k8s-app=nginx# 删除pod[root@k8s-master test]# kubectl delete -f nginx_demo.yaml # 或者 kubectl delete pod nginx-demopod &quot;nginx-demo&quot; deleted volume查看由上可知 pod nginx-demo 被调度到了 k8s-node02 节点。那么对应的volume信息如下： 站点信息 12345[root@k8s-node02 nginx]# ll /data/volumes/nginx/html/ # 宿主机目录已经被创建total 4-rw-r--r-- 1 root root 14 May 20 22:50 index.html[root@k8s-node02 nginx]# vim /data/volumes/nginx/html/index.html # 然后我们创建该文件，用于后续站点访问&lt;h1&gt;Test&lt;/h1&gt; 日志信息 1234[root@k8s-node02 nginx]# ll /data/volumes/nginx/log/ # 宿主机目录已经被创建，且下面的两个日志文件也是容器启动时创建的total 4-rw-r--r-- 1 root root 0 May 20 23:04 access.log-rw-r--r-- 1 root root 0 May 20 22:47 error.log nginx站点访问查看指定pod的基本信息。得到了该pod的IP 12345678[root@k8s-master k8s_study]# kubectl get pod nginx-demo -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnginx-demo 1/1 Running 0 61s 10.244.2.16 k8s-node02 &lt;none&gt; &lt;none&gt;[root@k8s-master k8s_study]# curl http://10.244.2.16 # curl 访问站点&lt;h1&gt;Test&lt;/h1&gt;# 由于pod调用到了k8s-node02【172.16.1.112】节点，因此可以通过节点+hostPort端口方式访问[root@k8s-master test]# curl 172.16.1.112:8090&lt;h1&gt;Test&lt;/h1&gt; 同时在k8s-node02节点机器也可见访问日志，信息如下： 12345[root@k8s-node02 nginx]# cat /data/volumes/nginx/log/access.log 10.244.0.0 - - [20/May/2020:15:04:42 +0000] &quot;GET / HTTP/1.1&quot; 200 14 &quot;-&quot; &quot;curl/7.29.0&quot; &quot;-&quot;10.244.0.0 - - [20/May/2020:15:34:50 +0000] &quot;GET / HTTP/1.1&quot; 200 14 &quot;-&quot; &quot;curl/7.29.0&quot; &quot;-&quot;172.16.1.110 - - [20/May/2020:15:35:50 +0000] &quot;GET / HTTP/1.1&quot; 200 14 &quot;-&quot; &quot;curl/7.29.0&quot; &quot;-&quot;172.16.1.110 - - [20/May/2020:15:36:50 +0000] &quot;GET / HTTP/1.1&quot; 200 14 &quot;-&quot; &quot;curl/7.29.0&quot; &quot;-&quot; K8S Pod Yaml文件参数详细说明特别说明：注意格式，层级与缩进。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081apiVersion: v1 #必选，版本号，例如v1，可以用 kubectl api-versions 查询到kind: Pod #必选，指yaml文件定义的k8s 资源类型或角色，比如：Podmetadata: #必选，元数据对象 name: string #必选，元数据对象的名字，自己定义，比如命名Pod的名字 namespace: string #必选，元数据对象的名称空间，默认为&quot;default&quot; labels: #自定义标签 key1: value1 #自定义标签键值对1 key2: value2 #自定义标签键值对2 annotations: #自定义注解 key1: value1 #自定义注解键值对1 key2: value2 #自定义注解键值对2spec: #必选，对象【如pod】的详细定义 containers: #必选，spec对象的容器信息 - name: string #必选，容器名称 image: string #必选，要用到的镜像名称 imagePullPolicy: [Always|Never|IfNotPresent] #获取镜像的策略；(1)Always：意思是每次都尝试重新拉取镜像；(2)Never：表示仅使用本地镜像，即使本地没有镜像也不拉取；(3) IfNotPresent：如果本地有镜像就使用本地镜像，没有就拉取远程镜像。默认：Always command: [string] #指定容器启动命令，由于是数组因此可以指定多个。不指定则使用镜像打包时指定的启动命令。 args: [string] #指定容器启动命令参数，由于是数组因此可以指定多个 workingDir: string #指定容器的工作目录 volumeMounts: #指定容器内部的存储卷配置 - name: string #指定可以被容器挂载的存储卷的名称。跟下面volume字段的name值相同表示使用这个存储卷 mountPath: string #指定可以被容器挂载的存储卷的路径，应少于512字符 readOnly: boolean #设置存储卷路径的读写模式，true或者false，默认为读写模式false ports: #需要暴露的端口号列表 - name: string #端口的名称 containerPort: int #容器监听的端口号 #除非绝对必要，否则不要为 Pod 指定 hostPort。 将 Pod 绑定到hostPort时，它会限制 Pod 可以调度的位置数 #DaemonSet 中的 Pod 可以使用 hostPort，从而可以通过节点 IP 访问到 Pod；因为DaemonSet模式下Pod不会被调度到其他节点。 #一般情况下 containerPort与hostPort值相同 hostPort: int #可以通过宿主机+hostPort的方式访问该Pod。例如：pod在/调度到了k8s-node02【172.16.1.112】，hostPort为8090，那么该Pod可以通过172.16.1.112:8090方式进行访问。 protocol: string #端口协议，支持TCP和UDP，默认TCP env: #容器运行前需设置的环境变量列表 - name: string #环境变量名称 value: string #环境变量的值 resources: #资源限制和资源请求的设置（设置容器的资源上线） limits: #容器运行时资源使用的上线 cpu: string #CPU限制，单位为core数，允许浮点数，如0.1等价于100m，0.5等价于500m；因此如果小于1那么优先选择如100m的形式，精度为1m。这个数字用作 docker run 命令中的 --cpu-quota 参数。 memory: string #内存限制，单位：E,P,T,G,M,K；或者Ei,Pi,Ti,Gi,Mi,Ki；或者字节数。将用于docker run --memory参数 requests: #容器启动和调度时的限制设定 cpu: string #CPU请求，容器启动时初始化可用数量，单位为core数，允许浮点数，如0.1等价于100m，0.5等价于500m；因此如果小于1那么优先选择如100m的形式，精度为1m。这个数字用作 docker run 命令中的 --cpu-shares 参数。 memory: string #内存请求,容器启动的初始化可用数量。单位：E,P,T,G,M,K；或者Ei,Pi,Ti,Gi,Mi,Ki；或者字节数 # 参见官网地址：https://kubernetes.io/zh/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/ livenessProbe: #对Pod内各容器健康检查的设置，当探测无响应几次后将自动重启该容器，检查方法有exec、httpGet和tcpSocket，对一个容器【只需设置其中一种方法即可】 exec: #对Pod内容器健康检查方式设置为exec方式 command: [string] #exec方式需要制定的命令或脚本 httpGet: #对Pod内容器健康检查方法设置为HttpGet，需要制定Path、port path: string #访问 HTTP 服务的路径 port: number #访问容器的端口号或者端口名。如果数字必须在 1 ~ 65535 之间。 host: string #当没有定义 &quot;host&quot; 时，使用 &quot;PodIP&quot; scheme: string #当没有定义 &quot;scheme&quot; 时，使用 &quot;HTTP&quot;，scheme 只允许 &quot;HTTP&quot; 和 &quot;HTTPS&quot; HttpHeaders: #请求中自定义的 HTTP 头。HTTP 头字段允许重复。 - name: string value: string tcpSocket: #对Pod内容器健康检查方式设置为tcpSocket方式 port: number initialDelaySeconds: 5 #容器启动完成后，kubelet在执行第一次探测前应该等待 5 秒。默认是 0 秒，最小值是 0。 periodSeconds: 60 #指定 kubelet 每隔 60 秒执行一次存活探测。默认是 10 秒。最小值是 1 timeoutSeconds: 3 #对容器健康检查探测等待响应的超时时间为 3 秒，默认1秒 successThreshold: 1 #检测到有1次成功则认为服务是`就绪` failureThreshold: 5 #检测到有5次失败则认为服务是`未就绪`。默认值是 3，最小值是 1。 restartPolicy: [Always|Never|OnFailure] #Pod的重启策略，默认Always。Always表示一旦不管以何种方式终止运行，kubelet都将重启；OnFailure表示只有Pod以非0退出码退出才重启；Nerver表示不再重启该Pod nodeSelector: #定义Node的label过滤标签，以key：value的格式指定。节点选择，先给主机打标签kubectl label nodes kube-node01 key1=value1 key1: value1 imagePullSecrets: #Pull镜像时使用的secret名称，以name：secretKeyName格式指定 - name: string hostNetwork: false #是否使用主机网络模式，默认为false。如果设置为true，表示使用宿主机网络，不使用docker网桥 # volumes 和 containers 是同层级 ****************************** # 参见官网地址：https://kubernetes.io/zh/docs/concepts/storage/volumes/ volumes: #定义了paues容器关联的宿主机或分布式文件系统存储卷列表 （volumes类型有很多种，选其中一种即可） - name: string #共享存储卷名称。 emptyDir: &#123;&#125; #类型为emtyDir的存储卷，与Pod同生命周期的一个临时目录。当Pod因为某些原因被从节点上删除时，emptyDir卷中的数据也会永久删除。 hostPath: string #类型为hostPath的存储卷，表示挂载Pod所在宿主机的文件或目录 path: string #在宿主机上文件或目录的路径 type: [|DirectoryOrCreate|Directory|FileOrCreate|File] #空字符串（默认）用于向后兼容，这意味着在安装 hostPath 卷之前不会执行任何检查。DirectoryOrCreate：如果给定目录不存在则创建，权限设置为 0755，具有与 Kubelet 相同的组和所有权。Directory：给定目录必须存在。FileOrCreate：如果给定文件不存在，则创建空文件，权限设置为 0644，具有与 Kubelet 相同的组和所有权。File：给定文件必须存在。 secret: #类型为secret的存储卷，挂载集群预定义的secre对象到容器内部。Secret 是一种包含少量敏感信息例如密码、token 或 key 的对象。放在一个 secret 对象中可以更好地控制它的用途，并降低意外暴露的风险。 secretName: string #secret 对象的名字 items: #可选，修改key 的目标路径 - key: username #username secret存储在/etc/foo/my-group/my-username 文件中而不是 /etc/foo/username 中。【此时存在spec.containers[].volumeMounts[].mountPath为/etc/foo】 path: my-group/my-username configMap: #类型为configMap的存储卷，挂载预定义的configMap对象到容器内部。ConfigMap 允许您将配置文件与镜像文件分离，以使容器化的应用程序具有可移植性。 name: string #提供你想要挂载的 ConfigMap 的名字 资源需求（Requests）和限制（Limits）说明对于每一个资源，container可以指定具体的资源需求（requests）和限制（limits）。 requests申请范围是0到node节点的最大配置，而limits申请范围是requests到无限，即0 &lt;= requests &lt;=Node Allocatable，requests &lt;= limits &lt;= Infinity。 对于CPU，如果pod中服务使用CPU超过设置的limits，pod不会被kill掉但会被限制。如果没有设置limits，pod可以使用全部空闲的cpu资源。 对于内存，当一个pod使用内存超过了设置的limits，【一个Pod可能有多个container】pod中container的进程会被kernel因OOM kill掉。当container因为OOM被kill掉时，系统倾向于在其原所在的机器上重启该container或本机或其他重新创建一个pod。 volumeMounts和volumes区别volumeMounts示例如下： 1234567volumeMounts: #定义容器挂载内容- name: nginx-site #使用的存储卷名称，跟下面volume字段的某个name值相同，这里表示使用volume的nginx-site这个存储卷 mountPath: /usr/share/nginx/html #挂载至容器中哪个目录 readOnly: false #读写挂载方式，默认为读写模式false- name: nginx-log mountPath: /var/log/nginx/ readOnly: false volumes示例如下： 123456789volumes: #volumes字段定义了paues容器关联的宿主机或分布式文件系统存储卷- name: nginx-site #存储卷名称 hostPath: #路径，为宿主机存储路径 path: /data/volumes/nginx/html/ #在宿主机上目录的路径 type: DirectoryOrCreate #定义类型，这表示如果宿主机没有此目录，则会自动创建- name: nginx-log hostPath: path: /data/volumes/nginx/log/ type: DirectoryOrCreate 相关阅读1、YAML 语言教程与使用案例 完毕！]]></content>
      <categories>
        <category>Kubernetes</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
        <tag>K8S</tag>
        <tag>CI/CD</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes K8S之SSL证书有效期修改]]></title>
    <url>%2F2020%2F08%2F02%2Fkubernetes03%2F</url>
    <content type="text"><![CDATA[如何修改Kubernetes的SSL证书有效期 主机配置规划 服务器名称(hostname) 系统版本 配置 内网IP 外网IP(模拟) k8s-master CentOS7.7 2C/4G/20G 172.16.1.110 10.0.0.110 k8s-node01 CentOS7.7 2C/4G/20G 172.16.1.111 10.0.0.111 k8s-node02 CentOS7.7 2C/4G/20G 172.16.1.112 10.0.0.112 为什么要修改证书有效期Kubernetes默认的证书有效期都是1年，因此需要我们每年都更新证书，显然这对我们实际生产环境来说是很不友好的；因此我们要对Kubernetes的SSL证书有效期进行修改。 证书有效期查看 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152[root@k8s-master pki]# pwd/etc/kubernetes/pki[root@k8s-master pki]# lltotal 56-rw-r--r-- 1 root root 1224 May 12 15:51 apiserver.crt-rw-r--r-- 1 root root 1090 May 12 15:51 apiserver-etcd-client.crt-rw------- 1 root root 1675 May 12 15:51 apiserver-etcd-client.key-rw------- 1 root root 1675 May 12 15:51 apiserver.key-rw-r--r-- 1 root root 1099 May 12 15:51 apiserver-kubelet-client.crt-rw------- 1 root root 1675 May 12 15:51 apiserver-kubelet-client.key-rw-r--r-- 1 root root 1025 May 12 15:51 ca.crt-rw------- 1 root root 1675 May 12 15:51 ca.keydrwxr-xr-x 2 root root 162 May 12 15:51 etcd-rw-r--r-- 1 root root 1038 May 12 15:51 front-proxy-ca.crt-rw------- 1 root root 1675 May 12 15:51 front-proxy-ca.key-rw-r--r-- 1 root root 1058 May 12 15:51 front-proxy-client.crt-rw------- 1 root root 1675 May 12 15:51 front-proxy-client.key-rw------- 1 root root 1679 May 12 15:51 sa.key-rw------- 1 root root 451 May 12 15:51 sa.pub[root@k8s-master pki]# [root@k8s-master pki]# for i in $(ls *.crt); do echo &quot;===== $i =====&quot;; openssl x509 -in $i -text -noout | grep -A 3 &apos;Validity&apos; ; done===== apiserver.crt ===== Validity Not Before: May 12 07:51:36 2020 GMT Not After : May 12 07:51:36 2021 GMT Subject: CN=kube-apiserver===== apiserver-etcd-client.crt ===== Validity Not Before: May 12 07:51:37 2020 GMT Not After : May 12 07:51:38 2021 GMT Subject: O=system:masters, CN=kube-apiserver-etcd-client===== apiserver-kubelet-client.crt ===== Validity Not Before: May 12 07:51:36 2020 GMT Not After : May 12 07:51:37 2021 GMT Subject: O=system:masters, CN=kube-apiserver-kubelet-client===== ca.crt ===== Validity Not Before: May 12 07:51:36 2020 GMT Not After : May 10 07:51:36 2030 GMT Subject: CN=kubernetes===== front-proxy-ca.crt ===== Validity Not Before: May 12 07:51:37 2020 GMT Not After : May 10 07:51:37 2030 GMT Subject: CN=front-proxy-ca===== front-proxy-client.crt ===== Validity Not Before: May 12 07:51:37 2020 GMT Not After : May 12 07:51:37 2021 GMT Subject: CN=front-proxy-client[root@k8s-master pki]# 由上可见，除了ca根证书，其他证书有效期都是1年。 证书有效时限修改go环境部署go语言中文网 1https://studygolang.com/ 在Linux命令行下载 123456[root@k8s-master software]# wget https://studygolang.com/dl/golang/go1.14.6.linux-amd64.tar.gz[root@k8s-master software]# tar xf go1.14.6.linux-amd64.tar.gz -C /usr/local/[root@k8s-master software]# vim /etc/profile # 最后面添加如下信息# go语言环境变量export PATH=$PATH:/usr/local/go/bin[root@k8s-master software]# source /etc/profile Kubernetes源码下载与更改证书策略当期k8s版本 123[root@k8s-master software]# kubectl versionClient Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;17&quot;, GitVersion:&quot;v1.17.4&quot;, GitCommit:&quot;8d8aa39598534325ad77120c120a22b3a990b5ea&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2020-03-12T21:03:42Z&quot;, GoVersion:&quot;go1.13.8&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;Server Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;17&quot;, GitVersion:&quot;v1.17.4&quot;, GitCommit:&quot;8d8aa39598534325ad77120c120a22b3a990b5ea&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2020-03-12T20:55:23Z&quot;, GoVersion:&quot;go1.13.8&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125; 根据k8s版本下载源码 操作步骤 1234567891011121314151617181920212223242526272829303132333435363738[root@k8s-master software]# wget https://github.com/kubernetes/kubernetes/archive/v1.17.4.tar.gz[root@k8s-master software]# tar xf v1.17.4.tar.gz &amp;&amp; cd kubernetes-1.17.4[root@k8s-master kubernetes-1.17.4]# vim cmd/kubeadm/app/util/pkiutil/pki_helpers.go………………func NewSignedCert(cfg *certutil.Config, key crypto.Signer, caCert *x509.Certificate, caKey crypto.Signer) (*x509.Certificate, error) &#123; // 添加如下行 有效时间 100 年 const effectyear = time.Hour * 24 * 365 * 100 serial, err := cryptorand.Int(cryptorand.Reader, new(big.Int).SetInt64(math.MaxInt64)) if err != nil &#123; return nil, err &#125; if len(cfg.CommonName) == 0 &#123; return nil, errors.New(&quot;must specify a CommonName&quot;) &#125; if len(cfg.Usages) == 0 &#123; return nil, errors.New(&quot;must specify at least one ExtKeyUsage&quot;) &#125; certTmpl := x509.Certificate&#123; Subject: pkix.Name&#123; CommonName: cfg.CommonName, Organization: cfg.Organization, &#125;, DNSNames: cfg.AltNames.DNSNames, IPAddresses: cfg.AltNames.IPs, SerialNumber: serial, NotBefore: caCert.NotBefore, // NotAfter: time.Now().Add(kubeadmconstants.CertificateValidity).UTC(), NotAfter: time.Now().Add(effectyear).UTC(), // 修改行 KeyUsage: x509.KeyUsageKeyEncipherment | x509.KeyUsageDigitalSignature, ExtKeyUsage: cfg.Usages, &#125;[root@k8s-master kubernetes-1.17.4]# # 注意路径[root@k8s-master kubernetes-1.17.4]# make WHAT=cmd/kubeadm GOFLAGS=-v# 将更新后的kubeadm拷贝到指定位置[root@k8s-master kubernetes-1.17.4]# cp -a _output/bin/kubeadm /root/kubeadm-new 更新kubeadm并备份原证书123456# kubeadm更新mv /usr/bin/kubeadm /usr/bin/kubeadm_20200725mv /root/kubeadm-new /usr/bin/kubeadmchmod 755 /usr/bin/kubeadm# 原证书备份cp -a /etc/kubernetes/pki/ /etc/kubernetes/pki_20200725 证书更新操作如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# 证书更新[root@k8s-master ~]# kubeadm alpha certs renew all --config=/root/k8s_install/kubeadm-config.yaml# 查看新证书有效期[root@k8s-master ~]# cd /etc/kubernetes/pki[root@k8s-master pki]# lltotal 56-rw-r--r-- 1 root root 1224 Jul 25 18:44 apiserver.crt-rw-r--r-- 1 root root 1094 Jul 25 18:44 apiserver-etcd-client.crt-rw------- 1 root root 1675 Jul 25 18:44 apiserver-etcd-client.key-rw------- 1 root root 1679 Jul 25 18:44 apiserver.key-rw-r--r-- 1 root root 1103 Jul 25 18:44 apiserver-kubelet-client.crt-rw------- 1 root root 1679 Jul 25 18:44 apiserver-kubelet-client.key-rw-r--r-- 1 root root 1025 May 12 15:51 ca.crt-rw------- 1 root root 1675 May 12 15:51 ca.keydrwxr-xr-x 2 root root 162 May 12 15:51 etcd-rw-r--r-- 1 root root 1038 May 12 15:51 front-proxy-ca.crt-rw------- 1 root root 1675 May 12 15:51 front-proxy-ca.key-rw-r--r-- 1 root root 1058 Jul 25 18:44 front-proxy-client.crt-rw------- 1 root root 1679 Jul 25 18:44 front-proxy-client.key-rw------- 1 root root 1679 May 12 15:51 sa.key-rw------- 1 root root 451 May 12 15:51 sa.pub[root@k8s-master pki]# [root@k8s-master pki]# for i in $(ls *.crt); do echo &quot;===== $i =====&quot;; openssl x509 -in $i -text -noout | grep -A 3 &apos;Validity&apos; ; done===== apiserver.crt ===== Validity Not Before: May 12 07:51:36 2020 GMT Not After : Jul 1 10:44:20 2120 GMT Subject: CN=kube-apiserver===== apiserver-etcd-client.crt ===== Validity Not Before: May 12 07:51:37 2020 GMT Not After : Jul 1 10:44:20 2120 GMT Subject: O=system:masters, CN=kube-apiserver-etcd-client===== apiserver-kubelet-client.crt ===== Validity Not Before: May 12 07:51:36 2020 GMT Not After : Jul 1 10:44:20 2120 GMT Subject: O=system:masters, CN=kube-apiserver-kubelet-client===== ca.crt ===== Validity Not Before: May 12 07:51:36 2020 GMT Not After : May 10 07:51:36 2030 GMT Subject: CN=kubernetes===== front-proxy-ca.crt ===== Validity Not Before: May 12 07:51:37 2020 GMT Not After : May 10 07:51:37 2030 GMT Subject: CN=front-proxy-ca===== front-proxy-client.crt ===== Validity Not Before: May 12 07:51:37 2020 GMT Not After : Jul 1 10:44:22 2120 GMT Subject: CN=front-proxy-client 由上可见，除了CA根证书，其他证书有效期已经改为 100 年。 kubeadm-config.yaml文件参见如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[root@k8s-master k8s_install]# pwd/root/k8s_install[root@k8s-master k8s_install]# kubeadm config print init-defaults &gt; kubeadm-config.yaml# 做了适当修改[root@k8s-master k8s_install]# cat kubeadm-config.yaml apiVersion: kubeadm.k8s.io/v1beta2bootstrapTokens:- groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authenticationkind: InitConfigurationlocalAPIEndpoint: # 改为本机内网IP advertiseAddress: 172.16.1.110 bindPort: 6443nodeRegistration: criSocket: /var/run/dockershim.sock name: k8s-master taints: - effect: NoSchedule key: node-role.kubernetes.io/master---apiServer: timeoutForControlPlane: 4m0sapiVersion: kubeadm.k8s.io/v1beta2certificatesDir: /etc/kubernetes/pkiclusterName: kubernetescontrollerManager: &#123;&#125;dns: type: CoreDNSetcd: local: dataDir: /var/lib/etcdimageRepository: k8s.gcr.iokind: ClusterConfiguration# 本次部署的版本为 v1.17.4kubernetesVersion: v1.17.4networking: dnsDomain: cluster.local # 添加如下行，指定pod网络的IP地址范围，因为flannel 就是这个网段 podSubnet: 10.244.0.0/16 # 默认值即可，无需改变。服务VIP使用可选的IP地址范围。默认10.96.0.0/12 serviceSubnet: 10.96.0.0/12scheduler: &#123;&#125;---# 添加如下配置段，调度方式从默认改为ipvs方式【如果上面初始化没有做ipvs，那么这段就不需要】apiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfigurationfeatureGates: SupportIPVSProxyMode: truemode: ipvs 相关阅读1、基于kubeadm快速部署kubernetes K8S V1.17.4集群-无坑完整版 完毕！]]></content>
      <categories>
        <category>Kubernetes</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
        <tag>K8S</tag>
        <tag>CI/CD</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于kubeadm快速部署kubernetes K8S V1.17.4集群-无坑完整版]]></title>
    <url>%2F2020%2F07%2F29%2Fkubernetes02%2F</url>
    <content type="text"><![CDATA[基于kubeadm快速部署kubernetes K8S V1.17.4集群，并部署Dashboard Web页面，实现可视化查看Kubernetes资源 主机配置规划 服务器名称(hostname) 系统版本 配置 内网IP 外网IP(模拟) k8s-master CentOS7.7 2C/4G/20G 172.16.1.110 10.0.0.110 k8s-node01 CentOS7.7 2C/4G/20G 172.16.1.111 10.0.0.111 k8s-node02 CentOS7.7 2C/4G/20G 172.16.1.112 10.0.0.112 注意：没有swap分区 预定完成目标项1、在所有节点上安装Docker和kubeadm 2、部署Kubernetes Master 3、部署容器网络插件 4、部署 Kubernetes Worker，并将节点加入Kubernetes集群中 5、部署Dashboard Web页面，可视化查看Kubernetes资源 关于二进制安装 kubeadm 是 Kubernetes 官方支持的安装方式，“二进制” 不是。本文档采用 kubernetes.io 官方推荐的 kubeadm 工具安装 kubernetes 集群。 架构图 安装docker、安装kubeadm，kubelet和kubectl注意：所有机器都要安装 脚本如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192[root@k8s-master k8s_install]# pwd/root/k8s_install# 脚本信息如下【支持多次执行】[root@k8s-master k8s_install]# cat install_kubelet.sh #!/bin/sh##### 在 master 节点和 worker 节点都要执行 【所有机器执行】# 加载环境变量. /etc/profile. /etc/bashrc################################################ 添加主机名与IP对应关系（每台主机必须设置主机名）# 如下命令：没有则添加信息 若使用请根据自身主机情况修改 ★★★★★ 「你需要修改处」grep &apos;172.16.1.110.*k8s-master&apos; /etc/hosts || echo &quot;172.16.1.110 k8s-master&quot; &gt;&gt; /etc/hostsgrep &apos;172.16.1.111.*k8s-node01&apos; /etc/hosts || echo &quot;172.16.1.111 k8s-node01&quot; &gt;&gt; /etc/hostsgrep &apos;172.16.1.112.*k8s-node02&apos; /etc/hosts || echo &quot;172.16.1.112 k8s-node02&quot; &gt;&gt; /etc/hosts################################################ 必要的基础配置或包安装## 必须安装 nfs-utils 才能挂载 nfs 网络存储yum install -y nfs-utils## wget 用于下载文件yum install -y wget## 其他必要包yum install -y conntrack ipvsadm ipset# 关闭 防火墙systemctl stop firewalldsystemctl disable firewalldsystemctl stop iptablessystemctl disable iptables# 关闭 SeLinuxsetenforce 0sed -i &quot;s/SELINUX=enforcing/SELINUX=disabled/g&quot; /etc/selinux/config# 关闭 swap ， 本次涉及的机器没有swap，因此注释了## 如果有swap分区则放开注释#swapoff -a#yes | cp /etc/fstab /etc/fstab_bak#cat /etc/fstab_bak | grep -v swap &gt; /etc/fstab# 时间设置## 时区设置：东八区，上海ls -l /etc/localtime | grep &apos;Asia/Shanghai&apos; || (rm -f /etc/localtime &amp;&amp; ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime)## 时间同步定时任务：没有则添加定，进行时间同步crontab -l | grep &apos;ntpdate&apos; || echo -e &quot;# time sync\n*/10 * * * * /usr/sbin/ntpdate ntp1.aliyun.com &gt;/dev/null 2&gt;&amp;1&quot; &gt;&gt; /var/spool/cron/root## 查看硬件时间 hwclock --show## 系统时间同步到硬件时间hwclock --systohc# 关闭邮件服务systemctl stop postfix.service &amp;&amp; systemctl disable postfix.service################################################ 修改 /etc/sysctl.conf# 开启 ip_forward 转发并解决流量路由不正确问题# 如果有配置，则修改sed -i &quot;s#^net.ipv4.ip_forward.*#net.ipv4.ip_forward = 1#g&quot; /etc/sysctl.confsed -i &quot;s#^net.ipv4.tcp_tw_recycle.*#net.ipv4.tcp_tw_recycle = 0#g&quot; /etc/sysctl.confsed -i &quot;s#^net.bridge.bridge-nf-call-ip6tables.*#net.bridge.bridge-nf-call-ip6tables = 1#g&quot; /etc/sysctl.confsed -i &quot;s#^net.bridge.bridge-nf-call-iptables.*#net.bridge.bridge-nf-call-iptables = 1#g&quot; /etc/sysctl.conf# IPv6 转发sed -i &quot;s#^net.ipv6.conf.all.forwarding.*#net.ipv6.conf.all.forwarding = 1#g&quot; /etc/sysctl.confsed -i &quot;s#^net.netfilter.nf_conntrack_max.*#net.netfilter.nf_conntrack_max = 2310720#g&quot; /etc/sysctl.conf## 如下两条非必要sed -i &quot;s#^fs.file-max.*#fs.file-max = 52706963#g&quot; /etc/sysctl.confsed -i &quot;s#^fs.nr_open.*#fs.nr_open = 52706963#g&quot; /etc/sysctl.conf# 如果没有，追加grep &apos;net.ipv4.ip_forward = 1&apos; /etc/sysctl.conf || echo &quot;net.ipv4.ip_forward = 1&quot; &gt;&gt; /etc/sysctl.confgrep &apos;net.ipv4.tcp_tw_recycle = 0&apos; /etc/sysctl.conf || echo &quot;net.ipv4.tcp_tw_recycle = 0&quot; &gt;&gt; /etc/sysctl.confgrep &apos;net.bridge.bridge-nf-call-ip6tables = 1&apos; /etc/sysctl.conf || echo &quot;net.bridge.bridge-nf-call-ip6tables = 1&quot; &gt;&gt; /etc/sysctl.confgrep &apos;net.bridge.bridge-nf-call-iptables = 1&apos; /etc/sysctl.conf || echo &quot;net.bridge.bridge-nf-call-iptables = 1&quot; &gt;&gt; /etc/sysctl.confgrep &apos;net.ipv6.conf.all.forwarding = 1&apos; /etc/sysctl.conf || echo &quot;net.ipv6.conf.all.forwarding = 1&quot; &gt;&gt; /etc/sysctl.confgrep &apos;net.netfilter.nf_conntrack_max = 2310720&apos; /etc/sysctl.conf || echo &quot;net.netfilter.nf_conntrack_max = 2310720&quot; &gt;&gt; /etc/sysctl.confgrep &apos;fs.file-max = 52706963&apos; /etc/sysctl.conf || echo &quot;fs.file-max = 52706963&quot; &gt;&gt; /etc/sysctl.confgrep &apos;fs.nr_open = 52706963&apos; /etc/sysctl.conf || echo &quot;fs.nr_open = 52706963&quot; &gt;&gt; /etc/sysctl.conf# 执行命令以生效sysctl -p#### 说明：上面的命令中/etc/sysctl.conf可以用/etc/sysctl.d/k8s.conf替换；生效使用sysctl -p /etc/sysctl.d/k8s.conf 命令################################################ kube-proxy 开启ipvs的前置条件【本步骤可忽略，但推荐使用IPVS】modprobe br_netfiltercat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt; EOF#!/bin/bashmodprobe -- ip_vsmodprobe -- ip_vs_rrmodprobe -- ip_vs_wrrmodprobe -- ip_vs_shmodprobe -- nf_conntrack_ipv4EOFchmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4################################################ 安装 docker## 参考文档如下# https://www.cnblogs.com/zhanglianghhh/p/9891293.html# https://docs.docker.com/install/linux/docker-ce/centos/ # https://docs.docker.com/install/linux/linux-postinstall/## 卸载旧版本 根据需要放开注释#yum remove -y docker \#docker-client \#docker-client-latest \#docker-common \#docker-latest \#docker-latest-logrotate \#docker-logrotate \#docker-selinux \#docker-engine-selinux \#docker-engine## 设置 docker yum repositoryyum install -y yum-utils device-mapper-persistent-data lvm2yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo## 安装docker# yum install -y docker-ceyum install -y docker-ce-19.03.8## 启动docker服务，这样可以创建/etc/docker目录systemctl start docker## 配置daemon## 1、修改docker Cgroup Driver为systemd；2、日志格式设定## 如果不修改，在添加 worker 节点时可能会碰到如下错误## [WARNING IsDockerSystemdCheck]: detected &quot;cgroupfs&quot; as the Docker cgroup driver. The recommended driver is &quot;systemd&quot;. ## Please follow the guide at https://kubernetes.io/docs/setup/cri/cat &gt; /etc/docker/daemon.json &lt;&lt; EOF&#123; &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;], &quot;log-driver&quot;: &quot;json-file&quot;, &quot;log-opts&quot;: &#123; &quot;max-size&quot;: &quot;100m&quot; &#125;&#125;EOF## 开机自启动systemctl stop docker &amp;&amp; systemctl daemon-reload &amp;&amp; systemctl enable docker &amp;&amp; systemctl start docker################################################ 配置K8S的yum源cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt;EOF[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF# 卸载旧版本 根据需要放开注释#yum remove -y kubelet kubeadm kubectl# 由于版本更新频繁，这里指定版本号部署## 安装kubelet、kubeadm、kubectl## 将 $&#123;1&#125; 替换为 kubernetes 版本号，例如 1.17.4## yum install -y kubelet-$&#123;1&#125; kubeadm-$&#123;1&#125; kubectl-$&#123;1&#125;yum install -y kubelet-1.17.4 kubeadm-1.17.4 kubectl-1.17.4# 重启 docker，并启动 kubeletsystemctl daemon-reloadsystemctl restart dockersystemctl enable kubelet &amp;&amp; systemctl start kubelet# 打印分割线echo &quot;=====================&quot;# 打印docker版本信息docker version 执行上述脚本 kubelet 服务错误说明用kubeadm的方法安装kubelet后，运行systemctl status kubelet 和 journalctl -f -u kubelet 发现kubelet服务启动失败，错误代码255。 后来查了资料，运行journalctl -xefu kubelet 命令查看systemd日志才发现，真正的错误是： 1failed to load Kubelet config file /var/lib/kubelet/config.yaml, error failed to read kubelet config file &quot;/var/lib/kubelet/config.yaml&quot;, error: open /var/lib/kubelet/config.yaml: no such file or directory 原因： 关键文件缺失，多发生于没有做 kubeadm init就运行了systemctl start kubelet。 暂时可以有不管，后面 kubeadm init 后会恢复正常。 部署Kubernetes Master与安装Pod网络插件（CNI）注意：仅在master节点操作 kubeadm init 配置详解的官网地址如下： 1https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/ 这里使用 flannel 网络 master节点所需镜像kubernetes 服务启动依赖很多镜像，这些镜像要是在国内没有(fan qiang)的话，是下载不下来的。这里我们可以去阿里云容器镜像服务【别人放好的镜像】搜寻并下载指定版本的镜像替代。 下载完成后，通过 docker tag … 命令修改成指定名称的镜像即可。 需要哪些镜像及版本需要哪些镜像及版本，查看方式如下： 12345678910[root@k8s-master ~]# kubeadm config images list --kubernetes-version v1.17.4W0728 16:31:09.770937 8119 validation.go:28] Cannot validate kube-proxy config - no validator is availableW0728 16:31:09.770998 8119 validation.go:28] Cannot validate kubelet config - no validator is availablek8s.gcr.io/kube-apiserver:v1.17.4k8s.gcr.io/kube-controller-manager:v1.17.4k8s.gcr.io/kube-scheduler:v1.17.4k8s.gcr.io/kube-proxy:v1.17.4k8s.gcr.io/pause:3.1k8s.gcr.io/etcd:3.4.3-0k8s.gcr.io/coredns:1.6.5 获取初始化默认配置文件并修改12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[root@k8s-master k8s_install]# pwd/root/k8s_install[root@k8s-master k8s_install]# kubeadm config print init-defaults &gt; kubeadm-config.yaml# 做了适当修改[root@k8s-master k8s_install]# cat kubeadm-config.yaml apiVersion: kubeadm.k8s.io/v1beta2bootstrapTokens:- groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authenticationkind: InitConfigurationlocalAPIEndpoint: # 改为本机内网IP advertiseAddress: 172.16.1.110 bindPort: 6443nodeRegistration: criSocket: /var/run/dockershim.sock name: k8s-master taints: - effect: NoSchedule key: node-role.kubernetes.io/master---apiServer: timeoutForControlPlane: 4m0sapiVersion: kubeadm.k8s.io/v1beta2certificatesDir: /etc/kubernetes/pkiclusterName: kubernetescontrollerManager: &#123;&#125;dns: type: CoreDNSetcd: local: dataDir: /var/lib/etcdimageRepository: k8s.gcr.iokind: ClusterConfiguration# 本次部署的版本为 v1.17.4kubernetesVersion: v1.17.4networking: dnsDomain: cluster.local # 添加如下行，指定pod网络的IP地址范围，因为flannel 就是这个网段 podSubnet: 10.244.0.0/16 # 默认值即可，无需改变。服务VIP使用可选的IP地址范围。默认10.96.0.0/12 serviceSubnet: 10.96.0.0/12scheduler: &#123;&#125;---# 添加如下配置段，调度方式从默认改为ipvs方式【如果上面初始化没有做ipvs，那么这段就不需要】apiVersion: kubeproxy.config.k8s.io/v1alpha1kind: KubeProxyConfigurationfeatureGates: SupportIPVSProxyMode: truemode: ipvs kubeadm init与flannel网络安装创建 init_master.sh文件并编写脚本进行批量下载镜像；之后修改镜像tag，与google的k8s镜像名称一致；再之后初始化并安装Pod网络插件。 脚本如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697[root@k8s-master k8s_install]# pwd/root/k8s_install[root@k8s-master k8s_install]# cat init_master.sh#!/bin/bash##### 在 k8s master 节点执行# 加载环境变量. /etc/profile. /etc/bashrc################################################ 从国内下载 master 节点所需镜像，并对镜像重命名 # src_registry=&quot;registry.aliyuncs.com/google_containers&quot;src_registry=&quot;registry.cn-beijing.aliyuncs.com/google_registry&quot;# 定义镜像集合数组# 具体版本信息根据 kubeadm config images list --kubernetes-version v1.17.4 得到的images=( kube-apiserver:v1.17.4 kube-controller-manager:v1.17.4 kube-scheduler:v1.17.4 kube-proxy:v1.17.4 pause:3.1 etcd:3.4.3-0 coredns:1.6.5)# 循环从国内获取的Docker镜像for img in $&#123;images[@]&#125;;do # 从国内源下载镜像 docker pull $&#123;src_registry&#125;/$img # 改变镜像名称 docker tag $&#123;src_registry&#125;/$img k8s.gcr.io/$img # 删除源始镜像 docker rmi $&#123;src_registry&#125;/$img # 打印分割线 echo &quot;======== $img download OK ========&quot;doneecho &quot;********** k8s master docker images pull OK! **********&quot;################################################ kubeadm 初始化##### 初始化方式1# 这个初始化过程需要几分钟，具体时间取决于你的网络。# --apiserver-advertise-address=x.x.x.x 本机内网地址 ★★★★★ 「你需要修改处」# --service-cidr=x.x.x.x 为服务VIP使用可选的IP地址范围。默认10.96.0.0/12# --pod-network-cidr=x.x.x.x 指定pod网络的IP地址范围。#kubeadm init \# --apiserver-advertise-address=172.16.1.110 \# --kubernetes-version v1.17.4 \# --service-cidr=10.96.0.0/12 \# --pod-network-cidr=10.244.0.0/16##### 初始化方式2 【推荐方式】# --upload-certs 自动颁发证书，高可用有意义，单机可选# kubeadm-config.yaml文件，通过上文已得到该文件kubeadm init --config=kubeadm-config.yaml --upload-certs | tee kubeadm-init.logecho &quot;********** kubeadm init OK! **********&quot;# 配置 kubectlmkdir -p $HOME/.kube/cp -i /etc/kubernetes/admin.conf $HOME/.kube/configchown $(id -u):$(id -g) $HOME/.kube/configecho &quot;********** kubectl config OK! **********&quot;################################################ Kubernetes CNI扁平化网络：Flannel、Calico、Canal和Weave# https://blog.csdn.net/RancherLabs/article/details/88885539# 安装 kube-flannel 网络# 若能够访问到quay.io这个registery，可以注释掉下面3行docker命令。 在 kube-flannel.yml 文件中可得到 flannel 版本信息 # 如果不能访问 quay.io这个registery，那么请使用下面3行docker pull $&#123;src_registry&#125;/flannel:v0.12.0-amd64docker tag $&#123;src_registry&#125;/flannel:v0.12.0-amd64 quay.io/coreos/flannel:v0.12.0-amd64docker rmi $&#123;src_registry&#125;/flannel:v0.12.0-amd64 # 如果下载失败，那么可通过 https://github.com/coreos/flannel/blob/v0.12.0/Documentation/kube-flannel.yml 页面拷贝到本地wget https://raw.githubusercontent.com/coreos/flannel/v0.12.0/Documentation/kube-flannel.ymlkubectl apply -f kube-flannel.ymlecho &quot;********** kube-flannel network OK! **********&quot;################################################ master节点验证所有pod状态echo &quot;********** kubectl get pods --all-namespaces -o wide **********&quot;# kubectl get pods -A -o wide # 与下面的命令等效kubectl get pods --all-namespaces -o wide# master节点查看node状态echo &quot;********** kubectl get nodes **********&quot;kubectl get nodesecho &quot;********** 获得 join 命令参数 **********&quot;kubeadm token create --print-join-command 执行上述脚本 部分命令说明：查看所有pods运行信息 12kubectl get pods --all-namespaces # 或者kubectl get pods -Akubectl get pods --all-namespaces -o wide 查看所有node节点信息，其中master也是一个节点 12kubectl get nodeskubectl get nodes -o wide 获得 join命令参数，复制后在其他node节点执行，执行后加入该master节点，形成一个集群 1kubeadm token create --print-join-command flannel扁平化网络查看通过ifconfig命令，可见已经存在了扁平化网络。 扁平化网络说明： Kubernetes 的网络模型假定了所有Pod都在一个可以直接连通的扁平化的网络空间中，这在GCE(Google Compute Engine)里面是现成的网络模型，Kubernetes 假定这个网络已经存在。而在私有云里搭建 Kubernetes 集群，就不能假定这个网络已经存在。我们需要自己实现这个网络假设，将不同节点上的 Docker 容器之间的互相访问先打通，然后运行 Kubernetes。 同一个Pod内的多个容器之间通讯：lo 各Pod之间的通讯：Overlay Network Pod与Service之间的通讯：各节点的Iptables规则或者ipvs Flannel 是 CoreOS 团队针对 Kubernetes 设计的一个网络规划服务。简单来说，它的功能是让集群中的不同节点主机创建的 Docker 容器都具有全集群唯一的虚拟IP地址。而且它还能在这些IP地址之间建立一个覆盖网络(Overlay Network)，通过这个覆盖网络，将数据包原封不动的传递到目标容器内。 flannel网络如下图： 部署Kubernetes Node并加入Kubernetes集群备注：仅在worker节点执行 获得 join命令初始化worker节点并加入master在master执行如下命令获取join参数。 12[root@k8s-master ~]# kubeadm token create --print-join-commandkubeadm join 172.16.1.110:6443 --token jb8qa8.wbjx2k7t8vuvqf4q --discovery-token-ca-cert-hash sha256:a694fbe124afd00c0024d4be102037d8f84c9e2e1da1c6638b7788ad71f556ef 有效时间，该 token 的有效时间为 2 个小时，2小时内可以使用此 token 初始化任意数量的 worker 节点。 本文根据规划在k8s-node01、k8s-node02机器上执行。 执行完毕后，在master机器查看节点信息： 1234567891011121314151617181920212223242526[root@k8s-master ~]# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-master Ready master 47m v1.17.4k8s-node01 NotReady &lt;none&gt; 30s v1.17.4k8s-node02 NotReady &lt;none&gt; 25s v1.17.4[root@k8s-master ~]# [root@k8s-master ~]# kubectl get nodes -o wideNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIMEk8s-master Ready master 48m v1.17.4 172.16.1.110 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-1062.el7.x86_64 docker://19.3.8k8s-node01 NotReady &lt;none&gt; 56s v1.17.4 172.16.1.111 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-1062.el7.x86_64 docker://19.3.8k8s-node02 NotReady &lt;none&gt; 51s v1.17.4 172.16.1.112 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-1062.el7.x86_64 docker://19.3.8# 因为此时在k8s-node01、k8s-node02没有flannel镜像，因此pod未启动[root@k8s-master ~]# kubectl get pods -A -o wideNAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESkube-system coredns-6955765f44-c9zfh 1/1 Running 0 48m 10.244.0.2 k8s-master &lt;none&gt; &lt;none&gt;kube-system coredns-6955765f44-lrz5q 1/1 Running 0 48m 10.244.0.3 k8s-master &lt;none&gt; &lt;none&gt;kube-system etcd-k8s-master 1/1 Running 0 48m 172.16.1.110 k8s-master &lt;none&gt; &lt;none&gt;kube-system kube-apiserver-k8s-master 1/1 Running 0 48m 172.16.1.110 k8s-master &lt;none&gt; &lt;none&gt;kube-system kube-controller-manager-k8s-master 1/1 Running 0 48m 172.16.1.110 k8s-master &lt;none&gt; &lt;none&gt;kube-system kube-flannel-ds-amd64-dngrk 1/1 Running 0 46m 172.16.1.110 k8s-master &lt;none&gt; &lt;none&gt;kube-system kube-flannel-ds-amd64-h4sn6 0/1 Init:0/1 0 76s 172.16.1.111 k8s-node01 &lt;none&gt; &lt;none&gt;kube-system kube-flannel-ds-amd64-m92wp 0/1 Init:0/1 0 71s 172.16.1.112 k8s-node02 &lt;none&gt; &lt;none&gt;kube-system kube-proxy-28dwj 0/1 ContainerCreating 0 76s 172.16.1.111 k8s-node01 &lt;none&gt; &lt;none&gt;kube-system kube-proxy-c875m 1/1 Running 0 48m 172.16.1.110 k8s-master &lt;none&gt; &lt;none&gt;kube-system kube-proxy-stg6w 0/1 ContainerCreating 0 71s 172.16.1.112 k8s-node02 &lt;none&gt; &lt;none&gt;kube-system kube-scheduler-k8s-master 1/1 Running 0 48m 172.16.1.110 k8s-master &lt;none&gt; &lt;none&gt; 说明：k8s-node01、k8s-node02状态为NotReady 获取worker节点镜像备注：仅在worker节点执行 创建 init_worker.sh文件并编写脚本批量下载镜像；之后修改镜像tag，与google的k8s镜像名称一致；再之后下载Pod网络插件。 脚本如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253[root@k8s-node01 k8s_install]# pwd/root/k8s_install[root@k8s-node01 k8s_install]# cat init_worker.sh #!/bin/bash##### 在 k8s worker 节点执行# 加载环境变量. /etc/profile. /etc/bashrc################################################ 从国内下载 node 节点所需镜像，并对镜像重命名 # src_registry=&quot;registry.aliyuncs.com/google_containers&quot;src_registry=&quot;registry.cn-beijing.aliyuncs.com/google_registry&quot;# 定义镜像集合数组# 具体版本信息根据 kubeadm config images list --kubernetes-version v1.17.4 得到的images=( kube-apiserver:v1.17.4 kube-controller-manager:v1.17.4 kube-scheduler:v1.17.4 kube-proxy:v1.17.4 pause:3.1 etcd:3.4.3-0 coredns:1.6.5)# 循环从国内获取的Docker镜像for img in $&#123;images[@]&#125;;do # 从国内源下载镜像 docker pull $&#123;src_registry&#125;/$img # 改变镜像名称 docker tag $&#123;src_registry&#125;/$img k8s.gcr.io/$img # 删除源始镜像 docker rmi $&#123;src_registry&#125;/$img # 打印分割线 echo &quot;======== $img download OK ========&quot;doneecho &quot;********** k8s node docker images pull OK! **********&quot;################################################ Kubernetes CNI网络：Flannel、Calico、Canal和Weave# https://blog.csdn.net/RancherLabs/article/details/88885539# 确保能够访问到quay.io这个registery。#docker pull quay.io/coreos/flannel:v0.12.0-amd64# 如果不能访问 quay.io这个registery，那么请使用下面3行。或从master那边将 flannel 镜像打包过来即可docker pull $&#123;src_registry&#125;/flannel:v0.12.0-amd64docker tag $&#123;src_registry&#125;/flannel:v0.12.0-amd64 quay.io/coreos/flannel:v0.12.0-amd64docker rmi $&#123;src_registry&#125;/flannel:v0.12.0-amd64echo &quot;********** flannel image OK! **********&quot; 本文根据规划在k8s-node01、k8s-node02机器上执行。 work执行完毕后在master机器查看信息执行完毕后，在master机器查看节点信息： 12345678910111213141516171819202122232425# 所有节点都是Ready状态了[root@k8s-master ~]# kubectl get nodesNAME STATUS ROLES AGE VERSIONk8s-master Ready master 56m v1.17.4k8s-node01 Ready &lt;none&gt; 9m35s v1.17.4k8s-node02 Ready &lt;none&gt; 9m30s v1.17.4[root@k8s-master ~]# kubectl get nodes -o wideNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIMEk8s-master Ready master 57m v1.17.4 172.16.1.110 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-1062.el7.x86_64 docker://19.3.8k8s-node01 Ready &lt;none&gt; 9m47s v1.17.4 172.16.1.111 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-1062.el7.x86_64 docker://19.3.8k8s-node02 Ready &lt;none&gt; 9m42s v1.17.4 172.16.1.112 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-1062.el7.x86_64 docker://19.3.8[root@k8s-master ~]# kubectl get pods -A -o wideNAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESkube-system coredns-6955765f44-c9zfh 1/1 Running 0 57m 10.244.0.2 k8s-master &lt;none&gt; &lt;none&gt;kube-system coredns-6955765f44-lrz5q 1/1 Running 0 57m 10.244.0.3 k8s-master &lt;none&gt; &lt;none&gt;kube-system etcd-k8s-master 1/1 Running 0 57m 172.16.1.110 k8s-master &lt;none&gt; &lt;none&gt;kube-system kube-apiserver-k8s-master 1/1 Running 0 57m 172.16.1.110 k8s-master &lt;none&gt; &lt;none&gt;kube-system kube-controller-manager-k8s-master 1/1 Running 0 57m 172.16.1.110 k8s-master &lt;none&gt; &lt;none&gt;kube-system kube-flannel-ds-amd64-dngrk 1/1 Running 0 55m 172.16.1.110 k8s-master &lt;none&gt; &lt;none&gt;kube-system kube-flannel-ds-amd64-h4sn6 1/1 Running 0 10m 172.16.1.111 k8s-node01 &lt;none&gt; &lt;none&gt;kube-system kube-flannel-ds-amd64-m92wp 1/1 Running 0 9m57s 172.16.1.112 k8s-node02 &lt;none&gt; &lt;none&gt;kube-system kube-proxy-28dwj 1/1 Running 0 10m 172.16.1.111 k8s-node01 &lt;none&gt; &lt;none&gt;kube-system kube-proxy-c875m 1/1 Running 0 57m 172.16.1.110 k8s-master &lt;none&gt; &lt;none&gt;kube-system kube-proxy-stg6w 1/1 Running 0 9m57s 172.16.1.112 k8s-node02 &lt;none&gt; &lt;none&gt;kube-system kube-scheduler-k8s-master 1/1 Running 0 57m 172.16.1.110 k8s-master &lt;none&gt; &lt;none&gt; 说明：k8s-node01、k8s-node02状态为Ready，且所有pod都正常运行 移除worker节点正常情况下，无需移除 worker 节点，如果添加到集群出错，可以移除 worker 节点，再重新尝试添加。 比如：移除k8s-node01节点，步骤如下： 在准备移除的 worker 节点上执行 12# 只在 worker 节点执行[root@k8s-node01 ~]# kubeadm reset 在 master 节点上执行 123# 只在 master 执行[root@k8s-master ~]# kubectl delete node k8s-node01node &quot;k8s-node01&quot; deleted 集群版本查看执行如下命令即可 123[root@k8s-master ~]# kubectl versionClient Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;17&quot;, GitVersion:&quot;v1.17.4&quot;, GitCommit:&quot;8d8aa39598534325ad77120c120a22b3a990b5ea&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2020-03-12T21:03:42Z&quot;, GoVersion:&quot;go1.13.8&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;Server Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;17&quot;, GitVersion:&quot;v1.17.4&quot;, GitCommit:&quot;8d8aa39598534325ad77120c120a22b3a990b5ea&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2020-03-12T20:55:23Z&quot;, GoVersion:&quot;go1.13.8&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125; 到这来，整个集群已经部署完毕，可以欢呼了！ 部署 Dashboard本次部署版本版本：v2.0.0-rc6。不能部署v1.10.0版本，因为该版本过低不匹配。 Dashboard 的GitHub地址： 1https://github.com/kubernetes/dashboard/ 版本选择 下载dashboard镜像镜像下载脚本，在所有节点执行，因为dashboard会根据调度部署在任意节点。 脚本如下： 123456789101112131415161718192021222324252627282930313233[root@k8s-master k8s_install]# pwd/root/k8s_install[root@k8s-master k8s_install]# cat install_dashboard.sh #!/bin/sh##### 在 master 节点和 worker 节点都要执行 【所有机器执行】# 加载环境变量. /etc/profile. /etc/bashrc################################################ 从国内下载 k8s dashboard 所需镜像，并对镜像重命名 src_registry=&quot;registry.cn-beijing.aliyuncs.com/google_registry&quot;# 定义镜像集合数组 # V1.17.x 版本对应的 dashboard 不能使用 dashboard:v1.10.1 该版本低，不匹配images=( dashboard:v2.0.0-rc6 metrics-scraper:v1.0.3)# 循环从国内获取的Docker镜像for img in $&#123;images[@]&#125;;do # 从国内源下载镜像 docker pull $&#123;src_registry&#125;/$img # 改变镜像名称 docker tag $&#123;src_registry&#125;/$img kubernetesui/$img # 删除源始镜像 docker rmi $&#123;src_registry&#125;/$img # 打印分割线 echo &quot;======== $img download OK ========&quot;doneecho &quot;********** k8s dashboard docker images OK! **********&quot; 部署 Dashboard在k8s-master上操作 获取dashboard的recommended.yaml 1wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-rc6/aio/deploy/recommended.yaml 修改recommended.yaml配置，如下： 12345678910111213141516171819202122232425262728293031323334353637383940[root@k8s-master k8s_install]# pwd/root/k8s_install[root@k8s-master k8s_install]# vim recommended.yaml………………kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboardspec: # 添加处 type: NodePort ports: - port: 443 targetPort: 8443 # 添加处 nodePort: 30001 selector: k8s-app: kubernetes-dashboard……………… template: metadata: labels: k8s-app: kubernetes-dashboard spec: containers: - name: kubernetes-dashboard image: kubernetesui/dashboard:v2.0.0-rc6 # 修改处 从 Always 改为了 IfNotPresent #imagePullPolicy: Always imagePullPolicy: IfNotPresent……………… spec: containers: - name: dashboard-metrics-scraper image: kubernetesui/metrics-scraper:v1.0.3 # 添加如下行 imagePullPolicy: IfNotPresent 启动dashboard 1kubectl apply -f recommended.yaml 查看dashboard运行情况 123456789101112131415161718192021[root@k8s-master ~]# kubectl get pods -A -o wideNAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESkube-system coredns-6955765f44-kdcj7 1/1 Running 1 3h6m 10.244.0.5 k8s-master &lt;none&gt; &lt;none&gt;kube-system coredns-6955765f44-v9pzk 1/1 Running 1 3h6m 10.244.0.4 k8s-master &lt;none&gt; &lt;none&gt;kube-system etcd-k8s-master 1/1 Running 1 3h6m 172.16.1.110 k8s-master &lt;none&gt; &lt;none&gt;kube-system kube-apiserver-k8s-master 1/1 Running 1 3h6m 172.16.1.110 k8s-master &lt;none&gt; &lt;none&gt;kube-system kube-controller-manager-k8s-master 1/1 Running 1 3h6m 172.16.1.110 k8s-master &lt;none&gt; &lt;none&gt;kube-system kube-flannel-ds-amd64-9nzhv 1/1 Running 1 167m 172.16.1.111 k8s-node01 &lt;none&gt; &lt;none&gt;kube-system kube-flannel-ds-amd64-cft6f 1/1 Running 1 3h6m 172.16.1.110 k8s-master &lt;none&gt; &lt;none&gt;kube-system kube-flannel-ds-amd64-sm7d7 1/1 Running 1 167m 172.16.1.112 k8s-node02 &lt;none&gt; &lt;none&gt;kube-system kube-proxy-dprqm 1/1 Running 1 167m 172.16.1.112 k8s-node02 &lt;none&gt; &lt;none&gt;kube-system kube-proxy-k9l5r 1/1 Running 1 3h6m 172.16.1.110 k8s-master &lt;none&gt; &lt;none&gt;kube-system kube-proxy-mdfsl 1/1 Running 1 167m 172.16.1.111 k8s-node01 &lt;none&gt; &lt;none&gt;kube-system kube-scheduler-k8s-master 1/1 Running 1 3h6m 172.16.1.110 k8s-master &lt;none&gt; &lt;none&gt;kubernetes-dashboard dashboard-metrics-scraper-7b8b58dc8b-hr7lf 1/1 Running 0 93s 10.244.2.2 k8s-node02 &lt;none&gt; &lt;none&gt;kubernetes-dashboard kubernetes-dashboard-755dcb9575-9c5h8 1/1 Running 0 93s 10.244.3.2 k8s-node01 &lt;none&gt; &lt;none&gt;[root@k8s-master ~]# [root@k8s-master ~]# kubectl get pods -n kubernetes-dashboard -o wide # -n 指定namespaceNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESdashboard-metrics-scraper-7b8b58dc8b-hr7lf 1/1 Running 0 2m50s 10.244.2.2 k8s-node02 &lt;none&gt; &lt;none&gt;kubernetes-dashboard-755dcb9575-9c5h8 1/1 Running 0 2m50s 10.244.3.2 k8s-node01 &lt;none&gt; &lt;none&gt; 查看services服务信息 12345678910111213# 或则使用 kubectl get svc -A[root@k8s-master ~]# kubectl get services --all-namespacesNAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdefault kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 3h19mkube-system kube-dns ClusterIP 10.96.0.10 &lt;none&gt; 53/UDP,53/TCP,9153/TCP 3h19mkubernetes-dashboard dashboard-metrics-scraper ClusterIP 10.104.251.180 &lt;none&gt; 8000/TCP 4m55skubernetes-dashboard kubernetes-dashboard NodePort 10.111.61.235 &lt;none&gt; 443:30001/TCP 4m55s[root@k8s-master ~]# # 或则使用 kubectl get svc -n kubernetes-dashboard[root@k8s-master ~]# kubectl get services --namespace=kubernetes-dashboardNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdashboard-metrics-scraper ClusterIP 10.104.251.180 &lt;none&gt; 8000/TCP 5m13skubernetes-dashboard NodePort 10.111.61.235 &lt;none&gt; 443:30001/TCP 5m13s 浏览器访问根据规划，访问如下地址： 1https://172.16.1.110:30001/ 如果谷歌浏览器不能打开，参见如下博文：解决Google浏览器不能打开kubernetes dashboard方法【转】 1https://blog.csdn.net/weixin_30535043/article/details/102395724 也可以使用火狐浏览器打开，浏览器添加例外后，访问得到如下页面： 使用令牌登录（需要创建能够访问 Dashboard 的用户）创建service account并绑定默认cluster-admin管理员集群角色 123456789101112131415161718192021222324[root@k8s-master k8s_install]# pwd/root/k8s_install[root@k8s-master k8s_install]# cat account.yaml # Create Service AccountapiVersion: v1kind: ServiceAccountmetadata: name: admin-user namespace: kube-system---# Create ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1kind: ClusterRoleBindingmetadata: name: admin-userroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects:- kind: ServiceAccount name: admin-user namespace: kube-system[root@k8s-master k8s_install]# kubectl apply -f account.yaml # 执行 yaml 文件 查看绑定信息 123456789[root@k8s-master k8s_install]# kubectl get clusterrolebindingNAME AGEadmin-user 58scluster-admin 5h47mflannel 5h45mkubeadm:kubelet-bootstrap 5h47mkubeadm:node-autoapprove-bootstrap 5h47mkubeadm:node-autoapprove-certificate-rotation 5h47m……………… 获取tocken 1kubectl describe secrets -n kube-system $(kubectl -n kube-system get secret | grep admin-user | awk &apos;&#123;print $1&#125;&apos;) 将得到的token信息用于访问，结果如下：【可能需要等几分钟页面才能刷出来】 完毕！]]></content>
      <categories>
        <category>Kubernetes</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
        <tag>K8S</tag>
        <tag>CI/CD</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes K8S 基本概述、设计架构和设计理念]]></title>
    <url>%2F2020%2F07%2F27%2Fkubernetes01%2F</url>
    <content type="text"><![CDATA[Kubernetes K8S 概述、特性与架构说明，以及核心技术概念和API对象详解 Kubernetes概述Kubernetes是一个开源的，用于管理云平台中多个主机上的容器化的应用，Kubernetes的目标是让部署容器化的应用简单并且高效（powerful），Kubernetes提供了应用部署、规划、更新、维护的一种机制。 在所有的容器编排工具中（类似的还有 docker swarm / mesos等），Kubernetes的生态系统更大、增长更快，有更多的支持、服务和工具可供用户选择。 Kubernetes是Google开源的容器集群管理系统。最初源于谷歌内部的Borg，是Google基于Borg开源的容器编排调度引擎。它构建在Docker技术之上，为跨主机的容器化应用提供资源调度、服务发现、高可用管理和弹性伸缩等一整套功能，它提供完善的管理工具，涵盖开发、部署测试、运维监控等各个环节。 它的目标不仅仅是一个编排系统，而是提供一个规范，可以让你来描述集群的架构，定义服务的最终状态，Kubernetes可以帮你将系统自动的达到和维持在这个状态。 重点：跨机器、跨平台；协调资源使用 容器化越来越流行，主要原因是它带来的诸多好处： 敏捷地创建和部署应用程序：相较于创建虚拟机镜像，创建容器镜像更加容易和快速 持续构建集成：可以更快更频繁地构建容器镜像、部署容器化的应用程序、并且轻松地回滚应用程序 分离开发和运维的关注点：在开发构建阶段就完成容器镜像的构建，构建好的镜像可以部署到多种基础设施上。这种做法将开发阶段需要关注的内容包含在如何构建容器镜像的过程中，将部署阶段需要关注的内容聚焦在如何提供基础设施以及如何使用容器镜像的过程中。降低了开发和运维的耦合度 可监控性：不仅可以查看操作系统级别的资源监控信息，还可以查看应用程序健康状态以及其他信号的监控信息 开发、测试、生产不同阶段的环境一致性：开发阶段在笔记本上运行的容器与测试、生产环境中运行的容器一致 跨云服务商、跨操作系统发行版的可移植性：容器可运行在 Ubuntu、RHEL、CoreOS、CentOS等不同的操作系统发行版上，可以运行在私有化部署、Google Kubernetes Engine、AWS、阿里云等不同的云供应商的环境中 以应用程序为中心的管理：虚拟机时代的考虑的问题是在虚拟硬件上运行一个操作系统，而容器化时代，问题的焦点则是在操作系统的逻辑资源上运行一个应用程序 松耦合、分布式、弹性、无约束的微服务应用程序被切分成更小的、独立的微服务，并可以动态部署和管理，而不是一个部署在专属机器上的庞大的单片应用程序 资源隔离：确保应用程序性能不受干扰 资源利用：资源高效、高密度利用 Kubernetes特性1、自动化部署：yaml部署到K8S，会根据应用程序计算资源需求，自动分配到node。 2、系统自愈：重启已经停机的容器；替换、kill 那些不满足自定义健康检查条件的容器；在容器就绪之前，避免调用者发现该容器。 3、水平扩展：HPA周期调度RC的副本数量，将用户定义的resource值匹配。 4、服务发现和负载均衡：内置服务发现功能。可以通过 DNS 名称或 IP 地址暴露容器的访问方式；并且可以在同组容器内分发负载以实现负载均衡。 5、存储编排：可以自动挂载指定的存储系统，例如 local stroage/nfs/云存储等。 6、自动更新和回滚：可以在 K8S 中声明你期望应用程序容器应该达到的状态，Kubernetes将以合适的速率调整容器的实际状态，并逐步达到最终期望的结果，不会同时杀掉应用。更新出错，自动恢复到原先状态。 Kubernetes架构设计 Master说明Master：集群控制节点，负责整个集群的管理和控制。 API Server：提供接口，资源增删改查入口。并提供认证、授权、访问控制、API注册和发现等机制。 Controller Manager：所有资源对象的自动化控制中心；负责维护集群的状态，比如故障检测、自动扩展、滚动更新等。 Scheduler：负责资源调度，按照预定的调度策略将Pod调度到相应的机器上。 Etcd：保存整个集群的状态。 Node说明Node：工作节点，听从master的工作分配 Kubelet：Pod容器创建、启停、集群管理等任务；同时也负责Volume（CVI）和网络（CNI）的管理。 Kube-proxy：实现service的通信与负载均衡组件。 Docker：docker引擎，负责本机容器的创建和管理工作。 其他组件说明除了核心组件，还有一些推荐的Add-ons： kube-dns：负责为整个集群提供DNS服务 Ingress Controller：为服务提供外网入口 Heapster/metrics-server：提供资源监控 Dashboard：提供GUI Federation：提供跨可用区的集群 Fluentd-elasticsearch：提供集群日志采集、存储与查询 示意图kubernetes master kubernetes node 分层架构Kubernetes设计理念和功能其实就是一个类似Linux的分层架构，如下图所示： 核心层：Kubernetes最核心的功能，对外提供API构建高层的应用，对内提供插件式应用执行环境 应用层：部署（无状态应用、有状态应用、批处理任务、集群应用等）和路由（服务发现、DNS解析等） 管理层：系统度量（如基础设施、容器和网络的度量），自动化（如自动扩展、动态Provision等）以及策略管理（RBAC、Quota、PSP、NetworkPolicy等） 接口层：kubectl命令行工具、客户端SDK以及集群联邦 生态系统：在接口层之上的庞大容器集群管理调度的生态系统，可以划分为两个范畴 「Kubernetes外部：日志、监控、配置管理、CI、CD、Workflow、FaaS、OTS应用、ChatOps等」 「Kubernetes内部：CRI、CNI、CVI、镜像仓库、Cloud Provider、集群自身的配置和管理等」 Kubernetes的核心技术概念和API对象API对象是K8s集群中的管理操作单元。K8s集群系统每支持一项新功能，引入一项新技术，一定会新引入对应的API对象，支持对该功能的管理操作。例如副本集Replica Set对应的API对象是RS。 每个API对象都有3大类属性：元数据metadata、规范spec和状态status。元数据是用来标识API对象的，每个对象都至少有3个元数据：namespace，name和uid；除此以外还有各种各样的标签labels用来标识和匹配不同的对象，例如用户可以用标签env来标识区分不同的服务部署环境，分别用env=dev、env=testing、env=production来标识开发、测试、生产的不同服务。规范描述了用户期望K8s集群中的分布式系统达到的理想状态（Desired State），例如用户可以通过复制控制器Replication Controller设置期望的Pod副本数为3；status描述了系统实际当前达到的状态（Status），例如系统当前实际的Pod副本数为2；那么复制控制器当前的程序逻辑就是自动启动新的Pod，争取达到副本数为3。 K8s中所有的配置都是通过API对象的spec去设置的，也就是用户通过配置系统的理想状态来改变系统，这是k8s重要设计理念之一，即所有的操作都是声明式（Declarative）的而不是命令式（Imperative）的。声明式操作在分布式系统中的好处是稳定，不怕丢操作或运行多次，例如设置副本数为3的操作运行多次也还是一个结果，而给副本数加1的操作就不是声明式的，运行多次结果就错了。 微服实例-PodK8s有很多技术概念，同时对应很多API对象，最重要的也是最基础的是微服务Pod。 Pod是在K8s集群中运行部署应用或服务的最小单元，它是可以支持多容器的。Pod的设计理念是支持多个容器在一个Pod中共享网络地址和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。 Pod是K8s集群中所有业务类型的基础，可以看作运行在K8s集群中的小机器人，不同类型的业务就需要不同类型的小机器人去执行。目前K8s中的业务主要可以分为长期伺服型（long-running）、批处理型（batch）、节点后台支撑型（node-daemon）和有状态应用型（stateful application）；分别对应的小机器人控制器为Deployment、Job、DaemonSet和StatefulSet，本文后面会一一介绍。 复制控制器-Replication Controller，RCRC是K8s集群中最早的保证Pod高可用的API对象。通过监控运行中的Pod来保证集群中运行指定数目的Pod副本。指定的数目可以是多个也可以是1个；少于指定数目，RC就会启动运行新的Pod副本；多于指定数目，RC就会杀死多余的Pod副本。 即使在指定数目为1的情况下，通过RC运行Pod也比直接运行Pod更明智，因为RC也可以发挥它高可用的能力，保证永远有1个Pod在运行。 RC是K8s较早期的技术概念，只适用于长期伺服型的业务类型，比如控制小机器人提供高可用的Web服务。 副本集(设置)- Replica Set，RSRS是新一代RC，提供同样的高可用能力，区别主要在于RS后来居上，能支持更多种类的匹配模式。副本集对象一般不单独使用，而是作为Deployment的理想状态参数使用。 部署- DeploymentDeployment表示用户对K8s集群的一次更新操作。Deployment是一个比RS应用模式更广的API对象，可以是创建一个新的服务，更新一个新的服务，也可以是滚动升级一个服务。 滚动升级一个服务，实际是创建一个新的RS，然后逐渐将新RS中副本数增加到理想状态，将旧RS中的副本数减小到0的复合操作；这样一个复合操作用一个RS是不太好描述的，所以用一个更通用的Deployment来描述。 以K8s的发展方向，未来对所有长期伺服型的的业务的管理，都会通过Deployment来管理。 服务-ServiceRC、RS和Deployment只是保证了支撑服务的微服务Pod的数量，但是没有解决如何访问这些服务的问题。 一个Pod只是一个运行服务的实例，随时可能在一个节点上停止，在另一个节点以一个新的IP启动一个新的Pod，因此不能以固定的IP和端口号提供服务。要稳定地提供服务需要服务发现和负载均衡能力。服务发现完成的工作，是针对客户端访问的服务，找到对应的的后端服务实例。 在K8s集群中，客户端需要访问的服务就是Service对象。每个Service会对应一个集群内部有效的虚拟IP，集群内部通过虚拟IP访问一个服务。在K8s集群中微服务的负载均衡是由Kube-proxy实现的。Kube-proxy是K8s集群内部的负载均衡器。 它是一个分布式代理服务器，在K8s的每个节点上都有一个；这一设计体现了它的伸缩性优势，需要访问服务的节点越多，提供负载均衡能力的Kube-proxy就越多，高可用节点也随之增多。与之相比，我们平时在服务器端做个反向代理做负载均衡，还要进一步解决反向代理的负载均衡和高可用问题。 任务-JobJob是K8s用来控制批处理型任务的API对象。批处理业务与长期伺服业务的主要区别是批处理业务的运行有头有尾，而长期伺服业务在用户不停止的情况下永远运行。 Job管理的Pod根据用户的设置把任务成功执行完成就自动退出了。成功完成的标志根据不同的spec.completions策略而不同：单Pod型任务有一个Pod成功就标志完成；定数成功型任务保证有N个任务全部成功；工作队列型任务根据应用确认的全局成功而标志成功。 后台支撑服务集-DaemonSet长期伺服型和批处理型服务的核心在业务应用，可能有些节点运行多个同类业务的Pod，有些节点上又没有这类Pod运行；而后台支撑型服务的核心关注点是在K8s集群中的节点（物理机或虚拟机），要保证每个节点上都有一个此类Pod运行。 节点可能是所有集群节点也可能是通过nodeSelector选定的一些特定节点。典型的后台支撑型服务包括：存储，日志和监控等在每个节点上支持K8s集群运行的服务。 有状态服务集-StatefulSetK8s在1.3版本里发布了Alpha版的PetSet功能。在云原生应用的体系里，有下面两组近义词；第一组是无状态（stateless）、牲畜（cattle）、无名（nameless）、可丢弃（disposable）；第二组是有状态（stateful）、宠物（pet）、有名（having name）、不可丢弃（non-disposable）。 RC和RS主要是控制提供无状态服务的，其所控制的Pod的名字是随机设置的，一个Pod出故障了就被丢弃掉，在另一个地方重启一个新的Pod，名字变了；名字和启动在哪儿都不重要，重要的只是Pod总数；而StatefulSet是用来控制有状态服务，StatefulSet中的每个Pod的名字都是事先确定的，不能更改。StatefulSet中Pod的名字的作用，是关联与该Pod对应的状态。 对于RC和RS中的Pod，一般不挂载存储或者挂载共享存储，保存的是所有Pod共享的状态，Pod和普通物品一样没有什么分别；对于StatefulSet中的Pod，每个Pod挂载自己独立的存储，如果一个Pod出现故障，从其他节点启动一个同样名字的Pod，要挂载上原来Pod的存储继续以它的状态提供服务。 适合于StatefulSet的业务包括数据库服务MySQL和PostgreSQL，集群化管理服务Zookeeper、etcd等有状态服务。StatefulSet的另一种典型应用场景是作为一种比普通容器更稳定可靠的模拟虚拟机的机制。传统的虚拟机正是一种有状态的物品，运维人员需要不断地维护它，容器刚开始流行时，我们用容器来模拟虚拟机使用，所有状态都保存在容器里，而这已被证明是非常不安全、不可靠的。使用StatefulSet，Pod仍然可以通过漂移到不同节点提供高可用，而存储也可以通过外挂的存储来提供高可靠性，StatefulSet做的只是将确定的Pod与确定的存储关联起来保证状态的连续性。 集群联邦-FederationK8s在1.3版本里发布了beta版的Federation功能。在云计算环境中，服务的作用距离范围从近到远一般可以有：同主机（Host，Node）、跨主机同可用区（Available Zone）、跨可用区同地区（Region）、跨地区同服务商（Cloud Service Provider）、跨云平台。K8s的设计定位是单一集群在同一个地域内，因为同一个地区的网络性能才能满足K8s的调度和计算存储连接要求。而联合集群服务就是为提供跨Region跨服务商K8s集群服务而设计的。 每个K8s Federation有自己的分布式存储、API Server和Controller Manager。用户可以通过Federation的API Server注册该Federation的成员K8s Cluster。当用户通过Federation的API Server创建、更改API对象时，Federation API Server会在自己所有注册的子K8s Cluster都创建一份对应的API对象。在提供业务请求服务时，K8s Federation会先在自己的各个子Cluster之间做负载均衡，而对于发送到某个具体K8s Cluster的业务请求，会依照这个K8s Cluster独立提供服务时一样的调度模式去做K8s Cluster内部的负载均衡。而Cluster之间的负载均衡是通过域名服务的负载均衡来实现的。 所有的设计都尽量不影响K8s Cluster现有的工作机制，这样对于每个子K8s集群来说，并不需要更外层的有一个K8s Federation，也就是意味着所有现有的K8s代码和机制不需要因为Federation功能有任何变化。 存储卷-VolumeK8s集群中的存储卷跟Docker的存储卷有些类似，只不过Docker的存储卷作用范围为一个容器，而K8s的存储卷的生命周期和作用范围是一个Pod。每个Pod中声明的存储卷由Pod中的所有容器共享。 K8s支持非常多的存储卷类型，特别是支持多种公有云平台的存储，包括AWS，Google和Azure云；支持多种分布式存储包括GlusterFS和Ceph；也支持较容易使用的主机本地目录hostPath和NFS。 K8s还支持使用Persistent Volume Claim即PVC这种逻辑存储，使用这种存储，使得存储的使用者可以忽略后台的实际存储技术（例如AWS，Google或GlusterFS和Ceph），而将有关存储实际技术的配置交给存储管理员通过Persistent Volume来配置。 持久存储卷-Persistent Volume，PV和持久存储卷声明-Persistent Volume Claim，PVCPV和PVC使得K8s集群具备了存储的逻辑抽象能力，使得在配置Pod的逻辑里可以忽略对实际后台存储技术的配置，而把这项配置的工作交给PV的配置者，即集群的管理者。 存储的PV和PVC的这种关系，跟计算的Node和Pod的关系是非常类似的；PV和Node是资源的提供者，根据集群的基础设施变化而变化，由K8s集群管理员配置；而PVC和Pod是资源的使用者，根据业务服务的需求变化而变化，由K8s集群的使用者即服务管理员来配置。 节点-NodeK8s集群中的计算能力由Node提供，最初Node称为服务节点Minion，后来改名为Node。K8s集群中的Node也就等同于Mesos集群中的Slave节点，是所有Pod运行所在的工作主机，可以是物理机也可以是虚拟机。 无论是物理机还是虚拟机，工作主机的统一特征是上面要运行kubelet管理节点。 密钥对象-SecretSecret是用来保存和传递密码、密钥、认证凭证这些敏感信息的对象。使用Secret的好处是可以避免把敏感信息明文写在配置文件里。 在K8s集群中配置和使用服务不可避免的要用到各种敏感信息实现登录、认证等功能，例如访问AWS存储的用户名密码。 为了避免将类似的敏感信息明文写在所有需要使用的配置文件中，可以将这些信息存入一个Secret对象，而在配置文件中通过Secret对象引用这些敏感信息。这种方式的好处包括：意图明确，避免重复，减少暴漏机会。 用户帐户-User Account和服务帐户-Service Account顾名思义，用户帐户为人提供账户标识，而服务账户为计算机进程和K8s集群中运行的Pod提供账户标识。 用户帐户和服务帐户的一个区别是作用范围；用户帐户对应的是人的身份，人的身份与服务的namespace无关，所以用户账户是跨namespace的；而服务帐户对应的是一个运行中程序的身份，与特定namespace是相关的。 名称空间-Namespace名称空间为K8s集群提供虚拟的隔离作用，K8s集群初始有两个名称空间，分别是默认名称空间default和系统名称空间kube-system，除此以外，管理员可以创建新的名称空间满足需要。 RBAC访问授权K8s在1.3版本中发布了alpha版的基于角色的访问控制（Role-based Access Control，RBAC）的授权模式。相对于基于属性的访问控制（Attribute-based Access Control，ABAC），RBAC主要是引入了角色（Role）和角色绑定（RoleBinding）的抽象概念。在ABAC中，K8s集群中的访问策略只能跟用户直接关联；而在RBAC中，访问策略可以跟某个角色关联，具体的用户再跟一个或多个角色相关联。 显然，RBAC像其他新功能一样，每次引入新功能，都会引入新的API对象，从而引入新的概念抽象，而这一新的概念抽象一定会使集群服务管理和使用变得更容易扩展和重用。 总结从K8s的系统架构、技术概念和设计理念，我们可以看到K8s系统最核心的两个设计理念：一个是容错性，一个是易扩展性。 容错性实际是保证K8s系统稳定性和安全性的基础；易扩展性是保证K8s对变更友好，可以快速迭代增加新功能的基础。]]></content>
      <categories>
        <category>Kubernetes</category>
        <category>K8S</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
        <tag>K8S</tag>
        <tag>CI/CD</tag>
        <tag>DevOps</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YAML 语言教程与使用案例]]></title>
    <url>%2F2020%2F05%2F17%2Fyaml01%2F</url>
    <content type="text"><![CDATA[YAML语言教程与使用案例，如何编与读懂写YAML文件。 YAML概要YAML 是 “YAML Ain’t a Markup Language”（YAML 不是一种标记语言）的递归缩写。在开发的这种语言时，YAML 的意思其实是：”Yet Another Markup Language”（仍是一种标记语言）。 YAML是一个类似 XML、JSON 的标记性语言。YAML 强调以数据为中心，并不是以标识语言为重点。因而 YAML 本身的定义比较简单，号称“一种人性化的数据格式语言”。 基本语法1、大小写敏感 2、使用缩进表示层级关系 3、缩进时不允许使用Tab键，只允许使用空格 4、缩进的空格数不重要，只要相同层级的元素左侧对齐即可。【实际使用中建议两个空格作为一个层级的缩进】 5、# 表示注释，从这个字符一直到行尾，都会被解释器忽略 6、冒号，以冒号结尾除外，其他所有冒号后面必须有空格 7、短横线，表示列表项，使用一个短横线加一个空格；多个项使用同样的缩进级别作为同一列表 支持的数据结构1、对象：键值对的集合，又称为映射（mapping）/ 哈希（hashes） / 字典（dictionary） 2、数组：一组按次序排列的值，又称为序列（sequence） / 列表（list） 3、字面量/纯量（数字、字符串、布尔值）(scalars)：单个的、不可再分的值 YAML 组织结构YAML 文件可以由一或多个文档组成（即相对独立的组织结构组成），文档间使用“---”（三个横线）在每文档开始作为分隔符(可选)。同时，文档也可以使用“...”（三个点号）作为结束符（可选）。如下图所示： 备注：如果只是单个文档，分隔符“---”可省略。 每个文档并不需要使用结束符“...”来表示结束，但是对于网络传输或者流来说，有明确结束的符号，有利于软件处理。（例如不需要知道流关闭就能知道文档结束） Python中yaml模块的使用Python pip 安装如果未安装pip，则可以使用以下方法来安装： 123# curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py # 下载安装脚本# python get-pip.py # 运行安装脚本 或则：python3 get-pip.py 根据使用的Python决定# pip --version # 版本查看 用哪个版本的 Python 运行安装脚本，pip 就被关联到哪个版本。 pip安装yaml与导入安装pyyaml 1# pip install pyyaml # 或者pip3 install pyyaml 检查是否安装成功： 1、命令行输入：python 2、再输入：import yaml 安装成功后，在脚本里导入的语句，都是 import yaml YAML-对象数据类型备注：之所以对yaml文件使用Python进行解析，是因为我们要测验yaml文件格式是否书写正确。 对象数据：是一组键值对，使用冒号结构表示。 单个对象文档文件yaml文件 1234[root@docker02 yaml]# cat demo_01_obj.yml---name: zhangage: 22 Python解析 1234567891011[root@docker02 yaml]# cat demo_01_obj.py#!/usr/bin/env python# -*- coding: utf-8 -*-# Author: zhangimport yamlfile_path = &quot;./demo_01_obj.yml&quot;file = open(file_path, &apos;r&apos;)ys = yaml.load(file.read(), Loader=yaml.Loader)print ys 输出结果 12[root@docker02 yaml]# python demo_01_obj.py &#123;&apos;age&apos;: 22, &apos;name&apos;: &apos;zhang&apos;&#125; 多个对象文档文件yaml文件，仔细对比下加了”---“和”...“ 的区别 123456789101112131415[root@docker02 yaml]# cat demo_02_obj.yml---name: zhangage: 22...---name: Janeage: 20key: child-key: value child-key2: value2...---obj: &#123;obj_key1: value1, obj_key2: value2&#125;... Python解析 12345678910111213141516[root@docker02 yaml]# cat demo_02_obj.py#!/usr/bin/env python# -*- coding: utf-8 -*-# Author: zhangimport yamlimport jsonfile_path = &quot;demo_02_obj.yml&quot;file = open(file_path, &apos;r&apos;)ys = yaml.load_all(file.read(), Loader=yaml.Loader)for y in ys: # 两种打印方式都尝试下 #print y print json.dumps(y, indent=2) 输出结果 123456789101112131415161718192021222324[root@docker02 yaml]# python demo_02_obj.py&#123;&apos;age&apos;: 22, &apos;name&apos;: &apos;zhang&apos;&#125;&#123;&apos;age&apos;: 20, &apos;name&apos;: &apos;Jane&apos;, &apos;key&apos;: &#123;&apos;child-key2&apos;: &apos;value2&apos;, &apos;child-key&apos;: &apos;value&apos;&#125;&#125;&#123;&apos;obj&apos;: &#123;&apos;obj_key1&apos;: &apos;value1&apos;, &apos;obj_key2&apos;: &apos;value2&apos;&#125;&#125;# 或者如下[root@docker02 yaml]# python demo_02_obj.py &#123; &quot;age&quot;: 22, &quot;name&quot;: &quot;zhang&quot;&#125;&#123; &quot;age&quot;: 20, &quot;name&quot;: &quot;Jane&quot;, &quot;key&quot;: &#123; &quot;child-key2&quot;: &quot;value2&quot;, &quot;child-key&quot;: &quot;value&quot; &#125;&#125;&#123; &quot;obj&quot;: &#123; &quot;obj_key1&quot;: &quot;value1&quot;, &quot;obj_key2&quot;: &quot;value2&quot; &#125;&#125; YAML-数组数据类型备注：之所以对yaml文件使用Python进行解析，是因为我们要测验yaml文件格式是否书写正确。 数组类型：一组连词线开头的行，构成一个数组 yaml文件 12345678910[root@docker02 yaml]# cat demo_03_list.yml # 书写方式1color:- red- blue- green- orange- white# 书写方式2：行内表示法fruits: [orange, apple, banana] Python解析 12345678910111213141516[root@docker02 yaml]# cat demo_03_list.py #!/usr/bin/env python# -*- coding: utf-8 -*-# Author: zhangimport yamlimport jsonfile_path = &quot;demo_03_list.yml&quot;file = open(file_path, &apos;r&apos;)ys = yaml.load_all(file.read(), Loader=yaml.Loader)for y in ys: # 两种打印方式都尝试下 #print y print json.dumps(y, indent=2) 输出结果 123456789101112131415161718[root@docker02 yaml]# python demo_03_list.py &#123;&apos;color&apos;: [&apos;red&apos;, &apos;blue&apos;, &apos;green&apos;, &apos;orange&apos;, &apos;white&apos;], &apos;fruits&apos;: [&apos;orange&apos;, &apos;apple&apos;, &apos;banana&apos;]&#125;# 或者结果如下[root@docker02 yaml]# python demo_03_list.py&#123; &quot;color&quot;: [ &quot;red&quot;, &quot;blue&quot;, &quot;green&quot;, &quot;orange&quot;, &quot;white&quot; ], &quot;fruits&quot;: [ &quot;orange&quot;, &quot;apple&quot;, &quot;banana&quot; ]&#125; YAML-复合结构备注：之所以对yaml文件使用Python进行解析，是因为我们要测验yaml文件格式是否书写正确。 复合结构：对象和数组可以结合使用，形成复合结构 yaml文件，注意其书序格式，并细细对比输出结果 123456789101112131415161718[root@docker02 yaml]# cat demo_04_compose.ymlshop: GoodShoppingaddress: BJgoods: Food: - sell_time: &quot;AM 08:30&quot; food01: rice food02: pork Fruits: - sell_time: &quot;AM 09:00&quot; - fruit01: orange price: 3.50 - fruit02: banana price: 3.00 clothes: - sell_time: &quot;AM 09:30&quot; - clothe01 - clothe02 Python解析 12345678910111213141516[root@docker02 yaml]# cat demo_04_compose.py #!/usr/bin/env python# -*- coding: utf-8 -*-# Author: zhangimport yamlimport jsonfile_path = &quot;demo_04_compose.yml&quot;file = open(file_path, &apos;r&apos;)ys = yaml.load_all(file.read(), Loader=yaml.Loader)for y in ys: # 两种打印方式都尝试下 print y #print json.dumps(y, indent=2) 输出结果，仔细对比下 12345678910111213141516171819202122232425262728293031323334353637[root@docker02 yaml]# python demo_04_compose.py &#123;&apos;shop&apos;: &apos;GoodShopping&apos;, &apos;goods&apos;: &#123;&apos;Food&apos;: [&#123;&apos;food02&apos;: &apos;pork&apos;, &apos;sell_time&apos;: &apos;AM 08:30&apos;, &apos;food01&apos;: &apos;rice&apos;&#125;], &apos;Fruits&apos;: [&#123;&apos;sell_time&apos;: &apos;AM 09:00&apos;&#125;, &#123;&apos;fruit01&apos;: &apos;orange&apos;, &apos;price&apos;: 3.5&#125;, &#123;&apos;price&apos;: 3.0, &apos;fruit02&apos;: &apos;banana&apos;&#125;], &apos;clothes&apos;: [&#123;&apos;sell_time&apos;: &apos;AM 09:30&apos;&#125;, &apos;clothe01&apos;, &apos;clothe02&apos;]&#125;, &apos;address&apos;: &apos;BJ&apos;&#125;# 或者结果如下[root@docker02 yaml]# python demo_04_compose.py &#123; &quot;shop&quot;: &quot;GoodShopping&quot;, &quot;goods&quot;: &#123; &quot;Food&quot;: [ &#123; &quot;food02&quot;: &quot;pork&quot;, &quot;sell_time&quot;: &quot;AM 08:30&quot;, &quot;food01&quot;: &quot;rice&quot; &#125; ], &quot;Fruits&quot;: [ &#123; &quot;sell_time&quot;: &quot;AM 09:00&quot; &#125;, &#123; &quot;fruit01&quot;: &quot;orange&quot;, &quot;price&quot;: 3.5 &#125;, &#123; &quot;price&quot;: 3.0, &quot;fruit02&quot;: &quot;banana&quot; &#125; ], &quot;clothes&quot;: [ &#123; &quot;sell_time&quot;: &quot;AM 09:30&quot; &#125;, &quot;clothe01&quot;, &quot;clothe02&quot; ] &#125;, &quot;address&quot;: &quot;BJ&quot;&#125; YAML-纯量数据类型备注：之所以对yaml文件使用Python进行解析，是因为我们要测验yaml文件格式是否书写正确。 纯量是最基本的，不可再分的值，包括： 1234567字符串布尔值整数浮点数Null时间日期 常用数据类型的表示格式进行了约定123456789101112131415161718192021[root@docker02 yaml]# cat demo_05_scalars.ymlboolean: - TRUE # true,True都可以 - FALSE # false，False都可以float: - 3.14 - 6.8523015e+5 #可以使用科学计数法int: - 123null: nodeName: &apos;node&apos; parent: ~ # 使用~表示nullstring: - 哈哈 - &apos;Hello world&apos; # 可以使用双引号或者单引号包裹特殊字符 - newline newline2 # 字符串可以拆成多行，非尾行的每一行换行符都转为空格date: - 2018-02-17 # 日期必须使用ISO 8601格式，即yyyy-MM-dddatetime: - 2018-02-17T15:02:31+08:00 #时间使用ISO 8601格式，时间和日期之间使用T连接，最后使用+代表时区 双叹号强制转换类型yaml文件 1234567[root@docker02 yaml]# cat demo_06_switch.yml # 原信息ori01: 3.14ori02: &quot;123&quot;# 强制转换int_str: !!str 3.14 # value 整数强制转换为字符串str_int: !!int &quot;123&quot; # value 字符串强制转换为整数 Python解析 12345678910111213141516[root@docker02 yaml]# cat demo_06_switch.py #!/usr/bin/env python# -*- coding: utf-8 -*-# Author: zhangimport yamlimport jsonfile_path = &quot;demo_06_switch.yml&quot;file = open(file_path, &apos;r&apos;)ys = yaml.load_all(file.read(), Loader=yaml.Loader)for y in ys: # 两种打印方式都尝试下 print y #print json.dumps(y, indent=2) 输出结果，仔细对比下 12345678910[root@docker02 yaml]# python demo_06_switch.py &#123;&apos;ori01&apos;: 3.14, &apos;int_str&apos;: &apos;3.14&apos;, &apos;ori02&apos;: &apos;123&apos;, &apos;str_int&apos;: 123&#125;# 或者输出如下[root@docker02 yaml]# python demo_06_switch.py &#123; &quot;ori01&quot;: 3.14, &quot;int_str&quot;: &quot;3.14&quot;, &quot;ori02&quot;: &quot;123&quot;, &quot;str_int&quot;: 123&#125; YAML-引用备注：之所以对yaml文件使用Python进行解析，是因为我们要测验yaml文件格式是否书写正确。 &amp; 用来建立锚点（defaults），&lt;&lt; 表示合并到当前数据，* 用来引用锚点。 yaml文件 1234567891011121314151617181920212223[root@docker02 yaml]# cat demo_07_anchor.yml---hr: - Mark McGwire # Following node labeled SS - &amp;SS Sammy Sosa # 定义要复制的数据rbi: - *SS # Subsequent occurrence 这里是数据复制目标 - Ken Griffey...---defaults: &amp;defaults adapter: postgres host: localhostdevelopment: database: myapp_development &lt;&lt;: *defaultstest: database: myapp_test info: *defaults... Python解析 12345678910111213141516[root@docker02 yaml]# cat demo_07_anchor.py #!/usr/bin/env python# -*- coding: utf-8 -*-# Author: zhangimport yamlimport jsonfile_path = &quot;demo_07_anchor.yml&quot;file = open(file_path, &apos;r&apos;)ys = yaml.load_all(file.read(), Loader=yaml.Loader)for y in ys: # 两种打印方式都尝试下 print y #print json.dumps(y, indent=2) 输出结果，仔细对比下 123456789101112131415161718192021222324252627282930313233[root@docker02 yaml]# python demo_07_anchor.py &#123;&apos;hr&apos;: [&apos;Mark McGwire&apos;, &apos;Sammy Sosa&apos;], &apos;rbi&apos;: [&apos;Sammy Sosa&apos;, &apos;Ken Griffey&apos;]&#125;&#123;&apos;development&apos;: &#123;&apos;adapter&apos;: &apos;postgres&apos;, &apos;host&apos;: &apos;localhost&apos;, &apos;database&apos;: &apos;myapp_development&apos;&#125;, &apos;test&apos;: &#123;&apos;info&apos;: &#123;&apos;adapter&apos;: &apos;postgres&apos;, &apos;host&apos;: &apos;localhost&apos;&#125;, &apos;database&apos;: &apos;myapp_test&apos;&#125;, &apos;defaults&apos;: &#123;&apos;adapter&apos;: &apos;postgres&apos;, &apos;host&apos;: &apos;localhost&apos;&#125;&#125;# 或者结果如下[root@docker02 yaml]# python demo_07_anchor.py &#123; &quot;hr&quot;: [ &quot;Mark McGwire&quot;, &quot;Sammy Sosa&quot; ], &quot;rbi&quot;: [ &quot;Sammy Sosa&quot;, &quot;Ken Griffey&quot; ]&#125;&#123; &quot;development&quot;: &#123; &quot;adapter&quot;: &quot;postgres&quot;, &quot;host&quot;: &quot;localhost&quot;, &quot;database&quot;: &quot;myapp_development&quot; &#125;, &quot;test&quot;: &#123; &quot;info&quot;: &#123; &quot;adapter&quot;: &quot;postgres&quot;, &quot;host&quot;: &quot;localhost&quot; &#125;, &quot;database&quot;: &quot;myapp_test&quot; &#125;, &quot;defaults&quot;: &#123; &quot;adapter&quot;: &quot;postgres&quot;, &quot;host&quot;: &quot;localhost&quot; &#125;&#125; YAML-字符串备注：之所以对yaml文件使用Python进行解析，是因为我们要测验yaml文件格式是否书写正确。 字符串是最常见，也是最复杂的一种数据类型。 字符串默认不使用引号表示。 1str: 这是一行字符串 如果字符串之中包含空格或特殊字符，需要放在引号之中。 1str: &apos;内容： 字符串&apos; 单引号和双引号都可以使用，双引号不会对特殊字符转义。 12s1: &apos;内容\n字符串&apos;s2: &quot;内容\n字符串&quot; 单引号之中如果还有单引号，必须连续使用两个单引号转义。 1str: &apos;labor&apos;&apos;s day&apos; 字符串可以写成多行，从第二行开始，必须有空格缩进。换行符会被转为空格。 123str: 这是一段 多行 字符串 多行字符串可以使用 | 保留换行符，也可以使用 &gt; 折叠换行。 123456this: | Foo Barthat: &gt; Foo Bar + 表示保留字符串行末尾的换行，- 表示删除字符串末尾的换行。 12345678s1: | Foos2: |+ Foos3: |- Foo 字符串之中可以插入 HTML 标记。 12345message: | &lt;p style=&quot;color: red&quot;&gt; 段落 &lt;/p&gt; 字符串测验案例yaml文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455[root@docker02 yaml]# cat demo_08_str.yml str01: zhangsanstr02: &apos;Q: What are you doing?&apos;str03: &apos;zhangsan\nlisi\nwangwu&apos;str04: &quot;zhangsan\nlisi\nwangwu&quot;str05: &apos;What&apos;&apos;s your name?&apos;str06: &quot;What&apos;s your name?&quot;str07: &apos;Ken: Hello, My name is Ken. What&apos;&apos;s your name?&apos;str08: &quot;Ken: Hello, My name is Ken. What&apos;s your name?&quot;str11: | 111 222 333str12: &gt; aaa xxx bbb yyy ccc zzz# 之后有2行空行str16: | zhangsan lisi wangwu# 之后有2行空行str17: |+ zhangsan lisi wangwu# 之后有2行空行str18: |- zhangsan lisi wangwumessage: | &lt;p style=&quot;color: red&quot;&gt; one line str &lt;/p&gt; Python解析 12345678910111213141516[root@docker02 yaml]# cat demo_08_str.py #!/usr/bin/env python# -*- coding: utf-8 -*-# Author: zhangimport yamlimport jsonfile_path = &quot;demo_08_str.yml&quot;file = open(file_path, &apos;r&apos;)ys = yaml.load_all(file.read(), Loader=yaml.Loader)for y in ys: # 两种打印方式都尝试下 print y #print json.dumps(y, indent=2) 输出结果，仔细对比下 1234567891011121314151617181920[root@docker02 yaml]# python demo_08_str.py &#123;&apos;str02&apos;: &apos;Q: What are you doing?&apos;, &apos;str01&apos;: &apos;zhangsan&apos;, &apos;str05&apos;: &quot;What&apos;s your name?&quot;, &apos;str08&apos;: &quot;Ken: Hello, My name is Ken. What&apos;s your name?&quot;, &apos;str06&apos;: &quot;What&apos;s your name?&quot;, &apos;str18&apos;: &apos;zhangsan\nlisi\nwangwu&apos;, &apos;str17&apos;: &apos;zhangsan\nlisi\nwangwu\n\n\n&apos;, &apos;str16&apos;: &apos;zhangsan\nlisi\nwangwu\n&apos;, &apos;message&apos;: u&apos;\n&lt;p style=&quot;color: red&quot;&gt;\n one line str\n&lt;/p&gt;\n&apos;, &apos;str03&apos;: &apos;zhangsan\\nlisi\\nwangwu&apos;, &apos;str04&apos;: &apos;zhangsan\nlisi\nwangwu&apos;, &apos;str12&apos;: &apos;aaa xxx bbb yyy ccc zzz\n&apos;, &apos;str11&apos;: &apos;111\n222\n333\n&apos;, &apos;str07&apos;: &quot;Ken: Hello, My name is Ken. What&apos;s your name?&quot;&#125;# 或者输出如下[root@docker02 yaml]# python demo_08_str.py &#123; &quot;str02&quot;: &quot;Q: What are you doing?&quot;, &quot;str01&quot;: &quot;zhangsan&quot;, &quot;str05&quot;: &quot;What&apos;s your name?&quot;, &quot;str08&quot;: &quot;Ken: Hello, My name is Ken. What&apos;s your name?&quot;, &quot;str06&quot;: &quot;What&apos;s your name?&quot;, &quot;str18&quot;: &quot;zhangsan\nlisi\nwangwu&quot;, &quot;str17&quot;: &quot;zhangsan\nlisi\nwangwu\n\n\n&quot;, &quot;str16&quot;: &quot;zhangsan\nlisi\nwangwu\n&quot;, &quot;message&quot;: &quot;\n&lt;p style=\&quot;color: red\&quot;&gt;\n one line str\n&lt;/p&gt;\n&quot;, &quot;str03&quot;: &quot;zhangsan\\nlisi\\nwangwu&quot;, &quot;str04&quot;: &quot;zhangsan\nlisi\nwangwu&quot;, &quot;str12&quot;: &quot;aaa xxx bbb yyy ccc zzz\n&quot;, &quot;str11&quot;: &quot;111\n222\n333\n&quot;, &quot;str07&quot;: &quot;Ken: Hello, My name is Ken. What&apos;s your name?&quot;&#125; 完毕！]]></content>
      <categories>
        <category>YAML</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>YAML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Harbor企业级私有Docker镜像仓库部署]]></title>
    <url>%2F2020%2F05%2F13%2Fdocker06%2F</url>
    <content type="text"><![CDATA[Harbor介绍与安装部署，并实现通过http和https协议【自签发SSL证书】访问，客户端如何通过Harbor镜像仓库实现镜像的上传【推送】与下载【拉取】。 Harbor介绍Harbor，是一个英文单词，意思是港湾，港湾是干什么的呢，就是停放货物的，而货物呢，是装在集装箱中的，说到集装箱，就不得不提到Docker容器，因为docker容器的技术正是借鉴了集装箱的原理。所以，Harbor正是一个用于存储Docker镜像的企业级Registry服务。 Docker容器应用的开发和运行离不开可靠的镜像管理，虽然Docker官方也提供了公共的镜像仓库，但是从安全和效率等方面考虑，部署我们私有环境内的Registry也是非常必要的。Harbor是由VMware公司开源的企业级的Docker Registry管理项目，它包括权限管理(RBAC)、LDAP、日志审核、管理界面、自我注册、镜像复制和中文支持等功能。 机器规划 服务器名称(hostname) 操作系统版本 内网IP 外网IP(模拟) 安装软件 docker01 CentOS7.7 172.16.1.31 10.0.0.31 docker、Harbor docker02 CentOS7.7 172.16.1.32 10.0.0.32 docker SSL证书创建如果要使用https访问Harbor。那么请按照如下生成SSL证书。 创建根证书1234## 创建CA私钥openssl genrsa -out ca.key 2048## 制作CA公钥openssl req -new -x509 -days 36500 -key ca.key -out ca.crt -subj &quot;/C=CN/ST=BJ/L=BeiJing/O=BTC/OU=MOST/CN=zhang/emailAddress=ca@test.com&quot; 选项参数说明： genrsa 生成私钥 -out filename 标准输出到filename文件 req 生成证书请求 -new 生成新证书签署请求 -x509 专用于CA生成自签证书；不自签的时候不要加该选项 -days num 证书的有效期限 -key file 生成请求时用到的私钥文件 -out filename 标准输出到filename文件 subj内容详解： 1234567C = 国家ST = 省/州L = 城市O = Organization NameOU = Organizational Unit NameCN = Common NameemailAddress = test@email.address 证书签发12345678## 创建私钥openssl genrsa -out httpd.key 1024## 生成签发请求openssl req -new -key httpd.key -out httpd.csr -subj &quot;/C=CN/ST=BJ/L=BeiJing/O=BTC/OU=OPS/CN=zhang/emailAddress=zhang@test.com&quot;## 使用CA证书进行签发openssl x509 -req -sha256 -in httpd.csr -CA ca.crt -CAkey ca.key -CAcreateserial -days 36500 -out httpd.crt## 验证签发证书是否有效openssl verify -CAfile ca.crt httpd.crt 生成结果如下图： 然后将httpd.key和httpd.crt，放到/etc/harbor/cert/目录下，后面会用到。 安装docker-ce安装脚本如下 12345678910111213141516171819202122232425262728293031323334353637[root@docker01 harbor]# pwd/root/harbor[root@docker01 harbor]# cat install_docker-ce.sh#!/bin/sh# 加载环境变量. /etc/profile. /etc/bashrc## 设置 docker yum repositoryyum install -y yum-utils device-mapper-persistent-data lvm2yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo## 安装dockeryum install -y docker-ce# yum install -y docker-ce-19.03.8## 启动docker服务，这样可以创建/etc/docker目录systemctl start docker## 配置daemon## 1、修改docker Cgroup Driver为systemd；2、日志格式设定## 如果不修改，可能会碰到如下错误## [WARNING IsDockerSystemdCheck]: detected &quot;cgroupfs&quot; as the Docker cgroup driver. The recommended driver is &quot;systemd&quot;. ## Please follow the guide at https://kubernetes.io/docs/setup/cri/cat &gt; /etc/docker/daemon.json &lt;&lt; EOF&#123; &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;], &quot;log-driver&quot;: &quot;json-file&quot;, &quot;log-opts&quot;: &#123; &quot;max-size&quot;: &quot;100m&quot; &#125;&#125;EOF## 开机自启动systemctl stop docker &amp;&amp; systemctl daemon-reload &amp;&amp; systemctl enable docker &amp;&amp; systemctl start docker 安装docker-compose下载地址： 1https://github.com/docker/compose 此次，我们使用的是 1.25.5 版本。 1234567891011[root@docker01 harbor]# lltotal 17180-rw-r--r-- 1 root root 17586312 May 12 23:16 docker-compose-Linux-x86_64-rw-r--r-- 1 root root 958 May 12 23:00 install_docker-ce.sh[root@docker01 harbor]# chmod +x docker-compose-Linux-x86_64 # 添加执行权限[root@docker01 harbor]# mv docker-compose-Linux-x86_64 /usr/local/sbin/docker-compose # 移到指定目录[root@docker01 harbor]# docker-compose version # 版本查看docker-compose version 1.25.5, build 8a1c60f6docker-py version: 4.1.0CPython version: 3.7.5OpenSSL version: OpenSSL 1.1.0l 10 Sep 2019 安装Harbor私有仓库官网下载地址1https://github.com/goharbor/harbor 此次，我们使用的是 v1.10.1 版本。 123456789101112131415[root@docker01 harbor]# lltotal 658284-rw-r--r-- 1 root root 674078519 May 12 17:25 harbor-offline-installer-v1.10.1.tgz-rw-r--r-- 1 root root 958 May 12 23:00 install_docker-ce.sh[root@docker01 harbor]# [root@docker01 harbor]# tar xf harbor-offline-installer-v1.10.1.tgz # 解压包[root@docker01 harbor]# cd harbor/[root@docker01 harbor]# lltotal 662120-rw-r--r-- 1 root root 3398 Feb 10 14:18 common.sh-rw-r--r-- 1 root root 677974489 Feb 10 14:19 harbor.v1.10.1.tar.gz-rw-r--r-- 1 root root 5882 Feb 10 14:18 harbor.yml-rwxr-xr-x 1 root root 2284 Feb 10 14:18 install.sh-rw-r--r-- 1 root root 11347 Feb 10 14:18 LICENSE-rwxr-xr-x 1 root root 1749 Feb 10 14:18 prepare harbor.yml配置文件修改内容【http访问】123456789101112131415161718192021222324# 这里的hostname怎么配置# 1、如果所有机器都在一个局域网，那么配置内网IP# 2、如果机器跨网络，只能通过公网访问，那么配置本机外网IP或域名hostname: 172.16.1.31# http端口改为了5000，默认80端口http: # port for http, default is 80. If https enabled, this port will redirect to https port port: 5000# 将https注释掉，不然会报 ERROR:root:Error: The protocol is https but attribute ssl_cert is not set# https related config#https: # https port for harbor, default is 443 #port: 443 # The path of cert and key files for nginx #certificate: /your/certificate/path #private_key: /your/private/key/path# admin用户的免密harbor_admin_password: Harbor12345# 数据存储路径data_volume: /data harbor.yml配置文件修改内容【https访问】放开了https配置，证书是自签发的。 1234567891011121314151617181920212223# 这里的hostname怎么配置# 1、如果所有机器都在一个局域网，那么配置内网IP# 2、如果机器跨网络，只能通过公网访问，那么配置本机外网IP或域名hostname: 172.16.1.31# http端口改为了5000，默认80端口http: # port for http, default is 80. If https enabled, this port will redirect to https port port: 5000# https related confighttps: # https port for harbor, default is 443 port: 443 # The path of cert and key files for nginx certificate: /etc/harbor/cert/httpd.crt private_key: /etc/harbor/cert/httpd.key# admin用户的免密harbor_admin_password: Harbor12345# 数据存储路径data_volume: /data 如果使用了https协议且端口是443，那么当使用http访问时，会自动跳转到https。 部署Harbor修改完配置文件后，在的当前目录执行./install.sh，Harbor服务就会根据当前目录下的docker-compose.yml开始下载依赖的镜像，检测并按照顺序依次启动。 123456789101112[root@docker01 harbor]# lltotal 662120drwxr-xr-x 3 root root 20 May 12 23:47 common-rw-r--r-- 1 root root 3398 Feb 10 14:18 common.sh-rw-r--r-- 1 root root 677974489 Feb 10 14:19 harbor.v1.10.1.tar.gz-rw-r--r-- 1 root root 5921 May 12 23:54 harbor.ymldrwxr-xr-x 2 root root 24 May 12 23:47 input-rwxr-xr-x 1 root root 2284 Feb 10 14:18 install.sh-rw-r--r-- 1 root root 11347 Feb 10 14:18 LICENSE-rwxr-xr-x 1 root root 1749 Feb 10 14:18 prepare[root@docker01 harbor]# [root@docker01 harbor]# ./install.sh # 启动harbor 启动结果如下图 停止与启动Harbor如果修改了Harbor的配置文件harbor.yml，因为Harbor是基于docker-compose服务编排的，我们可以使用docker-compose命令重启Harbor。 未修改配置文件，重启Harbor命令：docker-compose start | stop | restart 当然个人建议：如果修改了harbor.yml文件，那么停止使用docker-compose down，启动使用 ./install.sh 。 123456789101112131415161718192021222324252627282930313233##### 停止Harbor[root@docker01 harbor]# docker-compose down Stopping harbor-jobservice ... doneStopping nginx ... doneStopping harbor-core ... doneStopping registryctl ... doneStopping redis ... doneStopping harbor-portal ... doneStopping harbor-db ... doneStopping registry ... doneStopping harbor-log ... doneRemoving harbor-jobservice ... doneRemoving nginx ... doneRemoving harbor-core ... doneRemoving registryctl ... doneRemoving redis ... doneRemoving harbor-portal ... doneRemoving harbor-db ... doneRemoving registry ... doneRemoving harbor-log ... doneRemoving network harbor_harbor##### 启动Harbor[root@docker01 harbor]# docker-compose up -dCreating network &quot;harbor_harbor&quot; with the default driverCreating harbor-log ... doneCreating registryctl ... doneCreating harbor-db ... doneCreating redis ... doneCreating registry ... doneCreating harbor-portal ... doneCreating harbor-core ... doneCreating nginx ... doneCreating harbor-jobservice ... done 镜像信息和容器信息镜像信息和容器信息如下 1234567891011121314151617181920212223242526272829[root@docker01 ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZEgoharbor/chartmuseum-photon v0.9.0-v1.10.1 0245d66323de 3 months ago 128MBgoharbor/harbor-migrator v1.10.1 a4f99495e0b0 3 months ago 364MBgoharbor/redis-photon v1.10.1 550a58b0a311 3 months ago 111MBgoharbor/clair-adapter-photon v1.0.1-v1.10.1 2ec99537693f 3 months ago 61.6MBgoharbor/clair-photon v2.1.1-v1.10.1 622624e16994 3 months ago 171MBgoharbor/notary-server-photon v0.6.1-v1.10.1 e4ff6d1f71f9 3 months ago 143MBgoharbor/notary-signer-photon v0.6.1-v1.10.1 d3aae2fc17c6 3 months ago 140MBgoharbor/harbor-registryctl v1.10.1 ddef86de6480 3 months ago 104MBgoharbor/registry-photon v2.7.1-patch-2819-2553-v1.10.1 1a0c5f22cfa7 3 months ago 86.5MBgoharbor/nginx-photon v1.10.1 01276d086ad6 3 months ago 44MBgoharbor/harbor-log v1.10.1 1f5c9ea164bf 3 months ago 82.3MBgoharbor/harbor-jobservice v1.10.1 689368d30108 3 months ago 143MBgoharbor/harbor-core v1.10.1 14151d58ac3f 3 months ago 130MBgoharbor/harbor-portal v1.10.1 8a9856c37798 3 months ago 52.1MBgoharbor/harbor-db v1.10.1 18548720d8ad 3 months ago 148MBgoharbor/prepare v1.10.1 897a4d535ced 3 months ago 192MB[root@docker01 ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES6f57ce1d6a27 goharbor/nginx-photon:v1.10.1 &quot;nginx -g &apos;daemon of…&quot; 29 seconds ago Up 28 seconds (health: starting) 0.0.0.0:5000-&gt;8080/tcp nginxbd441d18ae54 goharbor/harbor-jobservice:v1.10.1 &quot;/harbor/harbor_jobs…&quot; 29 seconds ago Up 28 seconds (health: starting) harbor-jobservice374fad48780e goharbor/harbor-core:v1.10.1 &quot;/harbor/harbor_core&quot; 30 seconds ago Up 29 seconds (health: starting) harbor-core89f8f4312c24 goharbor/harbor-portal:v1.10.1 &quot;nginx -g &apos;daemon of…&quot; 31 seconds ago Up 29 seconds (health: starting) 8080/tcp harbor-portal4d0b294a38c4 goharbor/redis-photon:v1.10.1 &quot;redis-server /etc/r…&quot; 31 seconds ago Up 29 seconds (health: starting) 6379/tcp rediscd9fafa019f5 goharbor/harbor-registryctl:v1.10.1 &quot;/home/harbor/start.…&quot; 31 seconds ago Up 29 seconds (health: starting) registryctla62616384f6c goharbor/registry-photon:v2.7.1-patch-2819-2553-v1.10.1 &quot;/home/harbor/entryp…&quot; 31 seconds ago Up 29 seconds (health: starting) 5000/tcp registrydc453165b1fb goharbor/harbor-db:v1.10.1 &quot;/docker-entrypoint.…&quot; 31 seconds ago Up 29 seconds (health: starting) 5432/tcp harbor-db8256f54e69ee goharbor/harbor-log:v1.10.1 &quot;/bin/sh -c /usr/loc…&quot; 31 seconds ago Up 30 seconds (healthy) 127.0.0.1:1514-&gt;10514/tcp harbor-log 浏览器访问访问地址如下： 12http 访问：http://10.0.0.31:5000/ 或则 http://172.16.1.31:5000/https访问：https://10.0.0.31/ 或者 https://172.16.1.31/ 备注： 1、由于我使用的Vmware虚拟机，因此10.0.0.0/24网段【模拟外网】和172.16.1.0/24网络【内网】都可以访问。生产环境是访问内网还是外网，视具体情况而定。 2、这里的访问地址和harbor.yml中配置的hostname值无关。 登录后页面 Harbor实现Docker镜像上传与下载新建项目根据你的项目名新建项目，这样才能将镜像推动到harbor镜像中心。 客户端http设置Docker 默认不允许非 HTTPS 方式推送镜像。我们可以通过 Docker 的配置选项来取消这个限制。 如果直接【上传】或【拉取】镜像会失败，因为默认为https方式。 所有客户端都需要添加这个配置，然后重启 docker 服务。 12345678910[root@docker01 ~]# vim /etc/docker/daemon.json&#123; &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;], &quot;log-driver&quot;: &quot;json-file&quot;, &quot;log-opts&quot;: &#123; &quot;max-size&quot;: &quot;100m&quot; &#125;, &quot;insecure-registries&quot;: [&quot;172.16.1.31:5000&quot;]&#125;[root@docker01 ~]# systemctl restart docker # 重启docker服务 添加了 “insecure-registries”: [“172.16.1.31:5000”] 这行，其中172.16.1.31为内网IP地址。该文件必须符合 json 规范，否则 Docker 将不能启动。 如果在Harbor所在的机器重启了docker服务，记得要重新启动Harbor。 客户端登录Harbor客户端登录Harbor。 1# docker login 172.16.1.31:5000 -u admin -p Harbor12345 查看登录信息，这样客户端就可以直接拉取或者推送镜像了。 1234567891011[root@docker01 ~]# cat ~/.docker/config.json &#123; &quot;auths&quot;: &#123; &quot;172.16.1.31:5000&quot;: &#123; &quot;auth&quot;: &quot;YWRtaW46SGFyYm9yMTIzNDU=&quot; &#125; &#125;, &quot;HttpHeaders&quot;: &#123; &quot;User-Agent&quot;: &quot;Docker-Client/19.03.8 (linux)&quot; &#125;&#125; Docker push镜像上传123456789[root@docker02 ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE172.16.1.31:5000/zhang/nginx 1.17 ed21b7a8aee9 6 weeks ago 127MB[root@docker02 ~]# docker push 172.16.1.31:5000/zhang/nginx:1.17 # 上传镜像The push refers to repository [172.16.1.31:5000/zhang/nginx]d37eecb5b769: Pushed 99134ec7f247: Pushed c3a984abe8a8: Pushed 1.17: digest: sha256:7ac7819e1523911399b798309025935a9968b277d86d50e5255465d6592c0266 size: 948 说明：注意镜像名格式 Harbor页面信息 Docker pull镜像拉取1234567891011[root@docker01 ~]# docker images | grep &apos;zhang/nginx&apos;[root@docker01 ~]# docker pull 172.16.1.31:5000/zhang/nginx:1.17 # 镜像拉取1.17: Pulling from zhang/nginxc499e6d256d6: Pull complete 74cda408e262: Pull complete ffadbd415ab7: Pull complete Digest: sha256:7ac7819e1523911399b798309025935a9968b277d86d50e5255465d6592c0266Status: Downloaded newer image for 172.16.1.31:5000/zhang/nginx:1.17172.16.1.31:5000/zhang/nginx:1.17[root@docker01 ~]# docker images | grep &apos;zhang/nginx&apos;172.16.1.31:5000/zhang/nginx 1.17 ed21b7a8aee9 6 weeks ago 127MB Harbor页面信息 完毕！]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>docker</tag>
        <tag>Harbor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Linux CentOS 7搭建OpenVPN服务与管理]]></title>
    <url>%2F2020%2F05%2F09%2Fopenvpn01%2F</url>
    <content type="text"><![CDATA[在CentOS 7环境下搭建OpenVPN服务，Windows客户端、Linux客户端通过OpenVPN服务访问后端机器。 主机规划与架构 服务器名称(hostname) 操作系统版本 内网IP 外网IP(模拟) 角色 web01 CentOS7.7 172.16.10.191 无 被访问机器 web02 CentOS7.7 172.16.10.192 无 被访问机器 openvpn-server CentOS7.7 172.16.10.190 10.0.0.190 Openvpn-Server openvpn-client CentOS7.7 无 10.0.0.180 Openvpn-Client 本地笔记本电脑 Windows10 无 10.0.0.X Openvpn-Client OpenVPN软件版本123Linux 安装：openvpn-2.4.9.tar.gz # GitHub地址：https://github.com/OpenVPN/openvpnLinux 安装：easy-rsa-3.0.7.tar.gz # GitHub地址：https://github.com/OpenVPN/easy-rsawidows安装：openvpn-install-2.4.9-I601-Win10.exe # OpenVPN官网 如果widows安装软件在官方访问失败，那么可以从如下地址下载： 1https://www.techspot.com/downloads/5182-openvpn.html 架构图 OpenVPN机器配置必要修改开启转发功能并生效 123## 不存在该配置则添加# grep &apos;net.ipv4.ip_forward = 1&apos; /etc/sysctl.conf || echo &apos;net.ipv4.ip_forward = 1&apos; &gt;&gt; /etc/sysctl.conf# sysctl -p 原因：从客户端访问web01或web02机器需要通过VPN机器中转。 iptables配置 只需添加配置，不需要启动iptables服务 12345678910111213141516## 添加如下配置# iptables -t nat -A POSTROUTING -s 10.8.0.0/24 -o eth0 -j MASQUERADE# iptables-save &gt; /etc/sysconfig/iptables# iptables -L -n -t natChain PREROUTING (policy ACCEPT)target prot opt source destination Chain INPUT (policy ACCEPT)target prot opt source destination Chain OUTPUT (policy ACCEPT)target prot opt source destination Chain POSTROUTING (policy ACCEPT)target prot opt source destination MASQUERADE all -- 10.8.0.0/24 0.0.0.0/0 原因：客户端连接VPN后，默认分配的10.8.0.0/24网段地址，不能直接访问web01或web02机器【这两台是172.16.10.0/24网段】，因此需要在iptables进行nat配置。 删除上面的iptables配置信息命令如下。作用：对比正常的访问和异常的访问 1# iptables -t nat -D POSTROUTING 1 系统时间与硬件时间同步12345678[root@openvpn-server ~]# crontab -l # 定时任务同步系统时间# time sync by zhang at 2020-03-09*/10 * * * * /usr/sbin/ntpdate ntp1.aliyun.com &gt;/dev/null 2&gt;&amp;1[root@openvpn-server ~]# ll /etc/localtime # 使用上海时间lrwxrwxrwx. 1 root root 33 Mar 9 03:59 /etc/localtime -&gt; /usr/share/zoneinfo/Asia/Shanghai[root@openvpn-server ~]# hwclock --show # 查看硬件时间Sun 03 May 2020 03:34:37 PM CST -0.614806 seconds[root@openvpn-server ~]# hwclock --systohc # 系统时间同步到硬件时间 说明：如果时间不同步，那么VPN登录访问就可能存在问题。 Vmware虚拟机网络设置【可略】web01机器 网卡eth0内网设置 12345678910$ cat /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=eth0TYPE=EthernetONBOOT=yesNM_CONTROLLED=yesBOOTPROTO=noneIPV6INIT=yesUSERCTL=noIPADDR=172.16.10.191NETMASK=255.255.255.0 web02机器 网卡eth0内网设置 12345678910[zhang@zhang ~]$ cat /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=eth0TYPE=EthernetONBOOT=yesNM_CONTROLLED=yesBOOTPROTO=noneIPV6INIT=yesUSERCTL=noIPADDR=172.16.10.192NETMASK=255.255.255.0 openvpn-server机器 网卡eth0内网设置 12345678910$ cat /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=eth0TYPE=EthernetONBOOT=yesNM_CONTROLLED=yesBOOTPROTO=noneIPV6INIT=yesUSERCTL=noIPADDR=172.16.10.190NETMASK=255.255.255.0 网卡eth1模拟外网设置 12345678910111213$ cat /etc/sysconfig/network-scripts/ifcfg-eth1 DEVICE=eth1TYPE=EthernetONBOOT=yesNM_CONTROLLED=yesBOOTPROTO=noneIPV6INIT=yesUSERCTL=noIPADDR=10.0.0.190NETMASK=255.255.255.0GATEWAY=10.0.0.2DNS1=223.5.5.5DNS2=223.6.6.6 openvpn-client机器 网卡eth0模拟外网设置 12345678910111213$ cat /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=eth0TYPE=EthernetONBOOT=yesNM_CONTROLLED=yesBOOTPROTO=noneIPV6INIT=yesUSERCTL=noIPADDR=10.0.0.180NETMASK=255.255.255.0GATEWAY=10.0.0.2DNS1=223.5.5.5DNS2=223.6.6.6 安装openvpn根据主机规划，在openvpn-server【172.16.10.190、10.0.0.190】部署openvpn。 安装依赖包 1# yum install -y lz4-devel lzo-devel pam-devel openssl-devel systemd-devel sqlite-devel [备注如果是阿里云机器，可能还需要装如下包：] 123yum install -y autoconfyum install -y automakeyum install -y libtool libtool-ltdl 从github上下载openvpn源代码包并解压后编译安装，最后建立软连接 12345678# wget https://github.com/OpenVPN/openvpn/archive/v2.4.9.tar.gz# mv v2.4.9.tar.gz openvpn-2.4.9.tar.gz# tar xf openvpn-2.4.9.tar.gz # cd openvpn-2.4.9/# autoreconf -i -v -f# ./configure --prefix=/usr/local/openvpn --enable-lzo --enable-lz4 --enable-crypto --enable-server --enable-plugins --enable-port-share --enable-iproute2 --enable-pf --enable-plugin-auth-pam --enable-pam-dlopen --enable-systemd# make &amp;&amp; make install# ln -s /usr/local/openvpn/sbin/openvpn /usr/local/sbin/openvpn 配置文件修改 123# vim /usr/local/openvpn/lib/systemd/system/openvpn-server@.service### 找到 ExecStart 这行，改为如下ExecStart=/usr/local/openvpn/sbin/openvpn --config server.conf 配置系统服务，并开机自启动 12# cp -a /usr/local/openvpn/lib/systemd/system/openvpn-server@.service /usr/lib/systemd/system/openvpn.service# systemctl enable openvpn.service 生成证书easy-rsa下载与配置修改下载easy-rsa并解压 123# wget https://github.com/OpenVPN/easy-rsa/archive/v3.0.7.tar.gz # mv v3.0.7.tar.gz easy-rsa-3.0.7.tar.gz# tar xf easy-rsa-3.0.7.tar.gz 根据easy-rsa-3.0.7/easyrsa3/vars.example文件生成全局配置文件vars 12# cd easy-rsa-3.0.7/easyrsa3# cp -a vars.example vars 修改vars文件，根据需要去掉注释，并修改对应值；或者直接在文件末尾追加如下信息： 12345678910111213141516171819202122# 国家set_var EASYRSA_REQ_COUNTRY &quot;CN&quot;# 省set_var EASYRSA_REQ_PROVINCE &quot;BJ&quot;# 城市set_var EASYRSA_REQ_CITY &quot;BeiJing&quot;# 组织set_var EASYRSA_REQ_ORG &quot;zhang&quot;# 邮箱set_var EASYRSA_REQ_EMAIL &quot;zhang@test.com&quot;# 拥有者set_var EASYRSA_REQ_OU &quot;ZJ&quot;# 长度set_var EASYRSA_KEY_SIZE 2048# 算法set_var EASYRSA_ALGO rsa# CA证书过期时间，单位天set_var EASYRSA_CA_EXPIRE 36500# 签发证书的有效期是多少天，单位天set_var EASYRSA_CERT_EXPIRE 36500 生成服务端和客户端证书初始化与创建CA根证书 1# ./easyrsa init-pki 初始化，会在当前目录创建PKI目录，用于存储一些中间变量及最终生成的证书 1# ./easyrsa build-ca 在这部分需要输入PEM密码 PEM pass phrase，输入两次，此密码必须记住，不然以后不能为证书签名。还需要输入common name 通用名，如：openvpen，这个你自己随便设置个独一无二的。 生成服务端证书 1# ./easyrsa build-server-full server nopass 为服务端生成证书对并在本地签名。nopass参数生成一个无密码的证书；在此过程中会让你确认ca密码 1# ./easyrsa gen-dh 创建Diffie-Hellman，确保key穿越不安全网络的命令，时间会有点长，耐心等待 生成客户端证书 生成多个客户端证书 1234# ./easyrsa build-client-full client nopass # 无密码，实际应用中不推荐，客户端有密码可提高安全性# ./easyrsa build-client-full zhangsan # 让你输入密码，后续VPN连接时会使用# ./easyrsa build-client-full lisi # 让你输入密码，后续VPN连接时会使用# ./easyrsa build-client-full wangwu # 让你输入密码，后续VPN连接时会使用 为客户端生成证书对并在本地签名。nopass参数生成一个无密码的证书；在此过程中都会让你确认ca密码 为了提高安全性，生成ta.key 1# openvpn --genkey --secret ta.key 加强认证方式，防攻击。如果配置文件中启用此项(默认是启用的)，就需要执行上述命令，并把ta.key放到/etc/openvpn/server目录。配置文件中服务端第二个参数为0，同时客户端也要有此文件，且client.conf中此指令的第二个参数需要为1。【服务端有该配置，那么客户端也必须要有】 整理服务端证书 123456mkdir -p /etc/openvpn/server/cp -a pki/ca.crt /etc/openvpn/server/cp -a pki/private/server.key /etc/openvpn/server/cp -a pki/issued/server.crt /etc/openvpn/server/cp -a pki/dh.pem /etc/openvpn/server/cp -a ta.key /etc/openvpn/server/ 创建服务端配置文件参照openvpn-2.4.9/sample/sample-config-files/server.conf文件 服务端配置文件1234567891011121314151617181920212223242526272829# cat /etc/openvpn/server/server.conf # 配置文件内容local 0.0.0.0port 1194proto tcpdev tunca /etc/openvpn/server/ca.crtcert /etc/openvpn/server/server.crtkey /etc/openvpn/server/server.keydh /etc/openvpn/server/dh.pemserver 10.8.0.0 255.255.255.0ifconfig-pool-persist ipp.txtpush &quot;route 172.16.10.0 255.255.255.0&quot;;client-to-client;duplicate-cnkeepalive 10 120tls-auth /etc/openvpn/server/ta.key 0cipher AES-256-CBCcompress lz4-v2push &quot;compress lz4-v2&quot;;comp-lzomax-clients 1000user nobodygroup nobodypersist-keypersist-tunstatus openvpn-status.loglog /var/log/openvpn.logverb 3;explicit-exit-notify 1 配置文件参数说明参考：openvpn-2.4.9/sample/sample-config-files/server.conf 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586local 0.0.0.0表示openvpn服务端的监听地址port 1194监听的端口，默认是1194proto tcp使用的协议，有udp和tcp。建议选择tcpdev tun使用三层路由IP隧道(tun)还是二层以太网隧道(tap)。一般都使用tunca ca.crtcert server.crtkey server.keydh dh2048.pemca证书、服务端证书、服务端密钥和密钥交换文件。如果它们和server.conf在同一个目录下则可以不写绝对路径，否则需要写绝对路径调用server 10.8.0.0 255.255.255.0vpn服务端为自己和客户端分配IP的地址池。服务端自己获取网段的第一个地址(此处为10.8.0.1)，后为客户端分配其他的可用地址。以后客户端就可以和10.8.0.1进行通信。注意：该网段地址池不要和已有网段冲突或重复。其实一般来说是不用改的。除非当前内网使用了10.8.0.0/24的网段。ifconfig-pool-persist ipp.txt使用一个文件记录已分配虚拟IP的客户端和虚拟IP的对应关系，以后openvpn重启时，将可以按照此文件继续为对应的客户端分配此前相同的IP。也就是自动续借IP的意思。server-bridge XXXXXX使用tap模式的时候考虑此选项。push &quot;route 10.0.10.0 255.255.255.0&quot;push &quot;route 192.168.10.0 255.255.255.0&quot;vpn服务端向客户端推送vpn服务端内网网段的路由配置，以便让客户端能够找到服务端内网。多条路由就写多个Push指令client-to-client让vpn客户端之间可以互相看见对方，即能互相通信。默认情况客户端只能看到服务端一个人；默认是注释的，不能客户端之间相互看见duplicate-cn允许多个客户端使用同一个VPN帐号连接服务端默认是注释的，不支持多个客户登录一个账号keepalive 10 120每10秒ping一次，120秒后没收到ping就说明对方挂了tls-auth ta.key 0加强认证方式，防攻击。如果配置文件中启用此项(默认是启用的)需要执行openvpn --genkey --secret ta.key，并把ta.key放到/etc/openvpn/server目录服务端第二个参数为0；同时客户端也要有此文件，且client.conf中此指令的第二个参数需要为1。cipher AES-256-CBC# 选择一个密码。如果在服务器上使用了cipher选项，那么您也必须在这里指定它。注意，v2.4客户端/服务器将在TLS模式下自动协商AES-256-GCM。compress lz4-v2push &quot;compress lz4-v2&quot;openvpn 2.4版本的vpn才能设置此选项。表示服务端启用lz4的压缩功能，传输数据给客户端时会压缩数据包。Push后在客户端也配置启用lz4的压缩功能，向服务端发数据时也会压缩。如果是2.4版本以下的老版本，则使用用comp-lzo指令comp-lzo启用lzo数据压缩格式。此指令用于低于2.4版本的老版本。且如果服务端配置了该指令，客户端也必须要配置max-clients 100并发客户端的连接数persist-keypersist-tun通过ping得知超时时，当重启vpn后将使用同一个密钥文件以及保持tun连接状态status openvpn-status.log在文件中输出当前的连接信息，每分钟截断并重写一次该文件;log openvpn.log;log-append openvpn.log默认vpn的日志会记录到rsyslog中，使用这两个选项可以改变。log指令表示每次启动vpn时覆盖式记录到指定日志文件中，log-append则表示每次启动vpn时追加式的记录到指定日志中。但两者只能选其一，或者不选时记录到rsyslog中verb 3日志记录的详细级别。;mute 20沉默的重复信息。最多20条相同消息类别的连续消息将输出到日志。explicit-exit-notify 1当服务器重新启动时，通知客户端，以便它可以自动重新连接。仅在UDP协议是可用 启动openvpn服务并查看进程与端口12345# systemctl start openvpn.service# ps -ef | grep &apos;open&apos;nobody 19095 1 0 01:19 ? 00:00:00 /usr/local/openvpn/sbin/openvpn --config server.conf# netstat -lntup | grep &apos;19095&apos;tcp 0 0 0.0.0.0:1194 0.0.0.0:* LISTEN 19095/openvpn 通过ifconfig命令，也可见多个tun0网卡信息 Windows客户端配置与访问客户端安装 安装完毕后会在「网络连接」中会多出一个连接 客户端client用户配置文件备注：文件名 windows为client.ovpn，Linux为client.conf 需要的证书与配置文件如下图： 123说明：1、注意路径，在OpenVPN/config目录下建立了client目录2、ca.crt、client.crt、client.key、ta.key都是之前创建好的，只有client.ovpn需要单独下载并修改。 client.ovpn内容如下： 参照openvpn-2.4.9/sample/sample-config-files/client.conf文件 123456789101112131415161718192021;# 文件名 windows为client.ovpn，Linux为client.confclientdev tunproto tcpremote 10.0.0.190 1194resolv-retry infinitenobind;user nobody;group nobodypersist-keypersist-tunca ca.crtcert client.crtkey client.keyremote-cert-tls servertls-auth ta.key 1cipher AES-256-CBCcompress lz4-v2verb 3;mute 20 客户端zhangsan用户配置文件备注：文件名 windows为zhangsan.ovpn，Linux为zhangsan.conf 需要的证书与配置文件如下图： 123说明：1、注意路径，在OpenVPN/config目录下建立了zhangsan目录2、ca.crt、zhangsan.crt、zhangsan.key、ta.key都是之前创建好的，只有zhangsan.ovpn需要单独下载并修改。 zhangsan.ovpn内容如下： 参照openvpn-2.4.9/sample/sample-config-files/client.conf文件 123456789101112131415161718192021;# 文件名 windows为client.ovpn，Linux为client.confclientdev tunproto tcpremote 10.0.0.190 1194resolv-retry infinitenobind;user nobody;group nobodypersist-keypersist-tunca ca.crtcert zhangsan.crtkey zhangsan.keyremote-cert-tls servertls-auth ta.key 1cipher AES-256-CBCcompress lz4-v2verb 3;mute 20 其他用户如：lisi，wangwu参考上述进行配置即可。 配置文件参数说明参考：openvpn-2.4.9/sample/sample-config-files/client.conf 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# 文件名 windows为client.ovpn，Linux为client.confclient# 标识这是个客户端dev tun# 使用三层路由IP隧道(tun)还是二层以太网隧道(tap)。服务端是什么客户端就是什么proto tcp# 使用的协议，有udp和tcp。服务端是什么客户端就是什么remote 10.0.0.190 1194# 服务端的地址和端口resolv-retry infinite# 一直尝试解析OpenVPN服务器的主机名。# 在机器上非常有用，不是永久连接到互联网，如笔记本电脑。nobind# 大多数客户机不需要绑定到特定的本地端口号。;user nobody;group nobody# 初始化后的降级特权(仅非windows)persist-keypersist-tun# 尝试在重新启动时保留某些状态。ca ca.crtcert client.crtkey client.key# ca证书、客户端证书、客户端密钥# 如果它们和client.conf或client.ovpn在同一个目录下则可以不写绝对路径，否则需要写绝对路径调用remote-cert-tls server# 通过检查certicate是否具有正确的密钥使用设置来验证服务器证书。tls-auth ta.key 1# 加强认证方式，防攻击。服务端有配置，则客户端必须有cipher AES-256-CBC# 选择一个密码。如果在服务器上使用了cipher选项，那么您也必须在这里指定它。注意，v2.4客户端/服务器将在TLS模式下自动协商AES-256-GCM。compress lz4-v2# 服务端用的什么，客户端就用的什么# 表示客户端启用lz4的压缩功能，传输数据给客户端时会压缩数据包。verb 3# 日志级别;mute 20# 沉默的重复信息。最多20条相同消息类别的连续消息将输出到日志。 启动客户端并连接VPN 连接client用户 之前生成客户端证书的时候，加上 nopass 参数，因此直接连接无需输入密码。如果连接的是用户zhangsan，则需要输入密码。 连接成功后，会有如下提示 客户端访问后端的web01和web02机器本地笔记本电脑操作 VPN机器抓包 可见能够正常访问 备注：可以将VPN机器的iptables配置删除；然后断开客户端连接，再重新连接，之后客户端ping 访问试试。【用于对比】 Linux客户端配置与访问安装openvpn安装参见上文，上面说过了Linux安装OpenVPN，这里不单独说了。我们这里使用之前创建的wangwu客户端用户进行验证。 配置文件修改 12345678[root@openvpn-client ~]# vim /usr/local/openvpn/lib/systemd/system/openvpn-server@.service[Service]Type=notifyPrivateTmp=true#WorkingDirectory=/etc/openvpn/serverWorkingDirectory=/etc/openvpn/wangwu#ExecStart=/usr/local/openvpn/sbin/openvpn --status %t/openvpn-server/status-%i.log --status-version 2 --suppress-timestamps --config %i.confExecStart=/usr/local/openvpn/sbin/openvpn --config wangwu.conf 配置系统服务，并开机自启动【请根据需要加入开机自启动】 12# cp -a /usr/local/openvpn/lib/systemd/system/openvpn-server@.service /usr/lib/systemd/system/openvpn.service# systemctl enable openvpn.service 客户端wangwu客户配置备注：文件名 windows为wangwu.ovpn，Linux为wangwu.conf 需要的证书与配置文件如下： 123说明：1、注意路径，在/etc/openvpn/目录下建立了wangwu目录2、ca.crt、wangwu.crt、wangwu.key、ta.key都是之前创建好的，只有wangwu.ovpn需要单独下载并修改。 123456789[root@openvpn-client wangwu]# pwd/etc/openvpn/wangwu[root@openvpn-client wangwu]# lltotal 24-rw-r--r-- 1 root root 1164 May 2 23:08 ca.crt-rw-r--r-- 1 root root 636 May 2 23:46 ta.key-rw-r--r-- 1 root root 318 May 3 21:54 wangwu.conf-rw-r--r-- 1 root root 4422 May 2 23:14 wangwu.crt-rw-r--r-- 1 root root 1834 May 2 23:14 wangwu.key wangwu.conf内容如下： 参照openvpn-2.4.9/sample/sample-config-files/client.conf文件 12345678910111213141516171819202122[root@openvpn-client wangwu]# cat wangwu.conf ;# 文件名 windows为client.ovpn，Linux为client.confclientdev tunproto tcpremote 10.0.0.190 1194resolv-retry infinitenobinduser nobodygroup nobodypersist-keypersist-tunca ca.crtcert wangwu.crtkey wangwu.keyremote-cert-tls servertls-auth ta.key 1cipher AES-256-CBCcompress lz4-v2verb 3;mute 20 其他用户如：参考上述进行配置即可。 启动客户端并连接VPN1234[root@openvpn-client wangwu]# systemctl start openvpn.service Enter Private Key Password: ****** # 输入该用户创建时设定的密码[root@openvpn-client wangwu]# ps -ef | grep &apos;open&apos;nobody 11266 1 0 21:56 ? 00:00:00 /usr/local/openvpn/sbin/openvpn --config wangwu.conf 网卡信息如下图： 客户端访问后端的web01和web02机器客户端Linux操作 VPN机器抓包 可见能够正常访问 备注：可以将VPN机器的iptables配置删除；然后断开客户端连接，再重新连接，之后客户端ping 访问试试。【用于对比】 推荐阅读应用openvpn 1https://www.bbsmax.com/A/x9J23DRZ56/ OpenVPN服务搭建与管理 1https://my.oschina.net/u/3021599/blog/3048615 CentOS安装PPTP VPN 1http://www.ttlsa.com/linux/centos-install-pptp-vpn/ 完毕！]]></content>
      <categories>
        <category>OpenVPN</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>OpenVPN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何查看docker run启动参数命令]]></title>
    <url>%2F2020%2F04%2F29%2Fdocker-01%2F</url>
    <content type="text"><![CDATA[通过runlike去查看一个容器的docker run启动参数 安装pip1yum install -y python-pip 安装runlike1pip install runlike 查看docker run参数发布一个容器123456[root@docker01 ~]# docker run -d -v /data/nginx_test2:/data_volume_test2 -v /etc/hosts:/etc/hosts -p 8080:80 --name nginx105 nginx:1.17 # 发布容器[root@docker01 ~]# netstat -lntup | grep &apos;8080&apos; # 映射到本地的端口tcp6 0 0 :::8080 :::* LISTEN 5153/docker-proxy[root@docker01 ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES74d35e8f150c nginx:1.17 &quot;nginx -g &apos;daemon of…&quot; 57 seconds ago Up 56 seconds 0.0.0.0:8080-&gt;80/tcp nginx105 查看启动参数1234567891011121314151617# 格式：runlike -p &lt;容器名&gt;|&lt;容器ID&gt;[root@docker01 ~]# runlike -p nginx105 docker run \ --name=nginx105 \ --hostname=74d35e8f150c \ --env=PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \ --env=NGINX_VERSION=1.17.9 \ --env=NJS_VERSION=0.3.9 \ --env=&apos;PKG_RELEASE=1~buster&apos; \ --volume=/data/nginx_test2:/data_volume_test2 \ --volume=/etc/hosts:/etc/hosts \ -p 8080:80 \ --restart=no \ --label maintainer=&quot;NGINX Docker Maintainers &lt;docker-maint@nginx.com&gt;&quot; \ --detach=true \ nginx:1.17 \ nginx -g &apos;daemon off;&apos; 完毕！]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dcoker如何搭建私有registry镜像仓库]]></title>
    <url>%2F2020%2F04%2F29%2Fdocker05%2F</url>
    <content type="text"><![CDATA[Docker如何搭建私有镜像仓库，与如何访问私有镜像仓库，删除私有镜像仓库镜像 机器规划 服务器名称(hostname) 操作系统版本 内网IP 外网IP(模拟) 安装软件 docker01 CentOS7.7 172.16.1.31 10.0.0.31 docker docker02 CentOS7.7 172.16.1.32 10.0.0.32 docker 说明：在docker01机器有registry镜像和docker-registry-web镜像，用搭建私有镜像仓库和web页面访问。 访问仓库仓库（Repository）是集中存放镜像的地方。 一个容易混淆的概念是注册服务器（Registry）。实际上注册服务器是管理仓库的具体服务器，每个服务器上可以有多个仓库，而每个仓库下面有多个镜像。从这方面来说，仓库可以被认为是一个具体的项目或目录。例如对于仓库地址 docker.io/ubuntu 来说，docker.io 是注册服务器地址，ubuntu 是仓库名。 大部分时候，并不需要严格区分这两者的概念。 私有仓库搭建有时候使用 Docker Hub 这样的公共仓库可能不方便，用户可以创建一个本地仓库供私人使用。 本文介绍如何使用本地仓库。 docker-registry 是官方提供的工具，可以用于构建私有的镜像仓库。 获取镜像说明：registry 镜像选择 registry:2 和 registry:2.4.1 都可以。 拉取私有镜像仓库 123docker pull registry:2.4.1 # 当然 docker pull registry:2 也可以# 或者如下获取，然后通过docker tag 重命名docker pull registry.cn-beijing.aliyuncs.com/google_registry/registry:2.4.1 用于web页面查看仓库中的镜像 123docker pull hyper/docker-registry-web# 或者如下获取，然后通过docker tag 重命名docker pull registry.cn-beijing.aliyuncs.com/google_registry/docker-registry-web:latest 要得到的镜像信息 1234[root@docker01 ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZEhyper/docker-registry-web latest 0db5683824d8 3 years ago 599MBregistry 2.4.1 8ff6a4aae657 3 years ago 172MB 容器运行将上传的镜像放到本地的 /opt/data/registry 目录 1mkdir -p /opt/data/registry 启动私有仓库 1docker run -d -p 5000:5000 -v /opt/data/registry:/var/lib/registry --name registry registry:2.4.1 或则如下启动，目的：可以删除仓库中的镜像 1docker run -d -p 5000:5000 -v /opt/data/registry:/var/lib/registry -v /data/config.yml:/etc/docker/registry/config.yml --name registry registry:2.4.1 /data/config.yml 这个是什么呢？我们在下面删除仓库镜像介绍 这里需要说明一点，在启动仓库时，需在配置文件中的storage配置中增加delete=true配置项，允许删除镜像。默认的镜像是没有这个参数 123456789101112131415161718192021$ cat config.ymlversion: 0.1log: fields: service: registrystorage: delete: enabled: true cache: blobdescriptor: inmemory filesystem: rootdirectory: /var/lib/registryhttp: addr: :5000 headers: X-Content-Type-Options: [nosniff]health: storagedriver: enabled: true interval: 10s threshold: 3 启动web页面查看仓库镜像容器 123456docker run -d -p 8080:8080 --name registry-web --link registry \ -e REGISTRY_URL=http://172.16.1.31:5000/v2 \ -e REGISTRY_TRUST_ANY_SSL=true \ -e REGISTRY_BASIC_AUTH=&quot;YWRtaW46YWRtaW4=&quot; \ -e REGISTRY_NAME=10.0.0.31:5000 \ hyper/docker-registry-web 查看docker运行容器 1234[root@docker01 ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES2575cca9eace hyper/docker-registry-web &quot;start.sh&quot; 2 hours ago Up 2 hours 0.0.0.0:8080-&gt;8080/tcp registry-web9e4530dd82df registry:2.4.1 &quot;/entrypoint.sh /etc…&quot; 3 hours ago Up 2 hours 0.0.0.0:5000-&gt;5000/tcp registry 实现http访问私有仓库Docker 默认不允许非 HTTPS 方式推送镜像。我们可以通过 Docker 的配置选项来取消这个限制。 如果直接【上传】或【拉取】镜像会失败，因为默认为https方式，得到提示信息如下： 12345[root@docker02 ~]# docker push 172.16.1.31:5000/zhang/nginx:1.17 # 推动镜像失败The push refers to repository [172.16.1.31:5000/zhang/nginx]Get https://172.16.1.31:5000/v2/: http: server gave HTTP response to HTTPS client[root@docker02 ~]# docker pull 172.16.1.31:5000/zhang/flannel:v0.12.0-amd64 # 拉取镜像失败Error response from daemon: Get https://172.16.1.31:5000/v2/: http: server gave HTTP response to HTTPS client 实现http上传方式一添加如下配置，如果文件不存在则添加，在docker01和docker02机器都要添加，因为这两台机器都可能向仓库推送或拉取镜像。 1234[root@docker02 ~]# vim /etc/docker/daemon.json&#123; &quot;insecure-registries&quot;: [&quot;172.16.1.31:5000&quot;]&#125; 说明：该文件必须符合 json 规范，否则 Docker 将不能启动。 重启docker服务并向镜像中心推送镜像 1234567891011[root@docker02 ~]# systemctl restart docker.service [root@docker02 ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE172.16.1.31:5000/zhang/nginx 1.17 ed21b7a8aee9 2 weeks ago 127MB# 推送镜像到镜像仓库【注意镜像名称】[root@docker02 ~]# docker push 172.16.1.31:5000/zhang/nginx:1.17The push refers to repository [172.16.1.31:5000/zhang/nginx]d37eecb5b769: Layer already exists 99134ec7f247: Layer already exists c3a984abe8a8: Layer already exists 1.17: digest: sha256:7ac7819e1523911399b798309025935a9968b277d86d50e5255465d6592c0266 size: 948 可见推送成功。 实现http上传方式二在启动docker server时增加启动参数为默认使用http访问。在docker01和docker02机器都要添加，因为这两台机器都可能向仓库推送或拉取镜像。修改docker启动配置文件： 1[root@docker02 ~]# vim /usr/lib/systemd/system/docker.service 找到ExecStart，行尾追加信息，结果如下： 1ExecStart=/usr/bin/dockerd --insecure-registry 172.16.1.31:5000 重启docker服务并向镜像中心推送镜像 12345678910[root@docker02 ~]# systemctl daemon-reload[root@docker02 ~]# systemctl restart docker[root@docker02 ~]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE172.16.1.31:5000/zhang/centos 7.7.1908 08d05d1d5859 5 months ago 204MB# 推送镜像到镜像仓库【注意镜像名称】[root@docker02 ~]# docker push 172.16.1.31:5000/zhang/centos:7.7.1908The push refers to repository [172.16.1.31:5000/zhang/centos]034f282942cd: Pushed 7.7.1908: digest: sha256:8f2c78ca3141051eef77fb083066222abf20330a2345c970a5a61427aeb2dc7b size: 529 可见推送成功。 registry-web浏览器访问1http://10.0.0.31:8080/ Registry删除镜像、垃圾回收上传镜像123456[root@docker02 ~]# docker push 172.16.1.31:5000/zhang/nginx:1.17The push refers to repository [172.16.1.31:5000/zhang/nginx]d37eecb5b769: Layer already exists 99134ec7f247: Layer already exists c3a984abe8a8: Layer already exists 1.17: digest: sha256:7ac7819e1523911399b798309025935a9968b277d86d50e5255465d6592c0266 size: 948 说明：通过上面的方式可以获取镜像的sha256 信息。 查看数据大小，进入仓库容器中，通过du命令查看大小 123[root@docker01 ~]# docker exec -it registry sh # 进入容器# du -sh /var/lib/registry/ # 已经在容器内部了138M /var/lib/registry/ 删除镜像删除镜像对应的API如下： 1DELETE /v2/&lt;name&gt;/manifests/&lt;reference&gt; name:镜像名称 reference: 镜像对应sha256值 发送请求，删除刚才上传的镜像【注意格式】 1234567[root@docker02 ~]# curl -I -X DELETE http://172.16.1.31:5000/v2/zhang/nginx/manifests/sha256:7ac7819e1523911399b798309025935a9968b277d86d50e5255465d6592c0266HTTP/1.1 202 AcceptedDocker-Distribution-Api-Version: registry/2.0X-Content-Type-Options: nosniffDate: Tue, 21 Apr 2020 05:11:14 GMTContent-Length: 0Content-Type: text/plain; charset=utf-8 查看镜像1http://10.0.0.31:8080/ 可以看到镜像索引已经被删除 查看数据大小123[root@docker01 ~]# docker exec -it registry sh# du -sh /var/lib/registry/138M /var/lib/registry/ 可以看到数据大小没有变化（只删除了元数据） 垃圾回收进行容器执行垃圾回收命令 123456# registry garbage-collect /etc/docker/registry/config.yml INFO[0000] Deleting blob: /docker/registry/v2/blobs/sha256/74/74cda408e262b296a56beb25c60ce2cf938a3a2fa6a1a1ddc862e67ac1135c9f go.version=go1.6.2 instance.id=a4ea7e2a-71a1-4607-b313-274d0f725589INFO[0000] Deleting blob: /docker/registry/v2/blobs/sha256/7a/7ac7819e1523911399b798309025935a9968b277d86d50e5255465d6592c0266 go.version=go1.6.2 instance.id=a4ea7e2a-71a1-4607-b313-274d0f725589INFO[0000] Deleting blob: /docker/registry/v2/blobs/sha256/c4/c499e6d256d6d4a546f1c141e04b5b4951983ba7581e39deaf5cc595289ee70f go.version=go1.6.2 instance.id=a4ea7e2a-71a1-4607-b313-274d0f725589INFO[0000] Deleting blob: /docker/registry/v2/blobs/sha256/ed/ed21b7a8aee9cc677df6d7f38a641fa0e3c05f65592c592c9f28c42b3dd89291 go.version=go1.6.2 instance.id=a4ea7e2a-71a1-4607-b313-274d0f725589INFO[0000] Deleting blob: /docker/registry/v2/blobs/sha256/ff/ffadbd415ab7081b4d3fac4adf708f1bc2ed5d0b65d85c947342a5fa46257486 go.version=go1.6.2 instance.id=a4ea7e2a-71a1-4607-b313-274d0f725589 查看数据大小 12# du -sh /var/lib/registry/89M /var/lib/registry/ 可以看到镜像数据已被删除。 完毕！]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker Dockerfile 指令详解与实战案例]]></title>
    <url>%2F2020%2F04%2F28%2Fdocker04%2F</url>
    <content type="text"><![CDATA[Dockerfile介绍及常用指令，包括FROM，RUN，还提及了 COPY，ADD，EXPOSE，WORKDIR等，其实 Dockerfile 功能很强大，它提供了十多个指令。 Dockerfile介绍Dockerfile 是一个用来构建镜像的文本文件，文本内容包含了一条条构建镜像所需的指令和说明。 在Docker中创建镜像最常用的方式，就是使用Dockerfile。Dockerfile是一个Docker镜像的描述文件，我们可以理解成火箭发射的A、B、C、D…的步骤。Dockerfile其内部包含了一条条的指令，每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建。 FROM 指令-指定基础镜像所谓定制镜像，那一定是以一个镜像为基础，在其上进行定制。而 FROM 就是指定基础镜像，因此一个 Dockerfile 中 FROM 是必备的指令，并且必须是第一条指令。如下： 1FROM centos MAINTAINER 维护者信息该镜像是由谁维护的 1MAINTAINER lightzhang lightzhang@xxx.com ENV 设置环境变量格式有两种： 12ENV &lt;key&gt; &lt;value&gt;ENV &lt;key1&gt;=&lt;value1&gt; &lt;key2&gt;=&lt;value2&gt;... 这个指令很简单，就是设置环境变量而已，无论是后面的其它指令，如 RUN，还是运行时的应用，都可以直接使用这里定义的环境变量。 12ENV VERSION=1.0 DEBUG=on \ NAME=&quot;Happy Feet&quot; 这个例子中演示了如何换行，以及对含有空格的值用双引号括起来的办法，这和 Shell 下的行为是一致的。 下列指令可以支持环境变量展开： 1ADD、COPY、ENV、EXPOSE、FROM、LABEL、USER、WORKDIR、VOLUME、STOPSIGNAL、ONBUILD、RUN。 可以从这个指令列表里感觉到，环境变量可以使用的地方很多，很强大。通过环境变量，我们可以让一份 Dockerfile 制作更多的镜像，只需使用不同的环境变量即可。 ARG 构建参数1格式：ARG &lt;参数名&gt;[=&lt;默认值&gt;] 构建参数和 ENV 的效果一样，都是设置环境变量。所不同的是，ARG 所设置的构建环境的环境变量，在将来容器运行时是不会存在这些环境变量的。但是不要因此就使用 ARG 保存密码之类的信息，因为 docker history 还是可以看到所有值的。 Dockerfile 中的 ARG 指令是定义参数名称，以及定义其默认值。该默认值可以在构建命令 docker build 中用 --build-arg &lt;参数名&gt;=&lt;值&gt; 来覆盖。 在 1.13 之前的版本，要求 --build-arg 中的参数名，必须在 Dockerfile 中用 ARG 定义过了，换句话说，就是 --build-arg 指定的参数，必须在 Dockerfile 中使用了。如果对应参数没有被使用，则会报错退出构建。 从 1.13 开始，这种严格的限制被放开，不再报错退出，而是显示警告信息，并继续构建。这对于使用 CI 系统，用同样的构建流程构建不同的 Dockerfile 的时候比较有帮助，避免构建命令必须根据每个 Dockerfile 的内容修改。 RUN 执行命令RUN 指令是用来执行命令行命令的。由于命令行的强大能力，RUN 指令在定制镜像时是最常用的指令之一。其格式有两种： shell 格式：RUN &lt;命令&gt;，就像直接在命令行中输入的命令一样。如下： 1RUN echo &apos;&lt;h1&gt;Hello, Docker!&lt;/h1&gt;&apos; &gt; /usr/share/nginx/html/index.html exec 格式：RUN [“可执行文件”, “参数1”, “参数2”]，这更像是函数调用中的格式。 Dockerfile 中每一个指令都会建立一层，RUN 也不例外。每一个 RUN 的行为，都会新建立一层，在其上执行这些命令，执行结束后，commit 这一层的修改，构成新的镜像。 Dockerfile 不推荐写法： 12345678910FROM debian:stretchRUN apt-get updateRUN apt-get install -y gcc libc6-dev make wgetRUN wget -O redis.tar.gz &quot;http://download.redis.io/releases/redis-5.0.3.tar.gz&quot;RUN mkdir -p /usr/src/redisRUN tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1RUN make -C /usr/src/redisRUN make -C /usr/src/redis installRUN rm redis.tar.gz 上面的这种写法，创建了 8 层镜像。这是完全没有意义的，而且很多运行时不需要的东西，都被装进了镜像里，比如编译环境、更新的软件包等等。最后一行即使删除了软件包，那也只是当前层的删除；虽然我们看不见这个包了，但软件包却早已存在于镜像中并一直跟随着镜像，没有真正的删除。 结果就是产生非常臃肿、非常多层的镜像，不仅仅增加了构建部署的时间，也很容易出错。 这是很多初学 Docker 的人常犯的一个错误。 另外：Union FS 是有最大层数限制的，比如 AUFS，曾经是最大不得超过 42 层，现在是不得超过 127 层。 Dockerfile 正确写法： 1234567891011121314FROM debian:stretchRUN buildDeps=&apos;gcc libc6-dev make wget&apos; \ &amp;&amp; apt-get update \ &amp;&amp; apt-get install -y $buildDeps \ &amp;&amp; wget -O redis.tar.gz &quot;http://download.redis.io/releases/redis-5.0.3.tar.gz&quot; \ &amp;&amp; mkdir -p /usr/src/redis \ &amp;&amp; tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1 \ &amp;&amp; make -C /usr/src/redis \ &amp;&amp; make -C /usr/src/redis install \ &amp;&amp; rm -rf /var/lib/apt/lists/* \ &amp;&amp; rm redis.tar.gz \ &amp;&amp; rm -r /usr/src/redis \ &amp;&amp; apt-get purge -y --auto-remove $buildDeps 这里没有使用很多个 RUN 对应不同的命令，而是仅仅使用一个 RUN 指令，并使用 &amp;&amp; 将各个所需命令串联起来。将之前的 8 层，简化为了 1 层，且后面删除了不需要的包和目录。在撰写 Dockerfile 的时候，要经常提醒自己，这并不是在写 Shell 脚本，而是在定义每一层该如何构建。因此镜像构建时，一定要确保每一层只添加真正需要添加的东西，任何无关的东西都应该清理掉。 很多人初学 Docker 制作出了很臃肿的镜像的原因之一，就是忘记了每一层构建的最后一定要清理掉无关文件。 COPY 复制文件格式： 12COPY [--chown=&lt;user&gt;:&lt;group&gt;] &lt;源路径&gt;... &lt;目标路径&gt;COPY [--chown=&lt;user&gt;:&lt;group&gt;] [&quot;&lt;源路径1&gt;&quot;,... &quot;&lt;目标路径&gt;&quot;] COPY 指令将从构建上下文目录中 &lt;源路径&gt; 的文件/目录复制到新的一层的镜像内的 &lt;目标路径&gt; 位置。如： 1COPY package.json /usr/src/app/ &lt;源路径&gt; 可以是多个，甚至可以是通配符，其通配符规则要满足 Go 的 filepath.Match 规则，如： 12COPY hom* /mydir/COPY hom?.txt /mydir/ &lt;目标路径&gt; 可以是容器内的绝对路径，也可以是相对于工作目录的相对路径（工作目录可以用 WORKDIR 指令来指定）。目标路径不需要事先创建，如果目录不存在会在复制文件前先行创建缺失目录。 此外，还需要注意一点，使用 COPY 指令，源文件的各种元数据都会保留。比如读、写、执行权限、文件变更时间等。这个特性对于镜像定制很有用。特别是构建相关文件都在使用 Git 进行管理的时候。 在使用该指令的时候还可以加上 --chown=&lt;user&gt;:&lt;group&gt; 选项来改变文件的所属用户及所属组。 1234COPY --chown=55:mygroup files* /mydir/COPY --chown=bin files* /mydir/COPY --chown=1 files* /mydir/COPY --chown=10:11 files* /mydir/ ADD 更高级的复制文件ADD 指令和 COPY 的格式和性质基本一致。但是在 COPY 基础上增加了一些功能。 如果 &lt;源路径&gt; 为一个 tar 压缩文件的话，压缩格式为 gzip, bzip2 以及 xz 的情况下，ADD 指令将会自动解压缩这个压缩文件到 &lt;目标路径&gt; 去。 在某些情况下，如果我们真的是希望复制个压缩文件进去，而不解压缩，这时就不可以使用 ADD 命令了。 在 Docker 官方的 Dockerfile 最佳实践文档 中要求，尽可能的使用 COPY，因为 COPY 的语义很明确，就是复制文件而已，而 ADD 则包含了更复杂的功能，其行为也不一定很清晰。最适合使用 ADD 的场合，就是所提及的需要自动解压缩的场合。 特别说明：在 COPY 和 ADD 指令中选择的时候，可以遵循这样的原则，所有的文件复制均使用 COPY 指令，仅在需要自动解压缩的场合使用 ADD。 在使用该指令的时候还可以加上 --chown=&lt;user&gt;:&lt;group&gt; 选项来改变文件的所属用户及所属组。 1234ADD --chown=55:mygroup files* /mydir/ADD --chown=bin files* /mydir/ADD --chown=1 files* /mydir/ADD --chown=10:11 files* /mydir/ WORKDIR 指定工作目录1格式为 WORKDIR &lt;工作目录路径&gt; 使用 WORKDIR 指令可以来指定工作目录（或者称为当前目录），以后各层的当前目录就被改为指定的目录，如该目录不存在，WORKDIR 会帮你建立目录。 之前提到一些初学者常犯的错误是把 Dockerfile 等同于 Shell 脚本来书写，这种错误的理解还可能会导致出现下面这样的错误： 12RUN cd /appRUN echo &quot;hello&quot; &gt; world.txt 如果将这个 Dockerfile 进行构建镜像运行后，会发现找不到 /app/world.txt 文件，或者其内容不是 hello。原因其实很简单，在 Shell 中，连续两行是同一个进程执行环境，因此前一个命令修改的内存状态，会直接影响后一个命令；而在 Dockerfile 中，这两行 RUN 命令的执行环境根本不同，是两个完全不同的容器。这就是对 Dockerfile 构建分层存储的概念不了解所导致的错误。 之前说过每一个 RUN 都是启动一个容器、执行命令、然后提交存储层文件变更。第一层 RUN cd /app 的执行仅仅是当前进程的工作目录变更，一个内存上的变化而已，其结果不会造成任何文件变更。而到第二层的时候，启动的是一个全新的容器，跟第一层的容器更完全没关系，自然不可能继承前一层构建过程中的内存变化。 因此如果需要改变以后各层的工作目录的位置，那么应该使用 WORKDIR 指令。 USER 指定当前用户1格式：USER &lt;用户名&gt;[:&lt;用户组&gt;] USER 指令和 WORKDIR 相似，都是改变环境状态并影响以后的层。WORKDIR 是改变工作目录，USER 则是改变之后层的执行 RUN, CMD 以及 ENTRYPOINT 这类命令的身份。 当然，和 WORKDIR 一样，USER 只是帮助你切换到指定用户而已，这个用户必须是事先建立好的，否则无法切换。 123RUN groupadd -r redis &amp;&amp; useradd -r -g redis redisUSER redisRUN [ &quot;redis-server&quot; ] 如果以 root 执行的脚本，在执行期间希望改变身份，比如希望以某个已经建立好的用户来运行某个服务进程，不要使用 su 或者 sudo，这些都需要比较麻烦的配置，而且在 TTY 缺失的环境下经常出错。建议使用 gosu。 12345678# 建立 redis 用户，并使用 gosu 换另一个用户执行命令RUN groupadd -r redis &amp;&amp; useradd -r -g redis redis# 下载 gosuRUN wget -O /usr/local/bin/gosu &quot;https://github.com/tianon/gosu/releases/download/1.7/gosu-amd64&quot; \ &amp;&amp; chmod +x /usr/local/bin/gosu \ &amp;&amp; gosu nobody true# 设置 CMD，并以另外的用户执行CMD [ &quot;exec&quot;, &quot;gosu&quot;, &quot;redis&quot;, &quot;redis-server&quot; ] VOLUME 定义匿名卷格式为： 12VOLUME [&quot;&lt;路径1&gt;&quot;, &quot;&lt;路径2&gt;&quot;...]VOLUME &lt;路径&gt; 之前我们说过，容器运行时应该尽量保持容器存储层不发生写操作，对于数据库类需要保存动态数据的应用，其数据库文件应该保存于卷(volume)中。为了防止运行时用户忘记将动态文件所保存目录挂载为卷，在 Dockerfile 中，我们可以事先指定某些目录挂载为匿名卷，这样在运行时如果用户不指定挂载，其应用也可以正常运行，不会向容器存储层写入大量数据。 1VOLUME /data 这里的 /data 目录就会在运行时自动挂载为匿名卷，任何向 /data 中写入的信息都不会记录进容器存储层，从而保证了容器存储层的无状态化。当然，运行时可以覆盖这个挂载设置。比如： 1docker run -d -v mydata:/data xxxx 在这行命令中，就使用了 mydata 这个命名卷挂载到了 /data 这个位置，替代了 Dockerfile 中定义的匿名卷的挂载配置。 EXPOSE 声明端口1格式为 EXPOSE &lt;端口1&gt; [&lt;端口2&gt;...] EXPOSE 指令是声明运行时容器提供服务端口，这只是一个声明，在运行时并不会因为这个声明应用就会开启这个端口的服务。在 Dockerfile 中写入这样的声明有两个好处，一个是帮助镜像使用者理解这个镜像服务的守护端口，以方便配置映射；另一个用处则是在运行时使用随机端口映射时，也就是 docker run -P 时，会自动随机映射 EXPOSE 的端口。 要将 EXPOSE 和在运行时使用 -p &lt;宿主端口&gt;:&lt;容器端口&gt; 区分开来。-p，是映射宿主端口和容器端口，换句话说，就是将容器的对应端口服务公开给外界访问，而 EXPOSE 仅仅是声明容器打算使用什么端口而已，并不会自动在宿主进行端口映射。 ENTRYPOINT 入口点ENTRYPOINT 的格式和 RUN 指令格式一样，分为 exec 格式和 shell 格式。 ENTRYPOINT 的目的和 CMD 一样，都是在指定容器启动程序及参数。ENTRYPOINT 在运行时也可以替代，不过比 CMD 要略显繁琐，需要通过 docker run 的参数 --entrypoint 来指定。 当指定了 ENTRYPOINT 后，CMD 的含义就发生了改变，不再是直接的运行其命令，而是将 CMD 的内容作为参数【★★★★★】传给 ENTRYPOINT 指令，换句话说实际执行时，将变为： 1&lt;ENTRYPOINT&gt; &quot;&lt;CMD&gt;&quot; 那么有了 CMD 后，为什么还要有 ENTRYPOINT 呢？这种 &lt;ENTRYPOINT&gt; “&lt;CMD&gt;“ 有什么好处么？让我们来看几个场景。 场景一：让镜像变成像命令一样使用假设我们需要一个得知自己当前公网 IP 的镜像，那么可以先用 CMD 来实现： 12FROM centos:7.7.1908CMD [ &quot;curl&quot;, &quot;-s&quot;, &quot;ifconfig.io&quot; ] 假如我们使用 docker build -t myip . 来构建镜像的话，如果我们需要查询当前公网 IP，只需要执行： 12$ docker run myip183.226.75.148 嗯，这么看起来好像可以直接把镜像当做命令使用了，不过命令总有参数，如果我们希望加参数呢？比如从上面的 CMD 中可以看到实质的命令是 curl，那么如果我们希望显示 HTTP 头信息，就需要加上 -i 参数。那么我们可以直接加 -i 参数给 docker run myip 么？ 123$ docker run myip -idocker: Error response from daemon: OCI runtime create failed: container_linux.go:348: starting container process caused &quot;exec: \&quot;-i\&quot;: executable file not found in $PATH&quot;: unknown.ERRO[0000] error waiting for container: context canceled 我们可以看到可执行文件找不到的报错，executable file not found。之前我们说过，跟在镜像名后面的是 command【Usage: docker run [OPTIONS] IMAGE [COMMAND] [ARG…]】，运行时会替换 CMD 的默认值。因此这里的 -i 替换了原来的 CMD，而不是添加在原来的 curl -s ifconfig.io 后面。而 -i 根本不是命令，所以自然找不到。 那么如果我们希望加入 -i 这参数，我们就必须重新完整的输入这个命令： 1$ docker run myip curl -s ifconfig.io -i 这显然不是很好的解决方案，而使用 ENTRYPOINT 就可以解决这个问题。现在我们重新用 ENTRYPOINT 来实现这个镜像： 12FROM centos:7.7.1908ENTRYPOINT [ &quot;curl&quot;, &quot;-s&quot;, &quot;ifconfig.io&quot; ] 使用 docker build -t myip2 . 构建完成后，这次我们再来尝试直接使用 docker run myip2 -i： 1234567891011121314151617$ docker run myip2 183.226.75.148$ docker run myip2 -iHTTP/1.1 200 OKDate: Sun, 19 Apr 2020 02:20:48 GMTContent-Type: text/plain; charset=utf-8Content-Length: 15Connection: keep-aliveSet-Cookie: __cfduid=d76a2e007bbe7ec2d230b0a6636d115151587262848; expires=Tue, 19-May-20 02:20:48 GMT; path=/; domain=.ifconfig.io; HttpOnly; SameSite=LaxCF-Cache-Status: DYNAMICServer: cloudflareCF-RAY: 586326015c3199a1-LAXalt-svc: h3-27=&quot;:443&quot;; ma=86400, h3-25=&quot;:443&quot;; ma=86400, h3-24=&quot;:443&quot;; ma=86400, h3-23=&quot;:443&quot;; ma=86400cf-request-id: 0231d614d9000099a1e10d7200000001183.226.75.148 可以看到，这次成功了。这是因为当存在 ENTRYPOINT 后，CMD 的内容将会作为参数传给 ENTRYPOINT，而这里 -i 就是新的 CMD，因此会作为参数传给 curl，从而达到了我们预期的效果。 场景二：应用运行前的准备工作启动容器就是启动主进程，但有些时候，启动主进程前，需要一些准备工作。 比如 mysql 类的数据库，可能需要一些数据库配置、初始化的工作，这些工作要在最终的 mysql 服务器运行之前解决。 此外，可能希望避免使用 root 用户去启动服务，从而提高安全性，而在启动服务前还需要以 root 身份执行一些必要的准备工作，最后切换到服务用户身份启动服务。或者除了服务外，其它命令依旧可以使用 root 身份执行，方便调试等。 这些准备工作是和容器 CMD 无关的，无论 CMD 是什么，都需要事先进行一个预处理的工作。这种情况下，可以写一个脚本，然后放入 ENTRYPOINT 中去执行，而这个脚本会将接到的参数（也就是 &lt;CMD&gt;）作为命令，在脚本最后执行。比如官方镜像 redis 中就是这么做的： 12345678FROM alpine:3.4...RUN addgroup -S redis &amp;&amp; adduser -S -G redis redis...ENTRYPOINT [&quot;docker-entrypoint.sh&quot;]EXPOSE 6379CMD [ &quot;redis-server&quot; ] 可以看到其中为了 redis 服务创建了 redis 用户，并在最后指定了 ENTRYPOINT 为 docker-entrypoint.sh 脚本。 123456789#!/bin/sh...# allow the container to be started with `--user`if [ &quot;$1&quot; = &apos;redis-server&apos; -a &quot;$(id -u)&quot; = &apos;0&apos; ]; then chown -R redis . exec su-exec redis &quot;$0&quot; &quot;$@&quot;fiexec &quot;$@&quot; 该脚本的内容就是根据 CMD 的内容来判断，如果是 redis-server 的话，则切换到 redis 用户身份启动服务器，否则依旧使用 root 身份执行。比如： 12$ docker run -it redis iduid=0(root) gid=0(root) groups=0(root) CMD 容器启动命令CMD 指令的格式和 RUN 相似，也是两种格式和一种特殊格式： 123shell 格式：CMD &lt;命令&gt;exec 格式：CMD [&quot;可执行文件&quot;, &quot;参数1&quot;, &quot;参数2&quot;...]参数列表格式：CMD [&quot;参数1&quot;, &quot;参数2&quot;...]。在指定了 ENTRYPOINT 指令后，用 CMD 指定具体的参数。 之前介绍容器的时候曾经说过，Docker 不是虚拟机，容器就是进程。既然是进程，那么在启动容器的时候，需要指定所运行的程序及参数。CMD 指令就是用于指定默认的容器主进程的启动命令的。 在指令格式上，一般推荐使用 exec 格式，这类格式在解析时会被解析为 JSON 数组，因此一定要使用双引号 “，而不要使用单引号。 如果使用 shell 格式的话，实际的命令会被包装为 sh -c 的参数的形式进行执行。比如： 1CMD echo $HOME 在实际执行中，会将其变更为： 1CMD [ &quot;sh&quot;, &quot;-c&quot;, &quot;echo $HOME&quot; ] 这就是为什么我们可以使用环境变量的原因，因为这些环境变量会被 shell 进行解析处理。 提到 CMD 就不得不提容器中应用在前台执行和后台执行的问题。这是初学者常出现的一个混淆。 Docker 不是虚拟机，容器中的应用都应该以前台执行，而不是像虚拟机、物理机里面那样，用 systemd 去启动后台服务，容器内没有后台服务的概念。 对于容器而言，其启动程序就是容器应用进程，容器就是为了主进程而存在的，主进程退出，容器就失去了存在的意义，从而退出，其它辅助进程不是它需要关心的东西。 一些初学者将 CMD 写为： 1CMD service nginx start 使用 service nginx start 命令，则是希望 upstart 来以后台守护进程形式启动 nginx 服务。而刚才说了 CMD service nginx start 会被理解为 CMD [ “sh”, “-c”, “service nginx start”]，因此主进程实际上是 sh。那么当 service nginx start 命令结束后，sh 也就结束了，sh 作为主进程退出了，自然就会令容器退出。 正确的做法是直接执行 nginx 可执行文件，并且要求以前台形式运行。比如： 1CMD [&quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;] 构建镜像案例构建文件1234567891011121314151617181920212223242526272829303132333435363738394041[root@docker01 make03]# pwd/root/docker_test/make03[root@docker01 make03]# lltotal 12-rw-r--r-- 1 root root 720 Apr 19 16:46 Dockerfile-rw-r--r-- 1 root root 95 Apr 19 16:19 entrypoint.sh-rw-r--r-- 1 root root 22 Apr 19 16:18 index.html[root@docker01 make03]# cat Dockerfile # Dockerfile文件# 基础镜像FROM centos:7.7.1908# 维护者MAINTAINER lightzhang lightzhang@xxx.com# 命令:做了些什么操作RUN echo &apos;nameserver 223.5.5.5&apos; &gt; /etc/resolv.conf &amp;&amp; echo &apos;nameserver 223.6.6.6&apos; &gt;&gt; /etc/resolv.confRUN curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo &amp;&amp; curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repoRUN yum install -y nginx-1.16.1 &amp;&amp; yum clean allRUN echo &quot;daemon off;&quot; &gt;&gt; /etc/nginx/nginx.conf# 添加文件COPY index.html /usr/share/nginx/html/index.htmlCOPY entrypoint.sh /usr/local/bin/entrypoint.sh# 对外暴露端口声明EXPOSE 80# 执行ENTRYPOINT [&quot;sh&quot;, &quot;entrypoint.sh&quot;]# 执行命令CMD [&quot;nginx&quot;][root@docker01 make03]# cat index.html # 访问文件nginx in docker, html[root@docker01 make03]# cat entrypoint.sh # entrypoint 文件#!/bin/bashif [ &quot;$1&quot; = &apos;nginx&apos; ]; then exec nginx -c /etc/nginx/nginx.conffiexec &quot;$@&quot; 构建镜像123456789101112131415161718192021222324252627282930313233343536[root@docker01 make03]# docker build -t base/nginx:1.16.1 .Sending build context to Docker daemon 4.608kBStep 1/11 : FROM centos:7.7.1908 ---&gt; 08d05d1d5859Step 2/11 : MAINTAINER lightzhang lightzhang@xxx.com ---&gt; Using cache ---&gt; 1dc29e78d94fStep 3/11 : RUN echo &apos;nameserver 223.5.5.5&apos; &gt; /etc/resolv.conf &amp;&amp; echo &apos;nameserver 223.6.6.6&apos; &gt;&gt; /etc/resolv.conf ---&gt; Using cache ---&gt; 19398ad9b023Step 4/11 : RUN curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo &amp;&amp; curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo ---&gt; Using cache ---&gt; b2451c5856c5Step 5/11 : RUN yum install -y nginx-1.16.1 &amp;&amp; yum clean all ---&gt; Using cache ---&gt; 291f27cae4dfStep 6/11 : RUN echo &quot;daemon off;&quot; &gt;&gt; /etc/nginx/nginx.conf ---&gt; Using cache ---&gt; 115e07b6313eStep 7/11 : COPY index.html /usr/share/nginx/html/index.html ---&gt; Using cache ---&gt; 9d714d2e2a84Step 8/11 : COPY entrypoint.sh /usr/local/bin/entrypoint.sh ---&gt; Using cache ---&gt; b16983911b56Step 9/11 : EXPOSE 80 ---&gt; Using cache ---&gt; d8675d6c2d43Step 10/11 : ENTRYPOINT [&quot;sh&quot;, &quot;entrypoint.sh&quot;] ---&gt; Using cache ---&gt; 802a1a67db37Step 11/11 : CMD [&quot;nginx&quot;] ---&gt; Using cache ---&gt; f2517b4d5510Successfully built f2517b4d5510Successfully tagged base/nginx:1.16.1 发布容器与端口查看123456789101112131415161718192021[root@docker01 ~]# docker run -d -p 80:80 --name mynginx_v2 base/nginx:1.16.1 # 启动容器50a45a0894d8669308de7c70d47c96db8cd8990d3e34d1d125e5289ed062f126[root@docker01 ~]# [root@docker01 ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES50a45a0894d8 base/nginx:1.16.1 &quot;sh entrypoint.sh ng…&quot; 3 minutes ago Up 3 minutes 0.0.0.0:80-&gt;80/tcp mynginx_v2[root@docker01 ~]# netstat -lntup # 端口查看Active Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 1634/master tcp 0 0 0.0.0.0:111 0.0.0.0:* LISTEN 1/systemd tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1349/sshd tcp6 0 0 ::1:25 :::* LISTEN 1634/master tcp6 0 0 :::111 :::* LISTEN 1/systemd tcp6 0 0 :::80 :::* LISTEN 13625/docker-proxy tcp6 0 0 :::8080 :::* LISTEN 2289/docker-proxy tcp6 0 0 :::22 :::* LISTEN 1349/sshd udp 0 0 0.0.0.0:1021 0.0.0.0:* 847/rpcbind udp 0 0 0.0.0.0:111 0.0.0.0:* 1/systemd udp6 0 0 :::1021 :::* 847/rpcbind udp6 0 0 :::111 :::* 1/systemd 浏览器访问1http://172.16.1.31/ 完毕！]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker数据管理与挂载]]></title>
    <url>%2F2020%2F04%2F26%2Fdocker03%2F</url>
    <content type="text"><![CDATA[介绍如何在 Docker 内部以及容器之间管理数据；在容器中管理数据主要有两种方式：数据卷（Volumes）、挂载主机目录 (Bind mounts) 镜像来源12345[root@docker01 ~]# docker pull registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17[root@docker01 ~]# docker tag ed21b7a8aee9 nginx:1.17[root@docker01 ~]# docker images | grep &apos;nginx&apos;nginx 1.17 ed21b7a8aee9 2 weeks ago 127MBregistry.cn-beijing.aliyuncs.com/google_registry/nginx 1.17 ed21b7a8aee9 2 weeks ago 127MB 数据卷【-v, --volume】数据卷 是一个可供一个或多个容器使用的特殊目录，它绕过 UFS，可以提供很多有用的特性： 1、数据卷可以在容器之间共享和重用 2、对数据卷的修改会立马生效 3、对数据卷的更新，不会影响镜像 4、数据卷默认会一直存在，即使容器被删除 注意：数据卷的使用，类似于 Linux 下对目录或文件进行 mount，镜像中的被指定为挂载点的目录中的文件会隐藏掉，能显示看的是挂载的数据卷。 数据卷挂载方式1【重点，常用】包括挂载目录和挂载文件 宿主机挂载的目录和文件 123456[root@docker01 ~]# mkdir -p /data/nginx_test2 # 在宿主机创建目录[root@docker01 ~]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6# hosts info 2020-04-18 宿主机操作 1234567891011121314151617181920212223242526272829303132# /data/nginx_test2:/data_volume_test2 目录挂载 前面：宿主机目录； 后面：容器目录# /etc/hosts:/etc/hosts 文件挂载 前面：宿主机文件； 后面：容器文件[root@docker01 ~]# docker run -d -v /data/nginx_test2:/data_volume_test2 -v /etc/hosts:/etc/hosts --name nginx102 nginx:1.17 # 发布容器[root@docker01 ~]# docker inspect nginx102 # 然后在 Mounts 查看信息………… &quot;Mounts&quot;: [ &#123; &quot;Type&quot;: &quot;bind&quot;, &quot;Source&quot;: &quot;/etc/hosts&quot;, &quot;Destination&quot;: &quot;/etc/hosts&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;rprivate&quot; &#125;, &#123; &quot;Type&quot;: &quot;bind&quot;, &quot;Source&quot;: &quot;/data/nginx_test2&quot;, &quot;Destination&quot;: &quot;/data_volume_test2&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;rprivate&quot; &#125; ],…………# 在宿主机的挂载目录创建文件和目录[root@docker01 ~]# cd /data/nginx_test2/[root@docker01 nginx_test2]# echo &quot;aaa&quot; &gt; aaa[root@docker01 nginx_test2]# mkdir abc[root@docker01 nginx_test2]# lltotal 4-rw-r--r-- 1 root root 4 Apr 18 16:41 aaadrwxr-xr-x 2 root root 6 Apr 18 16:41 abc 容器操作 12345678910111213141516# 进入容器[root@docker01 ~]# docker exec -it nginx102 bash# 查看挂载目录信息root@8c8db5089c0e:/# ls bin boot data_volume_test2 dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr varroot@8c8db5089c0e:/# cd data_volume_test2/root@8c8db5089c0e:/data_volume_test2# ls -ltotal 4-rw-r--r-- 1 root root 4 Apr 18 08:41 aaadrwxr-xr-x 2 root root 6 Apr 18 08:41 abc# 查看挂载文件信息root@2537d3854b42:/# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6# hosts info 2020-04-18 数据卷挂载方式2【了解】宿主机操作 12345678910[root@docker01 ~]# docker run -d -v /data_volume --name nginx101 nginx:1.17 # 发布时，加入数据卷信息 -v /data_volume594ea376d9301263046b13c72304af9c74a2a7516d0f3d8292f020ceba94742d[root@docker01 ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES594ea376d930 nginx:1.17 &quot;nginx -g &apos;daemon of…&quot; 5 seconds ago Up 3 seconds 80/tcp nginx101 [root@docker01 ~]# docker inspect -f &#123;&#123;.Mounts&#125;&#125; nginx101 # 或者执行 docker inspect nginx101 然后在 Mounts 查看信息[&#123;volume b7bXXXX897 /var/lib/docker/volumes/b7bXXXX897/_data /data_volume local true &#125;][root@docker01 ~]# cd /var/lib/docker/volumes/b7bXXXX897/_data # 进入数据卷目录[root@docker01 _data]# echo &quot;1111&quot; &gt; 111 # 创建文件并写入数据[root@docker01 _data]# mkdir aaa/bbb -p # 创建多个目录 进入容器操作 12345678910111213[root@docker01 ~]# docker exec -it nginx101 bash # 进入容器root@594ea376d930:/# ls -l # 查看容器根目录信息total 8drwxr-xr-x 2 root root 4096 Mar 27 00:00 bindrwxr-xr-x 2 root root 6 Feb 1 17:09 bootdrwxr-xr-x 2 root root 6 Apr 17 14:35 data_volume # 存在该目录drwxr-xr-x 5 root root 340 Apr 17 14:35 dev…………root@594ea376d930:/# cd data_volume/ # 进入数据卷root@594ea376d930:/data_volume# ls -l # 可见文件和目录都在容器中存在total 4-rw-r--r-- 1 root root 5 Apr 17 14:44 111drwxr-xr-x 3 root root 17 Apr 17 14:44 aaa 查看容器的挂载信息1234root@594ea376d930:/# mount # 查看容器挂载信息………………/dev/sda2 on /data_volume type xfs (rw,relatime,attr2,inode64,noquota) # 找到数据卷信息……………… 挂载主机目录【--mount】包括挂载目录和挂载文件 宿主机挂载的目录和文件 123456[root@docker01 ~]# mkdir -p /data/nginx_test3 # 在宿主机创建目录[root@docker01 ~]# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6# hosts info 2020-04-18 宿主机操作 12345678910111213141516171819202122232425262728293031323334353637[root@docker01 ~]# docker run -d --mount type=bind,source=/data/nginx_test3,target=/data_volume_test3 --mount type=bind,source=/etc/hosts,target=/etc/hosts --name nginx103 nginx:1.17 # 发布容器[root@docker01 ~]# docker inspect nginx103 # 在Mounts可见容器的挂载信息………… &quot;Mounts&quot;: [ &#123; &quot;Type&quot;: &quot;bind&quot;, &quot;Source&quot;: &quot;/data/nginx_test3&quot;, &quot;Destination&quot;: &quot;/data_volume_test3&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;rprivate&quot; &#125;, &#123; &quot;Type&quot;: &quot;bind&quot;, &quot;Source&quot;: &quot;/etc/hosts&quot;, &quot;Destination&quot;: &quot;/etc/hosts&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;rprivate&quot; &#125; ],…………# 在宿主机的挂载目录创建文件和目录[root@docker01 ~]# cd /data/nginx_test3/[root@docker01 nginx_test3]# echo &quot;123&quot; &gt; 123[root@docker01 nginx_test3]# mkdir -p 111/222[root@docker01 nginx_test3]# lltotal 4drwxr-xr-x 3 root root 17 Apr 18 18:05 111-rw-r--r-- 1 root root 4 Apr 18 18:05 123[root@docker01 nginx_test3]# tree .├── 111│ └── 222└── 1232 directories, 1 file 容器操作 123456789101112131415# 进入容器[root@docker01 ~]# docker exec -it nginx103 bash# 查看挂载目录信息root@c9427fcbc26c:/# ls bin boot data_volume_test3 dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr varroot@c9427fcbc26c:/# ls -l data_volume_test3/total 4drwxr-xr-x 3 root root 17 Apr 18 10:05 111-rw-r--r-- 1 root root 4 Apr 18 10:05 123# 查看挂载文件信息root@c9427fcbc26c:/# cat /etc/hosts127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6# hosts info 2020-04-18 完毕！]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker镜像与容器的常用操作]]></title>
    <url>%2F2020%2F04%2F25%2Fdocker02%2F</url>
    <content type="text"><![CDATA[Docker镜像加速配置；Docker镜像常用操作；Dcoker容器常用操作。 镜像加速器国内从 Docker Hub 拉取镜像有时会遇到困难，此时可以配置镜像加速器。国内很多云服务商都提供了国内加速器服务，例如： 12网易云加速器 https://hub-mirror.c.163.com阿里云加速器(需登录账号获取): https://cr.console.aliyun.com/cn-hangzhou/mirrors 国内各大云服务商均提供了 Docker 镜像加速服务，建议根据运行 Docker 的云平台选择对应的镜像加速服务，具体请参考官方文档。 在CentOS7系统，请在 /etc/docker/daemon.json 中写入如下内容（如果文件不存在请新建该文件） 123456[root@docker01 ~]# vim /etc/docker/daemon.json&#123; &quot;registry-mirrors&quot;: [ &quot;https://hub-mirror.c.163.com&quot; ]&#125; 注意，一定要保证该文件符合 json 规范，否则 Docker 将不能启动。 之后重新启动服务。 12systemctl daemon-reloadsystemctl restart docker 检查加速器是否生效执行如下命令，如果从结果中看到了如下内容，说明配置成功。 12345678[root@docker01 ~]# docker info # 显示整个系统的信息………………Registry Mirrors: https://hub-mirror.c.163.com/Live Restore Enabled: falseWARNING: bridge-nf-call-iptables is disabledWARNING: bridge-nf-call-ip6tables is disabled Docker镜像操作说明：Docker 运行容器前需要本地存在对应的镜像，如果本地不存在该镜像，Docker 会从镜像仓库下载该镜像。 search搜索镜像12345678[root@docker01 ~]# docker search centosNAME DESCRIPTION STARS OFFICIAL AUTOMATEDcentos The official build of CentOS. 5934 [OK] ansible/centos7-ansible Ansible on Centos7 128 [OK]jdeathe/centos-ssh OpenSSH / Supervisor / EPEL/IUS/SCL Repos - … 114 [OK]consol/centos-xfce-vnc Centos container with &quot;headless&quot; VNC session… 114 [OK]centos/mysql-57-centos7 MySQL 5.7 SQL database server 74 ………… pull从镜像中心下载镜像123456# 格式：docker pull &lt;image_name&gt;:&lt;tag&gt; ，如果没有tag，默认为 latest[root@docker01 ~]# docker pull centos:latestlatest: Pulling from library/centos8a29a15cefae: Pull complete Digest: sha256:fe8d824220415eed5477b63addf40fb06c3b049404242b31982106ac204f6700Status: Downloaded newer image for centos:latest push推送镜像到镜像中心12格式：docker push &lt;image_name&gt;:&lt;tag&gt;[root@docker01 ~]# docker push registry.cn-beijing.aliyuncs.com/google_registry/centos:latest 说明：如果有疑问可先忽略，后面搭建私有仓库文章会再次说明的。 images列出镜像123[root@docker01 ~]# docker images # 或者 docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEcentos latest 470671670cac 2 months ago 237MB save镜像保存到本地12345# 格式：docker save -o &lt;保存的文件名&gt; &lt;image_name:tag&gt;|&lt;image_id&gt;[root@docker01 docker_test]# docker save -o centos_docker_20200413.tar centos:latest [root@docker01 docker_test]# ll -htotal 234M-rw------- 1 root root 234M Apr 13 16:21 centos_docker_20200413.tar rmi删除镜像1234567# 格式：docker rmi &lt;image_name:tag&gt;|&lt;image_id&gt;[root@docker01 docker_test]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEcentos latest 470671670cac 2 months ago 237MB[root@docker01 docker_test]# docker rmi 470671670cac # 删除镜像 [root@docker01 docker_test]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZE load导入镜像12345# 格式：docker load -i &lt;image_file&gt;[root@docker01 docker_test]# docker load -i centos_docker_20200413.tar[root@docker01 docker_test]# docker images REPOSITORY TAG IMAGE ID CREATED SIZEcentos latest 470671670cac 2 months ago 237MB tag标签123456789# 格式：docker tag SOURCE_IMAGE[:TAG] TARGET_IMAGE[:TAG][root@docker01 docker_test]# docker images REPOSITORY TAG IMAGE ID CREATED SIZEcentos latest 470671670cac 2 months ago 237MB [root@docker01 docker_test]# docker tag centos:latest centos:20200413[root@docker01 docker_test]# docker images REPOSITORY TAG IMAGE ID CREATED SIZEcentos 20200413 470671670cac 2 months ago 237MBcentos latest 470671670cac 2 months ago 237MB 使用：根据需要给docker镜像打一个新标签。 info显示整个系统的信息12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[root@docker01 ~]# docker info Containers: 0 Running: 0 Paused: 0 Stopped: 0Images: 1Server Version: 18.06.3-ceStorage Driver: overlay2 Backing Filesystem: xfs Supports d_type: true Native Overlay Diff: trueLogging Driver: json-fileCgroup Driver: cgroupfsPlugins: Volume: local Network: bridge host macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file logentries splunk syslogSwarm: inactiveRuntimes: runcDefault Runtime: runcInit Binary: docker-initcontainerd version: 468a545b9edcd5932818eb9de8e72413e616e86erunc version: a592beb5bc4c4092b1b1bac971afed27687340c5init version: fec3683Security Options: seccomp Profile: defaultKernel Version: 3.10.0-1062.el7.x86_64Operating System: CentOS Linux 7 (Core)OSType: linuxArchitecture: x86_64CPUs: 2Total Memory: 1.777GiBName: docker01ID: XIHU:XNWU:II7A:YXUH:BOZ3:JSGG:J3P2:CU2Z:5QHA:5Y64:PZ4V:62DIDocker Root Dir: /var/lib/dockerDebug Mode (client): falseDebug Mode (server): falseRegistry: https://index.docker.io/v1/Labels:Experimental: falseInsecure Registries: 127.0.0.0/8Registry Mirrors: https://hub-mirror.c.163.com/Live Restore Enabled: falseWARNING: bridge-nf-call-iptables is disabledWARNING: bridge-nf-call-ip6tables is disabled system镜像体积查看另外一个需要注意的问题是，docker image ls 列表中的镜像体积总和并非是所有镜像实际硬盘消耗。由于 Docker 镜像是多层存储结构，并且可以继承、复用，因此不同镜像可能会因为使用相同的基础镜像，从而拥有共同的层。由于 Docker 使用 Union FS，相同的层只需要保存一份即可，因此实际镜像硬盘占用空间很可能要比这个列表镜像大小的总和要小的多。 可以通过以下命令来便捷的查看镜像、容器、数据卷所占用的空间。 123456[root@docker01 docker_test]# docker system dfTYPE TOTAL ACTIVE SIZE RECLAIMABLEImages 2 0 440.1MB 440.1MB (100%)Containers 0 0 0B 0BLocal Volumes 0 0 0B 0BBuild Cache 0 0 0B 0B inspect显示镜像或容器的详情123# 格式：docker inspect &lt;镜像ID&gt;|&lt;镜像名&gt;|&lt;容器ID&gt;|&lt;容器名&gt;[root@docker01 ~]# docker inspect centos:latest # 显示镜像详情[root@docker01 ~]# docker inspect 67ba647b0151 # 显示容器详情 Docker容器操作run创建容器12345# 格式：docker run [OPTIONS] IMAGE [COMMAND] [ARG...][root@docker01 ~]# docker run -i -t --name centos01 centos:latest /bin/bash[root@f7c4da3cecad /]# # 此时已进入docker容器[root@f7c4da3cecad /]# exit # 退出容器，此时容器会停止【正常情况】[root@docker01 ~]# 参数说明： 1234-i 交互式操作-t 分配一个终端--name 运行的容器名称最后的/bin/bash 要执行的命令 ps查看容器123456[root@docker01 ~]# docker ps # 查看正在运行的容器CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES[root@docker01 ~]# [root@docker01 ~]# docker ps -a # 查看所有容器，包括运行和停止的CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESf7c4da3cecad centos:latest &quot;/bin/bash&quot; 4 minutes ago Exited (0) 36 seconds ago centos01 start启动容器123456789# 格式：docker start &lt;容器名&gt;|&lt;容器ID&gt;[root@docker01 ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESf7c4da3cecad centos:latest &quot;/bin/bash&quot; 10 minutes ago Exited (0) 3 seconds ago centos01[root@docker01 ~]# docker start f7c4da3cecad # 启动容器f7c4da3cecad[root@docker01 ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESf7c4da3cecad centos:latest &quot;/bin/bash&quot; 10 minutes ago Up 3 seconds centos01 restart重启容器123456789# 格式：docker restart &lt;容器名&gt;|&lt;容器ID&gt;[root@docker01 ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESf7c4da3cecad centos:latest &quot;/bin/bash&quot; 15 minutes ago Up 4 minutes centos01[root@docker01 ~]# docker restart f7c4da3cecad # 重启容器f7c4da3cecad[root@docker01 ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESf7c4da3cecad centos:latest &quot;/bin/bash&quot; 15 minutes ago Up 1 second centos01 stop停止容器123456789# 格式：docker stop &lt;容器名&gt;|&lt;容器ID&gt;[root@docker01 ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESf7c4da3cecad centos:latest &quot;/bin/bash&quot; 3 hours ago Up 4 minutes centos01[root@docker01 ~]# docker stop f7c4da3cecad # 停止容器f7c4da3cecad[root@docker01 ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESf7c4da3cecad centos:latest &quot;/bin/bash&quot; 3 hours ago Exited (0) 10 seconds ago centos01 rm删除容器123456# 格式：docker rm &lt;容器名&gt;|&lt;容器ID&gt;[root@docker01 ~]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESf7c4da3cecad centos:latest &quot;/bin/bash&quot; 3 hours ago Exited (0) 10 seconds ago centos01[root@docker01 ~]# docker rm f7c4da3cecad # 删除已停止的容器f7c4da3cecad 说明：如果要强制删除正在运行的容器，使用 docker rm -f &lt;容器ID&gt;。不过生产环境不建议强制删除容器，防止误删除。 rename容器重命名12345678# 格式：docker rename CONTAINER NEW_NAME[root@docker01 ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES67ba647b0151 centos:latest &quot;/bin/bash&quot; About a minute ago Up About a minute centos01[root@docker01 ~]# docker rename 67ba647b0151 centos001 # 容器重命名[root@docker01 ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES67ba647b0151 centos:latest &quot;/bin/bash&quot; 2 minutes ago Up About a minute centos001 exec进入容器或在运行容器中执行命令进入容器 12[root@docker01 ~]# docker exec -it f7c4da3cecad bash[root@f7c4da3cecad /]# 说明：不建议通过 docker attach 方式进入容器。 在容器外让指定容器执行命令 12345# 不要有 -t 选项，因为不需要分配 tty 终端[root@docker01 ~]# docker exec -i f7c4da3cecad bash -c &quot;ps -ef&quot;UID PID PPID C STIME TTY TIME CMDroot 1 0 0 10:12 pts/0 00:00:00 /bin/bashroot 78 0 0 10:19 ? 00:00:00 ps -ef cp复制文件或目录将宿主机文件或目录，拷贝到docker容器中 123456789101112131415[root@docker01 ~]# docker exec -i 67ba647b0151 bash -c &quot;ls -l /root&quot;total 12-rw------- 1 root root 2366 Jan 13 21:49 anaconda-ks.cfg-rw-r--r-- 1 root root 435 Jan 13 21:49 anaconda-post.log-rw------- 1 root root 2026 Jan 13 21:49 original-ks.cfg[root@docker01 ~]# [root@docker01 ~]# docker cp /usr/bin/telnet 67ba647b0151:/root/ # 拷贝文件[root@docker01 ~]# docker cp /root/basedOptimi 67ba647b0151:/root/ # 拷贝目录[root@docker01 ~]# docker exec -i 67ba647b0151 bash -c &quot;ls -l /root&quot;total 112-rw------- 1 root root 2366 Jan 13 21:49 anaconda-ks.cfg-rw-r--r-- 1 root root 435 Jan 13 21:49 anaconda-post.logdrwxr-xr-x 2 root root 30 Mar 8 19:59 basedOptimi-rw------- 1 root root 2026 Jan 13 21:49 original-ks.cfg-rwxr-xr-x 1 root root 101776 Aug 3 2017 telnet 将docker容器中的文件或目录，拷贝到宿主机中 12[root@docker01 ~]# docker cp 67ba647b0151:/root/original-ks.cfg /root/ # 拷贝文件[root@docker01 ~]# docker cp 67ba647b0151:/etc /root/ # 拷贝目录 logs查看容器日志123456[root@docker01 ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESf7c4da3cecad centos:latest &quot;/bin/bash&quot; 3 hours ago Up 1 second centos01# 获取容器日志[root@docker01 ~]# docker logs -f --tail 500 f7c4da3cecad………… 参数说明： 12-f 持续打印输出--tail 500 打印日志最后的500行 stats容器使用资源统计可用于监控 12[root@docker01 ~]# docker stats &lt;容器ID&gt;|&lt;容器名称&gt; # 持续监控[root@docker01 ~]# docker stats --no-stream &lt;容器ID&gt;|&lt;容器名称&gt; # 不是持续监控，只显示第一次返回的结果 top容器中运行的进程1234567[root@docker01 ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES67ba647b0151 centos:latest &quot;/bin/bash&quot; 2 hours ago Up 2 hours centos001 # 查看容器中运行的进程信息[root@docker01 ~]# docker top 67ba647b0151UID PID PPID C STIME TTY TIME CMDroot 3302 3285 0 21:13 pts/0 00:00:00 /bin/bash port容器映射特定端口容器映射端口有：随机端口映射、指定单个端口映射、指定多个端口映射 获取镜像 12345[root@docker01 ~]# docker pull registry.cn-beijing.aliyuncs.com/google_registry/nginx:1.17[root@docker01 ~]# docker tag ed21b7a8aee9 nginx:1.17[root@docker01 ~]# docker images | grep &apos;nginx&apos;nginx 1.17 ed21b7a8aee9 2 weeks ago 127MBregistry.cn-beijing.aliyuncs.com/google_registry/nginx 1.17 ed21b7a8aee9 2 weeks ago 127MB 映射随机端口 12345[root@docker01 ~]# docker run -d -P --name nginx01 nginx:1.17e90c9faaf8e3387920dd9763bf5c7df9dd17856673868bb512cec78741ff71dc[root@docker01 ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe90c9faaf8e3 nginx:1.17 &quot;nginx -g &apos;daemon of…&quot; 3 seconds ago Up 2 seconds 0.0.0.0:1025-&gt;80/tcp nginx01 说明： 10.0.0.0:1025-&gt;80/tcp 前面为宿主机端口，后面为容器端口 容器日志： 1[root@docker01 ~]# docker logs -f --tail 500 nginx01 浏览器访问： 映射单个指定端口 1234[root@docker01 ~]# docker run -d -p 81:80 --name nginx02 nginx:1.1704478222f0dc981883f25504164be3af7da49248886cee7386ccc89b80cc57a1[root@docker01 ~]# docker ps | grep &apos;nginx02&apos;04478222f0dc nginx:1.17 &quot;nginx -g &apos;daemon of…&quot; 29 seconds ago Up 28 seconds 0.0.0.0:81-&gt;80/tcp nginx02 浏览器访问： 映射多个指定端口 1234[root@docker01 ~]# docker run -d -p 85:80 -p 445:443 --name nginx03 nginx:1.175886e52ff8e934bc827c8d7753a532b9062bd045799d0658a008e371e6ecd09c[root@docker01 ~]# docker ps | grep &apos;nginx03&apos;5886e52ff8e9 nginx:1.17 &quot;nginx -g &apos;daemon of…&quot; 12 seconds ago Up 11 seconds 0.0.0.0:85-&gt;80/tcp, 0.0.0.0:445-&gt;443/tcp nginx03 推荐阅读1、 Docker简介与安装 完毕！]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker简介与安装]]></title>
    <url>%2F2020%2F04%2F24%2Fdocker01%2F</url>
    <content type="text"><![CDATA[Docker简介；Docker与虚拟机区别；Docker的优点及内部组件说明；如何安装Docker与镜像加速配置。 什么是DockerDocker提供了一个可以运行你的应用程序的封装，或者说容器。它原本是dotCloud的一个内部项目，开源的时候吸引了大量的关注与讨论，后续导致dotCloud把公司名称改为了Docker Inc。Docker 最初是在 Ubuntu 12.04 上开发实现的；Red Hat 则从 RHEL 6.5 开始对 Docker 进行支持；Google 也在其 PaaS 产品中广泛应用 Docker。 Docker 使用 Google 公司推出的 Go 语言进行开发实现并遵从Apache2.0协议开源，基于 Linux 内核的 cgroup，namespace，以及 OverlayFS 类的 Union FS 等技术，对进程进行封装隔离，属于操作系统层面的虚拟化技术。由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。最初实现是基于 LXC，从 0.7 版本以后开始去除 LXC，转而使用自行开发的 libcontainer，从 1.11 开始，则进一步演进为使用 runC 和 containerd。 Docker 在容器的基础上，进行了进一步的封装，从文件系统、网络互联到进程隔离等等，极大的简化了容器的创建和维护。使得 Docker 技术比虚拟机技术更为轻便、快捷。 Docker将应用程序与该程序的依赖，打包在一个文件里面。运行这个文件，就会生成一个虚拟容器。程序在这个虚拟容器里运行，就好像在真实的物理机上运行一样。有了Docker，就不用担心环境问题，这大大地提高了程序运行的灵活性和可移植性。 Docker属于Linux容器的一种封装，提供简单易用的容器使用接口。总体来说，Docker的接口相当简单，用户可以方便地创建和使用容器，把自己的应用放入容器。容器还可以进行版本管理、复制、分享、修改，就像管理普通的代码一样。 Docker与虚拟机区别下面的图片比较了 Docker 和传统虚拟化方式的不同之处。传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程；而容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便。 上图：传统虚拟化 上图：Docker 类别 Docker OpenStack/KVM 部署难度 非常简单 组件多，部署复杂，耗时 启动速度 秒级 分钟级 执行性能 和物理机几乎一致 VM会占用一些资源 镜像体积 镜像MB级别 镜像GB级别 管理效率 管理简单 组件相互依赖，管理复杂 隔离性 隔离性高 彻底隔离 可管理性 单进程、不建议启动SSH 完成的系统管理 网络连接 比较弱 可以灵活组建各类网络架构 为什么要使用 Docker更高效的利用系统资源 由于容器不需要进行硬件虚拟以及运行完整操作系统等额外开销，Docker 对系统资源的利用率更高。无论是应用执行速度、内存损耗或者文件存储速度，都要比传统虚拟机技术更高效。因此，相比虚拟机技术，一个相同配置的主机，往往可以运行更多数量的应用。 更快速的启动时间 传统的虚拟机技术启动应用服务往往需要数分钟，而 Docker 容器应用，由于直接运行于宿主内核，无需启动完整的操作系统，因此可以做到秒级、甚至毫秒级的启动时间。大大的节约了开发、测试、部署的时间。 一致的运行环境 开发过程中一个常见的问题是环境一致性问题。由于开发环境、测试环境、生产环境不一致，导致有些 bug 并未在开发过程中被发现。而 Docker 的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性，从而不会再出现 「这段代码在我机器上没问题啊」 这类问题。 持续交付和部署 对开发和运维（DevOps）人员来说，最希望的就是一次创建或配置，可以在任意地方正常运行。 使用 Docker 可以通过定制应用镜像来实现持续集成、持续交付、部署。开发人员可以通过 Dockerfile 来进行镜像构建，并结合 持续集成(Continuous Integration) 系统进行集成测试，而运维人员则可以直接在生产环境中快速部署该镜像，甚至结合 持续部署(Continuous Delivery/Deployment) 系统进行自动部署。 而且使用 Dockerfile 使镜像构建透明化，不仅仅开发团队可以理解应用运行环境，也方便运维团队理解应用运行所需条件，帮助更好的生产环境中部署该镜像。 更轻松的迁移 由于 Docker 确保了执行环境的一致性，使得应用的迁移更加容易。Docker 可以在很多平台上运行，无论是物理机、虚拟机、公有云、私有云，甚至是笔记本，其运行结果是一致的。因此用户可以很轻易的将在一个平台上运行的应用，迁移到另一个平台上，而不用担心运行环境的变化导致应用无法正常运行的情况。 更轻松的维护和扩展 Docker 使用的分层存储以及镜像的技术，使得应用重复部分的复用更为容易，也使得应用的维护更新更加简单，基于基础镜像进一步扩展镜像也变得非常简单。此外，Docker 团队同各个开源项目团队一起维护了一大批高质量的 官方镜像，既可以直接在生产环境使用，又可以作为基础进一步定制，大大的降低了应用服务的镜像制作成本。 Docker内部组件Docker 包括三个基本概念：镜像（Image）、容器（Container）、仓库（Repository） 理解了这三个概念，就理解了 Docker 的整个生命周期。 Docker 镜像我们都知道，操作系统分为内核和用户空间。对于 Linux 而言内核启动后，会挂载 root 文件系统为其提供用户空间支持。而 Docker 镜像（Image）就相当于是一个 root 文件系统。比如官方镜像 ubuntu:18.04 就包含了完整的一套 Ubuntu 18.04 最小系统的 root 文件系统。 Docker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。镜像不包含任何动态数据，其内容在构建之后也不会被改变。 分层存储 因为镜像包含操作系统完整的 root 文件系统，其体积往往是庞大的，因此在 Docker 设计时，就充分利用 Union FS 的技术，将其设计为分层存储的架构。所以严格来说，镜像并非是像一个 ISO 那样的打包文件，镜像只是一个虚拟的概念，其实际体现并非由一个文件组成，而是由一组文件系统组成，或者说，由多层文件系统联合组成。 镜像构建时，会一层层构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。比如，删除前一层文件的操作，实际不是真的删除前一层的文件，而是仅在当前层标记为该文件已删除。在最终容器运行的时候，虽然不会看到这个文件，但是实际上该文件会一直跟随镜像。因此，在构建镜像的时候，需要额外小心，每一层尽量只包含该层需要添加的东西，任何额外的东西应该在该层构建结束前清理掉。 分层存储的特征还使得镜像的复用、定制变的更为容易。甚至可以用之前构建好的镜像作为基础层，然后进一步添加新的层，以定制自己所需的内容，构建新的镜像。 Docker 容器镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的 类 和 实例 一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等。 容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的命名空间。因此容器可以拥有自己的 root 文件系统、自己的网络配置、自己的进程空间，甚至自己的用户 ID 空间。容器内的进程是运行在一个隔离的环境里，使用起来，就好像是在一个独立于宿主的系统下操作一样。这种特性使得容器封装的应用比直接在宿主运行更加安全。也因为这种隔离的特性，很多人初学 Docker 时常常会混淆容器和虚拟机。 前面讲过镜像使用的是分层存储，容器也是如此。每一个容器运行时，是以镜像为基础层，在其上创建一个当前容器的存储层，我们可以称这个为容器运行时读写而准备的存储层为容器存储层。 容器存储层的生存周期和容器一样，容器消亡时，容器存储层也随之消亡。因此，任何保存于容器存储层的信息都会随容器删除而丢失。 按照 Docker 最佳实践的要求，容器不应该向其存储层内写入任何数据，容器存储层要保持无状态化。所有的文件写入操作，都应该使用数据卷（Volume）、或者绑定宿主目录，在这些位置的读写会跳过容器存储层，直接对宿主（或网络存储）发生读写，其性能和稳定性更高。 数据卷的生存周期独立于容器，容器消亡，数据卷不会消亡。因此，使用数据卷后，容器删除或者重新运行之后，数据却不会丢失。 Docker Registry镜像构建完成后，可以很容易的在当前宿主机上运行，但是，如果需要在其它服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，Docker Registry 就是这样的服务。 一个 Docker Registry 中可以包含多个仓库（Repository）；每个仓库可以包含多个标签（Tag）；每个标签对应一个镜像。 通常，一个仓库会包含同一个软件不同版本的镜像，而标签就常用于对应该软件的各个版本。我们可以通过 &lt;仓库名&gt;:&lt;标签&gt; 的格式来指定具体是这个软件哪个版本的镜像。如果不给出标签，将以 latest 作为默认标签。 仓库名经常以 两段式路径 形式出现，比如 jwilder/nginx-proxy，前者往往意味着 Docker Registry 多用户环境下的用户名，后者则往往是对应的软件名。但这并非绝对，取决于所使用的具体 Docker Registry 的软件或服务。 Docker Registry 公开服务 Docker Registry 公开服务是开放给用户使用、允许用户管理镜像的 Registry 服务。一般这类公开服务允许用户免费上传、下载公开的镜像，并可能提供收费服务供用户管理私有镜像。 最常使用的 Registry 公开服务是官方的 Docker Hub，这也是默认的 Registry，并拥有大量的高质量的官方镜像。除此以外，还有 CoreOS 的 Quay.io，CoreOS 相关的镜像存储在这里；Google 的 Google Container Registry，Kubernetes 的镜像使用的就是这个服务。 由于某些原因，在国内访问这些服务可能会比较慢。国内的一些云服务商提供了针对 Docker Hub 的镜像服务（Registry Mirror），这些镜像服务被称为加速器。常见的有 阿里云加速器、DaoCloud 加速器 等。使用加速器会直接从国内的地址下载 Docker Hub 的镜像，比直接从 Docker Hub 下载速度会提高很多。在 安装 Docker 一节中有详细的配置方法。 国内也有一些云服务商提供类似于 Docker Hub 的公开服务。比如 网易云镜像服务、DaoCloud 镜像市场、阿里云镜像库 等。 12345Docker Hub：https://hub.docker.com/ Quay.io：https://quay.io/repository/ Google Container Registry：https://cloud.google.com/container-registry/阿里云加速器：https://cr.console.aliyun.com/#/acceleratorDaoCloud 加速器：https://www.daocloud.io/mirror#accelerator-doc Docker Registry 私有服务 除了使用公开服务外，用户还可以在本地搭建私有 Docker Registry。Docker 官方提供了 Docker Registry 镜像，可以直接使用做为私有 Registry 服务。 开源的 Docker Registry 镜像只提供了 Docker Registry API 的服务端实现，足以支持 docker 命令，不影响使用。但不包含图形界面，以及镜像维护、用户管理、访问控制等高级功能。在官方的商业化版本 Docker Trusted Registry 中，提供了这些高级功能。 除了官方的 Docker Registry 外，还有第三方软件实现了 Docker Registry API，甚至提供了用户界面以及一些高级功能。比如，Harbor 和 Sonatype Nexus。 安装docker备注：切勿在没有配置 Docker YUM 源的情况下直接使用 yum 命令安装 Docker.，因为这种方式安装的docker 版本较低。 系统要求：最好是CentOS 7版本及以上版本。 卸载旧版本使用以下命令卸载旧版本： 12345678910yum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-selinux \ docker-engine-selinux \ docker-engine docker安装指定版本123456789101112131415161718192021222324252627282930313233343536373839# 安装依赖包[root@docker01 ~]# yum install -y yum-utils device-mapper-persistent-data lvm2# 添加软件yum源信息[root@docker01 ~]# yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo# 更新 yum 软件源缓存【可选操作】[root@docker01 ~]# yum makecache fast# 查看可安装的版本信息[root@docker01 ~]# yum list docker-ce.x86_64 --showduplicates | sort -r * updates: mirrors.aliyun.comLoading mirror speeds from cached hostfileLoaded plugins: fastestmirror * extras: mirrors.aliyun.comdocker-ce.x86_64 3:18.09.2-3.el7 docker-ce-stabledocker-ce.x86_64 3:18.09.1-3.el7 docker-ce-stabledocker-ce.x86_64 3:18.09.0-3.el7 docker-ce-stabledocker-ce.x86_64 18.06.3.ce-3.el7 docker-ce-stabledocker-ce.x86_64 18.06.2.ce-3.el7 docker-ce-stabledocker-ce.x86_64 18.06.1.ce-3.el7 docker-ce-stabledocker-ce.x86_64 18.06.0.ce-3.el7 docker-ce-stabledocker-ce.x86_64 18.03.1.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 18.03.0.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.12.1.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.12.0.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.09.1.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.09.0.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.06.2.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.06.1.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.06.0.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.03.3.ce-1.el7 docker-ce-stabledocker-ce.x86_64 17.03.2.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.03.1.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.03.0.ce-1.el7.centos docker-ce-stable * base: mirrors.aliyun.comAvailable Packages# 安装指定版本的docker服务[root@docker01 ~]# yum -y install docker-ce-18.06.3.ce-3.el7# 版本信息查看[root@docker01 ~]# docker -v # 或者 docker versionDocker version 18.06.3-ce, build d7080c1 启动服务与加入开机自启动12345678910111213141516171819202122[root@docker01 ~]# systemctl status docker.service # 查看当前服务状态● docker.service - Docker Application Container Engine Loaded: loaded (/usr/lib/systemd/system/docker.service; disabled; vendor preset: disabled) # 未加入开机自启动 Active: inactive (dead) # 服务未启动 Docs: https://docs.docker.com[root@docker01 ~]# [root@docker01 ~]# systemctl enable docker.service # 加入开机自启动Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.[root@docker01 ~]# [root@docker01 ~]# systemctl start docker.service # 启动服务[root@docker01 ~]# [root@docker01 ~]# systemctl status docker.service # 查看当前服务状态● docker.service - Docker Application Container Engine Loaded: loaded (/usr/lib/systemd/system/docker.service; enabled; vendor preset: disabled) # 已加入开机自启动 Active: active (running) since Sun 2020-04-12 23:27:00 CST; 2s ago # 服务已启动 Docs: https://docs.docker.com Main PID: 2502 (dockerd) Tasks: 21 Memory: 46.3M CGroup: /system.slice/docker.service ├─2502 /usr/bin/dockerd └─2509 docker-containerd --config /var/run/docker/containerd/containerd.toml 镜像加速器国内从 Docker Hub 拉取镜像有时会遇到困难，此时可以配置镜像加速器。国内很多云服务商都提供了国内加速器服务，例如： 12网易云加速器 https://hub-mirror.c.163.com阿里云加速器(需登录账号获取): https://cr.console.aliyun.com/cn-hangzhou/mirrors 国内各大云服务商均提供了 Docker 镜像加速服务，建议根据运行 Docker 的云平台选择对应的镜像加速服务，具体请参考官方文档。 在CentOS7系统，请在 /etc/docker/daemon.json 中写入如下内容（如果文件不存在请新建该文件） 123456[root@docker01 ~]# vim /etc/docker/daemon.json&#123; &quot;registry-mirrors&quot;: [ &quot;https://hub-mirror.c.163.com&quot; ]&#125; 注意，一定要保证该文件符合 json 规范，否则 Docker 将不能启动。 之后重新启动服务。 12systemctl daemon-reloadsystemctl restart docker 检查加速器是否生效执行如下命令，如果从结果中看到了如下内容，说明配置成功。 12345678[root@docker01 ~]# docker info # 显示整个系统的信息………………Registry Mirrors: https://hub-mirror.c.163.com/Live Restore Enabled: falseWARNING: bridge-nf-call-iptables is disabledWARNING: bridge-nf-call-ip6tables is disabled docker服务启动异常处理123456789101112131415161718[root@docker01 tools]# systemctl start dockerJob for docker.service failed because the control process exited with error code. See &quot;systemctl status docker.service&quot; and &quot;journalctl -xe&quot; for details.[root@docker01 tools]# journalctl -xe # 查询具体信息-- Defined-By: systemd-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel-- -- Unit docker.service has begun starting up.Nov 01 17:40:55 docker01 dockerd[2493]: time=&quot;2018-11-01T17:40:55.181209947+08:00&quot; level=info msg=&quot;libcontainerd: new containerd process, pid: 2501&quot;Nov 01 17:40:56 docker01 dockerd[2493]: time=&quot;2018-11-01T17:40:56.187023899+08:00&quot; level=error msg=&quot;[graphdriver] prior storage driver overlay2 failed: driver not supported&quot;Nov 01 17:40:56 docker01 dockerd[2493]: Error starting daemon: error initializing graphdriver: driver not supportedNov 01 17:40:56 docker01 systemd[1]: docker.service: main process exited, code=exited, status=1/FAILURENov 01 17:40:56 docker01 systemd[1]: Failed to start Docker Application Container Engine.-- Subject: Unit docker.service has failed-- Defined-By: systemd-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel………………# 具体信息如下截图，解决方法如下，之后就可以正常起docker服务了[root@docker01 tools]# mv /var/lib/docker /var/lib/docker.old 推荐阅读1234Docker官网：https://www.docker.com/Docker中文社区：http://www.docker.org.cn/Docker HUB官方镜像仓库：https://hub.docker.com/Docker从入门到实践：https://yeasy.gitbooks.io/docker_practice/content/]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Xshell如何远程连接Linux服务器详解]]></title>
    <url>%2F2020%2F04%2F12%2Fxshell-01%2F</url>
    <content type="text"><![CDATA[如何通过Xshell远程连接Linux服务器，以及如何配置xshell各项参数。 说明：本文讲解通过xshell如何连接Linux服务器，不会讲解如何安装xshell。 创建会话创建会话目录 要求：一个项目则有一个会话目录，目录下对应着项目的所有会话。这样的规划更有利于后期会话的管理，免得所有会话一团糟连自己都看不下去了。 创建会话 会话连接设置 用户身份验证配置 会话终端设置 会话外观配置置 会话日志记录设置 这样所有会话的访问和操作日志都会在本地保留一份。可用于历史追述【这是一个好功能】。 本地电脑保存的日志，可以用于历史追述，日志名称如下： 日志内容具体的格式与信息如下： 会话文件传输【上传下载】配置 建议设置该项，方面文件传输与传输后文件的管理。 会话完成后显示的信息 备注： 如果后期有其他会话需要创建，那么直接复制上面创建好的会话即可，然后编辑「连接」和「用户身份验证」这两项就可了。不必完全重新创建新的会话。 必要的快捷键设置 鼠标选定的文本自动复制到剪贴板 快捷键创建/编辑 可以根据需要创建或修改所需的快捷键。 完毕！]]></content>
      <categories>
        <category>xshell</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>xshell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VMware如何克隆一个虚拟机]]></title>
    <url>%2F2020%2F03%2F15%2FVMware-03%2F</url>
    <content type="text"><![CDATA[如何在Vmware克隆一个虚拟机，并修改哪些配置。 克隆虚拟机步骤其中模板虚拟机的安装部署可参见：「VMware安装Linux CentOS 7.7系统」 找到克隆的模板机，并选择克隆。 进入克隆虚拟机向导 选择现有快照（如果有多个快照，请根据需要选择），而不是虚拟机的当前状态（该状态可能已经被你有意识或无意识改变）。 选择连接克隆 优点：可以使用更少的磁盘，节约磁盘空间。 缺点：不是完整克隆，因此必须保留模板虚拟机。如果模板虚拟机有问题或不存在了，那么链接克隆虚拟机也不能正常使用了。 克隆虚拟机名称和保存路径 克隆虚拟机完毕 必要的配置修改为什么要做必要的配置修改： 1、避免机器之间的IP地址冲突 2、避免主机名hostname一致，产生冲突。 主机名修改123[root@zhang ~]# hostname master[root@zhang ~]# vim /etc/hostname master 操作完毕后，重新登录下就能发现hostname已经改好了。 IP地址修改模板机的IP为：172.16.1.100/10.0.0.100；克隆后的虚拟机IP改为：172.16.1.110/10.0.0.110【也可以改为其他IP地址】 操作步骤如下： 123# vim /etc/sysconfig/network-scripts/ifcfg-eth0 # 将 IPADDR=172.16.1.100 改为 IPADDR=172.16.1.110# vim /etc/sysconfig/network-scripts/ifcfg-eth1 # 将 IPADDR=10.0.0.100 改为 IPADDR=10.0.0.110# systemctl restart network.service 再通过 ifconfig 命令，就可见eth0和eth1的IP已经修改好了。 数据目录创建创建一个目录 /app 【也可以为其他目录，看个人或公司情况】，专门用于存放应用程序、数据或日志。 这里有两种方式：1、只创建目录，然后目录权限为一个普通用户【当前系统已存在的普通用户】；2、创建一个yun普通用户，家目录为/app。 yun用户添加具体如下： 1、运维人员使用的登录账号； 2、所有的业务都放在 /app/ 下「yun用户的家目录」，避免应用程序、产生的数据和日志乱放； 3、因为几乎所有的生产环境都是禁止 root 远程登录的（因此该 yun 用户也进行了 sudo 提权）。 1234567# 使用一个专门的用户，避免直接使用root用户# 添加用户、指定家目录并指定用户密码# sudo提权# 让其它普通用户可以进入该目录查看信息useradd -u 1050 -d /app yun &amp;&amp; echo &apos;123456&apos; | /usr/bin/passwd --stdin yunecho &quot;yun ALL=(ALL) NOPASSWD: ALL&quot; &gt;&gt; /etc/sudoerschmod 755 /app/ 快照管理【重要★★★★★】上面的步骤都操作完毕后，一个克隆虚拟机就已经OK了。 但在我们使用中，可能会把克隆虚拟机给搞乱了，上面安装了一堆乱七八糟的东西，搞得我们自己都头痛不已。这时我们想要恢复到一个干净的环境，那怎么办呢。快照管理这时就可以昂首挺胸，闪亮登场了。 此时我们通过上文的操作后，就得到了一个到手即用的虚拟机。为了保证后续我们能回退到该状态，在此刻我们就需做一个快照。这样只要这个快照不被我们删除，那么我们就能随时可以回到该状态。具体操作步骤如下： 关闭客户机，为了节约磁盘空间，因此我们先关机，之后做快照。 点击快照管理 拍摄快照 写好快照名称与描述。一定要有意义，不然时间久了自己也不清楚是什么东东。 这样一个快照就制作完成了。由于我们是关机制作的快照因此占用磁盘空间小，而且耗时少、速度快。 Vmware虚拟机快照的应用场景为什么需要快照： 就是因为虚拟机的当前状态对我们有意义，后期可能还要回到该状态。比如：操作系统刚安装成功对我们有意义，这时制作一个快照；比如lnmp环境或WordPress系统安装成功对我们有意义，这时再制作一个快照。 快照的最大好处就是：即使我们把系统弄得乱七八糟，甚至系统干崩了，我们也不需要从零开始安装系统。直接通过快照让整个系统恢复到指定快照的那个状态。大大节省了我们的时间。 快照的应用场景举例： 1、虚拟机操作系统安装完毕，来一个快照 2、虚拟机操作系统初始化完毕，基础优化完毕（参见：VMware安装Linux CentOS 7.7系统），来一个快照。这个快照可以用于克隆虚拟机的克隆源。 3、克隆虚拟机初始化完毕，来一个快照（参见前文）。 4、在克隆虚拟机部署完毕一个系统（如：lnmp、WordPress），来一个快照 5、在克隆虚拟机做一个大动作，又不知道结果会怎样，为了保证能够回到操作之前的状态，这时来一个快照。 6、等等…………；只要你需要随时可以来一个快照 备注：建议关机后制作快照，不然耗磁盘空间，耗快照制作时间。 相关阅读「VMware如何创建虚拟机并设置虚拟机网络」 「VMware安装Linux CentOS7.7系统」 完毕！]]></content>
      <categories>
        <category>VMware</category>
        <category>虚拟机</category>
      </categories>
      <tags>
        <tag>VMware</tag>
        <tag>Linux</tag>
        <tag>虚拟机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VMware安装Linux CentOS 7.7系统]]></title>
    <url>%2F2020%2F03%2F15%2FVMware-02%2F</url>
    <content type="text"><![CDATA[如何在Vmware安装Linux CentOS 7.7系统，并且是最小化安装。之后进行必要的配置修改，并实现基础优化。最后做一个快照。 安装Linux CentOS 7.7安装要求：安装后的虚拟机用于服务器，因此要最小化安装，不要安装多余的软件，也不需要安装图形化界面。 虚拟机如何创建参见：「VMware如何创建虚拟机并设置虚拟机网络》」 启动虚拟机 安装CentOS 7选择（通过上下键切换选项）第一个，安装CentOS 7 键盘及语言选择选择键盘模式，选择语言以及语言所在国家（比如：美国英语，英国英语、印度英语、澳大利亚英语、加拿大英语等） 选择时区选择：亚洲/上海 软件安装最小化安装 磁盘分区自己手动分区 /boot 分区磁盘大小分配 / 根分区磁盘大小分配 分区后的结果 分区生效 安装系统并设置root密码 系统配置修改注意：安装好后，没有ifconfig命令 主机名修改不要使用默认的主机名 123[root@localhost ~]# vim /etc/hostnamezhang[root@localhost ~]# hostname zhang 操作完毕后，重新登录下就能发现hostname已经改好了。 网卡修改默认的网卡名不为eth0，eth1。因此要进行修改。 修改 grub 文件并生效修改 /etc/default/grub 文件，并在变量GRUB_CMDLINE_LINU中加入：net.ifnames=0来禁用新的命名规则。 12345678[root@zhang ~]# cat /etc/default/grub GRUB_TIMEOUT=5GRUB_DISTRIBUTOR=&quot;$(sed &apos;s, release .*$,,g&apos; /etc/system-release)&quot;GRUB_DEFAULT=savedGRUB_DISABLE_SUBMENU=trueGRUB_TERMINAL_OUTPUT=&quot;console&quot;GRUB_CMDLINE_LINUX=&quot;net.ifnames=0 crashkernel=auto spectre_v2=retpoline rhgb quiet&quot;GRUB_DISABLE_RECOVERY=&quot;true&quot; 保存后需要重新生成grub配置文件并更新内核参数，为此我们需要运行： 1# grub2-mkconfig -o /etc/grub2.cfg # 结果如下图 对网卡名重命名并修改文件配置网卡重命名 123# cd /etc/sysconfig/network-scripts/# mv ifcfg-ens33 ifcfg-eth0# mv ifcfg-ens37 ifcfg-eth1 eth0（内网）修改后文件内容 12345678910[root@zhang network-scripts]# cat ifcfg-eth0 DEVICE=eth0TYPE=EthernetONBOOT=yesNM_CONTROLLED=yesBOOTPROTO=noneIPV6INIT=yesUSERCTL=noIPADDR=172.16.1.100NETMASK=255.255.255.0 eth1（外网）修改后文件内容 12345678910111213[root@zhang network-scripts]# cat ifcfg-eth1DEVICE=eth1TYPE=EthernetONBOOT=yesNM_CONTROLLED=yesBOOTPROTO=noneIPV6INIT=yesUSERCTL=noIPADDR=10.0.0.100NETMASK=255.255.255.0GATEWAY=10.0.0.2DNS1=223.5.5.5DNS2=223.6.6.6 备注： eth1的IPADDR=10.0.0.100这里是当做外网IP使用，由于是在个人电脑上Vmware创建的虚拟机，不会有真实的公网IP配置。因此用了10.0.0.0/8 网段当做外网IP。 GATEWAY=10.0.0.2 是在Vmware虚拟机全局设置的。参见：「VMware如何创建虚拟机并设置虚拟机网络」 DNS1=223.5.5.5 与 DNS2=223.6.6.6 是阿里云的DNS地址。 最好不要使用谷歌的 8.8.8.8，因为这个国外的IP，可能会被限制。 1# systemctl restart network.service 重启网卡服务，就能生效了。 ifconfig安装 可见上网正常 如何安装ifconfig命令 12# yum search ifconfig # 查询该命令在哪个工具包中# yum install -y net-tools 执行ifconfig命令 安装完之后系统基础优化12# 相关网址：https://github.com/zhanglianghhh/system-install/blob/master/linux-CenetOS7/basedOptimi_7.sh 具体优化点： CentOS base镜像源和epel镜像源 关闭并禁止selinux 关闭并禁止firewall防护墙 创建一个普通用户并可提权（生产环境是不允许直接使用root登录的） 时间同步（同步阿里云的时间服务器） 若无任何操作多久断开连接与保存历史命令记录条数（脚本中是注释的，请根据需要是否去掉注释） 给一些命令添加别名。目的：显示颜色，更加方便查看信息。如：alias grep=’grep –color=auto’；alias ls=’ls –color=auto’等 将用户的操作记录保存到系统日志中。这样如果出现问题，有历史记录可追述 系统文件句柄数设置 内核参数优化 隐藏系统内核参数并设置登录欢迎语 SSH配置优化 必要的包安装。比如：bash-completion、lrzsz、sysstat、nmap、tree、telnet、dos2unix、nc、vim等。 快照管理【重要★★★★★】完成上述步骤后，整个Vmware Linux CentOS 7.7 部署实际已经完毕。但还有重要的一步也必须完成。 该虚拟机我们是用作模板的，后续其他虚拟机是从该虚拟机克隆过去的，并且克隆的就是此刻虚拟机的状态，还有就是为了保证该虚拟机当前状态即使被改变也能进行恢复。因此我们要做一个快照，用于保存虚拟机的当前状态。 关闭客户机，为了节约磁盘空间，因此我们先关机，之后做快照。 选择快照管理 写好快照名称与描述。一定要有意义，不然时间久了自己也不清楚是什么东东。 快照制作完毕 相关阅读「VMware如何创建虚拟机并设置虚拟机网络》」 完毕！]]></content>
      <categories>
        <category>VMware</category>
        <category>虚拟机</category>
      </categories>
      <tags>
        <tag>VMware</tag>
        <tag>Linux</tag>
        <tag>虚拟机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VMware如何创建Linux虚拟机并设置虚拟机网络]]></title>
    <url>%2F2020%2F03%2F15%2FVMware-01%2F</url>
    <content type="text"><![CDATA[如何在Vmware中创建Linux虚拟机并设置Vmware和虚拟机网络 创建Linux虚拟机创建虚拟机 配置类型选择自定义 虚拟机硬件兼容性，选择当前Vmware版本即可 选择要使用的Linux ISO镜像文件，镜像从“阿里云镜像站”下载。 虚拟机命名与虚拟机存放位置 虚拟机CPU核数配置 虚拟机内存设置 虚拟机网络连接选择 虚拟机 I/O控制器类型选择 虚拟机磁盘类型选择 虚拟机使用哪个磁盘 指定磁盘大小，选择是否拆分磁盘为多个文件 虚拟机创建完毕 虚拟机网络设置让Linux 虚拟机拥有两个网卡，其中网卡1为内网（eth0），网卡2为外网（eth1）。 编辑虚拟机设置 添加网络适配器 修改网络适配器的网络连接方式，网卡1（内网）选择：仅主机模式；网卡2（外网）选择：NAT模式 修改完毕后，虚拟机整体配置如下 Vmware设置虚拟网路针对虚拟机网络连接中：仅主机模式和NAT模式，进行设置。这样可以规范所有虚拟机中网络连接所使用的网段。 选择虚拟网络编辑器 仅主机设置 NAT设置 完毕！]]></content>
      <categories>
        <category>VMware</category>
        <category>虚拟机</category>
      </categories>
      <tags>
        <tag>VMware</tag>
        <tag>Linux</tag>
        <tag>虚拟机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux tcpdump 命令详解与示例]]></title>
    <url>%2F2020%2F02%2F20%2Ftcpdump-01%2F</url>
    <content type="text"><![CDATA[命令概要Linux作为网络服务器，特别是作为路由器和网关时，数据的采集和分析是不可少的。TcpDump 是 Linux 中强大的网络数据采集分析工具之一。 用简单的话来定义tcpdump，就是：dump the traffic on a network，根据使用者的定义对网络上的数据包进行截获的包分析工具。 作为互联网上经典的的系统管理员必备工具，tcpdump以其强大的功能，灵活的截取策略，成为每个高级的系统管理员分析网络，排查问题等所必备的工具之一。 TCPDump可以将网络中传送的数据包完全截获下来提供分析。它支持针对网络层、协议、主机、网络或端口的过滤，并提供and、or、not等逻辑语句来帮助你去掉无用的信息。 tcpdump 常用示例查看当前机器有哪些网络接口 12345678# tcpdump -D1.eth02.nflog (Linux netfilter log (NFLOG) interface)3.nfqueue (Linux netfilter queue (NFQUEUE) interface)4.eth15.usbmon1 (USB bus number 1)6.any (Pseudo-device that captures on all interfaces)7.lo [Loopback] 下面所有测试中都有 -i any的选项，表示抓取所有网络接口上的包，为了让测试方便 针对指定主机抓包 1# tcpdump -i any -n -nn host 192.168.1.10 -w ./$(date +%Y%m%d%H%M%S).pcap 针对指定端口抓包 1# tcpdump -i any -n -nn port 80 -w ./$(date +%Y%m%d%H%M%S).pcap 针对主机和端口抓包，两者关系 and 1# tcpdump -i any -n -nn host 192.168.1.10 and port 80 -w ./$(date +%Y%m%d%H%M%S).pcap 针对多个端口抓包 1# tcpdump -i any -n -nn port 111 or port 443 针对多个主机抓包 1# tcpdump -i any -n -nn host www.baidu.com or www.360.com 其他示例： 抓取访问destination 443端口的包 1234# tcpdump -i any -n dst port 443 # 然后我们做一个curl https://www.baidu.com的操作…………19:22:43.049262 IP 120.27.48.179.45008 &gt; 180.101.49.11.https: Flags [.], ack 2997781737, win 229, length 0………… 抓取源端口是80的包 12345# tcpdump -i any -nn src port 80 tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes19:32:45.325115 IP 100.100.45.131.80 &gt; 10.80.151.139.48500: Flags [.], ack 2072960929, win 2915, length 019:32:51.151735 IP 100.100.45.131.80 &gt; 10.80.151.139.48500: Flags [.], ack 959, win 2915, length 0 抓取源或者目标端口都是80的包 12345# tcpdump -i any -n -nn port 8019:35:19.465908 IP 120.27.48.179.40640 &gt; 180.101.49.11.80: Flags [P.], seq 1:78, ack 1, win 229, length 77: HTTP: GET / HTTP/1.119:35:19.487790 IP 180.101.49.11.80 &gt; 120.27.48.179.40640: Flags [.], ack 78, win 908, length 019:35:19.488832 IP 180.101.49.11.80 &gt; 120.27.48.179.40640: Flags [P.], seq 1:2782, ack 78, win 908, length 2781: HTTP: HTTP/1.1 200 OK19:35:19.488857 IP 120.27.48.179.40640 &gt; 180.101.49.11.80: Flags [.], ack 2782, win 272, length 0 表示抓取destination prot 在100到455之间的端口的数据 1234567# tcpdump -i any -n -nn dst portrange 100-455 # 在另外的面做curl https://www.baidu.com 以及 telnet www.baidu.com 111## 部分信息如下19:41:29.534311 IP 120.27.48.179.45588 &gt; 180.101.49.11.443: Flags [S], seq 956630279, win 29200, options [mss 1460,sackOK,…………19:41:29.550033 IP 120.27.48.179.45588 &gt; 180.101.49.11.443: Flags [.], ack 2690465329, win 229, length 0…………19:41:37.128895 IP 120.27.48.179.38202 &gt; 180.101.49.11.111: Flags [S], seq 946466181, win 29200, options [mss 1460,sackOK,TS val 732615405 ecr 0,nop,wscale 7], length 019:41:38.131297 IP 120.27.48.179.38202 &gt; 180.101.49.11.111: Flags [S], seq 946466181, win 29200, options [mss 1460,sackOK,TS val 732616408 ecr 0,nop,wscale 7], length 0 抓取源的端口是20-80的包 1# tcpdump -i any -n src portrange 20-80 抓取端口是20-80的包，不考虑源或目标 1# tcpdump -i any -n portrange 20-80 抓取destination为www.baidu.com的包 1# tcpdump -i any dst www.baidu.com # 然后ping www.baidu.com ,以及 curl www.baidu.com 抓取destination为192.168.1.[0-255]的包 1# tcpdump -i any -n -nn dst 192.168.1 # 可以指定范围 ★★★★★ 注意用法 不是一个完整的IP地址 抓取source为192.168.*.*的包 1# tcpdump -i any -n -nn src host 192.168 # 等价于 tcpdump -i any -n -nn src 192.168 抓取192.168的包(不管是source还是destination ) 1# tcpdump -i any -n -nn host 192.168 抓取包长度小于800的包 1# tcpdump -i any -n -nn less 800 抓取包长度大于800的包 1# tcpdump -i any -n -nn greater 800 只抓取tcp包 1# tcpdump -i any -n tcp 只抓取udp包 1# tcpdump -i any -n udp 只抓取icmp的包，internet控制包 1# tcpdump -i any -n icmp tcpdump 命令格式1# tcpdump option filter option 举例 -n，-i any 等 filter 是过滤包的条件，举例：tcp，portrange 1-1000，src port 58895，host www.itshouce.com.cn, filter可以进行组合 比如：dst port 3306 and src port 58895，portrange 1-1000 or src port 58895 not dst port 3306 option filter 举例：tcpdump -i any -n portrange 1-3306 or portrange 10000-58895 tcpdump 常见选项参见：man tcpdump 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137-A 以ASCII码方式显示每一个数据包(不会显示数据包中链路层头部信息). 在抓取包含网页数据的数据包时, 可方便查看数据(nt: 即Handy for capturing web pages).-B buffer_size/--buffer-size=buffer_size将操作系统捕获缓冲区大小设置为buffer_size，单位为KiB(1024字节)。-c count tcpdump将在接受到count个数据包后退出.-C file-size (nt: 此选项用于配合 -w file 选项使用)该选项使得tcpdump 在把原始数据包直接保存到文件中之前, 检查此文件大小是否超过file-size. 如果超过了, 将关闭此文件,另创一个文件继续用于原始数据包的记录. 新创建的文件名与 -w 选项指定的文件名一致, 但文件名后多了一个数字.该数字会从1开始随着新创建文件的增多而增加. file-size的单位是百万字节(nt: 这里指1,000,000个字节,并非1,048,576个字节, 后者是以1024字节为1k, 1024k字节为1M计算所得, 即1M=1024 ＊ 1024 ＝ 1,048,576)# tcpdump -C 1 -w a.cap-d 以容易阅读的形式,在标准输出上打印出编排过的包匹配码, 随后tcpdump停止.(nt | rt: human readable, 容易阅读的,通常是指以ascii码来打印一些信息. compiled, 编排过的. packet-matching code, 包匹配码,含义未知, 需补充)-dd 以C语言的形式打印出包匹配码.-ddd 以十进制数的形式打印出包匹配码(会在包匹配码之前有一个附加的&apos;count&apos;前缀).-D ★★★★★打印系统中所有tcpdump可以在其上进行抓包的网络接口. 每一个接口会打印出数字编号, 相应的接口名字, 以及可能的一个网络接口描述. 其中网络接口名字和数字编号可以用在tcpdump 的-i flag 选项(nt: 把名字或数字代替flag), 来指定要在其上抓包的网络接口.-e 每行的打印输出中将包括数据包的数据链路层头部信息-E spi@ipaddr algo:secret,... 可通过spi@ipaddr algo:secret 来解密IPsec ESP包(nt | rt:IPsec Encapsulating Security Payload,IPsec 封装安全负载, IPsec可理解为, 一整套对ip数据包的加密协议, ESP 为整个IP 数据包或其中上层协议部分被加密后的数据,前者的工作模式称为隧道模式; 后者的工作模式称为传输模式 . 工作原理, 另需补充). 需要注意的是, 在终端启动tcpdump 时, 可以为IPv4 ESP packets 设置密钥(secret）. 可用于加密的算法包括des-cbc, 3des-cbc, blowfish-cbc, rc3-cbc, cast128-cbc, 或者没有(none).默认的是des-cbc(nt: des, Data Encryption Standard, 数据加密标准, 加密算法未知, 另需补充).secret 为用于ESP 的密钥, 使用ASCII 字符串方式表达. 如果以 0x 开头, 该密钥将以16进制方式读入. 该选项中ESP 的定义遵循RFC2406, 而不是 RFC1827. 并且, 此选项只是用来调试的, 不推荐以真实密钥(secret)来使用该选项, 因为这样不安全: 在命令行中输入的secret 可以被其他人通过ps 等命令查看到. 除了以上的语法格式(nt: 指spi@ipaddr algo:secret), 还可以在后面添加一个语法输入文件名字供tcpdump 使用(nt：即把spi@ipaddr algo:secret,... 中...换成一个语法文件名). 此文件在接受到第一个ESP 包时会打开此文件, 所以最好此时把赋予tcpdump 的一些特权取消(nt: 可理解为, 这样防范之后, 当该文件为恶意编写时,不至于造成过大损害).-f 显示外部的IPv4 地址时(nt: foreign IPv4 addresses, 可理解为, 非本机ip地址), 采用数字方式而不是名字.(此选项是用来对付Sun公司的NIS服务器的缺陷(nt: NIS, 网络信息服务, tcpdump 显示外部地址的名字时会用到她提供的名称服务): 此NIS服务器在查询非本地地址名字时,常常会陷入无尽的查询循环). 由于对外部(foreign)IPv4地址的测试需要用到本地网络接口(nt: tcpdump 抓包时用到的接口)及其IPv4 地址和网络掩码. 如果此地址或网络掩码不可用, 或者此接口根本就没有设置相应网络地址和网络掩码(nt: linux 下的 &apos;any&apos; 网络接口就不需要设置地址和掩码, 不过此&apos;any&apos;接口可以收到系统中所有接口的数据包), 该选项不能正常工作.-F file 使用file 文件作为过滤条件表达式的输入, 此时命令行上的输入将被忽略.-i interface ★★★★★指定tcpdump 需要监听的接口. 如果没有指定, tcpdump 会从系统接口列表中搜寻编号最小的已配置好的接口(不包括 loopback 接口).一但找到第一个符合条件的接口, 搜寻马上结束. 在采用2.2版本或之后版本内核的Linux 操作系统上, &apos;any&apos; 这个虚拟网络接口可被用来接收所有网络接口上的数据包(nt: 这会包括目的是该网络接口的, 也包括目的不是该网络接口的). 需要注意的是如果真实网络接口不能工作在&apos;混杂&apos;模式(promiscuous)下,则无法在&apos;any&apos;这个虚拟的网络接口上抓取其数据包. 如果 -D 标志被指定, tcpdump会打印系统中的接口编号，而该编号就可用于此处的interface 参数.-l 对标准输出进行行缓冲(nt: 使标准输出设备遇到一个换行符就马上把这行的内容打印出来).在需要同时观察抓包打印以及保存抓包记录的时候很有用. 比如, 可通过以下命令组合来达到此目的: tcpdump -l | tee dat 或者 tcpdump -l &gt; dat &amp; tail -f dat (nt: 前者使用tee来把tcpdump 的输出同时放到文件dat和标准输出中, 而后者通过重定向操作&apos;&gt;&apos;, 把tcpdump的输出放到dat 文件中, 同时通过tail把dat文件中的内容放到标准输出中)-L 列出指定网络接口所支持的数据链路层的类型后退出.(nt: 指定接口通过-i 来指定)-m module 通过module 指定的file 装载SMI MIB 模块(nt: SMI，Structure of Management Information, 管理信息结构MIB, Management Information Base, 管理信息库. 可理解为, 这两者用于SNMP(Simple Network Management Protoco)协议数据包的抓取. 具体SNMP 的工作原理未知, 另需补充). 此选项可多次使用, 从而为tcpdump 装载不同的MIB 模块.-M secret 如果TCP 数据包(TCP segments)有TCP-MD5选项(在RFC 2385有相关描述), 则为其摘要的验证指定一个公共的密钥secret.-n ★★★★★不将主机网络地址转换为名称。-nn ★★★★★不将协议和端口号等转换成名称。-N 不打印出host 的域名部分. 比如, 如果设置了此选现, tcpdump 将会打印&apos;nic&apos; 而不是 &apos;nic.ddn.mil&apos;.-#/--number 在行的开头打印一个数据包序号。-O 不运行包匹配编码优化器。当怀疑某些bug是由优化代码引起的, 此选项将很有用.-p 一般情况下, 把网络接口设置为非&apos;混杂&apos;模式. 但必须注意 , 在特殊情况下此网络接口还是会以&apos;混杂&apos;模式来工作； 从而, &apos;-p&apos; 的设与不设, 不能当做以下选现的代名词:&apos;ether host &#123;local-hw-add&#125;&apos; 或 &apos;ether broadcast&apos;(nt: 前者表示只匹配以太网地址为host 的包, 后者表示匹配以太网地址为广播地址的数据包).-Q|-P direction/--direction=direction 选择发送/接收方向 direction 应该捕获数据包的方向。可能的值是 in、out和 inout。并非所有平台都可用。-q 快速(安静?)输出。打印更少的协议信息，因此输出行更短。-R 设定tcpdump 对 ESP/AH 数据包的解析按照 RFC1825而不是RFC1829(nt: AH, 认证头, ESP， 安全负载封装, 这两者会用在IP包的安全传输机制中). 如果此选项被设置, tcpdump 将不会打印出&apos;禁止中继&apos;域(nt: relay prevention field). 另外,由于ESP/AH规范中没有规定ESP/AH数据包必须拥有协议版本号域,所以tcpdump不能从收到的ESP/AH数据包中推导出协议版本号.-r file 从文件file 中读取包数据. 如果file 字段为 &apos;-&apos; 符号, 则tcpdump 会从标准输入中读取包数据.# tcpdump -n -r a.cap-S 打印TCP 数据包的顺序号时, 使用绝对的顺序号, 而不是相对的顺序号.(nt: 相对顺序号可理解为, 相对第一个TCP 包顺序号的差距,比如, 接受方收到第一个数据包的绝对顺序号为232323, 对于后来接收到的第2个,第3个数据包, tcpdump会打印其序列号为1, 2分别表示与第一个数据包的差距为1 和 2. 而如果此时-S 选项被设置, 对于后来接收到的第2个, 第3个数据包会打印出其绝对顺序号:232324, 232325).-s snaplen ★★★★★设置tcpdump的数据包抓取长度为snaplen, 如果不设置默认将会是262144字节【256 Kb】。需要注意的是, 采用长的抓取长度(nt: snaplen比较大), 会增加包的处理时间, 并且会减少tcpdump 可缓存的数据包的数量，从而会导致数据包的丢失。所以, 在能抓取我们想要的包的前提下, 抓取长度越小越好。将snaplen设置为0将其设置为262144的默认值，以便向后兼容最新版本的tcpdump。-T type 强制tcpdump按type指定的协议所描述的包结构来分析收到的数据包. 目前已知的type 可取的协议为: aodv (Ad-hoc On-demand Distance Vector protocol, 按需距离向量路由协议, 在Ad hoc(点对点模式)网络中使用), cnfp (Cisco NetFlow protocol), rpc(Remote Procedure Call), rtp (Real-Time Applications protocol), rtcp (Real-Time Applications con-trol protocol), snmp (Simple Network Management Protocol), tftp (Trivial File Transfer Protocol, 碎文件协议), vat (Visual Audio Tool, 可用于在internet 上进行电 视电话会议的应用层协议), 以及wb (distributed White Board, 可用于网络会议的应用层协议).-t 在每行输出中不打印时间戳-tt 不对每行输出的时间进行格式处理(nt: 这种格式一眼可能看不出其含义, 如时间戳打印成1261798315)-ttt tcpdump 输出时, 每两行打印之间会延迟一个段时间(以毫秒为单位)-tttt 在每行打印的时间戳之前添加日期的打印-u 打印出未加密的NFS 句柄(nt: handle可理解为NFS 中使用的文件句柄, 这将包括文件夹和文件夹中的文件)-U 使得当tcpdump在使用-w 选项时, 其文件写入与包的保存同步.(nt: 即, 当每个数据包被保存时, 它将及时被写入文件中,而不是等文件的输出缓冲已满时才真正写入此文件) -U 标志在老版本的libcap库(nt: tcpdump 所依赖的报文捕获库)上不起作用, 因为其中缺乏pcap_cump_flush()函数.-v 当分析和打印的时候, 产生详细的输出. 比如, 包的生存时间, 标识, 总长度以及IP包的一些选项. 这也会打开一些附加的包完整性检测, 比如对IP或ICMP包头部的校验和.-vv 产生比-v更详细的输出. 比如, NFS回应包中的附加域将会被打印, SMB数据包也会被完全解码.-vvv 产生比-vv更详细的输出. 比如, telent 时所使用的SB, SE 选项将会被打印, 如果telnet同时使用的是图形界面, 其相应的图形选项将会以16进制的方式打印出来(nt: telnet 的SB,SE选项含义未知, 另需补充).-w file ★★★★★把包数据直接写入文件而不进行分析和打印输出. 这些包数据可在随后通过-r 选项来重新读入并进行分析和打印。 file 为 - 时，表示标准输出 也就是输出到标准输出中# tcpdump -w - |strings 这是一个超级有用的命令,把包的数据，用字符展示出来-W 此选项与-C 选项配合使用, 这将限制可打开的文件数目, 并且当文件数据超过这里设置的限制时, 依次循环替代之前的文件, 这相当于一个拥有filecount 个文件的文件缓冲池. 同时, 该选项会使得每个文件名的开头会出现足够多并用来占位的0, 这可以方便这些文件被正确的排序.-x 当分析和打印时, tcpdump 会打印每个包的头部数据, 同时会以16进制打印出每个包的数据(但不包括连接层的头部).总共打印的数据大小不会超过整个数据包的大小与snaplen 中的最小值. 必须要注意的是, 如果高层协议数据没有snaplen 这么长,并且数据链路层(比如, Ethernet层)有填充数据, 则这些填充数据也会被打印.(nt: so for link layers that pad, 未能衔接理解和翻译, 需补充 )-xx tcpdump 会打印每个包的头部数据, 同时会以16进制打印出每个包的数据, 其中包括数据链路层的头部.-X 当分析和打印时, tcpdump 会打印每个包的头部数据, 同时会以16进制和ASCII码形式打印出每个包的数据(但不包括连接层的头部).这对于分析一些新协议的数据包很方便.-XX 当分析和打印时, tcpdump 会打印每个包的头部数据, 同时会以16进制和ASCII码形式打印出每个包的数据, 其中包括数据链路层的头部.这对于分析一些新协议的数据包很方便.-y datalinktype 设置tcpdump 只捕获数据链路层协议类型是datalinktype的数据包-Z user 使tcpdump 放弃自己的超级权限(如果以root用户启动tcpdump, tcpdump将会有超级用户权限), 并把当前tcpdump的用户ID设置为user, 组ID设置为user首要所属组的ID(nt: tcpdump 此处可理解为tcpdump 运行之后对应的进程) 此选项也可在编译的时候被设置为默认打开.(nt: 此时user 的取值未知, 需补充) tcpdump 条件表达式该表达式用于决定哪些数据包将被打印。如果不给定条件表达式，网络上所有被捕获的包都会被打印。否则，只有满足条件表达式的数据包被打印.(nt: all packets, 可理解为，所有被指定接口捕获的数据包)。 表达式由一个或多个表达元组成(nt：primitive，表达元，可理解为组成表达式的基本元素)。一个表达元通常由一个或多个修饰符(qualifiers)后跟一个名字或数字表示的id组成(nt：即 qualifiers id )。有三种不同类型的修饰符：type、dir以及 proto。 参见：man pcap-filter 1234type 修饰符指定id 所代表的对象类型, id可以是名字也可以是数字. 可选的对象类型有: host, net, port 以及portrange(nt: host 表明id是主机, net 表明id是网络, port 表明id是端口，而portrange 表明id 是一个端口范围). 如, &apos;host foo&apos;, &apos;net 128.3&apos;, &apos;port 20&apos;, &apos;portrange 6000-6008&apos;(nt: 分别表示主机 foo,网络 128.3, 端口 20, 端口范围 6000-6008). 如果不指定type 修饰符, id默认的修饰符为host. ★★ 1234dir 修饰符描述id 所对应的传输方向, 即发往id 还是从id 接收（nt: 而id 到底指什么需要看其前面的type 修饰符）。 可取的方向为: src, dst, src or dst, src and dst.(nt:分别表示, id是传输源, id是传输目的, id是传输源或者传输目的, id是传输源并且是传输目的). 例如, &apos;src foo&apos;,&apos;dst net 128.3&apos;, &apos;src or dst port ftp-data&apos;.(nt: 分别表示符合条件的数据包中, 源主机是foo, 目的网络是128.3, 源或目的端口为 ftp-data). 如果不指定dir修饰符, id 默认的修饰符为src or dst。★★ 123456proto 修饰符描述id 所属的协议. 可选的协议有: ether, fddi, tr, wlan, ip, ip6, arp, rarp, decnet, tcp以及 upd. (nt | rt: ether, fddi, tr, 具体含义未知, 需补充. 可理解为物理以太网传输协议, 光纤分布数据网传输协议,以及用于路由跟踪的协议. wlan, 无线局域网协议; ip,ip6 即通常的TCP/IP协议栈中所使用的ipv4以及ipv6网络层协议; arp, rarp 即地址解析协议,反向地址解析协议; decnet, Digital Equipment Corporation开发的, 最早用于PDP-11 机器互联的网络协议; tcp and udp, 即通常TCP/IP协议栈中的两个传输层协议). 表达式顺序：proto dir type 对于修饰符后跟id 的格式，可理解为 type id 是对包最基本的过滤条件：即对包相关的主机，网络，端口的限制；dir 表示对包的传送方向的限制；proto表示对包相关的协议限制 表达元之间还可以通过关键字and，or 以及 not 进行连接，从而可组成比较复杂的条件表达式。比如，host foo and not port ftp and not port ftp-data (nt：其过滤条件可理解为，数据包的主机为foo，并且端口不是ftp(端口21) 和ftp-data(端口20，常用端口和名字的对应可在linux 系统中的/etc/service 文件中找到))。 为了表示方便，同样的修饰符可以被省略，如 tcp dst port ftp or ftp-data or domain 与以下的表达式含义相同 tcp dst port ftp or tcp dst port ftp-data or tcp dst port domain。(nt：其过滤条件可理解为：包的协议为tcp，目的端口为ftp 或 ftp-data 或 domain(端口53) )。]]></content>
      <categories>
        <category>tcpdump</category>
      </categories>
      <tags>
        <tag>tcpdump</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 查看磁盘IO并找出占用IO读写很高的进程]]></title>
    <url>%2F2020%2F02%2F04%2Fdisk-io%2F</url>
    <content type="text"><![CDATA[根据磁盘IO告警，找到占用磁盘IO (util)读写很高的进程。 背景-线上告警线上一台服务器告警，磁盘利用率 disk.util &gt; 90，并持续告警。 登录该服务器后通过 iostat -x 1 10 查看了相关磁盘使用信息。相关截图如下： 12# 如果没有 iostat 命令，那么使用 yum install sysstat 进行安装# iostat -x 1 10 由上图可知，vdb磁盘的 %util【IO】几乎都在100%，原因是频繁的读取数据造成的。 其他字段说明 123456789101112131415161718192021Device：设备名称tps：每秒的IO读、写请求数量，多个逻辑请求可以组合成对设备的单个I/O请求。Blk_read/s (kB_read/s, MB_read/s)：从设备读取的数据量，以每秒若干块(千字节、兆字节)表示。块相当于扇区，因此块大小为512字节。Blk_wrtn/s (kB_wrtn/s, MB_wrtn/s)：写入设备的数据量，以每秒若干块(千字节、兆字节)表示。块相当于扇区，因此块大小为512字节。Blk_read (kB_read, MB_read)：读取块的总数(千字节、兆字节)。Blk_wrtn (kB_wrtn, MB_wrtn)：写入块的总数(千字节，兆字节)。rrqm/s：每秒合并到设备的读请求数。即delta(rmerge)/s wrqm/s：每秒合并到设备的写入请求数。即delta(wmerge)/s r/s：每秒完成的读I/O设备次数。即delta(rio)/s w/s：每秒完成的写I/0设备次数。即delta(wio)/s rsec/s (rkB/s, rMB/s)：每秒读取设备的扇区数(千字节、兆字节)。每扇区大小为512字节wsec/s (wkB/s, wMB/s)：每秒写入设备的扇区数(千字节、兆字节)。每扇区大小为512字节avgrq-sz：平均每次设备I/O操作的数据量(扇区为单位)。即delta(rsec+wsec)/delta(rio+wio) avgqu-sz：平均每次发送给设备的I/O队列长度。await：平均每次IO请求等待时间。(包括等待队列时间和处理时间，毫秒为单位)r_await：平均每次IO读请求等待时间。(包括等待队列时间和处理时间，毫秒为单位)w_await：平均每次IO写请求等待时间。(包括等待队列时间和处理时间，毫秒为单位)svctm：平均每次设备I/O操作的处理时间(毫秒)。警告！不要再相信这个字段值，这个字段将在将来的sysstat版本中删除。 %util：一秒中有百分之多少的时间用于I/O操作，或者说一秒中有多少时间I/O队列是非空的。当该值接近100%时，设备饱和发生。 找到 IO 占用高的进程通过 iotop 命令如果没有该命令，请通过 yum install iotop 进行安装。 1# iotop -oP 通过这个命令可以看见比较详细信息，如：进程号，磁盘读取量，磁盘写入量，IO百分比，涉及到的命令是什么「两个都是 grep 命令造成的IO读取量大」。 通过 pidstat 命令12# 命令的含义：展示I/O统计，每秒更新一次# pidstat -d 1 可见其中 grep 命令占用了大量的读IO，之后可根据 PID 查看相关进程信息。 说明：本图与上图的PID不同，原因是上图涉及的进程执行完了，本图是之后执行产生的进程【都执行的同一个脚本】。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>disk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible-实战指南-LNMP环境部署]]></title>
    <url>%2F2020%2F01%2F12%2Fansible-lnmp%2F</url>
    <content type="text"><![CDATA[主机规划 序号 主机名称 操作系统版本 内网IP 外网IP(模拟) 安装软件 0 对外提供访问 CentOS7.5 10.0.0.170 keepalived【无主机，只有一个虚拟VIP】 1 ansi-haproxy01 CentOS7.5 172.16.1.181 10.0.0.181 zabbix-agent、haproxy 2 ansi-haproxy02 CentOS7.5 172.16.1.182 10.0.0.182 zabbix-agent、haproxy 3 ansi-web01 CentOS7.5 172.16.1.183 10.0.0.183 zabbix-agent、nginx+php、mysql(master) 4 ansi-web02 CentOS7.5 172.16.1.184 10.0.0.184 zabbix-agent、nginx+php、mysql(slave) 5 ansi-web03 CentOS7.5 172.16.1.185 10.0.0.185 zabbix-agent、nginx+php、mysql(slave)、memcached 6 ansi-manager CentOS7.5 172.16.1.180 10.0.0.180 Ansible、zabbix-server、zabbix-agent、mariadb「zabbix使用」 系统初始化：必要的系统初始化 基础组件包括：zabbix监控，mariadb（用于存放zabbix监控信息） 业务组件包括：MySQL、memcached、nginx、PHP、haproxy、keepalived 添加用户账号说明： 1、 运维人员使用的登录账号； 2、 所有的业务都放在 /app/ 下「yun用户的家目录」，避免业务数据乱放； 3、 该用户也被 ansible 使用，因为几乎所有的生产环境都是禁止 root 远程登录的（因此该 yun 用户也进行了 sudo 提权）。 1234567# 使用一个专门的用户，避免直接使用root用户# 添加用户、指定家目录并指定用户密码# sudo提权# 让其它普通用户可以进入该目录查看信息useradd -u 1050 -d /app yun &amp;&amp; echo &apos;123456&apos; | /usr/bin/passwd --stdin yunecho &quot;yun ALL=(ALL) NOPASSWD: ALL&quot; &gt;&gt; /etc/sudoerschmod 755 /app/ 备注：记得在管理机 172.16.1.180 上实现对其他机器的免密登录。 Ansible 配置清单Inventory1234567891011121314151617181920212223242526272829303132333435363738394041[yun@ansi-manager ansible_info]$ cat hosts_key# 业务组件分组[manageservers]172.16.1.180:22[keepalivedserver]172.16.1.181172.16.1.182[proxyservers]172.16.1.18[1:2]:22[webservers]172.16.1.183 ansible_ssh_port=22172.16.1.184 ansible_ssh_port=22172.16.1.185 ansible_ssh_port=22[memservers]172.16.1.185[dbservers]172.16.1.183172.16.1.184172.16.1.185[dbservers_master]172.16.1.183[dbservers_slave]172.16.1.184172.16.1.185# 基础组件分组## 数据库组件分组[zabbixdbserver]172.16.1.180## 基础业务组件分组[zabbixserver]172.16.1.180 系统架构 实战项目GitHub地址该项目已经放在了GitHub上，地址如下： 1https://github.com/zhanglianghhh/ansible-example-lnmp 如需要请自行访问或下载。 项目任务分解获取需求并拿到机器的时候，这时需要我们做如下分析： 1、主机规划：每台主机用于部署什么模块【本文第一节实际已经规划好了】 2、普通用户创建与提权：如果机器是公司统一初始化的，那么可以不创建普通用户，只需提权即可。 3、ansible管理机到其他机器的免密登录。 4、具体任务分解：包括机器必要的初始化、基础组件部署与业务组件部署。如果公司对机器做了统一的初始化，那么视情况而定。 123456789101112131415161718192021222324## 系统初始化1、基础镜像源与epel镜像源2、必要的包安装3、指定环境变量，如：为history命令添加时间信息；操作命令记录到系统日志4、用户名、主机添加背景色，用于生产环境，这样可以减少人为的误操作5、别名配置，如：alias grep=&apos;grep --color=auto&apos;6、内核参数修改，生产中视情况而定7、创建web站点用户 www。8、创建必要的目录，如：软件包存放目录，后期运维脚本存放目录## 基础组件部署1、yum 安装mariadb，用于存放监控信息2、yum 安装zabbix server3、yum 安装zabbix agent## 业务组件部署1、MySQL 数据库部署2、MySQL 主从实现3、memcached 部署4、nginx 部署5、PHP 部署6、nginx、PHP整合，nginx、PHP、MySQL整合，nginx、PHP、memcached整合7、haproxy 部署8、keepalived 部署 项目编写与后续验证步骤请参见： 1https://github.com/zhanglianghhh/ansible-example-lnmp 这里包含：涉及目录与文件说明；服务部署；停止服务【因为是个人电脑通过虚拟机实现的】；服务验证。 为了避免重复这里就不单独说了，参见上面地址即可。]]></content>
      <categories>
        <category>ansbile</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible Roles详解]]></title>
    <url>%2F2020%2F01%2F09%2Fansible-12%2F</url>
    <content type="text"><![CDATA[主机规划 主机名称 操作系统版本 内网IP 外网IP(模拟) 安装软件 ansi-manager CentOS7.5 172.16.1.180 10.0.0.180 ansible ansi-haproxy01 CentOS7.5 172.16.1.181 10.0.0.181 ansi-haproxy02 CentOS7.5 172.16.1.182 10.0.0.182 ansi-web01 CentOS7.5 172.16.1.183 10.0.0.183 ansi-web02 CentOS7.5 172.16.1.184 10.0.0.184 ansi-web03 CentOS7.5 172.16.1.185 10.0.0.185 添加用户账号说明： 1、 运维人员使用的登录账号； 2、 所有的业务都放在 /app/ 下「yun用户的家目录」，避免业务数据乱放； 3、 该用户也被 ansible 使用，因为几乎所有的生产环境都是禁止 root 远程登录的（因此该 yun 用户也进行了 sudo 提权）。 1234567# 使用一个专门的用户，避免直接使用root用户# 添加用户、指定家目录并指定用户密码# sudo提权# 让其它普通用户可以进入该目录查看信息useradd -u 1050 -d /app yun &amp;&amp; echo &apos;123456&apos; | /usr/bin/passwd --stdin yunecho &quot;yun ALL=(ALL) NOPASSWD: ALL&quot; &gt;&gt; /etc/sudoerschmod 755 /app/ Ansible 配置清单Inventory之后文章都是如下主机配置清单 123456789101112131415[yun@ansi-manager ansible_info]$ pwd/app/ansible_info[yun@ansi-manager ansible_info]$ cat hosts_key # 方式1、主机 + 端口 + 密钥[manageservers]172.16.1.180:22[proxyservers]172.16.1.18[1:2]:22# 方式2：别名 + 主机 + 端口 + 密码[webservers]web01 ansible_ssh_host=172.16.1.183 ansible_ssh_port=22web02 ansible_ssh_host=172.16.1.184 ansible_ssh_port=22web03 ansible_ssh_host=172.16.1.185 ansible_ssh_port=22 Ansible Roles 基本概述前面已经学习了 变量、tasks 和 handlers，那怎样组织 playbook 才是最好的方式呢？ 简单的回答就是：使用 roles。roles 基于一个已知的文件结构，去自动的加载某些 vars_files，tasks 以及 handlers。以便 playbook 更好的调用。相比 playbook，roles 的结构更加的清晰有层次。 假如：无论我们安装什么软件都会安装时间同步服务，那么每个 playbook 都要编写时间同步服务的 task。此时我们可以将时间同步服务 task 写好，等到用的时候再调用即可。 注意事项：在编写 roles 的时候，最好能够将一个 task 拆分为一个文件，方便后续复用「彻底打散」。 Roles 目录结构在 roles 目录下，可以使用如下命令创建目录 1ansible-galaxy init nfs roles # 其中 nfs 为目录名称 这样创建的目录是全目录，但是我们可能只需要部分目录，因此实际应用中大多数都由我们自己创建目录，而不是用命令创建目录。 示例目录构造如下： 123456789101112131415161718192021[yun@ansi-manager tmp]$ tree ././├── sit.yml├── webservers.yml└── roles └── nfs # 角色名称 ├── defaults # 角色默认变量（最低优先级） │ └── main.yml ├── files # 文件存放 ├── handlers # 触发任务 │ └── main.yml ├── meta # 依赖关系 │ └── main.yml ├── README.md # 使用说明 ├── tasks # 具体任务 │ └── main.yml ├── templates # 模板文件 └── vars # 角色其他变量 └── main.yml10 directories, 10 files 目录说明： 1、首先要有 roles 目录，然后在 roles 目录下创建相应的目录。 2、roles 下的目录名最好见文知意，如 common 目录表示基础目录，是必要的；nfs 目录表示安装 nfs 服务；memcached 目录表示安装 memcached 服务；等等。 3、可以根据自身需要创建 roles 下的二级目录，不需要的目录可以不创建，没需要全目录创建。 4、roles 目录下的二级目录中，有些目录必须包含一个 main.yml 文件，以便 ansible 使用。 Roles 依赖关系roles 允许在使用 role 时自动引入其他 role。roles 的依赖关系存储在 role 目录中的 meta/main.yml 文件中。 例如：安装 WordPress 是需要先确保 Nginx 和 PHP 都能正常运行，此时都可以在 WordPress 的 role 中定义依赖 Nginx 和 php-fpm 的 role。 12345[yun@ansi-manager playbook]$ cat /app/roles/wordpress/meta/main.yml---dependencies: - &#123; role: nginx &#125; - &#123; role: php-fpm &#125; 此时 WordPress 的 role 会先执行 Nginx 的 role，然后执行 php-fpm 的 role，最后再执行 WordPress 本身的 role。 Ansible Roles 案例实战-部署 NFS 服务整体目录结构123456789101112131415161718192021222324252627282930[yun@ansi-manager ansible_roles]$ pwd/app/ansible_info/ansible_roles[yun@ansi-manager ansible_roles]$ lltotal 4drwxrwxr-x 2 yun yun 17 Sep 15 19:41 group_vars-rw-rw-r-- 1 yun yun 108 Sep 15 19:37 nfs_server.ymldrwxrwxr-x 4 yun yun 35 Sep 15 18:00 roles[yun@ansi-manager ansible_roles]$ tree # 目录结构.├── group_vars│ └── all├── nfs_server.yml└── roles ├── nfs # 服务端 │ ├── handlers │ │ └── main.yml │ ├── tasks │ │ ├── config.yml │ │ ├── install.yml │ │ ├── main.yml │ │ ├── mkdir.yml │ │ ├── start_NFS.yml │ │ └── start_rpcbind.yml │ └── templates │ └── exports.j2 └── nfs_client # 客户端 └── tasks └── main.yml9 directories, 11 files 服务端信息目录结构 1234567891011121314151617[yun@ansi-manager ansible_roles]$ pwd/app/ansible_info/ansible_roles[yun@ansi-manager ansible_roles]$ tree roles/nfsroles/nfs├── handlers│ └── main.yml├── tasks│ ├── config.yml│ ├── install.yml│ ├── main.yml│ ├── mkdir.yml│ ├── start_NFS.yml│ └── start_rpcbind.yml└── templates └── exports.j24 directories, 8 files tasks任务目录信息 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[yun@ansi-manager ansible_roles]$ cat roles/nfs/tasks/main.yml - include_tasks: install.yml- include_tasks: config.yml- include_tasks: mkdir.yml- include_tasks: start_rpcbind.yml- include_tasks: start_NFS.yml[yun@ansi-manager ansible_roles]$ cat roles/nfs/tasks/install.yml - name: &quot;install package NFS &quot; yum: name: - nfs-utils - rpcbind state: present[yun@ansi-manager ansible_roles]$ cat roles/nfs/tasks/config.yml - name: &quot;NFS server config and edit restart&quot; template: src: exports.j2 dest: /etc/exports owner: root group: root mode: &apos;644&apos; notify: &quot;reload NFS server&quot;[yun@ansi-manager ansible_roles]$ cat roles/nfs/tasks/mkdir.yml - name: &quot;create NFS dir&quot; file: path: /data owner: yun group: yun state: directory recurse: yes[yun@ansi-manager ansible_roles]$ cat roles/nfs/tasks/start_rpcbind.yml - name: &quot;rpcbind server start&quot; systemd: name: rpcbind state: started daemon_reload: yes enabled: yes[yun@ansi-manager ansible_roles]$ cat roles/nfs/tasks/start_NFS.yml - name: &quot;NFS server start&quot; systemd: name: nfs state: started daemon_reload: yes enabled: yes handlers任务目录信息 12345[yun@ansi-manager ansible_roles]$ cat roles/nfs/handlers/main.yml - name: &quot;reload NFS server&quot; systemd: name: nfs state: reloaded 模板目录信息 12[yun@ansi-manager ansible_roles]$ cat roles/nfs/templates/exports.j2 &#123;&#123; nfs_dir &#125;&#125; 172.16.1.0/24(rw,sync,root_squash,all_squash,anonuid=1050,anongid=1050) 客户端信息客户端就比较简单了，就一个挂载任务 12345678[yun@ansi-manager ansible_roles]$ cat roles/nfs_client/tasks/main.yml - name: &quot;mount NFS server&quot; mount: src: 172.16.1.180:&#123;&#123; nfs_dir &#125;&#125; path: /mnt fstype: nfs opts: defaults state: mounted 变量信息12345[yun@ansi-manager ansible_roles]$ pwd/app/ansible_info/ansible_roles[yun@ansi-manager ansible_roles]$ cat group_vars/all # NFS 服务端目录nfs_dir: /data playbook 信息12345678910[yun@ansi-manager ansible_roles]$ cat nfs_server.yml ---# NFS server- hosts: manageservers roles: - nfs- hosts: proxyservers roles: - nfs_client 任务执行123[yun@ansi-manager ansible_roles]$ ansible-playbook -b -i ../hosts_key --syntax-check nfs_server.yml # 语法检测[yun@ansi-manager ansible_roles]$ ansible-playbook -b -i ../hosts_key -C nfs_server.yml # 预执行，测试执行[yun@ansi-manager ansible_roles]$ ansible-playbook -b -i ../hosts_key nfs_server.yml # 执行 Ansible Roles 案例实战-部署 memcached 服务整体目录结构1234567891011121314151617181920[yun@ansi-manager ansible_roles]$ pwd/app/ansible_info/ansible_roles[yun@ansi-manager ansible_roles]$ lltotal 8-rw-rw-r-- 1 yun yun 71 Sep 16 09:05 memcached_server.ymldrwxrwxr-x 5 yun yun 52 Sep 16 08:38 roles[yun@ansi-manager ansible_roles]$ tree roles/roles/└── memcached ├── handlers │ └── main.yml ├── tasks │ ├── config.yml │ ├── install.yml │ ├── main.yml │ └── start.yml └── templates └── memcached.j211 directories, 15 files 服务信息目录结构 1234567891011121314151617181920[yun@ansi-manager memcached]$ pwd/app/ansible_info/ansible_roles/roles/memcached[yun@ansi-manager memcached]$ lltotal 0drwxrwxr-x 2 yun yun 22 Sep 16 08:56 handlersdrwxrwxr-x 2 yun yun 76 Sep 16 08:53 tasksdrwxrwxr-x 2 yun yun 26 Sep 16 08:55 templates[yun@ansi-manager memcached]$ tree.├── handlers│ └── main.yml├── tasks│ ├── config.yml│ ├── install.yml│ ├── main.yml│ └── start.yml└── templates └── memcached.j23 directories, 6 files tasks任务目录信息 12345678910111213141516171819202122232425262728[yun@ansi-manager memcached]$ cat tasks/main.yml - include_tasks: install.yml- include_tasks: config.yml- include_tasks: start.yml[yun@ansi-manager memcached]$ cat tasks/install.yml - name: &quot; install package memcached&quot; yum: name: memcached state: present[yun@ansi-manager memcached]$ cat tasks/config.yml - name: &quot;memcached server config and edit restart&quot; template: src: memcached.j2 dest: /etc/sysconfig/memcached owner: root group: root mode: &apos;644&apos; notify: &quot;restart memcached server&quot;[yun@ansi-manager memcached]$ cat tasks/start.yml - name: &quot;memcached server start&quot; systemd: name: memcached state: started daemon_reload: yes enabled: yes handlers任务目录信息 12345[yun@ansi-manager memcached]$ cat handlers/main.yml - name: &quot;restart memcached server&quot; systemd: name: memcached state: restarted 模板目录信息 123456[yun@ansi-manager memcached]$ cat templates/memcached.j2 PORT=&quot;11211&quot;USER=&quot;memcached&quot;MAXCONN=&quot;1024&quot;CACHESIZE=&quot;&#123;&#123; ansible_memtotal_mb // 2 &#125;&#125;&quot;OPTIONS=&quot;&quot; playbook 信息123456[yun@ansi-manager ansible_roles]$ cat memcached_server.yml ---# memcached server- hosts: manageservers roles: - memcached 任务执行123[yun@ansi-manager ansible_roles]$ ansible-playbook -b -i ../hosts_key --syntax-check memcached_server.yml # 语法检测[yun@ansi-manager ansible_roles]$ ansible-playbook -b -i ../hosts_key -C memcached_server.yml # 预执行，测试执行[yun@ansi-manager ansible_roles]$ ansible-playbook -b -i ../hosts_key memcached_server.yml # 执行 Ansible Roles 案例实战-部署 Rsync 服务整体目录结构12345678910111213141516171819202122232425262728[yun@ansi-manager ansible_roles]$ pwd/app/ansible_info/ansible_roles[yun@ansi-manager ansible_roles]$ lltotal 12drwxrwxr-x 2 yun yun 17 Sep 29 09:33 group_varsdrwxrwxr-x 7 yun yun 86 Sep 29 08:49 roles-rw-rw-r-- 1 yun yun 116 Sep 29 09:50 rsyncd_server.yml[yun@ansi-manager ansible_roles]$ tree roles/roles/├── rsync_client│ ├── tasks│ │ └── main.yml│ └── templates│ └── rsync.password.j2└── rsyncd ├── handlers │ └── main.yml ├── tasks │ ├── config.yml │ ├── install.yml │ ├── main.yml │ ├── mkdir.yml │ └── start_rsyncd.yml └── templates ├── rsyncd.conf.j2 └── rsync.password.j218 directories, 25 files 服务端信息目录结构 1234567891011121314151617[yun@ansi-manager rsyncd]$ pwd/app/ansible_info/ansible_roles/roles/rsyncd[yun@ansi-manager rsyncd]$ tree .├── handlers│ └── main.yml├── tasks│ ├── config.yml│ ├── install.yml│ ├── main.yml│ ├── mkdir.yml│ └── start_rsyncd.yml└── templates ├── rsyncd.conf.j2 └── rsync.password.j23 directories, 8 files tasks任务目录信息 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[yun@ansi-manager rsyncd]$ pwd/app/ansible_info/ansible_roles/roles/rsyncd[yun@ansi-manager rsyncd]$ cat tasks/main.yml - include_tasks: install.yml- include_tasks: config.yml- include_tasks: mkdir.yml- include_tasks: start_rsyncd.yml[yun@ansi-manager rsyncd]$ cat tasks/install.yml - name: &quot;Install package rsync&quot; yum: name: rsync state: present[yun@ansi-manager rsyncd]$ cat tasks/config.yml - name: &quot;rsyncd server config and edit restart&quot; template: src: rsyncd.conf.j2 dest: /etc/rsyncd.conf owner: root group: root mode: &apos;644&apos; notify: &quot;restart rsyncd server&quot;- name: &quot;rsyncd server password file&quot; template: src: rsync.password.j2 dest: /etc/rsync.password owner: root group: root mode: &apos;400&apos;[yun@ansi-manager rsyncd]$ cat tasks/mkdir.yml - name: &quot;create rsync business backup dir&quot; file: path: /backup/busi_data owner: root group: root state: directory recurse: yes- name: &quot;create rsync database backup dir&quot; file: path: /backup/database owner: root group: root state: directory recurse: yes[yun@ansi-manager rsyncd]$ cat tasks/start_rsyncd.yml- name: &quot;rsyncd server start&quot; systemd: name: rsyncd state: started daemon_reload: yes enabled: yes handlers任务目录信息 12345[yun@ansi-manager rsyncd]$ cat handlers/main.yml - name: &quot;restart rsyncd server&quot; systemd: name: rsyncd state: restarted 模板目录信息 1234567891011121314151617181920212223242526272829303132333435363738[yun@ansi-manager rsyncd]$ pwd/app/ansible_info/ansible_roles/roles/rsyncd[yun@ansi-manager rsyncd]$ cat templates/rsyncd.conf.j2 # 文件1# 备注：更多参数与更多详解，参见 man rsyncd.conf#rsync_config---------------startuid = rootgid = rootuse chroot = falsemax connections = 200timeout = 100pid file = /var/run/rsyncd.pidlock file = /var/run/rsync.locklog file = /var/log/rsyncd.logdont compress = *.gz *.tgz *.zip *.z *.Z *.rpm *.deb *.bz2ignore errors = trueread only = falselist = false## 注意为了避免困惑 hosts allow 和 hosts deny 请二选其一hosts allow = 172.16.1.0/24,10.9.0.0/16,120.27.48.179# hosts deny = 10.0.0.0/16# 支持多个认证账号auth users = &#123;&#123; auth_user &#125;&#125;secrets file = /etc/rsync.password# 数据备份 注意 path 目录的权限信息[back_data_module]path = /backup/busi_data/# 数据库备份 注意 path 目录的权限信息[back_db_module]path = /backup/database/#rsync_config---------------end[yun@ansi-manager rsyncd]$ cat templates/rsync.password.j2 # 文件2&#123;&#123; auth_user &#125;&#125;:&#123;&#123; auth_pawd &#125;&#125; 客户端信息123456789101112131415161718192021[yun@ansi-manager rsync_client]$ pwd/app/ansible_info/ansible_roles/roles/rsync_client[yun@ansi-manager rsync_client]$ tree # 目录结构.├── tasks│ └── main.yml└── templates └── rsync.password.j22 directories, 2 files[yun@ansi-manager rsync_client]$ cat tasks/main.yml # tasks 信息- name: &quot;rsync passwrod file config&quot; template: src: rsync.password.j2 dest: /etc/rsync.password owner: root group: root mode: &apos;400&apos;[yun@ansi-manager rsync_client]$ cat templates/rsync.password.j2 # 模板信息&#123;&#123; auth_pawd &#125;&#125; 变量信息12345678[yun@ansi-manager ansible_roles]$ pwd/app/ansible_info/ansible_roles[yun@ansi-manager ansible_roles]$ cat group_vars/all # NFS 服务端目录nfs_dir: /data# rsync daemon 使用auth_user: rsync_backupauth_pawd: rsync_backup_pwd playbook 信息12345678910[yun@ansi-manager ansible_roles]$ cat rsyncd_server.yml ---# rsyncd server- hosts: manageservers roles: - rsyncd- hosts: proxyservers roles: - rsync_client 任务执行123[yun@ansi-manager ansible_roles]$ ansible-playbook -b -i ../hosts_key --syntax-check rsyncd_server.yml # 语法检测[yun@ansi-manager ansible_roles]$ ansible-playbook -b -i ../hosts_key -C rsyncd_server.yml # 预执行，测试执行[yun@ansi-manager ansible_roles]$ ansible-playbook -b -i ../hosts_key rsyncd_server.yml # 执行 Ansible Galaxy1https://galaxy.ansible.com]]></content>
      <categories>
        <category>ansbile</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible Tests详解]]></title>
    <url>%2F2020%2F01%2F09%2Fansible-11%2F</url>
    <content type="text"><![CDATA[主机规划 主机名称 操作系统版本 内网IP 外网IP(模拟) 安装软件 ansi-manager CentOS7.5 172.16.1.180 10.0.0.180 ansible ansi-haproxy01 CentOS7.5 172.16.1.181 10.0.0.181 ansi-haproxy02 CentOS7.5 172.16.1.182 10.0.0.182 ansi-web01 CentOS7.5 172.16.1.183 10.0.0.183 ansi-web02 CentOS7.5 172.16.1.184 10.0.0.184 ansi-web03 CentOS7.5 172.16.1.185 10.0.0.185 添加用户账号说明： 1、 运维人员使用的登录账号； 2、 所有的业务都放在 /app/ 下「yun用户的家目录」，避免业务数据乱放； 3、 该用户也被 ansible 使用，因为几乎所有的生产环境都是禁止 root 远程登录的（因此该 yun 用户也进行了 sudo 提权）。 1234567# 使用一个专门的用户，避免直接使用root用户# 添加用户、指定家目录并指定用户密码# sudo提权# 让其它普通用户可以进入该目录查看信息useradd -u 1050 -d /app yun &amp;&amp; echo &apos;123456&apos; | /usr/bin/passwd --stdin yunecho &quot;yun ALL=(ALL) NOPASSWD: ALL&quot; &gt;&gt; /etc/sudoerschmod 755 /app/ Ansible 配置清单Inventory之后文章都是如下主机配置清单 123456789101112131415[yun@ansi-manager ansible_info]$ pwd/app/ansible_info[yun@ansi-manager ansible_info]$ cat hosts_key # 方式1、主机 + 端口 + 密钥[manageservers]172.16.1.180:22[proxyservers]172.16.1.18[1:2]:22# 方式2：别名 + 主机 + 端口 + 密码[webservers]web01 ansible_ssh_host=172.16.1.183 ansible_ssh_port=22web02 ansible_ssh_host=172.16.1.184 ansible_ssh_port=22web03 ansible_ssh_host=172.16.1.185 ansible_ssh_port=22 Tests 概述Tests 在 Jinja 中是一种评估模板表达式，并最终返回 True 或 False。Jinja 中就有自带的 Tests 清单，具体地址如下： 1http://docs.jinkan.org/docs/jinja2/templates.html#builtin-tests tests 和 filters 的主要区别在于Jinja tests 用于比较，而 filters 用于数据操作，两者在Jinja中有不同的应用。 与所有模板一样，tests 总是在 Ansible 控制机上执行，而不是在任务的目标机上，因为它们测验本地数据。 除了 Jinja2 tests 之外，Ansible还提供了一些 tests，用户也可以轻松创建自己的 tests。 测验字符串若要将字符串与子字符串或正则表达式匹配，请使用「match」、「search」或「regex」过滤。 match：必须有开头匹配 search：子串匹配 regex：正则匹配 示例： 12345678910111213141516171819202122232425262728293031323334353637383940414243[yun@ansi-manager ansi_tests]$ pwd/app/ansible_info/ansi_tests[yun@ansi-manager ansi_tests]$ cat tests_str.yml ---- hosts: manageservers vars: url: &quot;http://example.com/users/foo/resources/bar&quot; tasks: - debug: msg: &quot;matched pattern 1-1&quot; when: url is match(&quot;http://example.com/users/.*/resources/.*&quot;) # True - debug: msg: &quot;matched pattern 1-2&quot; when: url is match(&quot;http://example.com&quot;) # True - debug: msg: &quot;matched pattern 1-3&quot; when: url is match(&quot;.*://example.com&quot;) # True - debug: msg: &quot;matched pattern 1-4&quot; when: url is match(&quot;example.com/users/.*/resources/.*&quot;) # False - debug: msg: &quot;matched pattern 2-1&quot; when: url is search(&quot;/users/.*/resources/.*&quot;) # True - debug: msg: &quot;matched pattern 2-2&quot; when: url is search(&quot;/users/&quot;) # True - debug: msg: &quot;matched pattern 2-3&quot; when: url is search(&quot;/user/&quot;) # False - debug: msg: &quot;matched pattern 3&quot; when: url is regex(&quot;example.com/\w+/foo&quot;) # True[yun@ansi-manager ansi_tests]$ ansible-playbook -b -i ../hosts_key tests_str.yml # 注意查看执行 测验版本比较使用「version」，用于版本号比较。 「version」接受的运算符如下： 1&lt;, lt, &lt;=, le, &gt;, gt, &gt;=, ge, ==, =, eq, !=, &lt;&gt;, ne 「version」也可以接受「strict」参数，这个参数默认值为「False」，如果设置为「True」则ansible会进行更严格的版本检查： 1&#123;&#123; sample_version_var is version(&apos;1.0&apos;, operator=&apos;lt&apos;, strict=True) &#125;&#125; 示例： 1234567891011121314151617181920212223242526272829# 判断 ansible_python_version 版本是否 大于等于 2.7.3[yun@ansi-manager ansi_tests]$ pwd/app/ansible_info/ansi_tests[yun@ansi-manager ansi_tests]$ lltotal 8drwxrwxr-x 2 yun yun 35 Sep 12 15:14 file-rw-rw-r-- 1 yun yun 209 Sep 12 15:08 tests_version.yml[yun@ansi-manager ansi_tests]$ cat file/tests_version.conf.j2 # 涉及文件# Jinja2 版本测验&#123;% if ansible_python_version is version(&apos;2.7.3&apos;, &apos;&gt;=&apos;) %&#125;result True. ansible_python_version = &#123;&#123; ansible_python_version &#125;&#125;&#123;% else %&#125;result False&#123;% endif %&#125;[yun@ansi-manager ansi_tests]$ cat tests_version.yml # 涉及的playbook文件---# ansible tests Version Comparison- hosts: proxyservers tasks: - name: &quot;Tests Version Comparison&quot; template: src: ./file/tests_version.conf.j2 dest: /tmp/tests_version.conf[yun@ansi-manager ansi_tests]$ ansible-playbook -b -i ../hosts_key tests_version.yml # 执行 测验子集和超集关键字「superset」和「subset」，用于测验一个列表是否包含或被包含于另一个列表 示例： 1234567891011121314151617181920[yun@ansi-manager ansi_tests]$ pwd/app/ansible_info/ansi_tests[yun@ansi-manager ansi_tests]$ cat tests_set.yml ---# tests 子集和超集- hosts: manageservers vars: a: [1,2,3,4,5] b: [2,3] tasks: - debug: msg: &quot;A includes B&quot; when: a is superset(b) - debug: msg: &quot;B is included in A&quot; when: b is subset(a)[yun@ansi-manager ansi_tests]$ ansible-playbook -b -i ../hosts_key tests_set.yml # 注意查看执行 测验列表真假关键字「all」和「any」，用于检查列表里元素的真假，列表中所有为真或者任何一个为真。 all：一假则假 any：一真则真 1234567891011121314151617181920212223242526[yun@ansi-manager ansi_tests]$ pwd/app/ansible_info/ansi_tests[yun@ansi-manager ansi_tests]$ cat tests_list.yml ---# tests 测验 all any- hosts: manageservers vars: mylist: - 1 - &quot;&#123;&#123; 3 == 3 &#125;&#125;&quot; - True myotherlist: - False - True tasks: - debug: msg: &quot;all are true!&quot; when: mylist is all - debug: msg: &quot;at least one is true&quot; when: myotherlist is any[yun@ansi-manager ansi_tests]$ ansible-playbook -b -i ../hosts_key tests_list.yml # 注意查看执行 测验文件或目录用于测验目录、文件、软连接、是否已存在、是相对路径还是绝对路径等等。 1234567891011121314151617181920212223242526272829303132333435363738394041[yun@ansi-manager ansi_tests]$ pwd/app/ansible_info/ansi_tests[yun@ansi-manager ansi_tests]$ cat tests_path.yml ---- hosts: manageservers vars: # - mypath: /tmp/ - mypath: /tmp/yum_hard.sh - path2: /tmp/ # - path2: /tmp/yum_hard_2.sh tasks: - debug: msg: &quot;path is a directory&quot; when: mypath is directory - debug: msg: &quot;path is a file&quot; when: mypath is file - debug: msg: &quot;path is a symlink&quot; when: mypath is link - debug: msg: &quot;path already exists&quot; when: mypath is exists - debug: msg: &quot;path is &#123;&#123; (mypath is abs)|ternary(&apos;absolute&apos;,&apos;relative&apos;)&#125;&#125;&quot; - debug: msg: &quot;path is the same file as path2&quot; when: mypath is same_file(path2) - debug: msg: &quot;path is a mount&quot; when: mypath is mount[yun@ansi-manager ansi_tests]$ ansible-playbook -b -i ../hosts_key tests_path.yml # 注意查看执行 测验任务执行结果对任务执行结果进行测验。 123456789101112131415161718192021222324252627282930313233343536373839[yun@ansi-manager ansi_tests]$ pwd/app/ansible_info/ansi_tests[yun@ansi-manager ansi_tests]$ cat tests_result.yml ---- hosts: 172.16.1.180 ## 对如下3种情况一次测验 tasks: - shell: /usr/bin/foo #- shell: /usr/bin/true #- shell: /usr/bin/false register: result ignore_errors: True - debug: msg: &quot;&#123;&#123; result &#125;&#125;&quot; - debug: msg: &quot;it failed&quot; when: result is failed # in most cases you&apos;ll want a handler, but if you want to do something right now, this is nice - debug: msg: &quot;it changed&quot; when: result is changed - debug: msg: &quot;it succeeded in Ansible &gt;= 2.1&quot; when: result is succeeded - debug: msg: &quot;it succeeded&quot; when: result is success - debug: msg: &quot;it was skipped&quot; when: result is skipped[yun@ansi-manager ansi_tests]$ ansible-playbook -b -i ../hosts_key tests_result.yml # 注意查看执行]]></content>
      <categories>
        <category>ansbile</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible Jinja2 模板使用]]></title>
    <url>%2F2020%2F01%2F08%2Fansible-10%2F</url>
    <content type="text"><![CDATA[主机规划 主机名称 操作系统版本 内网IP 外网IP(模拟) 安装软件 ansi-manager CentOS7.5 172.16.1.180 10.0.0.180 ansible ansi-haproxy01 CentOS7.5 172.16.1.181 10.0.0.181 ansi-haproxy02 CentOS7.5 172.16.1.182 10.0.0.182 ansi-web01 CentOS7.5 172.16.1.183 10.0.0.183 ansi-web02 CentOS7.5 172.16.1.184 10.0.0.184 ansi-web03 CentOS7.5 172.16.1.185 10.0.0.185 添加用户账号说明： 1、 运维人员使用的登录账号； 2、 所有的业务都放在 /app/ 下「yun用户的家目录」，避免业务数据乱放； 3、 该用户也被 ansible 使用，因为几乎所有的生产环境都是禁止 root 远程登录的（因此该 yun 用户也进行了 sudo 提权）。 1234567# 使用一个专门的用户，避免直接使用root用户# 添加用户、指定家目录并指定用户密码# sudo提权# 让其它普通用户可以进入该目录查看信息useradd -u 1050 -d /app yun &amp;&amp; echo &apos;123456&apos; | /usr/bin/passwd --stdin yunecho &quot;yun ALL=(ALL) NOPASSWD: ALL&quot; &gt;&gt; /etc/sudoerschmod 755 /app/ Ansible 配置清单Inventory之后文章都是如下主机配置清单 123456789101112131415[yun@ansi-manager ansible_info]$ pwd/app/ansible_info[yun@ansi-manager ansible_info]$ cat hosts_key # 方式1、主机 + 端口 + 密钥[manageservers]172.16.1.180:22[proxyservers]172.16.1.18[1:2]:22# 方式2：别名 + 主机 + 端口 + 密码[webservers]web01 ansible_ssh_host=172.16.1.183 ansible_ssh_port=22web02 ansible_ssh_host=172.16.1.184 ansible_ssh_port=22web03 ansible_ssh_host=172.16.1.185 ansible_ssh_port=22 Jinja2 模板概述官网地址 1http://docs.jinkan.org/docs/jinja2/ Jinja2 是一个现代的，设计者友好的，仿照 Django 模板的 Python 模板语言。 它速度快，被广泛使用，并且提供了可选的沙箱模板执行环境保证安全。 Ansible 如何使用 jinja2 模板 Ansible 使用 jinja2 模板，也就是 template 模板。该模块和 copy 模块一样，都是将文件复制到目标机器，但是区别在于 template 模块可以获取要复制文件中的变量的值，而 copy 则是原封不动的把文件内容复制过去。 实际运用，比如：针对不同的主机定义不同的变量，template 会在将文件分发前读取变量到 jinja2 模板，之后再然后分发到不同的被管理主机上。 Jinja2 常用语法赋值 为变量赋值，优先级高于 playbook 中的优先级。 12&#123;% set username = &apos;zhang&apos; %&#125;&#123;% set navigation = [(&apos;index.html&apos;, &apos;Index&apos;), (&apos;about.html&apos;, &apos;About&apos;)] %&#125; 注释 1&#123;# ... #&#125;：要把模板中一行或多行注释掉，默认的注释语法。 变量 1&#123;&#123; ... &#125;&#125;：把表达式的结果打印到模板上。 你可以使用点（ . ）来访问变量的属性，作为替代，也可以使用所谓的“下标”语 法（ [] ）。如下示例： 12&#123;&#123; foo.bar &#125;&#125;&#123;&#123; foo[&apos;bar&apos;] &#125;&#125; 1示例：&#123;&#123; a_variable &#125;&#125;、&#123;&#123; foo.bar &#125;&#125;、&#123;&#123; foo[&apos;bar&apos;] &#125;&#125; 花括号不是变量的一部分，而是打印语句的重要一部分。 条件判断 Jinja 中的 if 语句可比 Python 中的 if 语句。 在最简单的形式中，你可以测试一个变量是否未定义，为空或 false: 简单形式： 123&#123;% if 条件表达式 %&#125; ……&#123;% endif %&#125; 多分支形式： 1234567&#123;% if 条件表达式 %&#125; ……&#123;% elif 条件表达式 %&#125; ……&#123;% else %&#125; ……&#123;% endif %&#125; 循环语句 for 循环语句 123&#123;% for user in users %&#125; &#123;&#123; user.username &#125;&#125;&#123;% endfor %&#125; 空白控制 默认配置中，模板引擎不会对空白做进一步修改，所以每个空白（空格、制表符、换行符 等等）都会原封不动返回。 此外，你也可以手动剥离模板中的空白。当你在块（比如一个 for 标签、一段注释或变量表达式）的开始或结束放置一个减号（ - ），可以移除块前或块后的空白。如下： 123&#123;% for item in range(1,9) -%&#125; &#123;&#123; item &#125;&#125;&#123;%- endfor %&#125; 输出的所有元素前后不会有任何空白，输出会是 123456789 。 转义 有时想要或甚至必要让 Jinja 忽略部分，而不会把它作为变量或块来处理。那么有如下两种方式： 单行转义：简单方式 需求：把 “{ {“ 作为原始字符串使用，而不是一个变量的开始部分。 1&#123;&#123; &apos;&#123;&#123;&apos; &#125;&#125; 多行转义： 需求：将如下一块代码不进行任何处理，直接打印输出。 1234567&#123;% raw %&#125; &lt;ul&gt; &#123;% for item in seq %&#125; &lt;li&gt;&#123;&#123; item &#125;&#125;&lt;/li&gt; &#123;% endfor %&#125; &lt;/ul&gt;&#123;% endraw %&#125; HTML 手动转义 如果你有一个可能包含 &gt;、&lt;、&amp; 或 &quot; 字符的变量，那么你需要转义它；否则会被 HTML 使用。 转义通过用管道传递到过滤器 |e 来实现，如： 1&#123;&#123; user.username|e &#125;&#125; 。 宏定义 宏类似常规编程语言中的函数。它们用于把常用行为作为可重用的函数，取代手动重复的工作。 如果宏在不同的模板中定义，你需要首先使用 import 。 示例： 12345678## 宏变量顺序和具体内容变量顺序一致「推荐写法」&#123;% macro input(name, value=&apos;&apos;, type=&apos;text&apos;, size=20) -%&#125; &lt;input name=&quot;&#123;&#123; name &#125;&#125;&quot; value=&quot;&#123;&#123; value|e &#125;&#125;&quot; type=&quot;&#123;&#123; type &#125;&#125;&quot; size=&quot;&#123;&#123; size &#125;&#125;&quot;&gt;&#123;%- endmacro %&#125;## 宏变量顺序和具体内容变量的顺序可以交错「不推荐写法」&#123;% macro input(name, value=&apos;&apos;, type=&apos;text&apos;, size=20) -%&#125; &lt;input type=&quot;&#123;&#123; type &#125;&#125;&quot; name=&quot;&#123;&#123; name &#125;&#125;&quot; value=&quot;&#123;&#123; value|e &#125;&#125;&quot; size=&quot;&#123;&#123; size &#125;&#125;&quot;&gt;&#123;%- endmacro %&#125; 宏变量解释： name：默认为空，引用时必填 value：默认为空字符串 type：默认为 text size：默认为 20 在命名空间中，宏之后可以像函数一样调用： 12&lt;p&gt;&#123;&#123; input(&apos;username&apos;) &#125;&#125;&lt;/p&gt; ## 结果为：&lt;p&gt;&lt;input name=&quot;username&quot; value=&quot;&quot; type=&quot;text&quot; size=&quot;20&quot;&gt;&lt;/p&gt;&lt;p&gt;&#123;&#123; input(&apos;password&apos;, type=&apos;password&apos;) &#125;&#125;&lt;/p&gt; ## 结果为：&lt;p&gt;&lt;input name=&quot;password&quot; value=&quot;&quot; type=&quot;password&quot; size=&quot;20&quot;&gt;&lt;/p&gt; import 导入 导入宏，并在不同的模板中使用。 12# 示例：导入 nginx 变量&#123;% from &apos;nginx_var.info&apos; import nginx_package_path, nginx_version %&#125; 过滤器 变量可以通过 过滤器 修改。过滤器与变量用管道符号（ | ）分割，并且也可以用圆括号传递可选参数。多个过滤器可以链式调用，前一个过滤器的输出会被作为后一个过滤器的输入。 1例如 &#123;&#123; name|striptags|title &#125;&#125;、&#123;&#123; list|join(&apos;, &apos;) &#125;&#125; 内置过滤器清单 1http://docs.jinkan.org/docs/jinja2/templates.html#builtin-filters ansible 自带过滤器 1https://docs.ansible.com/ansible/latest/user_guide/playbooks_filters.html?highlight=filter Tests 测验 除了过滤器，所谓的「Tests」也是可用的。要测验一个变量或表达式，你要在变量后加上一个 is 和 Tests 的名称。当然 Tests 也可以接受参数。 内置测验清单 1http://docs.jinkan.org/docs/jinja2/templates.html#builtin-tests ansible 自带测验 1https://docs.ansible.com/ansible/latest/user_guide/playbooks_tests.html 算术 Jinja 允许用计算值。这在模板中很少用到，但是为了完整性允许其存在。 支持如下运算符： 12345678910111213+：把两个对象加到一起。如：&#123;&#123; 1 + 1 &#125;&#125; 等于 2。但是如果两者是字符串或列表，你可以用这种方式来衔接它们【连接字符串推荐使用 ~ 运算符】。-：用第一个数减去第二个数。如：&#123;&#123; 3 - 2 &#125;&#125; 等于 1 。/：对两个数做除法。返回值会是一个浮点数。如：&#123;&#123; 1 / 2 &#125;&#125; 等于 &#123;&#123; 0.5 &#125;&#125; 。//：对两个数做除法，返回整数商。如：&#123;&#123; 20 // 7 &#125;&#125; 等于 2 。%：计算整数除法的余数。如：&#123;&#123; 11 % 7 &#125;&#125; 等于 4 。*：用右边的数乘左边的操作数。如：&#123;&#123; 2 * 2 &#125;&#125; 会返回 4。也可以用于重复一个字符串多次。如：&#123;&#123; &apos;=&apos; * 80 &#125;&#125; 会打印 80 个等号的横条。**：取左操作数的右操作数次幂。如：&#123;&#123; 2 ** 3 &#125;&#125; 会返回 8。 比较 ==：比较两个对象是否相等。 !=：比较两个对象是否不等。 &gt;：如果左边大于右边，返回 true。 &gt;=：如果左边大于等于右边，返回 true。 &lt;：如果左边小于右边，返回 true。 &lt;=：如果左边小于等于右边，返回 true。 逻辑 对于逻辑判断，在 for 过滤或 if 表达式中，它可以用于联合多个表达式： and：如果左操作数和右操作数同为真，返回 true。 or：如果左操作数或右操作数有一个为真，返回 true。 not：对一个表达式取反（见下）。 (expr)：表达式组。 提示：is 和 in 运算符同样支持使用中缀记法：foo is not bar 和 foo not in bar。所有的其它表达式需要前缀记法：not (foo and bar) 。 其它运算符 1234567in：运行序列/映射包含检查。如果左操作数 包含于 右操作数，返回 true 。比如 &#123;&#123; 1 in [1,2,3] &#125;&#125; 会返回 true。is：运行一个 测验。参见上述|：应用一个 过滤器。参见上述~：把所有的操作数转换为字符串，并且连接它们。 &#123;&#123; &quot;Hello &quot; ~ name ~ &quot;!&quot; &#125;&#125; 会返回（假设 name 值为 &apos;John&apos; ） Hello John!。 全局函数 range([start], stop[, step])：返回一个包含整等差级数的列表。 Ansible Jinja2 使用案例-常见功能本例包含：注释、赋值、变量、条件判断、循环、空白控制、转义。 目录结构 123456789[yun@ansi-manager jinja]$ pwd/app/ansible_info/jinja[yun@ansi-manager jinja]$ lltotal 4drwxrwxr-x 2 yun yun 65 Sep 5 19:34 file-rw-rw-r-- 1 yun yun 188 Sep 5 16:45 test_jinja2_01.yml[yun@ansi-manager jinja]$ ll file/total 16-rw-rw-r-- 1 yun yun 1562 Sep 5 19:36 test_jinja2_01.conf.j2 配置文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960[yun@ansi-manager jinja]$ cat file/test_jinja2_01.conf.j2 # 涉及的文件##### 注释、赋值、变量 示例# 为变量赋值，优先级高于 playbook 中的变量赋值&#123;# 注释 支持单行或多行 不会在受控机显示任何注释信息 #&#125;&#123;# 简单赋值 #&#125;&#123;% set username = &apos;zhangsan&apos; %&#125;&#123;# 赋值一个数组 #&#125;&#123;% set info = (&apos;name&apos;, &apos;age&apos;, &apos;address&apos;) %&#125; &#123;# 或者 &#123;% set info = [&apos;name&apos;, &apos;age&apos;, &apos;address&apos;] %&#125; #&#125;&#123;# 赋值一个二维数组 #&#125;&#123;% set navigation = [(&apos;index.html&apos;, &apos;Index&apos;), (&apos;about.html&apos;, &apos;About&apos;)] %&#125;&#123;# 或者如下写法&#123;% set navigation = ((&apos;index.html&apos;, &apos;Index&apos;), (&apos;about.html&apos;, &apos;About&apos;)) %&#125;#&#125;username = &#123;&#123; username &#125;&#125;info[1] = &#123;&#123; info[1] &#125;&#125;info = &#123;&#123; info &#125;&#125;navigation[0][1] = &#123;&#123; navigation[0][1] &#125;&#125;navigation = &#123;&#123; navigation &#125;&#125;# 变量 打印 DNS 信息 信息来自于 Ansible Factshost_DNS1 = &#123;&#123; ansible_dns[&apos;nameservers&apos;][0] &#125;&#125;host_DNS2 = &#123;&#123; ansible_dns.nameservers[0] &#125;&#125;##### 条件判断、循环、空白控制、转义 示例# 条件判断&#123;# &#123;% if username == &apos;zhangsan&apos; %&#125; #&#125;&#123;% if username == &apos;zhangsan1&apos; %&#125; username1&#123;# &#123;% elif username == &apos;zhangsan&apos; %&#125; #&#125;&#123;% elif username == &apos;zhangsan2&apos; %&#125; username2&#123;% else %&#125; username_other&#123;% endif %&#125;# 循环&#123;% for host_dns in ansible_dns[&apos;nameservers&apos;] %&#125; &#123;&#123; host_dns &#125;&#125;&#123;% endfor %&#125;# 去掉前后空白&#123;% for host_dns in ansible_dns[&apos;nameservers&apos;] -%&#125; &#123;&#123; host_dns &#125;&#125;&#123;%- endfor %&#125;# 单行转义&#123;&#123; &apos;&#123;&#123;&apos; &#125;&#125;# 多行转义## 块中的所有代码不做任何处理，直接原样输出&#123;% raw %&#125; &lt;ul&gt; &#123;% for item in range(1,5) %&#125; &lt;li&gt;&#123;&#123; item &#125;&#125;&lt;/li&gt; &#123;% endfor %&#125; &lt;/ul&gt;&#123;% endraw %&#125; playbook 文件 123456789101112[yun@ansi-manager jinja]$ cat test_jinja2_01.yml # playbook 文件---# ansible jinja2 测试案例1- hosts: proxyservers vars: - username: coco tasks: - name: &quot;test jinja2 01&quot; template: src: ./file/test_jinja2_01.conf.j2 dest: /tmp/test_jinja2_01.conf 文件执行 123[yun@ansi-manager jinja]$ ansible-playbook -b -i ../hosts_key --syntax-check test_jinja2_01.yml # 语法检测[yun@ansi-manager jinja]$ ansible-playbook -b -i ../hosts_key -C test_jinja2_01.yml # 预执行，测试执行[yun@ansi-manager jinja]$ ansible-playbook -b -i ../hosts_key test_jinja2_01.yml # 执行 Ansible Jinja2 使用案例-宏与导入本例包含：宏、from 导入 目录结构 12345678910[yun@ansi-manager jinja]$ pwd/app/ansible_info/jinja[yun@ansi-manager jinja]$ lltotal 8drwxrwxr-x 2 yun yun 95 Sep 5 20:00 file-rw-rw-r-- 1 yun yun 196 Sep 5 20:08 test_jinja2_02.yml[yun@ansi-manager jinja]$ ll file/total 12-rw-rw-r-- 1 yun yun 300 Sep 5 20:00 test_jinja2_02.conf.j2-rw-rw-r-- 1 yun yun 209 Sep 5 19:58 test_jinja2_macro.info 宏文件和配置文件 1234567891011121314151617181920[yun@ansi-manager jinja]$ cat file/test_jinja2_macro.info &#123;# 配置文件中使用宏 #&#125;&#123;% macro nginx_package_path(nginx_path=&quot;/tmp/package/nginx&quot;) -%&#125; &#123;&#123; nginx_path &#125;&#125;&#123;%- endmacro %&#125;&#123;% macro nginx_version(version=&apos;1.14.2&apos;) -%&#125; &#123;&#123; version &#125;&#125;&#123;%- endmacro %&#125;[yun@ansi-manager jinja]$ cat file/test_jinja2_02.conf.j2 &#123;# from 导入 nginx 变量 #&#125;&#123;% from &apos;test_jinja2_macro.info&apos; import nginx_package_path, nginx_version %&#125;&#123;# 没有指定，那么就是宏定义的默认值 #&#125;nginx_package_path = &#123;&#123; nginx_package_path() &#125;&#125;&#123;# 有给定值，那么会覆盖默认值 #&#125;nginx_version = &#123;&#123; nginx_version(&quot;1.12.4&quot;) &#125;&#125; playbook 文件 12345678910[yun@ansi-manager jinja]$ cat test_jinja2_02.yml ---# ansible jinja2 测试案例2- hosts: proxyservers tasks: - name: &quot;test jinja2 02&quot; template: src: ./file/test_jinja2_02.conf.j2 dest: /tmp/test_jinja2_02.conf 文件执行 123[yun@ansi-manager jinja]$ ansible-playbook -b -i ../hosts_key --syntax-check test_jinja2_02.yml # 语法检测[yun@ansi-manager jinja]$ ansible-playbook -b -i ../hosts_key -C test_jinja2_02.yml # 预执行，测试执行[yun@ansi-manager jinja]$ ansible-playbook -b -i ../hosts_key test_jinja2_02.yml # 执行 Ansible Jinja2 使用案例-算术、比较、逻辑本例包含：算术、比较、逻辑与其它运算符 目录结构 123456789[yun@ansi-manager jinja]$ pwd/app/ansible_info/jinja[yun@ansi-manager jinja]$ lltotal 8drwxrwxr-x 2 yun yun 95 Sep 5 20:00 file-rw-rw-r-- 1 yun yun 196 Sep 5 20:08 test_jinja2_03.yml[yun@ansi-manager jinja]$ ll file/total 12-rw-rw-r-- 1 yun yun 300 Sep 5 20:00 test_jinja2_03.conf.j2 配置文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172[yun@ansi-manager jinja]$ cat file/test_jinja2_03.conf.j2 ### 算术加法运算: 3.2 + 5.3 = &#123;&#123; 3.2 + 5.3 &#125;&#125;减法运算: 5.3 - 3.2 = &#123;&#123; 5.3 - 3.2 &#125;&#125;除法运算: 5.3 / 3.2 = &#123;&#123; 5.3 / 3.2 &#125;&#125;取商运算: 5.3 // 3.2 = &#123;&#123; 5.3 // 3.2 &#125;&#125;取余运算: 5.3 % 3.2 = &#123;&#123; 5.3 % 3.2 &#125;&#125;乘法运算: 3.2 * 3 = &#123;&#123; 3.2 * 3 &#125;&#125;次幂运算: 3.2 ** 3 = &#123;&#123; 3.2 ** 3 &#125;&#125;重复一个字符多次：&apos;$&apos; * 20 = &#123;&#123; &apos;$&apos; * 20 &#125;&#125;### 比较比较是否相等: &apos;zhang&apos; == &quot;zhang&quot; 为：&#123;&#123; &apos;zhang&apos; == &quot;zhang&quot; &#125;&#125;比较是否相等: &apos;zhang&apos; == &quot;zhang1&quot; 为：&#123;&#123; &apos;zhang&apos; == &quot;zhang1&quot; &#125;&#125;比较是否不等: &apos;zhang&apos; != &quot;zhang1&quot; 为：&#123;&#123; &apos;zhang&apos; != &quot;zhang1&quot; &#125;&#125;比较是否不等: &apos;zhang&apos; != &quot;zhang&quot; 为：&#123;&#123; &apos;zhang&apos; != &quot;zhang&quot; &#125;&#125;左边大于右边: 5.3 &gt; 3.2 为：&#123;&#123; 5.3 &gt; 3.2 &#125;&#125;左边大于右边: 3.2 &gt; 5.3 为：&#123;&#123; 3.2 &gt; 5.3 &#125;&#125;左边大于等于右边: 5.3 &gt;= 5.3 为：&#123;&#123; 5.3 &gt;= 5.3 &#125;&#125;左边大于等于右边: 3.2 &gt;= 5.3 为：&#123;&#123; 3.2 &gt;= 5.3 &#125;&#125;左边小于右边: 5.3 &lt; 3.2 为：&#123;&#123; 5.3 &lt; 3.2 &#125;&#125;左边小于右边: 3.2 &lt; 5.3 为：&#123;&#123; 3.2 &lt; 5.3 &#125;&#125;左边小于等于右边: 5.3 &lt;= 3.2 为：&#123;&#123; 5.3 &lt;= 3.2 &#125;&#125;左边小于等于右边: 5.3 &lt;= 5.3 为：&#123;&#123; 5.3 &lt;= 5.3 &#125;&#125;### 逻辑&#123;% set name1 = &apos;zhang&apos; %&#125;&#123;% set name2 = &quot;zhang&quot; %&#125;&#123;% set name3 = &quot;li&quot; %&#125;##### and 测验&#123;% if name1 == &apos;zhang&apos; and name2 == &apos;zhang&apos; %&#125;结果：True.&#123;% else %&#125;结果：False.&#123;% endif %&#125;&#123;% if name1 == &apos;zhang&apos; and name3 == &apos;zhang&apos; %&#125;结果：True.&#123;% else %&#125;结果：False.&#123;% endif %&#125;##### or 测验&#123;% if name1 == &apos;zhang&apos; or name2 == &apos;zhang&apos; %&#125;结果：True.&#123;% else %&#125;结果：False.&#123;% endif %&#125;&#123;% if name1 == &apos;zhang&apos; or name3 == &apos;zhang&apos; %&#125;结果：True.&#123;% else %&#125;结果：False.&#123;% endif %&#125;##### (expr) 测验&#123;% if name3 == &apos;zhang&apos; or ( name1 == &apos;zhang&apos; and name2 == &apos;zhang&apos; ) %&#125;结果：True.&#123;% else %&#125;结果：False.&#123;% endif %&#125;##### not 测验&#123;% if name3 == &apos;zhang&apos; or not (name1 == &apos;zhang&apos; and name2 == &apos;zhang&apos;) %&#125;结果：True.&#123;% else %&#125;结果：False.&#123;% endif %&#125; playbook 文件 1234567891011[yun@ansi-manager jinja]$ cat test_jinja2_03.yml ---# ansible jinja2 测试案例3- hosts: proxyservers tasks: - name: &quot;test jinja2 03&quot; template: src: ./file/test_jinja2_03.conf.j2 dest: /tmp/test_jinja2_03.conf 文件执行 123[yun@ansi-manager jinja]$ ansible-playbook -b -i ../hosts_key --syntax-check test_jinja2_03.yml # 语法检测[yun@ansi-manager jinja]$ ansible-playbook -b -i ../hosts_key -C test_jinja2_03.yml # 预执行，测试执行[yun@ansi-manager jinja]$ ansible-playbook -b -i ../hosts_key test_jinja2_03.yml # 执行 Ansible Jinja2 使用案例-其它运算本例包含：其它运算符 目录结构 123456789[yun@ansi-manager jinja]$ pwd/app/ansible_info/jinja[yun@ansi-manager jinja]$ lltotal 8drwxrwxr-x 2 yun yun 95 Sep 5 20:00 file-rw-rw-r-- 1 yun yun 196 Sep 5 20:08 test_jinja2_04.yml[yun@ansi-manager jinja]$ ll file/total 12-rw-rw-r-- 1 yun yun 300 Sep 5 20:00 test_jinja2_04.conf.j2 配置文件 1234567891011121314[yun@ansi-manager jinja]$ cat file/test_jinja2_04.conf.j2 # 其他运算符## in左操作数包含于右操作数: &#123;&#123; 1 in [1,2,3] &#125;&#125;左操作数包含于右操作数: &#123;&#123; 10 in [1,2,3] &#125;&#125;## is&#123;% set pawd = &apos;123abc&apos; %&#125;Test: &#123;&#123; pawd is defined &#125;&#125;## ~&#123;% set name = &apos;zhang&apos; %&#125;字符串连接: &quot;Hello&quot; ~ name ~ &apos;!&apos; ~ 123 = &#123;&#123; &quot;Hello&quot; ~ name ~ &apos;!&apos; ~ 123 &#125;&#125; playbook 文件 1234567891011[yun@ansi-manager jinja]$ cat test_jinja2_04.yml ---# ansible jinja2 测试案例4- hosts: proxyservers tasks: - name: &quot;test jinja2 04&quot; template: src: ./file/test_jinja2_04.conf.j2 dest: /tmp/test_jinja2_04.conf 文件执行 123[yun@ansi-manager jinja]$ ansible-playbook -b -i ../hosts_key --syntax-check test_jinja2_04.yml # 语法检测[yun@ansi-manager jinja]$ ansible-playbook -b -i ../hosts_key -C test_jinja2_04.yml # 预执行，测试执行[yun@ansi-manager jinja]$ ansible-playbook -b -i ../hosts_key test_jinja2_04.yml # 执行 Ansible Jinja2 使用案例-过滤器和测验本例包含：filters、tests 和 range。 内置过滤清单 1http://docs.jinkan.org/docs/jinja2/templates.html#builtin-tests 内置测验清单 1http://docs.jinkan.org/docs/jinja2/templates.html#tests 目录结构 123456789[yun@ansi-manager jinja]$ pwd/app/ansible_info/jinja[yun@ansi-manager jinja]$ lltotal 8drwxrwxr-x 2 yun yun 95 Sep 5 20:00 file-rw-rw-r-- 1 yun yun 196 Sep 5 20:08 test_jinja2_05.yml[yun@ansi-manager jinja]$ ll file/total 12-rw-rw-r-- 1 yun yun 300 Sep 5 20:00 test_jinja2_05.conf.j2 配置文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950[yun@ansi-manager jinja]$ cat file/test_jinja2_05.conf.j2 &#123;# 变量定义 #&#125;&#123;% set filter01 = -24.5 %&#125;&#123;% set filter02 = &apos;abcDEGg&apos; %&#125;&#123;% set filter03 = &quot; abCDef G hkIL &quot; %&#125;&#123;% set filter04 = [&apos;physics&apos;, &apos;chemistry&apos;, 1997, 2000] %&#125;# 过滤器 使用## 取绝对值&#123;&#123; filter01|abs() &#125;&#125; 或者 &#123;&#123; filter01|abs &#125;&#125;## 首字符大写，其他小写&#123;&#123; filter02|capitalize() &#125;&#125; 或者 &#123;&#123; filter02|capitalize &#125;&#125;## 去掉两端空格|&#123;&#123; filter03|trim() &#125;&#125;| 或者 |&#123;&#123; filter03|trim &#125;&#125;|## 返回序列的第一个&#123;&#123; filter04|first() &#125;&#125; 或者 &#123;&#123; filter04|first &#125;&#125;## 将序列变为字符串，可以指定每个元素间加入什么字符，默认空默认情况: &#123;&#123; filter04|join() &#125;&#125; 或者 &#123;&#123; filter04|join &#125;&#125;加入字符: &#123;&#123; filter04|join(&apos;|&apos;) &#125;&#125;# Tests 测验 使用## 变量是否定义未定义: &#123;&#123; filter00 is defined &#125;&#125;已定义: &#123;&#123; filter01 is defined &#125;&#125;## 变量是否是数字&#123;&#123; filter01 is number &#125;&#125; === &#123;&#123; filter02 is number &#125;&#125;## 变量是否是小写&#123;&#123; filter02 is lower &#125;&#125;## 变量是否是字符串&#123;&#123; filter02 is string &#125;&#125;## 变量在 if 中的判断是否已定义&#123;# ***** 变量已经被定义，直接判断表达式是 True或False ***** #&#125;&#123;# &#123;% if filter01 %&#125; #&#125;&#123;# 如果变量没有定义，那么执行会报错 #&#125;&#123;# 由于 filter00 之前未定义，因此这里定义为 false，不然执行会报错【生产中会在 playbook中定义】&#123;% set filter00 = false %&#125;&#123;% if filter00 %&#125;#&#125;&#123;# ***** 通过判断变量是否被定义，得到到True或False ***** #&#125;&#123;# &#123;% if filter01 is defined %&#125; #&#125;&#123;% if filter00 is defined %&#125;variable is defined&#123;% else %&#125;variable is undefined&#123;% endif %&#125;# range 使用&#123;% for i in range(1,20) %&#125; server 172.16.1.&#123;&#123; i &#125;&#125;:80&#123;% endfor %&#125; playbook 文件 12345678910[yun@ansi-manager jinja]$ cat test_jinja2_05.yml ---# ansible jinja2 测试案例5- hosts: proxyservers tasks: - name: &quot;test jinja2 05&quot; template: src: ./file/test_jinja2_05.conf.j2 dest: /tmp/test_jinja2_05.conf 文件执行 123[yun@ansi-manager jinja]$ ansible-playbook -b -i ../hosts_key --syntax-check test_jinja2_05.yml # 语法检测[yun@ansi-manager jinja]$ ansible-playbook -b -i ../hosts_key -C test_jinja2_05.yml # 预执行，测试执行[yun@ansi-manager jinja]$ ansible-playbook -b -i ../hosts_key test_jinja2_05.yml # 执行 Ansible Jinja2 使用案例-在playbook中使用本例在 ansible 的 playbook 中使用 jinja2. 目录结构 12345[yun@ansi-manager jinja]$ pwd/app/ansible_info/jinja[yun@ansi-manager jinja]$ lltotal 28-rw-rw-r-- 1 yun yun 1103 Jan 15 21:06 test_jinja2_06.yml playbook 文件 1234567891011121314151617181920212223242526272829303132333435[yun@ansi-manager jinja]$ cat test_jinja2_06.yml ---# 在 playbook 中使用 jinja2 模板- hosts: proxyservers tasks: - name: &quot;test_jinja2_06.info create&quot; file: path: &quot;/tmp/test_jinja2_06.info&quot; owner: yun group: yun state: touch # 这里用了 jinja2 的过滤器，计算，in 判断，赋值，条件判断，循环 - name: &quot;test jinja2 06&quot; blockinfile: path: &quot;/tmp/test_jinja2_06.info&quot; marker: &quot;### &#123;mark&#125; ANSIBLE MANAGED BLOCK jinja2 test ###&quot; block: | name: &quot;&#123;&#123; ansible_eth0[&apos;ipv4&apos;][&apos;address&apos;]|replace(&apos;.&apos;, &apos;-&apos;) &#125;&#125;&quot; host: &quot;&#123;&#123; ansible_eth0[&apos;ipv4&apos;][&apos;address&apos;] &#125;&#125;&quot; count: &quot;3.2 + 5.3 = &#123;&#123; 3.2 + 5.3 &#125;&#125;;5.3 - 3.2 = &#123;&#123; 5.3 - 3.2 &#125;&#125;;5.3 / 3.2 = &#123;&#123; 5.3 / 3.2 &#125;&#125;;5.3 % 3.2 = &#123;&#123; 5.3 % 3.2 &#125;&#125;;3.2 ** 3 = &#123;&#123; 3.2 ** 3 &#125;&#125;&quot; other_count: &quot;&#123;&#123; 1 in [1,2,3] &#125;&#125;&quot; ifelse: &#123;% set name1 = &apos;zhang&apos; %&#125; &#123;% if name1 == &apos;zhang&apos; %&#125; 结果：True. &#123;% else %&#125; 结果：False. &#123;% endif %&#125; loop: &#123;% for i in range(1,20) %&#125; server 172.16.1.&#123;&#123; i &#125;&#125;:80 &#123;% endfor %&#125; 文件执行 123[yun@ansi-manager jinja]$ ansible-playbook -b -i ../hosts_key --syntax-check test_jinja2_06.yml # 语法检测[yun@ansi-manager jinja]$ ansible-playbook -b -i ../hosts_key -C test_jinja2_06.yml # 预执行，测试执行[yun@ansi-manager jinja]$ ansible-playbook -b -i ../hosts_key test_jinja2_06.yml # 执行 受控机结果查看 1234567891011121314151617181920212223242526272829[yun@ansi-haproxy01 ~]$ cat /tmp/test_jinja2_06.info ### BEGIN ANSIBLE MANAGED BLOCK jinja2 test ###name: &quot;172-16-1-181&quot;host: &quot;172.16.1.181&quot;count: &quot;3.2 + 5.3 = 8.5;5.3 - 3.2 = 2.1;5.3 / 3.2 = 1.65625;5.3 % 3.2 = 2.1;3.2 ** 3 = 32.768&quot;other_count: &quot;True&quot;ifelse:结果：False.loop: server 172.16.1.1:80 server 172.16.1.2:80 server 172.16.1.3:80 server 172.16.1.4:80 server 172.16.1.5:80 server 172.16.1.6:80 server 172.16.1.7:80 server 172.16.1.8:80 server 172.16.1.9:80 server 172.16.1.10:80 server 172.16.1.11:80 server 172.16.1.12:80 server 172.16.1.13:80 server 172.16.1.14:80 server 172.16.1.15:80 server 172.16.1.16:80 server 172.16.1.17:80 server 172.16.1.18:80 server 172.16.1.19:80### END ANSIBLE MANAGED BLOCK jinja2 test ###]]></content>
      <categories>
        <category>ansbile</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible playbook Vault 加密]]></title>
    <url>%2F2020%2F01%2F08%2Fansible-09%2F</url>
    <content type="text"><![CDATA[主机规划 主机名称 操作系统版本 内网IP 外网IP(模拟) 安装软件 ansi-manager CentOS7.5 172.16.1.180 10.0.0.180 ansible ansi-haproxy01 CentOS7.5 172.16.1.181 10.0.0.181 ansi-haproxy02 CentOS7.5 172.16.1.182 10.0.0.182 ansi-web01 CentOS7.5 172.16.1.183 10.0.0.183 ansi-web02 CentOS7.5 172.16.1.184 10.0.0.184 ansi-web03 CentOS7.5 172.16.1.185 10.0.0.185 添加用户账号说明： 1、 运维人员使用的登录账号； 2、 所有的业务都放在 /app/ 下「yun用户的家目录」，避免业务数据乱放； 3、 该用户也被 ansible 使用，因为几乎所有的生产环境都是禁止 root 远程登录的（因此该 yun 用户也进行了 sudo 提权）。 1234567# 使用一个专门的用户，避免直接使用root用户# 添加用户、指定家目录并指定用户密码# sudo提权# 让其它普通用户可以进入该目录查看信息useradd -u 1050 -d /app yun &amp;&amp; echo &apos;123456&apos; | /usr/bin/passwd --stdin yunecho &quot;yun ALL=(ALL) NOPASSWD: ALL&quot; &gt;&gt; /etc/sudoerschmod 755 /app/ Ansible 配置清单Inventory之后文章都是如下主机配置清单 123456789101112131415[yun@ansi-manager ansible_info]$ pwd/app/ansible_info[yun@ansi-manager ansible_info]$ cat hosts_key # 方式1、主机 + 端口 + 密钥[manageservers]172.16.1.180:22[proxyservers]172.16.1.18[1:2]:22# 方式2：别名 + 主机 + 端口 + 密码[webservers]web01 ansible_ssh_host=172.16.1.183 ansible_ssh_port=22web02 ansible_ssh_host=172.16.1.184 ansible_ssh_port=22web03 ansible_ssh_host=172.16.1.185 ansible_ssh_port=22 Ansible Vault 概述当我们写的 playbook 中涉及敏感信息，如：数据库账号密码；MQ账号密码；主机账号密码。这时为了防止这些敏感信息泄露，就可以使用 vault 进行加密。 123456789101112131415161718192021[yun@ansi-manager ~]$ ansible-vault -hUsage: ansible-vault [create|decrypt|edit|encrypt|encrypt_string|rekey|view] [options] [vaultfile.yml]Options: --ask-vault-pass ask for vault password -h, --help show this help message and exit --new-vault-id=NEW_VAULT_ID the new vault identity to use for rekey --new-vault-password-file=NEW_VAULT_PASSWORD_FILE new vault password file for rekey --vault-id=VAULT_IDS the vault identity to use --vault-password-file=VAULT_PASSWORD_FILES vault password file -v, --verbose verbose mode (-vvv for more, -vvvv to enable connection debugging) --version show program&apos;s version number, config file location, configured module search path, module location, executable location and exit See &apos;ansible-vault &lt;command&gt; --help&apos; for more information on a specificcommand. 参数说明 create：创建一个加密文件，在创建时会首先要求输入 Vault 密码，之后才能进入文件中编辑。 decrypt：对 vault 加密的文件进行解密。 edit：对 vault 加密文件进行编辑。 encrypt：对提供的文件，进行 vault 加密。 encrypt_string：对提供的字符串进行 vault 加密。 rekey：对已 vault 加密的文件进行免密更改，需要提供之前的密码。 view：查看已加密的文件，需要提供密码。 Ansible Vault 交互式创建加密文件123456789101112131415161718192021222324252627[yun@ansi-manager object06]$ pwd/app/ansible_info/object06[yun@ansi-manager object06]$ ansible-vault create test_vault.ymlNew Vault password: # 输入密码Confirm New Vault password: # 确认密码---# vault test- hosts: proxyservers tasks: - name: &quot;touch file&quot; file: path: /tmp/with_itemstestfile state: touch[yun@ansi-manager object06]$ cat test_vault.yml # 加密后查看$ANSIBLE_VAULT;1.1;AES256336632396365303535643937313631616234623862666131653262353537623434656532353966396138353833366637383066366662666236666338333237610a303263336234303866623834663361393436336464343533343961626430636139643333373433363732326532666132646265643465666262633334353036620a63313631336438353632353137316434643666373966363135316666343438663962363032643163333266633662376538383134333862373961313166656536353734363537306262613661383838646538343366373932303634663366623061383230323733616565666632316536303939373632626631626138306536373963386164646437373364396633323334343630373137366130363064366337393837396664356335363738663130333436656238666233396466393137333064343432623139613936613135363863383832333032306139626637323236306636383135313236636438646166643937613761396564373033623637636166 对已加密的文件进行解密1234567891011121314[yun@ansi-manager object06]$ ansible-vault decrypt test_vault.ymlVault password: Decryption successful[yun@ansi-manager object06]$ [yun@ansi-manager object06]$ cat test_vault.yml # 解密后查看---# vault test- hosts: proxyservers tasks: - name: &quot;touch file&quot; file: path: /tmp/with_itemstestfile state: touch 对已存在文件进行加密12345678910111213141516[yun@ansi-manager object06]$ ansible-vault encrypt test_vault.ymlNew Vault password: Confirm New Vault password: Encryption successful[yun@ansi-manager object06]$ cat test_vault.yml $ANSIBLE_VAULT;1.1;AES256373139646631646134346566663232653764653034336334386130323037333631363162356230663930343836396537343333336432363732343936323937370a363239356233333634303464633539616132643630373138333637386238666437626666626461656465613436316464343838643733386334333162616332320a35303332353864356666656264633462363034393864626466356131656635633939653166326631303635363533613338326561666663623238396464383363613738323464373061636639333238363161656665323366643530383030363835643464366332353731666638346238346437363237383932356230616366636631373866323465613934613037303162626561383038373135616261616137326337633566306633343338306264646139396230613665356264353134373766366462666262363236633762303139643230346231333335393931313330653239643030303139366661353732333961323764613332316535323334343939 对已加密的文件进行编辑1234567891011[yun@ansi-manager object06]$ ansible-vault edit test_vault.ymlVault password: ---# vault test ==- hosts: proxyservers tasks: - name: &quot;touch file&quot; file: path: /tmp/with_itemstestfile state: touch 对已加密文件更改密码12345[yun@ansi-manager object06]$ ansible-vault rekey test_vault.ymlVault password: New Vault password: Confirm New Vault password: Rekey successful 对已加密文件进行查看1234567891011[yun@ansi-manager object06]$ ansible-vault view test_vault.ymlVault password: ---# vault test ==- hosts: proxyservers tasks: - name: &quot;touch file&quot; file: path: /tmp/with_itemstestfile state: touch 对提供的字符串进行加密1234567891011[yun@ansi-manager object06]$ ansible-vault encrypt_string &quot;111 222 333&quot;New Vault password: Confirm New Vault password: !vault | $ANSIBLE_VAULT;1.1;AES256 61343332386237363437623939633334626231613539353566313336306562373538633937363566 6537336166356466666431663037623835643964366137340a336439313066356265666636383430 36613661393232613134333961643936646164396130613663656237393837366566356631353061 3034326337303932610a303232643464633239383563393836306565353835666431363132303835 3635Encryption successful Ansible Vault 非交互式创建密码文件安全使用，记得使用 400 或 600 权限。 12345[yun@ansi-manager object06]$ echo &quot;111111&quot; &gt; vault_pwd[yun@ansi-manager object06]$ echo &quot;123456&quot; &gt; vault_pwd2[yun@ansi-manager object06]$ ll vault_pwd* # 权限 400-r-------- 1 yun yun 7 Aug 30 10:35 vault_pwd-r-------- 1 yun yun 7 Aug 30 10:39 vault_pwd2 创建加密文件12345678910[yun@ansi-manager object06]$ ansible-vault create test_vault02.yml --vault-password-file=vault_pwd---# vault test 2[yun@ansi-manager object06]$ cat test_vault02.yml $ANSIBLE_VAULT;1.1;AES256343563646138646561366163653833613866353163323638616563346432303661363133333763666638666536306162366263333037323231386365316238390a383139623435363738663832623533346665393930363833653330623330396438326162336237646131323039663965346166333263666131313833383761620a38353436356439383630623866613565613762303638653165393162336230613036333161613235393539633233663136653566366266353232386230383434 对已加密的文件进行解密12345[yun@ansi-manager object06]$ ansible-vault decrypt test_vault02.yml --vault-password-file=vault_pwdDecryption successful[yun@ansi-manager object06]$ cat test_vault02.yml ---# vault test 2 对已存在文件进行加密12345678910[yun@ansi-manager object06]$ ansible-vault encrypt test_vault02.yml --vault-password-file=vault_pwdEncryption successful[yun@ansi-manager object06]$ [yun@ansi-manager object06]$ cat test_vault02.yml $ANSIBLE_VAULT;1.1;AES256656530353932303663653636373431376363376636383464633035326231393531373661623965363533393766313339393665386463613831323366623962650a643365653833636663653938613966393230373966353332366632393164313434613465623937313635373138656235343965336539313638363937626635390a30396265336635313837313962323735663765623038656566336462643831613837383338323065346634323632396339323635323766386236623038616233 对已加密的文件进行编辑123[yun@ansi-manager object06]$ ansible-vault edit test_vault02.yml --vault-password-file=vault_pwd---# vault test 2 ## 对已加密文件更改密码12[yun@ansi-manager object06]$ ansible-vault rekey test_vault02.yml --vault-password-file=vault_pwd --new-vault-password-file=vault_pwd2Rekey successful 对已加密文件进行查看123[yun@ansi-manager object06]$ ansible-vault view test_vault02.yml --vault-password-file=vault_pwd2---# vault test 2 ## 对提供的字符串进行加密123456789[yun@ansi-manager object06]$ ansible-vault encrypt_string &quot;test info&quot; --vault-password-file=vault_pwd2!vault | $ANSIBLE_VAULT;1.1;AES256 30313766613263363963316663623664353862623032323331356563626636646239636666343766 6633363733303334373831303732326435396566313066630a373562633530333832613335393835 34396161313862656466353433313835643030633966383032656561343331616234373831623233 6636396135306436640a313531373835663633383665396139343464613861313034386365393137 6133Encryption successful Playbook 使用 vault 文件123456789101112131415# 其中 test_vault.yml 的 vault 密码为 vault_pwd 中的信息[yun@ansi-manager object06]$ ansible-vault view test_vault.yml --vault-password-file=vault_pwd---# vault test ==- hosts: proxyservers tasks: - name: &quot;touch file&quot; file: path: /tmp/with_itemstestfile state: touch[yun@ansi-manager object06]$ ansible-playbook -b -i ../hosts_key --syntax-check test_vault.yml --vault-password-file=vault_pwd # 语法检测[yun@ansi-manager object06]$ ansible-playbook -b -i ../hosts_key -C test_vault.yml --vault-password-file=vault_pwd # 预执行，测试执行[yun@ansi-manager object06]$ ansible-playbook -b -i ../hosts_key test_vault.yml --vault-password-file=vault_pwd # 执行]]></content>
      <categories>
        <category>ansbile</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible playbook 编程]]></title>
    <url>%2F2020%2F01%2F08%2Fansible-08%2F</url>
    <content type="text"><![CDATA[主机规划 主机名称 操作系统版本 内网IP 外网IP(模拟) 安装软件 ansi-manager CentOS7.5 172.16.1.180 10.0.0.180 ansible ansi-haproxy01 CentOS7.5 172.16.1.181 10.0.0.181 ansi-haproxy02 CentOS7.5 172.16.1.182 10.0.0.182 ansi-web01 CentOS7.5 172.16.1.183 10.0.0.183 ansi-web02 CentOS7.5 172.16.1.184 10.0.0.184 ansi-web03 CentOS7.5 172.16.1.185 10.0.0.185 添加用户账号说明： 1、 运维人员使用的登录账号； 2、 所有的业务都放在 /app/ 下「yun用户的家目录」，避免业务数据乱放； 3、 该用户也被 ansible 使用，因为几乎所有的生产环境都是禁止 root 远程登录的（因此该 yun 用户也进行了 sudo 提权）。 1234567# 使用一个专门的用户，避免直接使用root用户# 添加用户、指定家目录并指定用户密码# sudo提权# 让其它普通用户可以进入该目录查看信息useradd -u 1050 -d /app yun &amp;&amp; echo &apos;123456&apos; | /usr/bin/passwd --stdin yunecho &quot;yun ALL=(ALL) NOPASSWD: ALL&quot; &gt;&gt; /etc/sudoerschmod 755 /app/ Ansible 配置清单Inventory之后文章都是如下主机配置清单 123456789101112131415[yun@ansi-manager ansible_info]$ pwd/app/ansible_info[yun@ansi-manager ansible_info]$ cat hosts_key # 方式1、主机 + 端口 + 密钥[manageservers]172.16.1.180:22[proxyservers]172.16.1.18[1:2]:22# 方式2：别名 + 主机 + 端口 + 密码[webservers]web01 ansible_ssh_host=172.16.1.183 ansible_ssh_port=22web02 ansible_ssh_host=172.16.1.184 ansible_ssh_port=22web03 ansible_ssh_host=172.16.1.185 ansible_ssh_port=22 条件判断-whenwhen 判断在 ansible 任务中的使用频率非常高。 例如判断主机是否已经安装指定的软件包；对机器的操作系统进行判断然后再根据不同的方法「yum或apt等」进行软件包安装；根据操作系统的版本判断进行软件包的安装「是安装MySQL还是Mariadb」等。 示例：根据主机名的不同，下载不同的文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182[yun@ansi-manager object04]$ pwd/app/ansible_info/object04[yun@ansi-manager object04]$ lltotal 4-rw-rw-r-- 1 yun yun 950 Oct 26 10:22 test_when.yml[yun@ansi-manager object04]$ cat test_when.yml ---# 根据 hostname 的不同下载不同的图片# 特殊组 all，对所有机器有效- hosts: all tasks: - name: &quot;download picture jvm-01-01.png&quot; get_url: url: http://www.zhangblog.com/uploads/jvm/jvm-01-01.png dest: /tmp/ when: ansible_hostname == &quot;ansi-haproxy01&quot; - name: &quot;download picture jvm-01-02.png&quot; get_url: url: http://www.zhangblog.com/uploads/jvm/jvm-01-02.png dest: /tmp/ when: ansible_hostname == &quot;ansi-haproxy02&quot; - name: &quot;other download picture jvm-01-03.png&quot; get_url: url: http://www.zhangblog.com/uploads/jvm/jvm-01-03.png dest: /tmp/ # 从 facts 中获取的变量，ansible_facts[&apos;ansible_hostname&apos;] != &quot;ansi-haproxy01&quot; 错误写法；ansible_hostname != &quot;ansi-haproxy01&quot; 正确写法 #when: (ansible_hostname != &quot;ansi-haproxy01&quot;) and (ansible_hostname != &quot;ansi-haproxy02&quot;) # 写法一 #或者如下3行 列表之间关系是 (and 与) 等同于上一行 #when: # - ansible_hostname != &quot;ansi-haproxy01&quot; # - ansible_hostname != &quot;ansi-haproxy02&quot; #when: ansible_hostname is not match &quot;ansi-haproxy0*&quot; # 写法二 when: (ansible_hostname is match &quot;ansi-manager&quot;) or (ansible_hostname is match &quot;ansi-web*&quot;) # 写法三[yun@ansi-manager object04]$ ansible-playbook -b -i ../hosts_key --syntax-check test_when.yml # 语法检测[yun@ansi-manager object04]$ ansible-playbook -b -i ../hosts_key -C test_when.yml # 预执行，测试执行[yun@ansi-manager object04]$ ansible-playbook -b -i ../hosts_key test_when.yml # 执行PLAY [all] *******************************************************************************************************TASK [Gathering Facts] *******************************************************************************************ok: [web01]ok: [web02]ok: [web03]ok: [172.16.1.180]ok: [172.16.1.181]ok: [172.16.1.182]TASK [download picture jvm-01-01.png] ****************************************************************************skipping: [172.16.1.180]skipping: [web01]skipping: [web02]skipping: [web03]skipping: [172.16.1.182]changed: [172.16.1.181]TASK [download picture jvm-01-02.png] ****************************************************************************skipping: [172.16.1.180]skipping: [web01]skipping: [web02]skipping: [web03]skipping: [172.16.1.181]changed: [172.16.1.182]TASK [other download picture jvm-01-03.png] **********************************************************************skipping: [172.16.1.181]skipping: [172.16.1.182]changed: [web02]changed: [web01]changed: [172.16.1.180]changed: [web03]PLAY RECAP *******************************************************************************************************172.16.1.180 : ok=2 changed=1 unreachable=0 failed=0 skipped=2 rescued=0 ignored=0172.16.1.181 : ok=2 changed=1 unreachable=0 failed=0 skipped=2 rescued=0 ignored=0172.16.1.182 : ok=2 changed=1 unreachable=0 failed=0 skipped=2 rescued=0 ignored=0web01 : ok=2 changed=1 unreachable=0 failed=0 skipped=2 rescued=0 ignored=0web02 : ok=2 changed=1 unreachable=0 failed=0 skipped=2 rescued=0 ignored=0web03 : ok=2 changed=1 unreachable=0 failed=0 skipped=2 rescued=0 ignored=0 标准循环注意： 1、循环语法有两种：loop 和 with_。 2、loop 是在ansible 2.5 添加的，with_ 是一直存在的，推荐使用 loop。在未来 with_ 可能被弃用。 简单列表循环如果我们需要在 playbook 中启动多个服务，或者下载多个文件；按照之前所学的，那么我们需要写多个 task。但这样会使得 playbook 变得臃肿，因此这时我们就需要引进循环了。 示例：一次启动多个服务，下载多个文件 使用 loop 方式【推荐】 1234567891011121314151617181920212223242526272829303132[yun@ansi-manager object04]$ pwd/app/ansible_info/object04[yun@ansi-manager object04]$ lltotal 20-rw-rw-r-- 1 yun yun 594 Aug 23 22:10 test_loop.yml[yun@ansi-manager object04]$ cat test_loop.yml ---# 启动多个服务 和下载多个文件- hosts: proxyservers tasks: - name: &quot;start httpd, rpcbind, network server&quot; service: name: &quot;&#123;&#123; item &#125;&#125;&quot; # 需要用引号引起来 state: started loop: - httpd - rpcbind - network - name: &quot;download multiple file&quot; get_url: url: &quot;&#123;&#123; item &#125;&#125;&quot; # 需要用引号引起来 dest: /tmp/ loop: - http://www.zhangblog.com/uploads/jvm/jvm-01-01.png - http://www.zhangblog.com/uploads/jvm/jvm-01-02.png - http://www.zhangblog.com/uploads/jvm/jvm-01-03.png[yun@ansi-manager object04]$ ansible-playbook -b -i ../hosts_key --syntax-check test_loop.yml # 语法检测[yun@ansi-manager object04]$ ansible-playbook -b -i ../hosts_key -C test_loop.yml # 预执行，测试执行[yun@ansi-manager object04]$ ansible-playbook -b -i ../hosts_key test_loop.yml # 执行 备注：以上方法可用在 yum 模块中。 使用 with_items 方式 其中 playbook 文件中仅把 loop 变为了 with_items。 1234567891011121314151617181920212223242526272829303132[yun@ansi-manager object04]$ pwd/app/ansible_info/object04[yun@ansi-manager object04]$ lltotal 20-rw-rw-r-- 1 yun yun 594 Aug 23 22:10 test_with_items.yml[yun@ansi-manager object04]$ cat test_with_items.yml ---# 启动多个服务 和下载多个文件- hosts: proxyservers tasks: - name: &quot;start httpd, rpcbind, network server&quot; service: name: &quot;&#123;&#123; item &#125;&#125;&quot; # 需要用引号引起来 state: started with_items: - httpd - rpcbind - network - name: &quot;download multiple file&quot; get_url: url: &quot;&#123;&#123; item &#125;&#125;&quot; # 需要用引号引起来 dest: /tmp/ with_items: - http://www.zhangblog.com/uploads/jvm/jvm-01-01.png - http://www.zhangblog.com/uploads/jvm/jvm-01-02.png - http://www.zhangblog.com/uploads/jvm/jvm-01-03.png[yun@ansi-manager object04]$ ansible-playbook -b -i ../hosts_key --syntax-check test_with_items.yml # 语法检测[yun@ansi-manager object04]$ ansible-playbook -b -i ../hosts_key -C test_with_items.yml # 预执行，测试执行[yun@ansi-manager object04]$ ansible-playbook -b -i ../hosts_key test_with_items.yml # 执行 如果用在 yum 模块中则会报如下弃用告警，因此该方法不适用于 yum 模块。 遍历哈希列表如果我们需要创建多个用户并且每个用户都有指定的附加组；或者要创建多个文件，每个文件属主、属组、权限不一样；或者需要拷贝文件，但是每个文件的位置不一样，且属主、属组、权限不一样等等；那之前所学的简单循环就不能满足我们的需求了。这时「哈希列表循环」就闪亮登场了。 示例： 使用 loop 方式【推荐】 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748[yun@ansi-manager object04]$ pwd/app/ansible_info/object04[yun@ansi-manager object04]$ lltotal 16drwxrwxr-x 2 yun yun 56 Oct 26 16:03 file-rw-rw-r-- 1 yun yun 1205 Oct 26 16:02 test_loop_hash.yml[yun@ansi-manager object04]$ cat file/config_test.conf.j2 111[yun@ansi-manager object04]$ cat file/yml_test_j2.yml 222[yun@ansi-manager object04]$ cat test_loop_hash.yml ---# 使用循环字典创建多个用户，创建多个文件，拷贝多个文件- hosts: proxyservers tasks: - name: &quot;Create multiple user&quot; user: name: &quot;&#123;&#123; item.user &#125;&#125;&quot; groups: &quot;&#123;&#123; item.groups &#125;&#125;&quot; loop: - &#123; user: &quot;testuser1&quot;, groups: &quot;root&quot; &#125; - &#123; user: &quot;testuser2&quot;, groups: &quot;root,yun&quot; &#125; - name: &quot;Create multiple file or dir&quot; file: path: &quot;&#123;&#123; item.path &#125;&#125;&quot; owner: &quot;&#123;&#123; item.owner &#125;&#125;&quot; group: &quot;&#123;&#123; item.group &#125;&#125;&quot; mode: &quot;&#123;&#123; item.mode &#125;&#125;&quot; state: &quot;&#123;&#123; item.state &#125;&#125;&quot; loop: - &#123; path: &quot;/tmp/with_items_testdir&quot;, owner: &quot;yun&quot;, group: &quot;root&quot;, mode: &quot;755&quot;, state: &quot;directory&quot; &#125; - &#123; path: &quot;/tmp/with_items_testfile&quot;, owner: &quot;bin&quot;, group: &quot;bin&quot;, mode: &quot;644&quot;, state: &quot;touch&quot; &#125; - name: &quot;copy multiple file&quot; copy: src: &quot;&#123;&#123; item.src &#125;&#125;&quot; dest: &quot;&#123;&#123; item.dest &#125;&#125;&quot; owner: &quot;&#123;&#123; item.owner &#125;&#125;&quot; group: &quot;&#123;&#123; item.group &#125;&#125;&quot; loop: - &#123; src: &quot;./file/config_test.conf.j2&quot;, dest: &quot;/tmp/with_items_testdir/&quot;, owner: &quot;yun&quot;, group: &quot;root&quot; &#125; - &#123; src: &quot;./file/yml_test_j2.yml&quot;, dest: &quot;/tmp/yml_test.yml&quot;, owner: &quot;yun&quot;, group: &quot;yun&quot; &#125;[yun@ansi-manager object04]$ ansible-playbook -b -i ../hosts_key --syntax-check test_loop_hash.yml # 语法检测[yun@ansi-manager object04]$ ansible-playbook -b -i ../hosts_key -C test_loop_hash.yml # 预执行，测试执行[yun@ansi-manager object04]$ ansible-playbook -b -i ../hosts_key test_loop_hash.yml # 执行 使用 with_items 方式 其中 playbook 文件中仅把 loop 变为了 with_items。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748[yun@ansi-manager object04]$ pwd/app/ansible_info/object04[yun@ansi-manager object04]$ lltotal 16drwxrwxr-x 2 yun yun 56 Oct 26 16:03 file-rw-rw-r-- 1 yun yun 1205 Oct 26 16:02 test_with_items_hash.yml[yun@ansi-manager object04]$ cat file/config_test.conf.j2 111[yun@ansi-manager object04]$ cat file/yml_test_j2.yml 222[yun@ansi-manager object04]$ cat test_with_items_hash.yml ---# 使用循环字典创建多个用户，创建多个文件，拷贝多个文件- hosts: proxyservers tasks: - name: &quot;Create multiple user&quot; user: name: &quot;&#123;&#123; item.user &#125;&#125;&quot; groups: &quot;&#123;&#123; item.groups &#125;&#125;&quot; with_items: - &#123; user: &quot;testuser1&quot;, groups: &quot;root&quot; &#125; - &#123; user: &quot;testuser2&quot;, groups: &quot;root,yun&quot; &#125; - name: &quot;Create multiple file or dir&quot; file: path: &quot;&#123;&#123; item.path &#125;&#125;&quot; owner: &quot;&#123;&#123; item.owner &#125;&#125;&quot; group: &quot;&#123;&#123; item.group &#125;&#125;&quot; mode: &quot;&#123;&#123; item.mode &#125;&#125;&quot; state: &quot;&#123;&#123; item.state &#125;&#125;&quot; with_items: - &#123; path: &quot;/tmp/with_items_testdir&quot;, owner: &quot;yun&quot;, group: &quot;root&quot;, mode: &quot;755&quot;, state: &quot;directory&quot; &#125; - &#123; path: &quot;/tmp/with_items_testfile&quot;, owner: &quot;bin&quot;, group: &quot;bin&quot;, mode: &quot;644&quot;, state: &quot;touch&quot; &#125; - name: &quot;copy multiple file&quot; copy: src: &quot;&#123;&#123; item.src &#125;&#125;&quot; dest: &quot;&#123;&#123; item.dest &#125;&#125;&quot; owner: &quot;&#123;&#123; item.owner &#125;&#125;&quot; group: &quot;&#123;&#123; item.group &#125;&#125;&quot; with_items: - &#123; src: &quot;./file/config_test.conf.j2&quot;, dest: &quot;/tmp/with_items_testdir/&quot;, owner: &quot;yun&quot;, group: &quot;root&quot; &#125; - &#123; src: &quot;./file/yml_test_j2.yml&quot;, dest: &quot;/tmp/yml_test.yml&quot;, owner: &quot;yun&quot;, group: &quot;yun&quot; &#125;[yun@ansi-manager object04]$ ansible-playbook -b -i ../hosts_key --syntax-check test_with_items_hash.yml # 语法检测[yun@ansi-manager object04]$ ansible-playbook -b -i ../hosts_key -C test_with_items_hash.yml # 预执行，测试执行[yun@ansi-manager object04]$ ansible-playbook -b -i ../hosts_key test_with_items_hash.yml # 执行 遍历字典示例： 使用 loop 方式【推荐】 1234567891011121314151617181920212223242526272829[yun@ansi-manager object04]$ pwd/app/ansible_info/object04[yun@ansi-manager object04]$ lltotal 28-rw-rw-r-- 1 yun yun 452 Oct 26 16:46 test_loop_dict.yml[yun@ansi-manager object04]$ cat test_loop_dict.yml ---# 打印信息- hosts: manageservers vars: users: alice: name: Alice Appleworth telephone: 123-456-7890 bob: name: Bob Bananarama telephone: 987-654-3210 tasks: - name: &quot;print user info&quot; debug: msg: &quot;User &#123;&#123; item.key &#125;&#125;, userfullname: &#123;&#123; item.value.name &#125;&#125; (&#123;&#123; item.value.telephone &#125;&#125;)&quot; # 将字典转换为适合循环的项表 第一种方式推荐 loop: &quot;&#123;&#123; users|dict2items &#125;&#125;&quot; #loop: &quot;&#123;&#123; lookup(&apos;dict&apos;, users) &#125;&#125;&quot;[yun@ansi-manager object04]$ ansible-playbook -b -i ../hosts_key --syntax-check test_loop_dict.yml # 语法检测[yun@ansi-manager object04]$ ansible-playbook -b -i ../hosts_key -C test_loop_dict.yml # 预执行，测试执行[yun@ansi-manager object04]$ ansible-playbook -b -i ../hosts_key test_loop_dict.yml # 执行 使用 with_items 方式 12345678910111213141516171819202122232425262728[yun@ansi-manager object04]$ pwd/app/ansible_info/object04[yun@ansi-manager object04]$ lltotal 28-rw-rw-r-- 1 yun yun 458 Oct 26 16:47 test_with_items_dict.yml[yun@ansi-manager object04]$ cat test_with_items_dict.yml ---# 打印信息- hosts: manageservers vars: users: alice: name: Alice Appleworth telephone: 123-456-7890 bob: name: Bob Bananarama telephone: 987-654-3210 tasks: - name: &quot;print user info&quot; debug: msg: &quot;User &#123;&#123; item.key &#125;&#125;, userfullname: &#123;&#123; item.value.name &#125;&#125; (&#123;&#123; item.value.telephone &#125;&#125;)&quot; # with_dict 会直接解析字典 with_dict: &quot;&#123;&#123; users &#125;&#125;&quot;[yun@ansi-manager object04]$ ansible-playbook -b -i ../hosts_key --syntax-check test_with_items_dict.yml # 语法检测[yun@ansi-manager object04]$ ansible-playbook -b -i ../hosts_key -C test_with_items_dict.yml # 预执行，测试执行[yun@ansi-manager object04]$ ansible-playbook -b -i ../hosts_key test_with_items_dict.yml # 执行 变量循环-vars针对yum 安装多个包很有用，其他则会报出警告。 123456789101112131415161718192021222324[yun@ansi-manager object04]$ pwd/app/ansible_info/object04[yun@ansi-manager object04]$ lltotal 36-rw-rw-r-- 1 yun yun 252 Oct 26 17:46 test_cycle_vars.yml[yun@ansi-manager object04]$ cat test_cycle_vars.yml ---# 批量包安装- hosts: proxyservers tasks: - name: &quot;Install multiple packages&quot; yum: name: &quot;&#123;&#123; multi_package &#125;&#125;&quot; state: present vars: multi_package: - tree - nc - tcpdump[yun@ansi-manager object04]$ ansible-playbook -b -i ../hosts_key --syntax-check test_cycle_vars.yml # 语法检测[yun@ansi-manager object04]$ ansible-playbook -b -i ../hosts_key -C test_cycle_vars.yml # 预执行，测试执行[yun@ansi-manager object04]$ ansible-playbook -b -i ../hosts_key test_cycle_vars.yml # 执行 该方法不一定适用于其他模块 触发器-handlers当我们修改了服务的配置文件时，这时我们需要去重启服务，那么 handlers 就可以派上用场了。 注意事项： 1、无论多少个 task 通知了相同的 handlers，handlers 仅会在所有 tasks 结束后运行一次。 2、只有 task 发生改变了才会通知 handlers，没有改变则不会通知和触发 handlers。 3、不能用 handlers 替代 task 。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869[yun@ansi-manager object05]$ pwd/app/ansible_info/object05[yun@ansi-manager object05]$ lltotal 24drwxrwxr-x 2 yun yun 129 Aug 24 11:41 file-rw-rw-r-- 1 yun yun 1029 Aug 24 11:57 test_handlers.yml[yun@ansi-manager object05]$ ll file/ # 涉及配置文件total 20-rw-r--r-- 1 yun yun 11767 Aug 24 11:41 httpd.conf.j2[yun@ansi-manager object05]$ vim file/httpd.conf.j2 # 配置文件修改的地方…………# Change this to Listen on specific IP addresses as shown below to # prevent Apache from glomming onto all bound IP addresses.##Listen 12.34.56.78:80###### 端口改为变量Listen &#123;&#123; httpd_port &#125;&#125;…………[yun@ansi-manager object05]$ cat test_handlers.yml # yml 文件---# 比如安装配置启动 httpd。当我们修改配置文件，重启 httpd 服务# 要求：修改配置，重启一个或多个服务- hosts: proxyservers # 这里为了演示方便，因此变量直接就写在了该文件中 vars: - httpd_port: 8081 tasks: - name: &quot;Install httpd&quot; yum: name: &quot;&#123;&#123; packages &#125;&#125;&quot; state: present vars: packages: - httpd - httpd-tools - name: &quot;Httpd config&quot; template: src: ./file/httpd.conf.j2 dest: /etc/httpd/conf/httpd.conf # 一个通知 # notify: &quot;Restart httpd server&quot; # 多个通知 notify: - &quot;Restart httpd server&quot; - &quot;Restart crond server&quot; - name: &quot;Start httpd server&quot; systemd: name: httpd state: started enabled: yes handlers: - name: &quot;Restart httpd server&quot; systemd: name: httpd state: restarted - name: &quot;Restart crond server&quot; systemd: name: crond state: restarted[yun@ansi-manager object05]$ ansible-playbook -b -i ../hosts_key --syntax-check test_handlers.yml # 语法检测[yun@ansi-manager object05]$ ansible-playbook -b -i ../hosts_key -C test_handlers.yml # 预执行，测试执行[yun@ansi-manager object05]$ ansible-playbook -b -i ../hosts_key test_handlers.yml # 执行 任务标签-tags默认情况下，当我们执行一个 playbook 时，会执行该 playbook 中所有的任务。如果只想执行一个 task 或者部分 task 用于调试或者需求就是执行部分 task。那么可以使用 ansible 的标签（tags）功能给单独 task 或者全部 task 打上标签。之后利用这些标签来指定要运行哪些 playbook 任务，或不运行哪些 playbook 任务。 打标签方式 对一个 task 打一个标签； 对一个 task 打多个标签； 对多个 task 打一个标签 标签如何运用 -t TAGS, --tags=TAGS：执行指定的 tag 标签任务；多个标签使用逗号分开 --skip-tags=SKIP_TAGS：跳过指定标签不执行，执行指定外的 task「标签作用于 task 上，即使该task还有其他标签，这个 task 也不会被执行」；多个标签使用逗号分开 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[yun@ansi-manager object05]$ pwd/app/ansible_info/object05[yun@ansi-manager object05]$ lltotal 8drwxrwxr-x 2 yun yun 27 Oct 26 18:07 file-rw-rw-r-- 1 yun yun 1004 Oct 26 19:21 test_tags.yml[yun@ansi-manager object05]$ cat test_tags.yml ---# tags 标签测试- hosts: proxyservers # 这里为了演示方便，因此变量直接就写在了该文件中 vars: - httpd_port: 8081 tasks: - name: &quot;Install httpd&quot; yum: name: &quot;&#123;&#123; packages &#125;&#125;&quot; state: present vars: packages: - httpd - httpd-tools tags: - httpd_server - httpd_install - name: &quot;Httpd config&quot; template: src: ./file/httpd.conf.j2 dest: /etc/httpd/conf/httpd.conf notify: &quot;Restart httpd server&quot; tags: - httpd_server - httpd_config - name: &quot;Start httpd server&quot; systemd: name: httpd state: started enabled: yes tags: - httpd_server - httpd_start - name: &quot;Create dir&quot; file: path: /tmp/with_items_testdir state: directory tags: create_dir handlers: - name: &quot;Restart httpd server&quot; systemd: name: httpd state: restarted playbook 标签查看 12345678910111213141516171819[yun@ansi-manager object05]$ ansible-playbook -b -i ../hosts_key --syntax-check test_tags.yml # 语法检测## 查看 playbook 中的任务和标签[yun@ansi-manager object05]$ ansible-playbook -b -i ../hosts_key test_tags.yml --list-tasks playbook: test_tags.yml play #1 (proxyservers): proxyservers TAGS: [] tasks: Install httpd TAGS: [httpd_install, httpd_server] Httpd config TAGS: [httpd_config, httpd_server] Start httpd server TAGS: [httpd_server, httpd_start] Create dir TAGS: [create_dir]## 查看 playbook 中的标签[yun@ansi-manager object05]$ ansible-playbook -b -i ../hosts_key test_tags.yml --list-tagsplaybook: test_tags.yml play #1 (proxyservers): proxyservers TAGS: [] TASK TAGS: [create_dir, httpd_config, httpd_install, httpd_server, httpd_start] playbook 执行 12345678910## 单个标签执行[yun@ansi-manager object05]$ ansible-playbook -b -i ../hosts_key test_tags.yml -t httpd_install [yun@ansi-manager object05]$ ansible-playbook -b -i ../hosts_key test_tags.yml -t httpd_server## 多个标签执行[yun@ansi-manager object05]$ ansible-playbook -b -i ../hosts_key test_tags.yml -t httpd_install,httpd_config,httpd_start## 跳过哪些标签不执行「标签作用于 task 上，即使该 task 还有其他标签，这个 task 也不会被执行」[yun@ansi-manager object05]$ ansible-playbook -b -i ../hosts_key test_tags.yml --skip-tags httpd_server[yun@ansi-manager object05]$ ansible-playbook -b -i ../hosts_key test_tags.yml --skip-tags httpd_install,create_dir## 执行整个 playbook [yun@ansi-manager object05]$ ansible-playbook -b -i ../hosts_key test_tags.yml 文件引用/复用-include与import在实际应用中，是不可能将所有 task 写在一个 playbook 中的，需要进行拆分，方便后期重复使用。这样后面写其他 playbook 的时候，如果有重复的，那么直接引用之前写的即可。 Includes 与 Imports 1、include 和 import 虽然功能相近，但是 ansible 执行引擎对他们的处理却截然不同。 2、所有 import* 语句都会在解析 playbook 时进行预处理。「提前准备好工具」 3、所有 include* 语句都是在执行 playbook 时遇到再处理。「需要什么工具，再拿什么工具」 PS：include 模块：这个模块还将支持一段时间，但在不久的将来可能会弃用「最好不要使用这个模块」。 示例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364[yun@ansi-manager object05]$ pwd/app/ansible_info/object05[yun@ansi-manager object05]$ lltotal 32drwxrwxr-x 2 yun yun 103 Aug 24 19:51 file_yml-rw-rw-r-- 1 yun yun 518 Aug 24 19:56 test_include.yml[yun@ansi-manager object05]$ ll file_yml/total 16-rw-rw-r-- 1 yun yun 136 Aug 24 19:51 httpd_config.yml-rw-rw-r-- 1 yun yun 133 Aug 24 19:41 httpd_install.yml-rw-rw-r-- 1 yun yun 80 Aug 24 19:42 httpd_restart.yml-rw-rw-r-- 1 yun yun 93 Aug 24 19:41 httpd_start.yml## 每个小 yml 文件的具体内容[yun@ansi-manager object05]$ cat file_yml/httpd_install.yml - name: &quot;Install httpd&quot; yum: name: &quot;&#123;&#123; packages &#125;&#125;&quot; state: present vars: packages: - httpd - httpd-tools[yun@ansi-manager object05]$ cat file_yml/httpd_config.yml - name: &quot;Httpd config&quot; template: src: ./file/httpd.conf.j2 dest: /etc/httpd/conf/httpd.conf notify: &quot;Restart httpd server&quot;[yun@ansi-manager object05]$ cat file_yml/httpd_start.yml - name: &quot;Start httpd server&quot; systemd: name: httpd state: started enabled: yes[yun@ansi-manager object05]$ cat file_yml/httpd_restart.yml - name: &quot;Restart httpd server&quot; systemd: name: httpd state: restarted###### 主调用 yml 文件内容 ######[yun@ansi-manager object05]$ cat test_include.yml ---# 调用其他 yml 文件- hosts: proxyservers # 这里为了演示方便，因此变量直接就写在了该文件中 vars: - httpd_port: 8083 tasks: - include_tasks: ./file_yml/httpd_install.yml - include_tasks: ./file_yml/httpd_config.yml - include_tasks: ./file_yml/httpd_start.yml handlers: # 使用 import 进行预处理，这样防止 notify 时，在 handlers 找不到对应的信息 - import_tasks: ./file_yml/httpd_restart.yml[yun@ansi-manager object05]$ ansible-playbook -b -i ../hosts_key --syntax-check test_include.yml # 语法检测[yun@ansi-manager object05]$ ansible-playbook -b -i ../hosts_key -C test_include.yml # 预执行，测试执行[yun@ansi-manager object05]$ ansible-playbook -b -i ../hosts_key test_include.yml # 执行 忽略错误-ignore_errors在 playbook 执行过程中，默认情况下如果有错误发生，那么后面的 task 就不执行，并且退出当前的 playbook。 如果我们对某些 task 执行结果不关心，不管执行是否成功，后面的 task 也要继续执行。那就需要通过 ignore_errors 来忽略当前 task 的错误结果，让后面的 task 继续往下执行。 123456789101112131415161718192021222324252627282930313233[yun@ansi-manager object05]$ pwd/app/ansible_info/object05[yun@ansi-manager object05]$ lltotal 36-rw-rw-r-- 1 yun yun 479 Aug 26 09:24 test_ignore_errors.yml[yun@ansi-manager object05]$ cat test_ignore_errors.yml ---# ignore_errors 测试- hosts: proxyservers tasks: - name: &quot;Install httpd&quot; yum: name: &quot;&#123;&#123; packages &#125;&#125;&quot; state: present vars: packages: - httpd - httpd-tools - name: &quot;Shell false&quot; shell: /bin/false # 是否忽略该 task 的错误 「打开或关闭注释，对比」 ignore_errors: True - name: &quot;Create dir&quot; file: path: /tmp/with_items_testdir state: directory[yun@ansi-manager object05]$ ansible-playbook -b -i ../hosts_key --syntax-check test_ignore_errors.yml # 语法检测[yun@ansi-manager object05]$ ansible-playbook -b -i ../hosts_key -C test_ignore_errors.yml # 预执行，测试执行[yun@ansi-manager object05]$ ansible-playbook -b -i ../hosts_key test_ignore_errors.yml # 执行 默认情况 使用了 ignore_errors 的情况 自定义错误判定条件-failed_when命令不依赖返回状态码来判定是否执行失败，而是要查看命令返回内容来决定，比如返回内容中包括 command not found 字符串，则判定为失败。 123456789101112131415161718192021222324[yun@ansi-manager object05]$ pwd/app/ansible_info/object05[yun@ansi-manager object05]$ lltotal 48-rw-rw-r-- 1 yun yun 369 Aug 29 16:12 test_custom_error.yml[yun@ansi-manager object05]$ cat test_custom_error.yml ---# 自定义错误条件- hosts: proxyservers tasks: - name: &quot;this command prints &apos;command not found&apos; if not find&quot; shell: &quot;kkk -x&quot; # 测试一 #shell: &quot;/bin/kkk -x&quot; # 测试二 register: shell_result failed_when: &quot;&apos;command not found&apos; in shell_result[&apos;stderr&apos;]&quot; - name: &quot;print shell_result info&quot; debug: msg: &quot;&#123;&#123; shell_result[&apos;stderr&apos;] &#125;&#125;&quot;[yun@ansi-manager object05]$ ansible-playbook -b -i ../hosts_key --syntax-check test_custom_error.yml # 语法检测[yun@ansi-manager object05]$ ansible-playbook -b -i ../hosts_key -C test_custom_error.yml # 预执行，测试执行[yun@ansi-manager object05]$ ansible-playbook -b -i ../hosts_key test_custom_error.yml # 执行 强制调用触发器-force_handlers通常情况下，当 task 执行失败后，playbook 会终止。任何在此之前已经被 task notify 的 handlers 都不会被执行。 此时，如果你在 playbook 中设置了 force_handlers: yes 参数，则被通知的 handlers 就会被强制执行（有些特殊场景可能会使用到）。 如示例，在一个 playbook 中，如果配置文件的 task 已经被执行成功，并且 notify 了 handlers，之后必须重启服务。那么我们会强制要求：即使后续的 task 执行失败，之前被通知的 handlers 也必须执行。 如果不强制执行就变成了，第一次执行时：配置文件修改成功，但由于之后有 task 执行失败，导致 playbook 终止，后续 handlers 没有被调用，对应服务没有重启；第二次执行时：配置文件没发生改变「因此第一次已经更新了配置文件」，因此不会通知 handlers。最终结果就是配置改变了，但是就是没有重启服务。显然不符合我们的初衷。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[yun@ansi-manager object05]$ pwd/app/ansible_info/object05[yun@ansi-manager object05]$ lltotal 40drwxrwxr-x 2 yun yun 129 Aug 24 14:28 file-rw-rw-r-- 1 yun yun 909 Aug 29 12:23 test_error_deal.yml[yun@ansi-manager object05]$ cat test_error_deal.yml ---# 即使 task 执行错误，之前已 notify 的 handlers 必须被执行- hosts: proxyservers # 这里为了演示方便，因此变量直接就写在了该文件中 vars: - httpd_port: 8087 # 即使 task 执行错误，之前已 notify 的 handlers 必须被执行 force_handlers: yes tasks: - name: &quot;Install httpd&quot; yum: name: &quot;&#123;&#123; packages &#125;&#125;&quot; state: present vars: packages: - httpd - httpd-tools - name: &quot;Httpd config&quot; template: src: ./file/httpd.conf.j2 dest: /etc/httpd/conf/httpd.conf notify: &quot;Restart httpd server&quot; - name: &quot;Start httpd server&quot; systemd: name: httpd state: started enabled: yes # /bin/false 返回状态码为1，不为0 - name: &quot;Shell task&quot; shell: /bin/false - name: &quot;Create dir&quot; file: path: /tmp/with_items_testdir state: directory handlers: - name: &quot;Restart httpd server&quot; systemd: name: httpd state: restarted[yun@ansi-manager object05]$ ansible-playbook -b -i ../hosts_key --syntax-check test_error_deal.yml # 语法检测[yun@ansi-manager object05]$ ansible-playbook -b -i ../hosts_key -C test_error_deal.yml # 预执行，测试执行[yun@ansi-manager object05]$ ansible-playbook -b -i ../hosts_key test_error_deal.yml # 执行 抑制changed状态-changed_whenansible 会自动判断模块执行状态，command、shell 及其它模块如果修改了远程主机状态则被判定为 changed 状态，不过也可以自己决定达到 changed 状态的条件。 当我们在 playbook 中使用 shell 或者 command 模块时，每次 task 执行状态都是 changed。原因是因为每次我们都去执行获取当前数据，而不是一个固化的状态。 但在实际应用中，我们可能不需要 shell 或者 command 模块执行后的 changed 状态，这时我们就需要通过 changed_when: false 来抑制这个改变。 当然上述的 changed_when: false 可以在任何模块中使用，不局限于 shell 和 command 模块，只是我们常用于这两个模块而已。 12345678910111213141516171819202122232425[yun@ansi-manager object05]$ pwd/app/ansible_info/object05[yun@ansi-manager object05]$ lltotal 44-rw-rw-r-- 1 yun yun 299 Aug 29 14:47 test_changed_when.yml[yun@ansi-manager object05]$ cat test_changed_when.yml ---# 使用 changed_when: false 抑制 changed 状态- hosts: proxyservers tasks: - name: &quot;Shell task&quot; shell: netstat -lntp | grep &apos;httpd&apos; register: check_httpd # changed_when: false # 任何时候，都不为 changed 状态 #### check_httpd[&apos;stdout&apos;] 不包含 httpd 为 true，否则 false changed_when: &quot;&apos;httpd&apos; not in check_httpd[&apos;stdout&apos;]&quot; # 结果为 false - name: &quot;Debug output&quot; debug: msg: &quot;&#123;&#123; check_httpd.stdout &#125;&#125;&quot;[yun@ansi-manager object05]$ ansible-playbook -b -i ../hosts_key --syntax-check test_changed_when.yml # 语法检测[yun@ansi-manager object05]$ ansible-playbook -b -i ../hosts_key -C test_changed_when.yml # 预执行，测试执行[yun@ansi-manager object05]$ ansible-playbook -b -i ../hosts_key test_changed_when.yml # 执行]]></content>
      <categories>
        <category>ansbile</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible Facts 变量详解]]></title>
    <url>%2F2020%2F01%2F06%2Fansible-07%2F</url>
    <content type="text"><![CDATA[主机规划 主机名称 操作系统版本 内网IP 外网IP(模拟) 安装软件 ansi-manager CentOS7.5 172.16.1.180 10.0.0.180 ansible ansi-haproxy01 CentOS7.5 172.16.1.181 10.0.0.181 ansi-haproxy02 CentOS7.5 172.16.1.182 10.0.0.182 ansi-web01 CentOS7.5 172.16.1.183 10.0.0.183 ansi-web02 CentOS7.5 172.16.1.184 10.0.0.184 ansi-web03 CentOS7.5 172.16.1.185 10.0.0.185 添加用户账号说明： 1、 运维人员使用的登录账号； 2、 所有的业务都放在 /app/ 下「yun用户的家目录」，避免业务数据乱放； 3、 该用户也被 ansible 使用，因为几乎所有的生产环境都是禁止 root 远程登录的（因此该 yun 用户也进行了 sudo 提权）。 1234567# 使用一个专门的用户，避免直接使用root用户# 添加用户、指定家目录并指定用户密码# sudo提权# 让其它普通用户可以进入该目录查看信息useradd -u 1050 -d /app yun &amp;&amp; echo &apos;123456&apos; | /usr/bin/passwd --stdin yunecho &quot;yun ALL=(ALL) NOPASSWD: ALL&quot; &gt;&gt; /etc/sudoerschmod 755 /app/ Ansible 配置清单Inventory之后文章都是如下主机配置清单 123456789101112131415[yun@ansi-manager ansible_info]$ pwd/app/ansible_info[yun@ansi-manager ansible_info]$ cat hosts_key # 方式1、主机 + 端口 + 密钥[manageservers]172.16.1.180:22[proxyservers]172.16.1.18[1:2]:22# 方式2：别名 + 主机 + 端口 + 密码[webservers]web01 ansible_ssh_host=172.16.1.183 ansible_ssh_port=22web02 ansible_ssh_host=172.16.1.184 ansible_ssh_port=22web03 ansible_ssh_host=172.16.1.185 ansible_ssh_port=22 Facts 概述Ansible Facts 是 Ansible 在被托管主机上自动收集的变量。它是通过在执行 Ad-Hoc 以及 Playbook 时使用 setup 模块进行收集的，并且这个操作是默认的。 因为这个收集托管主机上的 Facts 比较耗费时间，所以可以在不需要的时候关闭 setup 模块。收集的 Facts 中包含了托管主机特有的信息，这些信息可以像变量一样在 Playbook 中使用。 收集的 Facts 中包含了以下常用的信息： 主机名、内核版本、网卡接口、IP 地址、操作系统版本、环境变量、CPU 核数、可用内存、可用磁盘 等等……。 使用场景： 通过 facts 检查 CPU，生成对应的 Nginx 配置文件 通过 facts 检查内存情况，定义不同的 MySQL 配置文件或 Redis 配置文件 通过 facts 检查主机 hostname，生成不同的 zabbix 配置文件 获取指定受控端的 facts 信息 12345678910[yun@ansi-manager ansible_info]$ pwd/app/ansible_info[yun@ansi-manager ansible_info]$ ansible 172.16.1.181 -m setup -i ./hosts_key 172.16.1.181 | SUCCESS =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;ansible_all_ipv4_addresses&quot;: [ &quot;10.0.0.181&quot;, &quot;172.16.1.181&quot; ],……………… 如何在 playbook 中关闭 facts 1234567891011121314[yun@ansi-manager object03]$ pwd/app/ansible_info/object03[yun@ansi-manager object03]$ cat test_facts.yml ---# facts 使用- hosts: proxyservers # 关闭 facts 变量 gather_facts: no # 这时就不能取到 ansible_hostname、ansible_eth0.ipv4.address、ansible_eth1 [&apos;ipv4&apos;][&apos;address&apos;] 变量信息 tasks: - name: &quot;get ansible facts var&quot; debug: msg: &quot;This host name is &#123;&#123; ansible_hostname &#125;&#125; ,eth0: &#123;&#123; ansible_eth0.ipv4.address &#125;&#125;, eth1: &#123;&#123; ansible_eth1[&apos;ipv4&apos;][&apos;address&apos;] &#125;&#125;&quot; Facts 案例-获取主机名和网卡信息获取受控端的主机名，内网地址和外网地址 1234567891011121314151617181920212223242526272829303132333435[yun@ansi-manager object03]$ pwd/app/ansible_info/object03[yun@ansi-manager object03]$ lltotal 4-rw-rw-r-- 1 yun yun 241 Aug 22 10:41 test_facts.yml[yun@ansi-manager object03]$ cat test_facts.yml ---# facts 使用- hosts: proxyservers tasks: - name: &quot;get ansible facts var&quot; debug: msg: &quot;This host name is &#123;&#123; ansible_hostname &#125;&#125; ,eth0: &#123;&#123; ansible_eth0.ipv4.address &#125;&#125;, eth1: &#123;&#123; ansible_eth1[&apos;ipv4&apos;][&apos;address&apos;] &#125;&#125;&quot; #### 上面写了两种方式引用变量，推荐使用后一种引用方式[yun@ansi-manager object03]$ ansible-playbook -b -i ../hosts_key test_facts.yml PLAY [proxyservers] ***********************************************************************************************TASK [Gathering Facts] ********************************************************************************************ok: [172.16.1.181]ok: [172.16.1.182]TASK [get ansible facts var] **************************************************************************************ok: [172.16.1.181] =&gt; &#123; &quot;msg&quot;: &quot;This host name is ansi-haproxy01 ,eth0: 172.16.1.181, eth1: 10.0.0.181&quot;&#125;ok: [172.16.1.182] =&gt; &#123; &quot;msg&quot;: &quot;This host name is ansi-haproxy02 ,eth0: 172.16.1.182, eth1: 10.0.0.182&quot;&#125;PLAY RECAP ********************************************************************************************************172.16.1.181 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 172.16.1.182 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Facts 案例-模拟zabbix客户端配置根据受控端主机名的不同，在受控端生成不同的配置文件 12345678910111213141516171819202122232425262728293031323334[yun@ansi-manager object03]$ pwd/app/ansible_info/object03[yun@ansi-manager object03]$ lltotal 32drwxrwxr-x 2 yun yun 58 Aug 22 12:31 file-rw-rw-r-- 1 yun yun 224 Aug 22 12:33 test_zabbix_agentd.yml[yun@ansi-manager object03]$ cat file/vars_file.yml # playbook 变量zabbix_server: 172.16.1.180[yun@ansi-manager object03]$ cat file/zabbix_agentd_temp.conf.j2 # 模拟 zabbix_agentd 配置文件# 模拟 zabbix_agentd 配置文件# zabbix 服务端配置Server=&#123;&#123; zabbix_server &#125;&#125;ServerActive=&#123;&#123; zabbix_server &#125;&#125;# zabbix 客户端配置Hostname=&#123;&#123; ansible_hostname &#125;&#125;[yun@ansi-manager object03]$ cat test_zabbix_agentd.yml # 具体的 yml 文件---# zabbix 配置- hosts: proxyservers vars_files: ./file/vars_file.yml tasks: - name: config zabbix_agentd template: src: ./file/zabbix_agentd_temp.conf.j2 dest: /tmp/zabbix_agentd_temp.conf[yun@ansi-manager object03]$ ansible-playbook -b -i ../hosts_key --syntax-check test_zabbix_agentd.yml # 语法检测[yun@ansi-manager object03]$ ansible-playbook -b -i ../hosts_key -C test_zabbix_agentd.yml # 预执行，测试执行[yun@ansi-manager object03]$ ansible-playbook -b -i ../hosts_key test_zabbix_agentd.yml # 执行 受控端1配置文件查看 123456789[yun@ansi-haproxy01 ~]$ cat /tmp/zabbix_agentd_temp.conf # 模拟 zabbix_agentd 配置文件# zabbix 服务端配置Server=172.16.1.180ServerActive=172.16.1.180# zabbix 客户端配置Hostname=ansi-haproxy01 受控端2配置文件查看 123456789[yun@ansi-haproxy02 ~]$ cat /tmp/zabbix_agentd_temp.conf # 模拟 zabbix_agentd 配置文件# zabbix 服务端配置Server=172.16.1.180ServerActive=172.16.1.180# zabbix 客户端配置Hostname=ansi-haproxy02]]></content>
      <categories>
        <category>ansbile</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible Playbook 变量与 register 详解]]></title>
    <url>%2F2020%2F01%2F06%2Fansible-06%2F</url>
    <content type="text"><![CDATA[主机规划 主机名称 操作系统版本 内网IP 外网IP(模拟) 安装软件 ansi-manager CentOS7.5 172.16.1.180 10.0.0.180 ansible ansi-haproxy01 CentOS7.5 172.16.1.181 10.0.0.181 ansi-haproxy02 CentOS7.5 172.16.1.182 10.0.0.182 ansi-web01 CentOS7.5 172.16.1.183 10.0.0.183 ansi-web02 CentOS7.5 172.16.1.184 10.0.0.184 ansi-web03 CentOS7.5 172.16.1.185 10.0.0.185 添加用户账号说明： 1、 运维人员使用的登录账号； 2、 所有的业务都放在 /app/ 下「yun用户的家目录」，避免业务数据乱放； 3、 该用户也被 ansible 使用，因为几乎所有的生产环境都是禁止 root 远程登录的（因此该 yun 用户也进行了 sudo 提权）。 1234567# 使用一个专门的用户，避免直接使用root用户# 添加用户、指定家目录并指定用户密码# sudo提权# 让其它普通用户可以进入该目录查看信息useradd -u 1050 -d /app yun &amp;&amp; echo &apos;123456&apos; | /usr/bin/passwd --stdin yunecho &quot;yun ALL=(ALL) NOPASSWD: ALL&quot; &gt;&gt; /etc/sudoerschmod 755 /app/ Ansible 配置清单Inventory之后文章都是如下主机配置清单 123456789101112131415[yun@ansi-manager ansible_info]$ pwd/app/ansible_info[yun@ansi-manager ansible_info]$ cat hosts_key # 方式1、主机 + 端口 + 密钥[manageservers]172.16.1.180:22[proxyservers]172.16.1.18[1:2]:22# 方式2：别名 + 主机 + 端口 + 密码[webservers]web01 ansible_ssh_host=172.16.1.183 ansible_ssh_port=22web02 ansible_ssh_host=172.16.1.184 ansible_ssh_port=22web03 ansible_ssh_host=172.16.1.185 ansible_ssh_port=22 ansible 定义变量的三种方式1、命令行中定义，通过 -e EXTRA_VARS, --extra-vars=EXTRA_VARS 定义 2、在 playbook 的 yml 文件中定义 3、在 inventory 清单中定义 4、变量名可以有下划线，但不能有中横线。 优先级：命令行定义变量 &gt; playbook定义变量 &gt; inventory 定义变量 如下内容也是按照优先级从低到高写的，如果是为了测试优先级那么你可以直接按照如下步骤测验。 测验内容：在不同的位置定义相同的变量，但变量值不同，然后在阿里云镜像源下载不同版本的 zabbix rpm 包。 1地址：https://mirrors.aliyun.com/zabbix/zabbix/ 在 inventory 清单中定义有三种定义方式： 1、直接在 inventory 清单文件中定义变量「了解即可，不推荐使用」 2、通过 group_vars 定义变量 3、通过 host_vars 定义变量 自身内部的优先级：host_vars/主机名【或别名】 定义变量 &gt; group_vars/清单组名 定义变量 &gt; group_vars/all 定义变量&gt; inventory 文件中直接定义变量 在 inventory 清单文件中定义变量「了解」12345678910111213141516171819202122232425262728293031323334[yun@ansi-manager object02]$ pwd/app/ansible_info/object02[yun@ansi-manager object02]$ cat ../hosts_key # 方式1、主机 + 端口 + 密钥[manageservers]172.16.1.180:22[proxyservers]172.16.1.18[1:2]:22[proxyservers:vars]zabbix_version=2.2zabbix_rpm=zabbix-release-2.2-1.el7.noarch.rpm### ************ 定义变量如上# 方式2：别名 + 主机 + 端口 + 密码[webservers]web01 ansible_ssh_host=172.16.1.183 ansible_ssh_port=22web02 ansible_ssh_host=172.16.1.184 ansible_ssh_port=22web03 ansible_ssh_host=172.16.1.185 ansible_ssh_port=22[yun@ansi-manager object02]$ cat test_vars.yml ---# 下载 zabbix rpm包- hosts: proxyservers tasks: - name: &quot;download zabbix rpm&quot; get_url: url: https://mirrors.aliyun.com/zabbix/zabbix/&#123;&#123; zabbix_version &#125;&#125;/rhel/7/x86_64/&#123;&#123; zabbix_rpm &#125;&#125; dest: /tmp/[yun@ansi-manager object02]$ ansible-playbook -b -i ../hosts_key --syntax-check test_vars.yml # 语法检测[yun@ansi-manager object02]$ ansible-playbook -b -i ../hosts_key -C test_vars.yml # 预执行，测试执行[yun@ansi-manager object02]$ ansible-playbook -b -i ../hosts_key test_vars.yml # 执行 然后就可以在目标机器 172.16.1.181、172.16.1.182 查看下载的 zabbix-release-2.2-1.el7.noarch.rpm 包了。 通过 group_vars 定义变量注意事项： 1、要创建的 group_vars 目录要与 inventory 清单文件在同一个目录，或者与要执行的 playbook 的 yml 文件在同一个目录。 2、group_vars 目录下的文件名是 inventory 清单文件中的组名。或者文件名为 all「特殊组」，表示对所有机器主机生效。 在 group_vars/all 定义变量 12345678910111213141516171819202122232425[yun@ansi-manager object02]$ pwd/app/ansible_info/object02[yun@ansi-manager object02]$ ll /app/ansible_info/total 4drwxrwxr-x 2 yun yun 17 Oct 15 14:55 group_vars-rw-rw-r-- 1 yun yun 348 Oct 11 20:36 hosts_keydrwxrwxr-x 2 yun yun 27 Oct 11 20:56 object02 [yun@ansi-manager object02]$ cat /app/ansible_info/group_vars/all # all「特殊组」变量文件zabbix_version: 2.4zabbix_rpm: zabbix-release-2.4-1.el7.noarch.rpm[yun@ansi-manager object02]$ cat test_vars.yml ---# 下载 zabbix rpm包- hosts: proxyservers tasks: - name: &quot;download zabbix rpm&quot; get_url: url: https://mirrors.aliyun.com/zabbix/zabbix/&#123;&#123; zabbix_version &#125;&#125;/rhel/7/x86_64/&#123;&#123; zabbix_rpm &#125;&#125; dest: /tmp/[yun@ansi-manager object02]$ ansible-playbook -b -i ../hosts_key --syntax-check test_vars.yml # 语法检测[yun@ansi-manager object02]$ ansible-playbook -b -i ../hosts_key -C test_vars.yml # 预执行，测试执行[yun@ansi-manager object02]$ ansible-playbook -b -i ../hosts_key test_vars.yml # 执行 然后就可以在目标机器 172.16.1.181、172.16.1.182 查看下载的 zabbix-release-2.4-1.el7.noarch.rpm 包了。 在 group_vars/组 定义变量 12345678910111213141516171819202122232425[yun@ansi-manager object02]$ pwd/app/ansible_info/object02[yun@ansi-manager object02]$ ll /app/ansible_info/total 4drwxrwxr-x 2 yun yun 17 Oct 15 14:55 group_vars-rw-rw-r-- 1 yun yun 348 Oct 11 20:36 hosts_keydrwxrwxr-x 2 yun yun 27 Oct 11 20:56 object02 [yun@ansi-manager object02]$ cat /app/ansible_info/group_vars/proxyservers # inventory 清单文件中的组名 变量文件zabbix_version: 3.0zabbix_rpm: zabbix-release-3.0-1.el7.noarch.rpm[yun@ansi-manager object02]$ cat test_vars.yml ---# 下载 zabbix rpm包- hosts: proxyservers tasks: - name: &quot;download zabbix rpm&quot; get_url: url: https://mirrors.aliyun.com/zabbix/zabbix/&#123;&#123; zabbix_version &#125;&#125;/rhel/7/x86_64/&#123;&#123; zabbix_rpm &#125;&#125; dest: /tmp/[yun@ansi-manager object02]$ ansible-playbook -b -i ../hosts_key --syntax-check test_vars.yml # 语法检测[yun@ansi-manager object02]$ ansible-playbook -b -i ../hosts_key -C test_vars.yml # 预执行，测试执行[yun@ansi-manager object02]$ ansible-playbook -b -i ../hosts_key test_vars.yml # 执行 然后就可以在目标机器 172.16.1.181、172.16.1.182 查看下载的 zabbix-release-3.0-1.el7.noarch.rpm 包了。 通过 host_vars 定义变量注意事项： 1、要创建的 host_vars 目录要与 inventory 清单文件在同一目录，或者与要执行的 playbook 的 yml 文件在同一个目录。 2、host_vars 目录下的文件名是 inventory 清单文件中的主机名或别名。如果有别名那么文件名为别名 12345678910111213141516171819202122232425[yun@ansi-manager object02]$ pwd/app/ansible_info/object02[yun@ansi-manager object02]$ ll /app/ansible_info/total 4-rw-rw-r-- 1 yun yun 348 Oct 11 20:36 hosts_keydrwxrwxr-x 2 yun yun 26 Oct 15 15:50 host_varsdrwxrwxr-x 2 yun yun 27 Oct 11 20:56 object02 [yun@ansi-manager object02]$ cat /app/ansible_info/host_vars/172.16.1.181 # inventory 清单文件中的主机名或别名 变量文件zabbix_version: 3.4zabbix_rpm: zabbix-release-3.4-2.el7.noarch.rpm[yun@ansi-manager object02]$ cat test_vars.yml ---# 下载 zabbix rpm包- hosts: proxyservers tasks: - name: &quot;download zabbix rpm&quot; get_url: url: https://mirrors.aliyun.com/zabbix/zabbix/&#123;&#123; zabbix_version &#125;&#125;/rhel/7/x86_64/&#123;&#123; zabbix_rpm &#125;&#125; dest: /tmp/[yun@ansi-manager object02]$ ansible-playbook -b -i ../hosts_key --syntax-check test_vars.yml # 语法检测[yun@ansi-manager object02]$ ansible-playbook -b -i ../hosts_key -C test_vars.yml # 预执行，测试执行[yun@ansi-manager object02]$ ansible-playbook -b -i ../hosts_key test_vars.yml # 执行 然后就可以在目标机器 172.16.1.181 查看下载的 zabbix-release-3.4-2.el7.noarch.rpm 包了。 使用 playbook 定义变量使用 playbook 定义变量有两种方式： 1、直接在要执行的 playbook 文件中定义变量。缺点是：这些变量无法与其他 playbook 共用。 2、将 playbook 中的变量抽出来，单独一个或多个文件 yml 文件。其他 playbook 文件要使用变量时，可以调用这些变量文件。好处是：可对变量集中管理，降低后期维护成本。「推荐使用」 自身内部优先级：playbook 引用文件变量 &gt; 直接定义在要执行的 yml 文件中 直接在 playbook 中定义变量变量写在要使用变量的 playbook 文件中。 1234567891011121314151617181920212223[yun@ansi-manager object02]$ pwd/app/ansible_info/object02[yun@ansi-manager object02]$ lltotal 4-rw-rw-r-- 1 yun yun 323 Oct 15 16:06 test_vars.yml[yun@ansi-manager object02]$ cat test_vars.yml ---# 下载 zabbix rpm包- hosts: proxyservers # 变量定义 vars: - zabbix_version: 3.5 - zabbix_rpm: zabbix-release-3.5-1.el7.noarch.rpm tasks: - name: &quot;download zabbix rpm&quot; get_url: url: https://mirrors.aliyun.com/zabbix/zabbix/&#123;&#123; zabbix_version &#125;&#125;/rhel/7/x86_64/&#123;&#123; zabbix_rpm &#125;&#125; dest: /tmp/[yun@ansi-manager object02]$ ansible-playbook -b -i ../hosts_key --syntax-check test_vars.yml # 语法检测[yun@ansi-manager object02]$ ansible-playbook -b -i ../hosts_key -C test_vars.yml # 预执行，测试执行[yun@ansi-manager object02]$ ansible-playbook -b -i ../hosts_key test_vars.yml # 执行 然后就可以在目标机器 172.16.1.181、172.16.1.182 查看下载的 zabbix-release-3.5-1.el7.noarch.rpm 包了。 playbook 引用变量文件将 playbook 变量抽出来，使用单独的文件进行管理 1234567891011121314151617181920212223242526272829303132[yun@ansi-manager object02]$ pwd/app/ansible_info/object02[yun@ansi-manager object02]$ ll /app/ansible_info/total 8-rw-rw-r-- 1 yun yun 456 Oct 15 15:56 hosts_keydrwxrwxr-x 2 yun yun 27 Oct 15 16:08 object02-rw-rw-r-- 1 yun yun 69 Oct 15 16:11 playbood_vars.yml[yun@ansi-manager object02]$ cat /app/ansible_info/playbood_vars.yml # 单独文件定义的变量zabbix_version: 4.0zabbix_rpm: zabbix-release-4.0-1.el7.noarch.rpm[yun@ansi-manager object02]$ cat test_vars.yml ---# 下载 zabbix rpm包- hosts: proxyservers # 变量引用 vars_files: ../playbood_vars.yml # 变量定义 vars: - zabbix_version: 3.5 - zabbix_rpm: zabbix-release-3.5-1.el7.noarch.rpm tasks: - name: &quot;download zabbix rpm&quot; get_url: url: https://mirrors.aliyun.com/zabbix/zabbix/&#123;&#123; zabbix_version &#125;&#125;/rhel/7/x86_64/&#123;&#123; zabbix_rpm &#125;&#125; dest: /tmp/[yun@ansi-manager object02]$ ansible-playbook -b -i ../hosts_key --syntax-check test_vars.yml # 语法检测[yun@ansi-manager object02]$ ansible-playbook -b -i ../hosts_key -C test_vars.yml # 预执行，测试执行[yun@ansi-manager object02]$ ansible-playbook -b -i ../hosts_key test_vars.yml # 执行 然后就可以在目标机器 172.16.1.181、172.16.1.182 查看下载的 zabbix-release-4.0-1.el7.noarch.rpm 包了。 使用命令行定义变量优先级别最高，但只是临时使用。 1234567891011121314151617181920212223242526[yun@ansi-manager object02]$ pwd/app/ansible_info/object02[yun@ansi-manager object02]$ ll total 4-rw-rw-r-- 1 yun yun 393 Oct 15 16:14 test_vars.yml[yun@ansi-manager object02]$ cat test_vars.yml ---# 下载 zabbix rpm包- hosts: proxyservers # 变量引用 vars_files: ../playbood_vars.yml # 变量定义 vars: - zabbix_version: 3.5 - zabbix_rpm: zabbix-release-3.5-1.el7.noarch.rpm tasks: - name: &quot;download zabbix rpm&quot; get_url: url: https://mirrors.aliyun.com/zabbix/zabbix/&#123;&#123; zabbix_version &#125;&#125;/rhel/7/x86_64/&#123;&#123; zabbix_rpm &#125;&#125; dest: /tmp/[yun@ansi-manager object02]$ ansible-playbook -b -i ../hosts_key -e &quot;zabbix_version=4.1 zabbix_rpm=zabbix-release-4.1-1.el7.noarch.rpm&quot; --syntax-check test_vars.yml # 语法检测[yun@ansi-manager object02]$ ansible-playbook -b -i ../hosts_key -e &quot;zabbix_version=4.1 zabbix_rpm=zabbix-release-4.1-1.el7.noarch.rpm&quot; -C test_vars.yml # 预执行，测试执行[yun@ansi-manager object02]$ ansible-playbook -b -i ../hosts_key -e &quot;zabbix_version=4.1 zabbix_rpm=zabbix-release-4.1-1.el7.noarch.rpm&quot; test_vars.yml # 执行 然后就可以在目标机器 172.16.1.181、172.16.1.182 查看下载的 zabbix-release-4.1-1.el7.noarch.rpm 包了。 多层级变量写法与引用层级变量的写法与两种引用方式。 123456789101112131415161718192021222324252627282930[yun@ansi-manager object02]$ pwd/app/ansible_info/object02[yun@ansi-manager object02]$ ll /app/ansible_info/total 12-rw-rw-r-- 1 yun yun 456 Oct 15 15:56 hosts_keydrwxrwxr-x 2 yun yun 27 Oct 15 16:14 object02-rw-rw-r-- 1 yun yun 76 Oct 15 16:26 playbood_vars_mult.yml[yun@ansi-manager object02]$ cat /app/ansible_info/playbood_vars_mult.yml # 多层级变量书写zabbix_rpm_info: version: 4.2 rpm: zabbix-release-4.2-1.el7.noarch.rpm[yun@ansi-manager object02]$ cat test_vars_mult.yml # 具体的 playbook 信息---# 下载 zabbix rpm包- hosts: proxyservers # 变量引用 vars_files: ../playbood_vars_mult.yml tasks: - name: &quot;download zabbix rpm&quot; get_url: # 下面写了两种方式引用变量，推荐使用后一种引用方式 url: https://mirrors.aliyun.com/zabbix/zabbix/&#123;&#123; zabbix_rpm_info.version &#125;&#125;/rhel/7/x86_64/&#123;&#123; zabbix_rpm_info[&apos;rpm&apos;] &#125;&#125; dest: /tmp/[yun@ansi-manager object02]$ ansible-playbook -b -i ../hosts_key --syntax-check test_vars_mult.yml # 语法检测[yun@ansi-manager object02]$ ansible-playbook -b -i ../hosts_key -C test_vars_mult.yml # 预执行，测试执行[yun@ansi-manager object02]$ ansible-playbook -b -i ../hosts_key test_vars_mult.yml # 执行 然后就可以在目标机器 172.16.1.181、172.16.1.182 查看下载的 zabbix-release-4.2-1.el7.noarch.rpm 包了。 register 注册变量如何使用 register 注册变量 1234567891011121314151617181920212223[yun@ansi-manager ansible_info]$ pwd/app/ansible_info[yun@ansi-manager ansible_info]$ lltotal 24-rw-rw-r-- 1 yun yun 483 Aug 18 09:12 hosts_key-rw-rw-r-- 1 yun yun 245 Aug 18 21:55 test_debug_register.yml[yun@ansi-manager ansible_info]$ cat test_debug_register.yml ---# 如何使用 debug 模块与 register 变量- hosts: proxyservers tasks: - name: &quot;get host port info&quot; shell: netstat -lntp register: host_port - name: &quot;print host port&quot; debug: #msg: &quot;&#123;&#123; host_port &#125;&#125;&quot; # 输出全部信息 #msg: &quot;&#123;&#123; host_port.cmd &#125;&#125;&quot; # 引用方式一 msg: &quot;&#123;&#123; host_port[&apos;stdout_lines&apos;] &#125;&#125;&quot; # 引用方式二[yun@ansi-manager ansible_info]$ ansible-playbook -b -i ./hosts_key test_debug_register.yml # 执行 第一个 task 中，使用了 register 注册变量，名为 host_port ；当 shell 模块执行完毕后，会将数据放到该变量中。 第二给 task 中，使用了 debug 模块，并从 host_port 中获取数据。]]></content>
      <categories>
        <category>ansbile</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible Playbook 初识]]></title>
    <url>%2F2020%2F01%2F06%2Fansible-05%2F</url>
    <content type="text"><![CDATA[主机规划 主机名称 操作系统版本 内网IP 外网IP(模拟) 安装软件 ansi-manager CentOS7.5 172.16.1.180 10.0.0.180 ansible ansi-haproxy01 CentOS7.5 172.16.1.181 10.0.0.181 ansi-haproxy02 CentOS7.5 172.16.1.182 10.0.0.182 ansi-web01 CentOS7.5 172.16.1.183 10.0.0.183 ansi-web02 CentOS7.5 172.16.1.184 10.0.0.184 ansi-web03 CentOS7.5 172.16.1.185 10.0.0.185 添加用户账号说明： 1、 运维人员使用的登录账号； 2、 所有的业务都放在 /app/ 下「yun用户的家目录」，避免业务数据乱放； 3、 该用户也被 ansible 使用，因为几乎所有的生产环境都是禁止 root 远程登录的（因此该 yun 用户也进行了 sudo 提权）。 1234567# 使用一个专门的用户，避免直接使用root用户# 添加用户、指定家目录并指定用户密码# sudo提权# 让其它普通用户可以进入该目录查看信息useradd -u 1050 -d /app yun &amp;&amp; echo &apos;123456&apos; | /usr/bin/passwd --stdin yunecho &quot;yun ALL=(ALL) NOPASSWD: ALL&quot; &gt;&gt; /etc/sudoerschmod 755 /app/ Ansible 配置清单Inventory之后文章都是如下主机配置清单 123456789101112131415[yun@ansi-manager ansible_info]$ pwd/app/ansible_info[yun@ansi-manager ansible_info]$ cat hosts_key # 方式1、主机 + 端口 + 密钥[manageservers]172.16.1.180:22[proxyservers]172.16.1.18[1:2]:22# 方式2：别名 + 主机 + 端口 + 密码[webservers]web01 ansible_ssh_host=172.16.1.183 ansible_ssh_port=22web02 ansible_ssh_host=172.16.1.184 ansible_ssh_port=22web03 ansible_ssh_host=172.16.1.185 ansible_ssh_port=22 Playbook 基本概述什么是 playbook，playbook 翻译过来就是“剧本”，那 playbook 组成如下： 1、play：定义主机角色「比作：剧本中的角色」 2、task：定义具体执行任务「比作：剧本中的角色要做什么事儿」 3、playbook：由一个或多个play组成，一个 play 可以包含多个 task 任务 简单理解：使用不同的模块完成一项事情 Playbook 优势1、功能上比 Ad-Hoc 更全 2、能很好的控制先后执行顺序，以及依赖关系 3、语法更加直观，可读性更好 4、Ad-Hoc 是临时的无法持久使用，Playbook 可以持久化使用。 Playbook 书写格式Playbook 是有 yml 语法书写，结构清晰，可读性强。 语法 描述 缩进 YAML 使用固定的缩进风格表示层级结构，每个缩进由两个空格组成，不能使用 Tab 键 冒号 以冒号结尾除外，其他所有冒号后面必须有空格 短横线 表示列表项，使用一个短横线加一个空格；多个项使用同样的缩进级别作为同一列表 Playbook 示例-安装部署 httpd要求：在 172.16.1.180、172.16.1.181、172.16.1.182 安装 httpd 服务。 test_httpd.yml 脚本内容说明：其中 172.16.1.181 和 172.16.1.182 已经安装好了 httpd 服务。 脚本参数说明： host：指定主机 remote_user：要使用什么用户操作 tasks：具体执行什么任务 12345678910111213141516171819202122232425262728293031[yun@ansi-manager object01]$ pwd/app/ansible_info/object01[yun@ansi-manager object01]$ ll ../ # 主机清单存放位置total 16-rw-rw-r-- 1 yun yun 226 Oct 8 16:07 hosts_key # 主机清单文件drwxrwxr-x 2 yun yun 28 Oct 11 19:24 object01[yun@ansi-manager object01]$ lltotal 4-rw-rw-r-- 1 yun yun 937 Oct 11 19:24 test_httpd.yml [yun@ansi-manager object01]$ cat test_httpd.yml # playbook 内容---# 上面一行表明这是一个 yml 文件，无其他作用# test playbook- hosts: manageservers, proxyservers # 定义主机组 「可以有多个主机组 使用 逗号加空格 隔开」 # remote_user: yun # 可省略该行 定义用户的身份「我们按之前的约定使用的 yun 用户，因此这里也是 yun 用户」 tasks: # 定义运行什么样的任务 - name: &quot;Installed httpd server&quot; # 第一个任务 描述 yum: name=httpd state=present # 执行什么模块「第一种书写方式，如果该行很长，会有折行的情况，不怎么美观」 - name: &quot;site update&quot; # 第二个任务 描述「第二种书写方式，推荐，更规范人性化」 copy: content: &quot;Test WEB&quot; dest: &quot;/var/www/html/index.html&quot; - name: &quot;start httpd server&quot; systemd: name: httpd daemon_reload: yes state: started enabled: yes 语法检查12345# 由于我们使用的是 yun 普通用户# 因此有时需要使用 -b 选择进行提权[yun@ansi-manager object01]$ ansible-playbook -b -i ../hosts_key --syntax-check test_httpd.ymlplaybook: test_httpd.yml 预执行如果存在依赖关系【如：软件安装，配置，启动】，那么可能预执行报错，但是执行是正常的。 因为后面的「启动」依赖前面的「配置」，「配置」依赖前面的「软件安装」。 因此如果报错，那么详细看看。确定是存在问题还是依赖关系导致的。 12345678910111213141516171819202122232425262728[yun@ansi-manager object01]$ ansible-playbook -b -i ../hosts_key -C test_httpd.ymlPLAY [manageservers, proxyservers] ***************************************************************************************TASK [Gathering Facts] **************************************************************************************************ok: [172.16.1.180]ok: [172.16.1.182]ok: [172.16.1.181]TASK [Installed httpd server] *******************************************************************************************ok: [172.16.1.182]ok: [172.16.1.181]changed: [172.16.1.180]TASK [site update] ******************************************************************************************************changed: [172.16.1.180]changed: [172.16.1.181]changed: [172.16.1.182]TASK [start httpd server] ***********************************************************************************************changed: [172.16.1.180]changed: [172.16.1.181]changed: [172.16.1.182]PLAY RECAP **************************************************************************************************************172.16.1.180 : ok=3 changed=2 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 172.16.1.181 : ok=3 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 172.16.1.182 : ok=3 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 执行 test_httpd.yml12345678910111213141516171819202122232425262728[yun@ansi-manager object01]$ ansible-playbook -b -i ../hosts_key test_httpd.ymlPLAY [manageservers, proxyservers] **************************************************************************************TASK [Gathering Facts] *************************************************************************************************ok: [172.16.1.182]ok: [172.16.1.180]ok: [172.16.1.181]TASK [Installed httpd server] ******************************************************************************************ok: [172.16.1.182]ok: [172.16.1.181]ok: [172.16.1.180]TASK [site update] *****************************************************************************************************changed: [172.16.1.181]changed: [172.16.1.182]changed: [172.16.1.180]TASK [start httpd server] **********************************************************************************************ok: [172.16.1.180]ok: [172.16.1.182]ok: [172.16.1.181]PLAY RECAP *************************************************************************************************************172.16.1.180 : ok=4 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 172.16.1.181 : ok=4 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 172.16.1.182 : ok=4 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 然后在 172.16.1.180、172.16.1.181、172.16.1.182 可见 httpd 安装部署已完成、服务已启动且已加入开机自启动；浏览器也能正常访问。 Playbook 示例-安装部署 nfs要求：在 172.16.1.180 安装 NFS 服务端，在 172.16.1.181、172.16.1.182 挂载 NFS。 test_nfs.yml 脚本内容12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879[yun@ansi-manager object01]$ pwd/app/ansible_info/object01[yun@ansi-manager object01]$ lltotal 8-rw-rw-r-- 1 yun yun 937 Oct 11 19:24 test_httpd.yml-rw-rw-r-- 1 yun yun 1434 Oct 11 19:50 test_nfs.yml[yun@ansi-manager ansible_info]$ cat test_nfs.yml # playbook 内容---# 必要包安装- hosts: manageservers, proxyservers tasks: - name: &quot;NFS package install&quot; yum: name: - nfs-utils - rpcbind state: present# 服务端配置与启动- hosts: manageservers tasks: - name: &quot;create NFS dir&quot; file: path: /data owner: nfsnobody group: nfsnobody state: directory recurse: yes - name: &quot;NFS server config and start&quot; copy: content: &quot;/data 172.16.1.0/24(rw,sync,root_squash,all_squash)\n&quot; dest: /etc/exports owner: root group: root mode: &apos;644&apos; # 当配置文件改变时，需要重加载 NFS 服务 notify: &quot;reload NFS server&quot; - name: &quot;rpcbind server start&quot; systemd: name: rpcbind state: started daemon_reload: yes enabled: yes - name: &quot;NFS server start&quot; systemd: name: nfs state: started daemon_reload: yes enabled: yes handlers: - name: &quot;reload NFS server&quot; systemd: name: nfs state: reloaded# 客户端操作- hosts: proxyservers tasks: - name: &quot;rpcbind server start&quot; systemd: name: rpcbind state: started daemon_reload: yes enabled: yes - name: &quot;mount NFS&quot; mount: src: 172.16.1.180:/data path: /mnt fstype: nfs state: mounted backup: True 语法检查12345# 由于我们使用的是 yun 普通用户# 因此有时需要使用 -b 选择进行提权[yun@ansi-manager object01]$ ansible-playbook -b -i ../hosts_key --syntax-check test_nfs.yml playbook: test_nfs.yml 预执行如果存在依赖关系【如：软件安装，配置，启动】，那么可能预执行报错，但是执行是正常的。 因为后面的「启动」依赖前面的「配置」，「配置」依赖前面的「软件安装」。 因此如果报错，那么详细看看。确定是存在问题还是依赖关系导致的。 1[yun@ansi-manager object01]$ ansible-playbook -b -i ../hosts_key -C test_nfs.yml # 具体打印内容，这里省略 执行 test_nfs.yml1[yun@ansi-manager object01]$ ansible-playbook -b -i ../hosts_key test_nfs.yml # 具体打印内容，这里省略 然后在 172.16.1.180 可见 NFS 服务已经部署且启动成功。 在 172.16.1.181、172.16.1.182 可见 NFS 挂载成功，且已写入 /etc/fstab 文件。]]></content>
      <categories>
        <category>ansbile</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible Ad-Hoc与常用模块]]></title>
    <url>%2F2020%2F01%2F02%2Fansible-04%2F</url>
    <content type="text"><![CDATA[主机规划 主机名称 操作系统版本 内网IP 外网IP(模拟) 安装软件 ansi-manager CentOS7.5 172.16.1.180 10.0.0.180 ansible ansi-haproxy01 CentOS7.5 172.16.1.181 10.0.0.181 ansi-haproxy02 CentOS7.5 172.16.1.182 10.0.0.182 ansi-web01 CentOS7.5 172.16.1.183 10.0.0.183 ansi-web02 CentOS7.5 172.16.1.184 10.0.0.184 ansi-web03 CentOS7.5 172.16.1.185 10.0.0.185 添加用户账号说明： 1、 运维人员使用的登录账号； 2、 所有的业务都放在 /app/ 下「yun用户的家目录」，避免业务数据乱放； 3、 该用户也被 ansible 使用，因为几乎所有的生产环境都是禁止 root 远程登录的（因此该 yun 用户也进行了 sudo 提权）。 1234567# 使用一个专门的用户，避免直接使用root用户# 添加用户、指定家目录并指定用户密码# sudo提权# 让其它普通用户可以进入该目录查看信息useradd -u 1050 -d /app yun &amp;&amp; echo &apos;123456&apos; | /usr/bin/passwd --stdin yunecho &quot;yun ALL=(ALL) NOPASSWD: ALL&quot; &gt;&gt; /etc/sudoerschmod 755 /app/ Ansible 配置清单Inventory之后文章都是如下主机配置清单 123456789101112131415[yun@ansi-manager ansible_info]$ pwd/app/ansible_info[yun@ansi-manager ansible_info]$ cat hosts_key # 方式1、主机 + 端口 + 密钥[manageservers]172.16.1.180:22[proxyservers]172.16.1.18[1:2]:22# 方式2：别名 + 主机 + 端口 + 密码[webservers]web01 ansible_ssh_host=172.16.1.183 ansible_ssh_port=22web02 ansible_ssh_host=172.16.1.184 ansible_ssh_port=22web03 ansible_ssh_host=172.16.1.185 ansible_ssh_port=22 Ansible执行返回 =&gt; 颜色信息说明黄色：成功执行并且伴随着状态的改变 1ansible proxyservers -m command -a &apos;cat /etc/hosts&apos; -i hosts_key 绿色：成功执行并且没有发生状态的改变，或者只是对远程节点状态信息进行查看 1ansible proxyservers -m ping -i hosts_key 红色：操作执行命令有异常 1ansible proxyservers -m command -a &apos;ll /tmp&apos; -i hosts_key 紫色：表示对命令执行发出警告信息（可能存在的问题，给你一下建议） 12# 其中 hosts_kkk 文件不存在ansible proxyservers -m command -a &apos;ll /tmp&apos; -i hosts_kkk Ansible 之 Ad-HocAnsible中有两种模式, 分别是 Ad-Hoc 模式和 Playbooks 模式。 ad-hoc简而言之，就是“临时命令”，不会保存。 ad-hoc模式的使用场景场景一，在多台机器上，查看某个进程是否启动 场景二，在多台机器上，拷贝指定日志文件到本地，等等 ad-hoc模式的命令使用 Ansible查看帮助方法123[yun@ansi-manager ~]$ ansible-doc -l # 查看所有模块与简要说明[yun@ansi-manager ~]$ ansible-doc copy # 查看指定模块方法「可优先查看 EXAMPLES 信息」★★★★★[yun@ansi-manager ~]$ ansible-doc -s copy # 查看指定模块的 Playbooks 代码段 Ansible常用模块工作目录与主机清单 当前所在的工作目录和主机清单 123456789101112131415161718[yun@ansi-manager ansible_info]$ pwd/app/ansible_info[yun@ansi-manager ansible_info]$ lltotal 4-rw-rw-r-- 1 yun yun 226 Oct 8 16:07 hosts_key[yun@ansi-manager ansible_info]$ cat hosts_key # 方式1、主机 + 端口 + 密钥[manageservers]172.16.1.180:22[proxyservers]172.16.1.18[1:2]:22# 方式2：别名 + 主机 + 端口 + 密码[webservers]web01 ansible_ssh_host=172.16.1.183 ansible_ssh_port=22web02 ansible_ssh_host=172.16.1.184 ansible_ssh_port=22web03 ansible_ssh_host=172.16.1.185 ansible_ssh_port=22 command 命令模块默认模块, 用于执行命令。但不支持管道或重定向。 正常示例 123[yun@ansi-manager ansible_info]$ ansible proxyservers -a &apos;df -h&apos; -i hosts_key# 或者[yun@ansi-manager ansible_info]$ ansible proxyservers -m command -a &apos;df -h&apos; -i hosts_key 异常示例 1234# 重定向不支持ansible proxyservers -m command -a &apos;df -h &gt; /tmp/df.info&apos; -i hosts_key# 管道不支持ansible proxyservers -m command -a &quot;df -h | grep &apos;boot&apos;&quot; -i hosts_key shell 命令模块功能和 command 相同，且支持管道和重定向。与 command 相比，优先使用该模块。 示例 12ansible proxyservers -m shell -a &quot;df -h | grep &apos;boot&apos;&quot; -i hosts_keyansible proxyservers -m shell -a &quot;df -h &gt; /tmp/df.info&quot; -i hosts_key script 脚本模块在本地运行模块，等同于在远程执行。且不需要将脚本文件推送到目标主机进行执行。 示例 脚本中有 sudo 提权 123456# 在 ansible 管理机操作[yun@ansi-manager ansible_info]$ cat /app/yunwei/yum_test.sh #!/bin/sh# 由于使用的是 yun 用户，而不是 root 用户，因此需要 sudo 提权sudo yum install -y iftop[yun@ansi-manager ansible_info]$ ansible proxyservers -m script -a &quot;/app/yunwei/yum_test.sh&quot; -i hosts_key 脚本中无 sudo 提权 12345678# 在 ansible 管理机操作[yun@ansi-manager ansible_info]$ cat /app/yunwei/yum_test.sh #!/bin/sh yum install -y iftop######################################## 由于我们使用的是 yun 普通用户# 因此这里需要使用 -b 选择进行提权，这样在远程会以 root 用户执行[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m script -a &quot;/app/yunwei/yum_test.sh&quot; -i hosts_key 在目标机器查看是否在 yum 安装 iftop 1234[root@ansi-haproxy01 ~]# ps -ef | grep &apos;iftop&apos;root 3867 3866 0 23:25 pts/1 00:00:00 sudo yum install -y iftoproot 3868 3867 48 23:25 pts/1 00:00:12 /usr/bin/python /bin/yum install -y iftoproot 4144 3155 0 23:25 pts/0 00:00:00 grep --color=auto iftop yum 安装软件模块在目标机器实现 yum 安装软件 12345# 查看 yum 模块方法「可优先查看 EXAMPLES 信息的使用案例，知晓如何使用」[yun@ansi-manager ansible_info]$ ansible-doc yum # 由于我们使用的是 yun 普通用户# 因此必须使用 -b 选择进行提权[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m yum -a &apos;name=httpd state=present&apos; -i ./hosts_key 相关选项说明： name：要安装软件包的名称 state：状态说明 &nbsp;&nbsp;&nbsp;&nbsp; ‘present’ 和 ‘installed’ 简单地确保安装了所需的包。「优先使用 present」 &nbsp;&nbsp;&nbsp;&nbsp; ‘latest’ 将更新指定的软件包，如果它不是最新可用的版本。 &nbsp;&nbsp;&nbsp;&nbsp; ‘absent’ and ‘removed’ 将删除指定的包【慎用！！！】。「如要使用优先使用 absent」 download_only：只下载包，不安装 copy 文件拷贝模块「本地到远端」将控制机的文件或目录拷贝到受控机，并且可以指定目标文件/目录的属性信息。 控制机操作 1234567891011121314# 查看 yum 模块方法「可优先查看 EXAMPLES 信息的使用案例，知晓如何使用」[yun@ansi-manager ansible_info]$ ansible-doc copy # 由于我们使用的是 yun 普通用户# 因此有时需要使用 -b 选择进行提权# 将 content 中的内容直接写入目标文件中[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m copy -a &quot;content=&apos;123\n&apos; dest=/tmp/copy_test2 owner=root group=root mode=0644 backup=yes&quot; -i ./hosts_key ## 拷贝文件[yun@ansi-manager ansible_info]$ cat /tmp/copy_test 111111222222333333[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m copy -a &quot;src=/tmp/copy_test dest=/tmp/ owner=root group=root mode=0644 backup=yes&quot; -i ./hosts_key## 拷贝目录[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m copy -a &quot;src=/app/yunwei dest=/tmp/ owner=root group=root mode=0644 backup=yes&quot; -i ./hosts_key 被控端查看 123456[yun@ansi-haproxy01 tmp]$ cat copy_test2123[yun@ansi-haproxy01 tmp]$ cat /tmp/copy_test 111111222222333333 相关选项说明： src：源文件「可以是绝对路径或相对路径」 remote_src：为 False「默认」，则源文件在本地；为 True ，则源文件在远端「了解」 dest：推送数据的目标路径或目标文件 owner：指定远端文件的属主 group：指定远端文件的属组 mode：指定远端文件的权限 backup：如果推送的目标文件存在且与源文件内容不同，那么会对目标文件进行备份「通过 checksum 校验」 content：将 content 中的内容直接写入目标文件中 注意事项： 1、同一个源文件，如果源文件内容没有任何修改，那么进行第二次相同的操作时，检测到要拷贝的文件和目标文件内容相同「通过 checksum 校验」，且目标文件属性前后未发生改变，那么就不会进行拷贝。由于目标文件属性未发生改变，所以返回数据颜色为绿色。如下图所示。 2、同一个源文件，如果源文件内容没有任何修改，那么进行第二次相同的操作时，如果源文件和目标文件内容相同「通过 checksum 校验」，但目标文件属性前后要求发生改变「属主、属组、权限」，那样也不会进行拷贝。但由于目标文件属性发生改变，所以返回数据颜色为黄色。 fetch 文件拷贝模块「远端到本地」该模块功能类似于 copy 模块，但是是反向的。将远端的文件拷贝到本地。备注：当前仅支持文件，暂不支持递归拷贝。 由于ansible使用的是 yun 用户，因此从远端拷贝过来的文件属主、属组都是 yun。 控制机操作 1234# 查看 yum 模块方法「可优先查看 EXAMPLES 信息的使用案例，知晓如何使用」[yun@ansi-manager ansible_info]$ ansible-doc fetch # 由于我们使用的是 yun 普通用户# 因此有时需要使用 -b 选择进行提权 被控端文件准备1 12345678[test1@ansi-haproxy01 tmp]$ ll /tmp/test1 -rw-rw-r-- 1 test1 test1 20 Nov 2 11:04 /tmp/test1[test1@ansi-haproxy01 tmp]$ cat /tmp/test1 111222333aaabbb 被控端文件准备2 1234[test1@ansi-haproxy02 tmp]$ ll /tmp/test1 -rw-rw-r-- 1 test1 test1 20 Nov 2 11:04 /tmp/test1[test1@ansi-haproxy02 tmp]$ cat /tmp/test1 1111 拷贝方式1 1234567891011121314151617[yun@ansi-manager ansible_info]$ ansible 172.16.1.181 -b -m fetch -a &quot;src=/tmp/test1 dest=/tmp/&quot; -i ./hosts_key # 拷贝 172.16.1.181 主机的或者[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m fetch -a &quot;src=/tmp/test1 dest=/tmp/&quot; -i ./hosts_key # 拷贝 proxyservers 主机组的## 查看拷贝结果「注意目录层次」[yun@ansi-manager ansible_info]$ ll /tmp/total 4drwxrwxr-x 3 yun yun 17 Nov 2 11:21 172.16.1.181drwxrwxr-x 3 yun yun 17 Nov 2 11:23 172.16.1.182[yun@ansi-manager ansible_info]$ tree /tmp/172.16.1.18*/tmp/172.16.1.181└── tmp └── test1/tmp/172.16.1.182└── tmp └── test12 directories, 2 files 拷贝方式2 123456789101112131415161718192021# 如果使用 flat=yes，那么最好只拷贝一台远端主机的文件，如果是多台那么后面执行的结果，会把前面的覆盖掉。# dest 路径有 / 结尾[yun@ansi-manager ansible_info]$ ansible 172.16.1.181 -b -m fetch -a &quot;src=/tmp/test1 dest=/tmp/kkk2/ flat=yes&quot; -i ./hosts_key # 推荐，只拷贝一台# dest 路径无 / 结尾[yun@ansi-manager ansible_info]$ ansible 172.16.1.181 -b -m fetch -a &quot;src=/tmp/test1 dest=/tmp/kkk flat=yes&quot; -i ./hosts_key # 推荐，只拷贝一台[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m fetch -a &quot;src=/tmp/test1 dest=/tmp/kkk flat=yes&quot; -i ./hosts_key # 不推荐，会产生覆盖## 查看拷贝结果[yun@ansi-manager ansible_info]$ cat /tmp/kkk2/test1 111222333aaabbb[yun@ansi-manager ansible_info]$ ll /tmp/kkk -rw-rw-r-- 1 yun yun 20 Nov 2 11:25 /tmp/kkk[yun@ansi-manager ansible_info]$ cat /tmp/kkk # 该文件没有 &quot;11111&quot;信息； 产生了覆盖效应111222333aaabbb 相关选项说明： src：源文件，当前仅支持文件，不支持目录 dest：推送数据的目标路径，默认为：dest[路径]/hostname/src[路径]。参见上面示例 flat： 默认 False。当为 yes/True 时，那么拷贝效果类似于本地的 copy。 template 模板使用该模块功能类似于 copy 模块，但 copy 模块不支持变量，不支持模板。 template 模块支持变量，支持 Jinja 模板。因此如果生成中的配置文件涉及变量，那么请使用 template 模块。 涉及到 playbook、变量和 Jinja ，这些在后面的文章会有详解。 1234567891011121314151617181920212223242526272829303132333435363738394041# 查看 yum 模块方法「可优先查看 EXAMPLES 信息的使用案例，知晓如何使用」[yun@ansi-manager ansible_info]$ ansible-doc template # 由于我们使用的是 yun 普通用户# 因此有时需要使用 -b 选择进行提权# palybook[yun@ansi-manager ansible_info]$ pwd/app/ansible_info[yun@ansi-manager ansible_info]$ lltotal 16drwxrwxr-x 2 yun yun 35 Oct 11 11:23 file-rw-rw-r-- 1 yun yun 226 Oct 8 16:07 hosts_key-rw-rw-r-- 1 yun yun 304 Oct 11 11:40 test_template.yml[yun@ansi-manager ansible_info]$ ll file/ # 涉及的文件total 4-rw-rw-r-- 1 yun yun 175 Oct 11 11:23 test_template.conf.j2[yun@ansi-manager ansible_info]$ cat file/test_template.conf.j2 # facts 变量dns_info=&#123;&#123; ansible_dns[&apos;nameservers&apos;][0] &#125;&#125;mem_total=&#123;&#123; ansible_memtotal_mb &#125;&#125;# 自定义变量listen_port=&#123;&#123; listen_port &#125;&#125;access_addr=&#123;&#123; access_addr &#125;&#125;[yun@ansi-manager ansible_info]$ [yun@ansi-manager ansible_info]$ cat test_template.yml # 涉及的 playbook---# template 示例- hosts: proxyservers vars: - listen_port: 8080 - access_addr: zhangblog.com tasks: - name: &quot;template conf&quot; template: src: ./file/test_template.conf.j2 dest: /tmp/test_template.conf owner: root group: yun mode: &apos;0600&apos;[yun@ansi-manager ansible_info]$ ansible-playbook -b -i ./hosts_key test_template.yml # 执行 目标机器查看 123456789[root@ansi-haproxy01 tmp]# pwd/tmp[root@ansi-haproxy01 tmp]# cat test_template.conf # facts 变量dns_info=223.5.5.5mem_total=1821# 自定义变量listen_port=8080access_addr=zhangblog.com 相关选项说明： src：源文件「可以是绝对路径或相对路径」 dest：推送数据的目标路径或目标文件 owner：指定远端文件的属主 group：指定远端文件的属组 mode：指定远端文件的权限 backup：如果推送的目标文件存在且与源文件内容不同，那么会对目标文件进行备份 file 文件配置模块在受控机创建文件或目录，或修改属性信息「如：属主、属组、权限」 123456789101112131415161718# 查看 yum 模块方法「可优先查看 EXAMPLES 信息的使用案例，知晓如何使用」[yun@ansi-manager ansible_info]$ ansible-doc file # 由于我们使用的是 yun 普通用户# 因此有时需要使用 -b 选择进行提权## 在受控机改变目标文件的属性【该文件是已存在的】[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m file -a &quot;path=/tmp/yum_test.sh owner=yun group=yun mode=0600&quot; -i ./hosts_key ## 在受控机创建软连接[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m file -a &quot;src=/tmp/yum_test.sh dest=/tmp/yum_link.sh owner=yun group=yun state=link&quot; -i ./hosts_key## 在受控机创建硬链接[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m file -a &quot;src=/tmp/yum_test.sh dest=/tmp/yum_hard.sh owner=yun group=root state=hard&quot; -i ./hosts_key## 在受控机，如果目标文件不存在则创建[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m file -a &quot;path=/tmp/dest_file owner=yun group=yun state=touch&quot; -i ./hosts_key## 在控制机，如果目标目录不存在则创建「可创建多级目录」[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m file -a &quot;path=/tmp/desc_dir/aaa/bbb owner=yun group=root mode=700 state=directory&quot; -i ./hosts_key## 在控制机，改变目标目录和目录下所有目录或文件的属性信息「递归修改」[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m file -a &quot;path=/tmp/desc_dir owner=yun group=zhang mode=766 state=directory recurse=yes&quot; -i ./hosts_key## 在受控机，如果目标文件或目录存在，则删除「慎用！！！」[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m file -a &quot;path=/tmp/desc_dir state=absent &quot; -i ./hosts_key 相关选项说明： path：指定目标文件或目录 owner：指定目标文件的属主 group：指定目标文件的属组 mode：指定目标文件的权限 state：状态说明 &nbsp;&nbsp;&nbsp;&nbsp;file：默认值，指定文件 &nbsp;&nbsp;&nbsp;&nbsp;link：创建软连接 &nbsp;&nbsp;&nbsp;&nbsp;hard：创建硬链接 &nbsp;&nbsp;&nbsp;&nbsp;touch：如果文件不存在则创建 &nbsp;&nbsp;&nbsp;&nbsp;directory：如果目录不存在则创建 &nbsp;&nbsp;&nbsp;&nbsp;absent：如果目标文件或目录存在，则删除「慎用！！！」 recurse：递归授权 lineinfile 行编辑模块此模块确保文件中有特定的行，或者使用反向引用的正则表达式替换现有的行。当您只想更改文件中的一行时，这非常有用。 如果您想要更改多个相似的行，请查看[replace]模块。如果你想要插入/更新/删除文件中的一个行块，请查看[blockinfile]模块。对于其他情况，请参见[copy]或[template]模块。 数据文件准备 1234567891011[yun@ansi-manager tmp]$ cat /tmp/lineinfile_test # disabled - No SELinux policy is loaded.SELINUX=disabled1# disabled - No SELinux policy is loaded.SELINUX=disabled2# disabled - No SELinux policy is loaded.SELINUX=disabled3# SELINUXTYPE= can take one of three two values:# httpd listen portListen 80 1234# 查看 yum 模块方法「可优先查看 EXAMPLES 信息的使用案例，知晓如何使用」[yun@ansi-manager ansible_info]$ ansible-doc lineinfile # 由于我们使用的是 yun 普通用户# 因此有时需要使用 -b 选择进行提权 行插入 12345678910# 如果文件中没有 line 中的字符串，那么就追加在文件末尾；有则不作任何操作。[yun@ansi-manager ansible_info]$ ansible manageservers -b -m lineinfile -a &quot;path=/tmp/lineinfile_test line=&apos;# who are you?&apos;&quot; -i ./hosts_key# 首先保证line 中的字符串在文件中没有，如果有则不会添加# 其次会使用insertafter中的正则规则进行正则匹配，匹配成功则在最后一次匹配行后面插入line，如果没匹配成功则在文件末尾插入[yun@ansi-manager ansible_info]$ ansible manageservers -b -m lineinfile -a &quot;path=/tmp/lineinfile_test insertafter=&apos;SELINUX&apos; line=&apos;insertafter test1&apos;&quot; -i ./hosts_key[yun@ansi-manager ansible_info]$ ansible manageservers -b -m lineinfile -a &quot;path=/tmp/lineinfile_test insertafter=&apos;SELINUXXX&apos; line=&apos;insertafter test2&apos;&quot; -i ./hosts_key# 首先保证line 中的字符串在文件中没有，如果有则不会添加# 其次会使用insertbefore中的正则规则进行正则匹配，匹配成功则在最后一次匹配行前面插入line，如果没匹配成功则在文件末尾插入[yun@ansi-manager ansible_info]$ ansible manageservers -b -m lineinfile -a &quot;path=/tmp/lineinfile_test insertbefore=&apos;SELINUX&apos; line=&apos;insertbefore test1&apos;&quot; -i ./hosts_key[yun@ansi-manager ansible_info]$ ansible manageservers -b -m lineinfile -a &quot;path=/tmp/lineinfile_test insertbefore=&apos;SELINUXXX&apos; line=&apos;insertbefore test2&apos;&quot; -i ./hosts_key 行替换 1234# state=present时，如果多次匹配，那么最后一次匹配会被修改；如果没有匹配成功则在文件末尾追加，不管line是否存在。[yun@ansi-manager ansible_info]$ ansible manageservers -b -m lineinfile -a &quot;path=/tmp/lineinfile_test regexp=&apos;^SELINUX=&apos; line=&apos;SELINUX=enforcing&apos;&quot; -i ./hosts_key# state=present时，如果多次匹配，那么最后一次匹配会被修改；如果没有匹配成功则文件保持不变，使用backrefs=yes。[yun@ansi-manager ansible_info]$ ansible manageservers -b -m lineinfile -a &quot;path=/tmp/lineinfile_test regexp=&apos;^ELINUX=&apos; line=&apos;SELINUX=enforcing&apos; backrefs=yes&quot; -i ./hosts_key 行删除 1234# state=absent时，如果多次匹配，那么每一次匹配都会删除匹配行[yun@ansi-manager ansible_info]$ ansible manageservers -b -m lineinfile -a &quot;path=/tmp/lineinfile_test regexp=&apos;^SELINUX=&apos; state=absent&quot; -i ./hosts_key# 根据line匹配，如果匹配则删除匹配行[yun@ansi-manager ansible_info]$ ansible manageservers -b -m lineinfile -a &quot;path=/tmp/lineinfile_test line=&apos;# httpd listen port&apos; state=absent&quot; -i ./hosts_key 相关选项说明： path：要修改的文件。 line：与 state=present 配合使用；在文件中要插入或者替换的行。 state：状态说明 &nbsp;&nbsp;&nbsp;&nbsp;present：添加或修改，默认值 &nbsp;&nbsp;&nbsp;&nbsp;absent：删除 backrefs：与 state=present 配合使用；如果设置了 line 可以包含反向引用(位置和命名)，当‘regexp’匹配，就会填充反向引用。 &nbsp;&nbsp;&nbsp;&nbsp;这个参数稍微改变了模块的操作；’insertbefore’ 和 ‘insertafter’ 将被忽略，如果 ‘regexp’ 与文件中的任何地方不匹配，文件将保持不变。 regexp：对文件的每行进行正则匹配；对于 state=present 只有最后的一次行匹配会被替换；对于 state=absent 只要匹配就会删除该行。 backup：对源文件备份。默认：False create：与 state=present 配合使用；如果不存在则创建文件。默认：False insertafter：与 state=present 配合使用；在匹配行后插入。使用正则表达式，在指定正则表达式的最后一次匹配之后插入该行。如果需要第一个匹配，则使用(firstmatch=yes)。 &nbsp;&nbsp;&nbsp;&nbsp;如果没有匹配成功，那么会在文件末尾处插入。优先级低于 regexp。 insertbefore：与 state=present 配合使用；在匹配行前插入。使用正则表达式，在指定正则表达式的最后一次匹配之前插入该行。如果需要第一个匹配，则使用(firstmatch=yes)。 &nbsp;&nbsp;&nbsp;&nbsp;如果没有匹配成功，那么会在文件末尾处插入。优先级低于 regexp。 firstmatch：与 insertafter 或 insertbefore 配合使用；在 insertafter 或 insertbefore 的首次正则匹配。默认：False owner：指定远端文件的属主 group：指定远端文件的属组 mode：指定远端文件的权限 blockinfile 多行编辑模块该模块可以帮助我们在指定的文件中插入”一段文本”，这段文本是被标记过的。 换句话说就是，我们在这段文本上做了记号，以便在以后的操作中可以通过”标记”找到这段文本，然后修改或者删除它 1234# 查看 yum 模块方法「可优先查看 EXAMPLES 信息的使用案例，知晓如何使用」[yun@ansi-manager ansible_info]$ ansible-doc blockinfile# 由于我们使用的是 yun 普通用户# 因此有时需要使用 -b 选择进行提权 对不存在的文件进行——多行插入与修改 1234567891011121314151617# /tmp/blockinfile_test 文件是不存在的，因此使用了create选项# 插入/修改 如果没有则插入，如果之前有信息则修改##### 这里之前没有，则是创建文件并【插入】信息[yun@ansi-manager ansible_info]$ ansible manageservers -b -m blockinfile -a &apos;path=/tmp/blockinfile_test block=&quot;Match User ansible-agent\nPasswordAuthentication no&quot; create=yes&apos; -i ./hosts_key[root@ansi-manager ansible_info]# cat /tmp/blockinfile_test # 查看文件信息# BEGIN ANSIBLE MANAGED BLOCKMatch User ansible-agentPasswordAuthentication no# END ANSIBLE MANAGED BLOCK##### 文件已存在，mark标记已存在，这里是【修改】[yun@ansi-manager ansible_info]$ ansible manageservers -b -m blockinfile -a &apos;path=/tmp/blockinfile_test block=&quot;iface eth0 inet static\n address 192.0.2.23\n netmask 255.255.255.0&quot; create=yes&apos; -i ./hosts_key[root@ansi-manager ansible_info]# cat /tmp/blockinfile_test # 查看文件信息# BEGIN ANSIBLE MANAGED BLOCKiface eth0 inet static address 192.0.2.23 netmask 255.255.255.0# END ANSIBLE MANAGED BLOCK 对已存在的文件进行——多行插入与修改 1234567891011121314151617181920212223242526272829303132333435[root@ansi-manager ansible_info]# cat /tmp/blockinfile_test2 # 查看文件信息&lt;!DOCTYPE HTML PUBLIC &quot;-//IETF//DTD HTML 2.0//EN&quot;&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;blockinfile info&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;welcome to here.&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;##### 插入多行标记块信息 这里也使用了 insertafter 选项[yun@ansi-manager ansible_info]$ ansible manageservers -b -m blockinfile -a &apos;path=/tmp/blockinfile_test2 marker=&quot;&lt;!-- &#123;mark&#125; ANSIBLE MANAGED BLOCK --&gt;&quot; insertafter=&quot;&lt;body&gt;&quot; block=&quot;&lt;h1&gt;Welcome to blockinfile&lt;/h1&gt;\n&lt;p&gt;Last Login By you!&lt;/p&gt;&quot;&apos; -i ./hosts_key[root@ansi-manager ansible_info]# cat /tmp/blockinfile_test2 # 查看文件信息&lt;!DOCTYPE HTML PUBLIC &quot;-//IETF//DTD HTML 2.0//EN&quot;&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;blockinfile info&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;!-- BEGIN ANSIBLE MANAGED BLOCK --&gt;&lt;h1&gt;Welcome to blockinfile&lt;/h1&gt;&lt;p&gt;Last Login By you!&lt;/p&gt;&lt;!-- END ANSIBLE MANAGED BLOCK --&gt;&lt;h1&gt;welcome to here.&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;##### 删除标记块信息[yun@ansi-manager ansible_info]$ ansible manageservers -b -m blockinfile -a &apos;path=/tmp/blockinfile_test2 marker=&quot;&lt;!-- &#123;mark&#125; ANSIBLE MANAGED BLOCK --&gt;&quot; block=&quot;&quot;&apos; -i ./hosts_key[root@ansi-manager ansible_info]# cat /tmp/blockinfile_test2 # 查看文件信息&lt;!DOCTYPE HTML PUBLIC &quot;-//IETF//DTD HTML 2.0//EN&quot;&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;blockinfile info&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;welcome to here.&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; 相关选项说明： path：要修改的文件。 block：要插入标记行内的文本。如果该选项缺失或是一个空字符串，该块将被删除，就像 “state” 被指定为 “absent” 。（别名：content） state：状态说明 &nbsp;&nbsp;&nbsp;&nbsp;present：添加或修改，默认值 &nbsp;&nbsp;&nbsp;&nbsp;absent：删除 create：文件不存在则创建。默认：False backup：对源文件备份。默认：False insertafter：在匹配行后插入。使用正则表达式，在指定正则表达式的最后一次匹配之后插入该block。 &nbsp;&nbsp;&nbsp;&nbsp;如果没有匹配成功，那么会在文件末尾处插入。 insertbefore：在匹配行前插入。使用正则表达式，在指定正则表达式的最后一次匹配之前插入该block。 &nbsp;&nbsp;&nbsp;&nbsp;如果没有匹配成功，那么会在文件末尾处插入。 marker：标记线模板。’{mark}’ 将被替换为 ‘in marker_begin’ 的值【默认：BEGIN】和 ‘marker_end’ 的值【默认：END】 marker_begin：开始的标记变量信息。默认：BEGIN marker_end：结尾的标记变量信息。默认：END owner：指定远端文件/目录的属主 group：指定远端文件/目录的属组 mode：指定远端文件/目录的权限 ini_file INI格式配置模块在一个ini的文件中管理(添加、删除、更改)单独的配置，而不必使用[template]或[assemble]来管理整个文件。添加不存在的section。 1234# 查看 yum 模块方法「可优先查看 EXAMPLES 信息的使用案例，知晓如何使用」[yun@ansi-manager ansible_info]$ ansible-doc ini_file# 由于我们使用的是 yun 普通用户# 因此有时需要使用 -b 选择进行提权 添加与修改 12345678# 第一次执行，如果文件不存在，默认会创建 添加 section 和 option 信息[yun@ansi-manager ansible_info]$ ansible manageservers -b -m ini_file -a &quot;path=/tmp/test.ini section=drinks option=username value=bobe mode=&apos;0644&apos;&quot; -i ./hosts_key# 修改 option 信息[yun@ansi-manager ansible_info]$ ansible manageservers -b -m ini_file -a &quot;path=/tmp/test.ini section=drinks option=username value=alice&quot; -i ./hosts_key# 添加另外一个section信息[yun@ansi-manager ansible_info]$ ansible manageservers -b -m ini_file -a &quot;path=/tmp/test.ini section=base_info option=address value=&apos;BeiJing&apos;&quot; -i ./hosts_key# 添加一个option信息，还是放在 drinks 这个section下[yun@ansi-manager ansible_info]$ ansible manageservers -b -m ini_file -a &quot;path=/tmp/test.ini section=drinks option=password value=123456&quot; -i ./hosts_key 删除 1234# 删除整个section， 删除drinks这个section[yun@ansi-manager ansible_info]$ ansible manageservers -b -m ini_file -a &quot;path=/tmp/test.ini section=drinks state=absent&quot; -i ./hosts_key# 删除指定section下的option[yun@ansi-manager ansible_info]$ ansible manageservers -b -m ini_file -a &quot;path=/tmp/test.ini section=base_info option=address state=absent&quot; -i ./hosts_key 相关选项说明： path：INI格式文件路径。如果没有则默认会创建。 section：INI文件中的 section 名。当设置单个值时，如果是 ‘state=present’ 则会自动添加这个值。如果留空或设置为 ‘null’，’option’ 将放在第一个 ‘section’ 之前。 &nbsp;&nbsp;&nbsp;&nbsp;如果配置格式不支持 section，也需要使用 “null”。 option：如果设置（需要更改的 value），这是选项的名称。如果添加/删除整个“section”，可以省略。默认：null value：option 的值。删除 “option” 时可以省略。 state：状态说明 &nbsp;&nbsp;&nbsp;&nbsp;present：添加或修改，默认值 &nbsp;&nbsp;&nbsp;&nbsp;absent：删除 backup：对源文件备份。默认：False create：如果不存在则创建文件。默认：True owner：指定远端文件的属主 group：指定远端文件的属组 mode：指定远端文件的权限 no_extra_spaces：在 = 符号前后不插入空格。默认：False replace 多行替换模块对文件所匹配的内容进行替换/删除。 数据文件准备 12345678910[yun@ansi-manager ~]$ cat /tmp/replace_test 172.16.1.181 test1.zhangblog.com172.16.1.182 test2.zhangblog.org172.16.1.183 test3.zhangblog.net# httpd listen portListen 80ServerRoot &quot;/etc/httpd&quot;User apacheGroup apache 1234567891011121314151617# 查看 yum 模块方法「可优先查看 EXAMPLES 信息的使用案例，知晓如何使用」[yun@ansi-manager ansible_info]$ ansible-doc replace# 由于我们使用的是 yun 普通用户# 因此有时需要使用 -b 选择进行提权# 将regexp正则匹配到的行，替换为replace的内容；且这里使用了反向引用。[yun@ansi-manager ansible_info]$ ansible manageservers -b -m replace -a &quot;path=/tmp/replace_test regexp=&apos;(\s+)test\d+\.zhangblog\.(\w+)?$&apos; replace=&apos;\1new.host.name.\2&apos;&quot; -i ./hosts_key# 将 after 之后的每行都替换为 replace 中的内容[yun@ansi-manager ansible_info]$ ansible manageservers -b -m replace -a &quot;path=/tmp/replace_test after=&apos;# httpd listen port&apos; regexp=&apos;^(.+)$&apos; replace=&apos;after replace&apos;&quot; -i ./hosts_key# 如果 after没有匹配完一行，那么 after匹配之后未匹配的该行内容也会替换为replace 中的内容[yun@ansi-manager ansible_info]$ ansible manageservers -b -m replace -a &quot;path=/tmp/replace_test after=&apos;# httpd listen&apos; regexp=&apos;^(.+)$&apos; replace=&apos;# after replace&apos;&quot; -i ./hosts_key# 将 before 之前的每行都替换为 replace 中的内容[yun@ansi-manager ansible_info]$ ansible manageservers -b -m replace -a &quot;path=/tmp/replace_test before=&apos;listen port&apos; regexp=&apos;^(.+)$&apos; replace=&apos;before replace&apos;&quot; -i ./hosts_key# 如果 before没有匹配完一行，那么 before匹配之前未匹配的该行内容也会替换为replace 中的内容[yun@ansi-manager ansible_info]$ ansible manageservers -b -m replace -a &quot;path=/tmp/replace_test before=&apos;listen port&apos; regexp=&apos;^(.+)$&apos; replace=&apos;before replace&apos;&quot; -i ./hosts_key# 删除匹配内容，行数不会改变。如果整行匹配，则最终为空行。[yun@ansi-manager ansible_info]$ ansible manageservers -b -m replace -a &quot;path=/tmp/replace_test regexp=&apos;zhangblog&apos;&quot; -i ./hosts_key[yun@ansi-manager ansible_info]$ ansible manageservers -b -m replace -a &quot;path=/tmp/replace_test regexp=&apos;.*test.*&apos;&quot; -i ./hosts_key 相关选项说明： path：要操作的文件路径。 after：如果指定，只有after匹配之后的内容将被替换/删除。可以与before组合使用。after可能匹配一行也可能匹配一行的部分；且不支持正则匹配。 before：如果指定，只有before匹配之前的内容将被替换/删除。可以与after组合使用。before可能匹配一行也可能匹配一行的部分；不支持正则匹配。 regexp：要在文件内容中查找的正则表达式。 replace：替换regexp匹配项的字符串。可能包含反向引用，如果regexp匹配，将使用regexp捕获组展开这些反向引用。如果没有设置，则完全删除匹配项。 &nbsp;&nbsp;&nbsp;&nbsp;反向引用可以像 ‘\1’ 那样含糊地使用，也可以像 ‘\g&lt;1&gt;’ 那样显式地使用。 backup：对源文件备份。默认：False encoding：用于读写文件的字符编码。默认：utf-8 owner：指定远端文件的属主 group：指定远端文件的属组 mode：指定远端文件的权限 sysctl 修改内核参数模块这个模块操作 sysctl 条目，并在更改它们之后可选地执行 /sbin/sysctl -p 。 1234567# 查看 yum 模块方法「可优先查看 EXAMPLES 信息的使用案例，知晓如何使用」[yun@ansi-manager ansible_info]$ ansible-doc sysctl# 由于我们使用的是 yun 普通用户# 因此有时需要使用 -b 选择进行提权# 会修改 /etc/sysctl.conf 文件并执行 /sbin/sysctl -p 使其生效[yun@ansi-manager ansible_info]$ ansible manageservers -b -m sysctl -a &apos;name=net.ipv4.ip_forward value=1&apos; -i ./hosts_key[yun@ansi-manager ansible_info]$ ansible manageservers -b -m sysctl -a &apos;name=vm.swappiness value=5&apos; -i ./hosts_key 相关选项说明： name：内核参数变量名。（别名：key） value：sysctl键的期望值。（别名：val） reload：当为 yes 时，如果 sysctl_file 文件被修改，那么会执行 /sbin/sysctl -p，使修改的参数生效。当为 no 时，则不重载 sysctl ，尽管 sysctl_file 已被修改。 state：状态说明 &nbsp;&nbsp;&nbsp;&nbsp;present：添加或修改，默认值 &nbsp;&nbsp;&nbsp;&nbsp;absent：删除 sysctl_file：指定 sysctl.conf 文件的绝对路径。默认：/etc/sysctl.conf sysctl_set：使用sysctl命令验证令牌值，必要时使用 -w 进行设置。默认：no ignoreerrors：使用此选项可忽略关于未知键的错误。默认：no get_url 文件下载模块通过 HTTP，HTTPS或 FTP 下载一个文件。 123456# 查看 yum 模块方法「可优先查看 EXAMPLES 信息的使用案例，知晓如何使用」[yun@ansi-manager ansible_info]$ ansible-doc get_url ## 下载一个文件[yun@ansi-manager ansible_info]$ ansible proxyservers -m get_url -a &quot;url=&apos;http://www.zhangblog.com/uploads/jvm/jvm-01-01.png&apos; dest=/tmp/ mode=0640&quot; -i ./hosts_key## 下载前会比对校验和，如果不匹配则不下载[yun@ansi-manager ansible_info]$ ansible proxyservers -m get_url -a &quot;url=&apos;http://www.zhangblog.com/uploads/jvm/jvm-01-01.png&apos; dest=/tmp/jvm-001.png checksum=&apos;md5:9af3f6066ea46ea81c0b3c9d719dbce0&apos;&quot; -i ./hosts_key 相关选项说明： mode：指定目标文件的权限 url：指定文件来源「支持 HTTP，HTTPS 和 FTP」 dest：指定目标存放的目录或文件 checksum：校验和「支持 sha256 和 md5」 timeout：请求超时时间，默认 10 秒。 owner：指定远端文件的属主 group：指定远端文件的属组 mode：指定远端文件的权限 backup：源文件备份 service、systemd 服务管理模块如果在 CentOS 6 及以下版本，优先使用 service 。 如果在 CentOS 7 及以上版本，优先使用 systemd 。 这里考虑到我们使用的 CentOS 7，因此使用的是 systemd 。至于 service 请自行查看文档。 1234567891011# 查看 yum 模块方法「可优先查看 EXAMPLES 信息的使用案例，知晓如何使用」[yun@ansi-manager ansible_info]$ ansible-doc service [yun@ansi-manager ansible_info]$ ansible-doc systemd # 由于我们使用的是 yun 普通用户# 因此有时需要使用 -b 选择进行提权## 启动 httpd 服务，并且加入开机自启动[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m systemd -a &quot;name=httpd state=started enabled=yes&quot; -i ./hosts_key## 重启 httpd 服务，并且重新加载 /usr/lib/systemd/system/httpd.service 服务配置文件，且加入开机自启动[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m systemd -a &quot;name=httpd state=restarted daemon_reload=yes enabled=yes&quot; -i ./hosts_key## 停止 httpd 服务，并且不加入开机自启动[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m systemd -a &quot;name=httpd state=stopped enabled=no&quot; -i ./hosts_key 相关选项说明： name：服务名称 state：服务状态 &nbsp;&nbsp;&nbsp;&nbsp;started：启动服务 &nbsp;&nbsp;&nbsp;&nbsp;stopped：停止服务 &nbsp;&nbsp;&nbsp;&nbsp;reloaded：重加载服务 &nbsp;&nbsp;&nbsp;&nbsp;restarted：重启动服务 enabled：是否加入开机自启动「yes 加入， no 不加入， 默认 null」 daemon_reload：当我们修改了服务管理配置文件，是否重加载其配置「yes 重加载服务配置文件， no 不加载，默认值」 group 组模块创建或删除用户组 12345678910# 查看 yum 模块方法「可优先查看 EXAMPLES 信息的使用案例，知晓如何使用」[yun@ansi-manager ansible_info]$ ansible-doc group # 由于我们使用的是 yun 普通用户# 因此有时需要使用 -b 选择进行提权## 创建 test 组，并指定组ID[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m group -a &quot; name=test gid=9001&quot; -i ./hosts_key## 创建 testsystem 组，并指定为系统组[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m group -a &quot;name=testsystem system=true&quot; -i ./hosts_key## 删除 testsystem 组[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m group -a &quot;name=testsystem state=absent&quot; -i ./hosts_key 相关选项说明： gid：指定组ID，默认 null name：指定组名称 state：组状态 &nbsp;&nbsp;&nbsp;&nbsp;present：创建组，默认 &nbsp;&nbsp;&nbsp;&nbsp;absent：删除组 system：是否为系统组 &nbsp;&nbsp;&nbsp;&nbsp;true：是系统组 &nbsp;&nbsp;&nbsp;&nbsp;false：不是系统组 user 用户模块创建或删除用户 1234# 查看 yum 模块方法「可优先查看 EXAMPLES 信息的使用案例，知晓如何使用」[yun@ansi-manager ansible_info]$ ansible-doc user # 由于我们使用的是 yun 普通用户# 因此有时需要使用 -b 选择进行提权 1、创建用户，指定 UID，指定附加组，不创建家目录，不可以登录 1[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m user -a &quot;name=zhangtest uid=1005 groups=zhang,yun create_home=no shell=/sbin/nologin&quot; -i ./hosts_key 受控机查看信息 1234[yun@ansi-haproxy02 ~]$ id zhangtestuid=1005(zhangtest) gid=1005(zhangtest) groups=1005(zhangtest),1000(zhang),1050(yun)[yun@ansi-haproxy02 ~]$ tail -n1 /etc/passwdzhangtest:x:1005:1005::/home/zhangtest:/sbin/nologin # /home/zhangtest 该目录不存在 2、删除用户，但不删除用户的家目录 1[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m user -a &quot;name=zhangtest state=absent&quot; -i ./hosts_key 3、为用户创建密码或SSH key 秘钥 1[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m user -a &quot;name=zhangtest2 generate_ssh_key=yes ssh_key_bits=2048 ssh_key_file=.ssh/id_rsa&quot; -i ./hosts_key 4、为用户创建密码 1234567# 得到密码串[yun@ansi-manager ansible_info]$ ansible localhost -m debug -a &quot;msg=&#123;&#123; &apos;123456&apos; | password_hash(&apos;sha512&apos;, &apos;salt&apos;) &#125;&#125;&quot;localhost | SUCCESS =&gt; &#123; &quot;msg&quot;: &quot;$6$salt$MktMKPZJ6t59GfxcJU20DwcwQzfMvOlHFVZiOVD71w.igcOo1R7vBYR65JquIQ/7siC7VRpmteKvZmfSkNc69.&quot;&#125;## 注意 -a &apos;&apos; 是单引号，而不是双引号。如果使用双引号，特殊字符会被解析[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m user -a &apos;name=zhangtest2 password=$6$salt$MktMKPZJ6t59GfxcJU20DwcwQzfMvOlHFVZiOVD71w.igcOo1R7vBYR65JquIQ/7siC7VRpmteKvZmfSkNc69.&apos; -i ./hosts_key 相关选项说明： name：用户名称 state：用户状态 &nbsp;&nbsp;&nbsp;&nbsp;present：默认值，创建用户 &nbsp;&nbsp;&nbsp;&nbsp;absent：删除用户 password：用户密码，默认：null shell：设置用户的shell uid：指定用户 UID group：设置用户主组，默认：null groups：设置用户附加组，默认：null system：如果为 yes 则创建系统用户，默认：false home：设置用户家目录 create_home：是否创建家目录，默认：true，如果不创建为：no comment：用户描述，默认：null expires：用户账号失效日期，默认：null update_password：更新密码 &nbsp;&nbsp;&nbsp;&nbsp;always：如果密码不同，则更新，默认值 &nbsp;&nbsp;&nbsp;&nbsp;on_create：只有新建用户时使用 remove：只有在 state=absent 时生效，作用：删除用户家目录 generate_ssh_key：是否生成SSH key 密钥对，默认：false ssh_key_bits：设置 SSH key 字节长度，默认：有 ssh-keygen 设置 ssh_key_comment：设置 SSH key 的描述，默认为：$HOSTNAME ssh_key_file：指定 SSH key 的文件名，默认：null 【即为：.ssh/id_rsa】 ssh_key_passphrase：指定 SSH key 密码，如果不提供则没有密码，默认：null ssh_key_type：设置 SSH key 类型，默认：rsa cron 定时任务模块创建、注释或删除定时任务 1234# 查看 yum 模块方法「可优先查看 EXAMPLES 信息的使用案例，知晓如何使用」[yun@ansi-manager ansible_info]$ ansible-doc cron # 由于我们使用的是 yun 普通用户# 因此有时需要使用 -b 选择进行提权 123456## 创建定时任务， name 为定时任务说明[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m cron -a &apos;name=&quot;crond test&quot; minute=0 hour=&quot;2,5&quot; job=&quot;ls /tmp &gt;/dev/null 2&gt;&amp;1&quot;&apos; -i ./hosts_key## 注释指定定时任务，其中 name ，时间参数，和 job 都需要。[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m cron -a &apos;name=&quot;crond test&quot; minute=0 hour=&quot;2,5&quot; job=&quot;ls /tmp &gt;/dev/null 2&gt;&amp;1&quot; disabled=true&apos; -i ./hosts_key## 删除定时任务[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m cron -a &apos;name=&quot;crond test&quot; state=absent&apos; -i ./hosts_key 相关选项说明： name：定时任务描述 state：状态 &nbsp;&nbsp;&nbsp;&nbsp;present：添加，默认值 &nbsp;&nbsp;&nbsp;&nbsp;absent：删除 user：指定哪个用户的定时任务会修改，默认是 root 用户，默认：null minute：哪个分钟执行 ( 0-59, *, */2, etc )，默认：* hour：哪个小时执行 ( 0-23, *, */2, etc )，默认 * day：每月的哪天执行 ( 1-31, *, */2, 等 )，默认：* month：哪个月执行 ( 1-12, *, */2, etc )，默认：* weekday：每周的哪天执行 ( 0-6 为 Sunday-Saturday, *, etc )，默认：* disabled：是否注释指定定时任务，默认：false job：定时任务要操作的具体信息 mount 文件系统挂载模块文件系统的挂载与取消。 1234# 查看 yum 模块方法「可优先查看 EXAMPLES 信息的使用案例，知晓如何使用」[yun@ansi-manager ansible_info]$ ansible-doc mount # 由于我们使用的是 yun 普通用户# 因此有时需要使用 -b 选择进行提权 案例示例： 在 ansi-manager 机器作为 NFS 的服务端，ansi-haproxy01、ansi-haproxy02 作为 NFS 的客户端。 1、在所有机器安装 NFS 必要的包 1[yun@ansi-manager ansible_info]$ ansible manageservers,proxyservers -b -m yum -a &quot;name=nfs-utils,rpcbind state=present&quot; -i ./hosts_key 2、NFS 服务端配置与启动 1234567[yun@ansi-manager ansible_info]$ ansible manageservers -b -m copy -a &apos;content=&quot;/data 172.16.1.0/24(rw,sync,root_squash,all_squash)\n&quot; dest=/etc/exports&apos; -i ./hosts_key# 创建 /data 目录，属主、属组为 nfsnobody[yun@ansi-manager ansible_info]$ ansible manageservers -b -m file -a &quot;path=/data owner=nfsnobody group=nfsnobody state=directory&quot; -i ./hosts_key[yun@ansi-manager ansible_info]$ ansible manageservers -b -m systemd -a &quot;name=rpcbind state=started enabled=yes&quot; -i ./hosts_key[yun@ansi-manager ansible_info]$ ansible manageservers -b -m systemd -a &quot;name=nfs state=started enabled=yes&quot; -i ./hosts_key## 查看 Export list 信息[yun@ansi-manager ansible_info]$ ansible manageservers -b -m shell -a &quot;showmount -e&quot; -i ./hosts_key 3、NFS 客户端操作 123[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m systemd -a &quot;name=rpcbind state=started enabled=yes&quot; -i ./hosts_key## 检查共享信息[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m shell -a &quot;showmount -e 172.16.1.180&quot; -i ./hosts_key 4、NFS 客户端挂载 12345678## 不挂载设备，仅在 /etc/fstab 中写入挂载配置信息[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m mount -a &quot;src=172.16.1.180:/data path=/mnt fstype=nfs state=present backup=yes&quot; -i ./hosts_key## 挂载设备，并在 /etc/fstab 中写入挂载配置信息[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m mount -a &quot;src=172.16.1.180:/data path=/mnt fstype=nfs state=mounted&quot; -i ./hosts_key## 不卸载设备，仅在 /etc/fstab 中删除挂载配置信息[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m mount -a &quot;src=172.16.1.180:/data path=/mnt fstype=nfs state=unmounted&quot; -i ./hosts_key## 卸载设备，并在 /etc/fstab 中删除挂载配置信息[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m mount -a &quot;src=172.16.1.180:/data path=/mnt fstype=nfs state=absent&quot; -i ./hosts_key 相关选项说明： src：要挂在的设备/磁盘 path：挂载点 opts：挂载参数选项，如：ro,noauto；默认：null fstype：文件系统类型 state：状态 &nbsp;&nbsp;&nbsp;&nbsp;present：不挂载设备，仅在 /etc/fstab 中写入挂载配置信息 &nbsp;&nbsp;&nbsp;&nbsp;mounted：挂载设备，并在 /etc/fstab 中写入挂载配置信息 &nbsp;&nbsp;&nbsp;&nbsp;unmounted：不卸载设备，仅在 /etc/fstab 中删除挂载配置信息 &nbsp;&nbsp;&nbsp;&nbsp;absent：卸载设备，并在 /etc/fstab 中删除挂载配置信息 backup：对之前的文件备份 debug 调试模块与 register 变量这里会使用 playbook 书写。具体的 playbook 详解，参见后面的文章。 涉及到 playbook、变量和 Jinja ，这些在后面的文章会有详解。 1234567891011121314151617181920212223242526272829303132# 查看 yum 模块方法「可优先查看 EXAMPLES 信息的使用案例，知晓如何使用」[yun@ansi-manager ansible_info]$ ansible-doc debug # 由于我们使用的是 yun 普通用户# 因此有时需要使用 -b 选择进行提权# Ad-Hoc 方式[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m debug -i ./hosts_key [yun@ansi-manager ansible_info]$ ansible proxyservers -b -m debug -a &apos;msg=&quot;print customized message&quot;&apos; -i ./hosts_key################################################### Playbooks 方式[yun@ansi-manager ansible_info]$ pwd/app/ansible_info[yun@ansi-manager ansible_info]$ lltotal 24-rw-rw-r-- 1 yun yun 483 Aug 18 09:12 hosts_key-rw-rw-r-- 1 yun yun 245 Aug 18 21:55 test_debug_register.yml[yun@ansi-manager ansible_info]$ cat test_debug_register.yml ---# 如何使用 debug 模块与 register 变量- hosts: proxyservers tasks: - name: &quot;get host port info&quot; shell: netstat -lntp register: host_port - name: &quot;print host port&quot; debug: #msg: &quot;&#123;&#123; host_port &#125;&#125;&quot; # 输出全部信息 #msg: &quot;&#123;&#123; host_port.cmd &#125;&#125;&quot; # 引用方式一 msg: &quot;&#123;&#123; host_port[&apos;stdout_lines&apos;] &#125;&#125;&quot; # 引用方式二[yun@ansi-manager ansible_info]$ ansible-playbook -b -i ./hosts_key test_debug_register.yml 第一个 task 中，使用了 register 注册变量，名为 host_port ；当 shell 模块执行完毕后，会将数据放到该变量中。 第二给 task 中，使用了 debug 模块，并从 host_port 中获取数据。 assert 断言模块【了解】对自定义消息断言。 涉及到 playbook、变量和 Jinja ，这些在后面的文章会有详解。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# 查看 yum 模块方法「可优先查看 EXAMPLES 信息的使用案例，知晓如何使用」[yun@ansi-manager ansible_info]$ ansible-doc assert # 由于我们使用的是 yun 普通用户# 因此有时需要使用 -b 选择进行提权# Ad-Hoc 方式[yun@ansi-manager ansible_info]$ ansible proxyservers -m assert -a &quot;that=&apos;2 == 3&apos; success_msg=ok fail_msg=fail&quot; -i ./hosts_key # 断言失败[yun@ansi-manager ansible_info]$ ansible proxyservers -m assert -a &quot;that=&apos;2 &lt;= 3&apos; success_msg=ok fail_msg=fail&quot; -i ./hosts_key # 断言成功[yun@ansi-manager ansible_info]$ ansible proxyservers -m assert -a &quot;that=&apos;2 &lt; 3&apos; success_msg=ok fail_msg=fail&quot; -i ./hosts_key # 断言成功[yun@ansi-manager ansible_info]$ ansible proxyservers -m assert -a &quot;that=&apos;2 &lt; 3 and 4 == 4&apos; success_msg=ok fail_msg=fail quiet=yes&quot; -i ./hosts_key # 断言成功[yun@ansi-manager ansible_info]$ ansible proxyservers -m assert -a &quot;that=&apos;2 &lt; 3 and 4 &gt; 4&apos; success_msg=ok fail_msg=fail quiet=yes&quot; -i ./hosts_key # 断言失败[yun@ansi-manager ansible_info]$ ansible proxyservers -m assert -a &quot;that=&apos;3 &lt; 3 or 4 == 4&apos; success_msg=ok fail_msg=fail quiet=yes&quot; -i ./hosts_key # 断言成功################################################### Playbooks 方式[yun@ansi-manager ansible_info]$ pwd/app/ansible_info[yun@ansi-manager ansible_info]$ lltotal 12-rw-rw-r-- 1 yun yun 226 Oct 8 16:07 hosts_key-rw-rw-r-- 1 yun yun 902 Oct 11 10:57 test_assert.yml[yun@ansi-manager ansible_info]$ cat test_assert.yml # playbook 信息---# assert 示例- hosts: proxyservers # 使用如下变量，分别测试下 vars: #- my_param: 20 #- my_param: -2 - my_param: 200 tasks: - name: &quot;assert example 1&quot; assert: # ansible_os_family 为 facts 中的变量信息 that: - ansible_os_family == &quot;RedHat&quot; success_msg: &quot;success info&quot; fail_msg: &quot;fail info&quot; - name: &quot;assert example 2&quot; assert: # that 下面的列表，为 &amp;&amp; 关系 that: - my_param &gt;= 0 - my_param &lt;= 100 fail_msg: &quot;&apos;my_param&apos; must be between 0 and 100&quot; success_msg: &quot;success info&quot; # 是否忽略该 task 的错误 ignore_errors: True - name: &quot;assert example 3&quot; assert: that: - my_param &lt;= 10 or my_param &gt;= 100 fail_msg: &quot;&apos;my_param&apos; must be &lt;= 10 or &gt;= 100&quot; success_msg: &quot;success info&quot;[yun@ansi-manager ansible_info]$ ansible-playbook -b -i ./hosts_key test_assert.yml # 执行 相关选项说明： that：列表字符串表达式 success_msg：当断言成功时输出的信息 fail_msg：别名 msg，当断言失败时输出的信息 quiet：默认 False，设置为 yes 避免冗长输出 selinux 安全模块配置 SELinux 。 123456# 查看 yum 模块方法「可优先查看 EXAMPLES 信息的使用案例，知晓如何使用」[yun@ansi-manager ansible_info]$ ansible-doc selinux # 由于我们使用的是 yun 普通用户# 因此有时需要使用 -b 选择进行提权## 关闭 selinux[yun@ansi-manager ansible_info]$ ansible proxyservers -b -m selinux -a &quot;state=disabled&quot; -i ./hosts_key 相关选项说明： state：状态 &nbsp;&nbsp;&nbsp;&nbsp;disabled：不可用 &nbsp;&nbsp;&nbsp;&nbsp;enforcing：强制执行 &nbsp;&nbsp;&nbsp;&nbsp;permissive：会提醒]]></content>
      <categories>
        <category>ansbile</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible-免密登录与主机清单Inventory]]></title>
    <url>%2F2020%2F01%2F01%2Fansible-03%2F</url>
    <content type="text"><![CDATA[主机规划 主机名称 操作系统版本 内网IP 外网IP(模拟) 安装软件 ansi-manager CentOS7.5 172.16.1.180 10.0.0.180 ansible ansi-haproxy01 CentOS7.5 172.16.1.181 10.0.0.181 ansi-haproxy02 CentOS7.5 172.16.1.182 10.0.0.182 ansi-web01 CentOS7.5 172.16.1.183 10.0.0.183 ansi-web02 CentOS7.5 172.16.1.184 10.0.0.184 ansi-web03 CentOS7.5 172.16.1.185 10.0.0.185 在实际使用中并不需要对ansible配置进行修改，或者说只有需要的时候才修改ansible配置。 添加用户账号说明： 1、 运维人员使用的登录账号； 2、 所有的业务都放在 /app/ 下「yun用户的家目录」，避免业务数据乱放； 3、 该用户也被 ansible 使用，因为几乎所有的生产环境都是禁止 root 远程登录的（因此该 yun 用户也进行了 sudo 提权）。 1234567# 使用一个专门的用户，避免直接使用root用户# 添加用户、指定家目录并指定用户密码# sudo提权# 让其它普通用户可以进入该目录查看信息useradd -u 1050 -d /app yun &amp;&amp; echo &apos;123456&apos; | /usr/bin/passwd --stdin yunecho &quot;yun ALL=(ALL) NOPASSWD: ALL&quot; &gt;&gt; /etc/sudoerschmod 755 /app/ 基于密码连接「了解」在实际生产环境中，建议使用基于秘钥连接而不是密码连接。 原因如下： 1、将密码直接写入文件中，有安全隐患； 2、生产环境的密码可能会定期更换，如果基于密码连接，那么我们也会频繁的维护，造成维护成本高； 3、基于秘钥连接，我们只需要做一次秘钥分发，后期连接无需任何修改。 清单配置1234567891011121314151617[yun@ansi-manager ansible_info]$ pwd/app/ansible_info[yun@ansi-manager ansible_info]$ cat hosts_pwd # 未分组机器，放在所有组前面# 默认端口22，可省略# 方式1：主机 + 端口 + 密码172.16.1.180 ansible_ssh_port=22 ansible_ssh_user=yun ansible_ssh_pass=&apos;123456&apos;# 方式2：主机 + 端口 + 密码[proxyservers]172.16.1.18[1:2] ansible_ssh_port=22 ansible_ssh_user=yun ansible_ssh_pass=&apos;123456&apos;# 方式3：主机 + 端口 + 密码[webservers]172.16.1.18[3:5] ansible_ssh_port=22 ansible_ssh_user=yun[webservers:vars]ansible_ssh_pass=&apos;123456&apos; 测验连接12345678[yun@ansi-manager ansible_info]$ ansible 172.16.1.180 -m ping -i ./hosts_pwd # 普通用户执行172.16.1.180 | FAILED! =&gt; &#123; &quot;msg&quot;: &quot;Using a SSH password instead of a key is not possible because Host Key checking is enabled and sshpass does not support this. Please add this host&apos;s fingerprint to your known_hosts file to manage this host.&quot;&#125;[yun@ansi-manager ansible_info]$ sudo ansible 172.16.1.180 -m ping -i ./hosts_pwd # 提权使用 root 用户执行172.16.1.180 | FAILED! =&gt; &#123; &quot;msg&quot;: &quot;Using a SSH password instead of a key is not possible because Host Key checking is enabled and sshpass does not support this. Please add this host&apos;s fingerprint to your known_hosts file to manage this host.&quot;&#125; 大概提示信息：因为启用了主机密钥检查，而 sshpass 不支持这一点。请将此主机「172.16.1.180」的指纹添加到你本机的known_hosts文件中以管理此主机。 跳过主机密钥检查，有两种方式：方式1：修改 Linux 系统配置 12345[root@ansi-manager ssh]# vim /etc/ssh/ssh_config # AddressFamily any# ConnectTimeout 0# StrictHostKeyChecking ask # 将该配置的注释打开，并改为 StrictHostKeyChecking no 这样针对所有用户都不会在进行 「主机密钥检查」了# IdentityFile ~/.ssh/identity 但是这个是 Linux 自带的配置，我们不能随意去更改。因此不建议如此操作。 方式2：修改 ansible 配置 12345[root@ansi-manager ansible]# pwd/etc/ansible[root@ansi-manager ansible]# vim ansible.cfg# uncomment this to disable SSH key host checkinghost_key_checking = False # 将该配置的注释去掉 改配置仅对 root 用户生效，其他普通用户是不生效的。这里使用该方法。 再次连接测试1234567891011121314[yun@ansi-manager ansible_info]$ ansible 172.16.1.180 -m ping -i ./hosts_pwd # 普通用户还是不行172.16.1.180 | FAILED! =&gt; &#123; &quot;msg&quot;: &quot;Using a SSH password instead of a key is not possible because Host Key checking is enabled and sshpass does not support this. Please add this host&apos;s fingerprint to your known_hosts file to manage this host.&quot;&#125;[yun@ansi-manager ansible_info]$ sudo ansible 172.16.1.180 -m ping -i ./hosts_pwd # 提权使用 root 用户执行172.16.1.180 | SUCCESS =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python&quot; &#125;, &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;[yun@ansi-manager ansible_info]$ sudo ansible proxyservers -m ping -i ./hosts_pwd # 正常[yun@ansi-manager ansible_info]$ sudo ansible webservers -m ping -i ./hosts_pwd # 正常 基于秘钥连接「推荐」在实际生产环境中，建议使用基于秘钥连接而不是密码连接。 原因如下： 1、将密码直接写入文件中，有安全隐患； 2、生产环境的密码可能会定期更换，如果基于密码连接，那么我们也会频繁的维护，造成维护成本高； 3、基于秘钥连接，我们只需要做一次秘钥分发，后期连接无需任何修改。 实现yun用户免秘钥登录要求：根据规划实现 172.16.1.180 到 172.16.1.180、172.16.1.181、172.16.1.182、172.16.1.183、172.16.1.184、172.16.1.185 免秘钥登录 因此需要在 172.16.1.180 机器创建秘钥，然后分发到受控机器。 创建秘钥12345678[yun@ansi-manager ~]$ ssh-keygen -t rsa # 一路回车即可 注意使用的是 yun 用户# 生成之后会在用户的根目录生成一个 “.ssh”的文件夹[yun@ansi-manager ~]$ ll -d .ssh/drwx------ 2 yun yun 38 Jul 25 10:51 .ssh/[yun@ansi-manager ~]$ ll .ssh/total 8-rw------- 1 yun yun 1675 Jul 25 10:51 id_rsa-rw-r--r-- 1 yun yun 398 Jul 25 10:51 id_rsa.pub 分发密钥123456[yun@ansi-manager ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub 172.16.1.180[yun@ansi-manager ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub 172.16.1.181[yun@ansi-manager ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub 172.16.1.182[yun@ansi-manager ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub 172.16.1.183[yun@ansi-manager ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub 172.16.1.184[yun@ansi-manager ~]$ ssh-copy-id -i ~/.ssh/id_rsa.pub 172.16.1.185 测验免密登录是否成功123456[yun@ansi-manager ~]$ ssh 172.16.1.180 # 等价于 ssh yun@172.16.1.180[yun@ansi-manager ~]$ ssh 172.16.1.181[yun@ansi-manager ~]$ ssh 172.16.1.182[yun@ansi-manager ~]$ ssh 172.16.1.183[yun@ansi-manager ~]$ ssh 172.16.1.184[yun@ansi-manager ~]$ ssh 172.16.1.185 注意：必须保证每台机器都免密登录成功，因此需要每台机器都验证。 .ssh目录中的文件说明12345678910111213[yun@ansi-manager .ssh]$ pwd/app/.ssh[yun@ansi-manager .ssh]$ lltotal 16-rw------- 1 yun yun 398 Jul 25 11:01 authorized_keys-rw------- 1 yun yun 1675 Jul 25 10:51 id_rsa-rw-r--r-- 1 yun yun 398 Jul 25 10:51 id_rsa.pub-rw-r--r-- 1 yun yun 1120 Jul 25 11:04 known_hosts ########################################################################################authorized_keys ：存放要远程免密登录机器的公钥，主要通过这个文件记录多台要远程登录机器的公钥id_rsa : 生成的私钥文件id_rsa.pub ：生成的公钥文件know_hosts : 已知的主机公钥清单 清单配置1234567891011121314151617[yun@ansi-manager ansible_info]$ pwd/app/ansible_info[yun@ansi-manager ansible_info]$ cat hosts_key # 未分组机器，放在所有组前面# 默认端口22，可省略# 方式1、主机 + 端口 + 密钥172.16.1.180:22# 方式2：主机 + 端口 + 密钥[proxyservers]172.16.1.18[1:2]:22# 方式3：别名 + 主机 + 端口 + 密码[webservers]web01 ansible_ssh_host=172.16.1.183 ansible_ssh_port=22web02 ansible_ssh_host=172.16.1.184 ansible_ssh_port=22web03 ansible_ssh_host=172.16.1.185 ansible_ssh_port=22 测验连接测验一 12345678[yun@ansi-manager ansible_info]$ ansible 172.16.1.180 -m ping -i ./hosts_key 172.16.1.180 | SUCCESS =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python&quot; &#125;, &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125; 测验二 123456789101112131415[yun@ansi-manager ansible_info]$ ansible proxyservers -m ping -i ./hosts_key 172.16.1.181 | SUCCESS =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python&quot; &#125;, &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;172.16.1.182 | SUCCESS =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python&quot; &#125;, &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125; 测验三 12345678910111213141516171819202122[yun@ansi-manager ansible_info]$ ansible webservers -m ping -i ./hosts_key web03 | SUCCESS =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python&quot; &#125;, &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;web01 | SUCCESS =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python&quot; &#125;, &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;web02 | SUCCESS =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python&quot; &#125;, &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125; 混合方式和主机组方式清单配置123456789101112131415161718192021[yun@ansi-manager ansible_info]$ pwd/app/ansible_info[yun@ansi-manager ansible_info]$ cat hosts_group # 未分组机器，放在所有组前面# 默认端口22，可省略# 方式1、主机 + 端口 + 密钥172.16.1.180# 方式一、主机组变量 + 主机 + 密码[proxyservers]172.16.1.18[1:2] ansible_ssh_port=22 ansible_ssh_user=yun ansible_ssh_pass=&apos;123456&apos;# 方式二、主机组变量 + 主机 + 密钥[webservers]172.16.1.18[3:5]:22# 定义多组，多组汇总整合# website 组包括两个子组[proxyservers, webservers][website:children]proxyserverswebservers 说明：定义多组使用没有问题。但是不能像上面一样既有密码配置，又有秘钥配置，这样会增加维护成本。这里为了演示因此用了密码和秘钥配置。 测验连接测验一 1234567891011121314151617181920# 如果 ~/.ssh/known_hosts 文件中没有添加受控机指纹，那么必须提权操作[yun@ansi-manager ansible_info]$ sudo ansible proxyservers -m ping -i ./hosts_group --list-hosts hosts (2): 172.16.1.181 172.16.1.182[yun@ansi-manager ansible_info]$ sudo ansible proxyservers -m ping -i ./hosts_group 172.16.1.182 | SUCCESS =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python&quot; &#125;, &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125;172.16.1.181 | SUCCESS =&gt; &#123; &quot;ansible_facts&quot;: &#123; &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python&quot; &#125;, &quot;changed&quot;: false, &quot;ping&quot;: &quot;pong&quot;&#125; 测验二 1234567[yun@ansi-manager ansible_info]$ ansible webservers -m ping -i ./hosts_group --list-hosts hosts (3): 172.16.1.183 172.16.1.184 172.16.1.185[yun@ansi-manager ansible_info]$ ansible webservers -m ping -i ./hosts_group ……………… 测验三 12345678[yun@ansi-manager ansible_info]$ ansible website -m ping -i ./hosts_group --list-hosts hosts (5): 172.16.1.181 172.16.1.182 172.16.1.183 172.16.1.184 172.16.1.185[yun@ansi-manager ansible_info]$ ansible website -m ping -i ./hosts_group 测验四 特殊组：all 123456789[yun@ansi-manager ansible_info]$ ansible all -m ping -i ./hosts_group --list-hosts hosts (6): 172.16.1.180 172.16.1.181 172.16.1.182 172.16.1.183 172.16.1.184 172.16.1.185[yun@ansi-manager ansible_info]$ ansible all -m ping -i ./hosts_group]]></content>
      <categories>
        <category>ansbile</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible-安装配置]]></title>
    <url>%2F2020%2F01%2F01%2Fansible-02%2F</url>
    <content type="text"><![CDATA[主机规划 主机名称 操作系统版本 内网IP 外网IP(模拟) 安装软件 ansi-manager CentOS7.5 172.16.1.180 10.0.0.180 ansible ansi-haproxy01 CentOS7.5 172.16.1.181 10.0.0.181 ansi-haproxy02 CentOS7.5 172.16.1.182 10.0.0.182 ansi-web01 CentOS7.5 172.16.1.183 10.0.0.183 ansi-web02 CentOS7.5 172.16.1.184 10.0.0.184 ansi-web03 CentOS7.5 172.16.1.185 10.0.0.185 在实际使用中并不需要对ansible配置进行修改，或者说只有需要的时候才修改ansible配置。 添加用户账号说明： 1、 运维人员使用的登录账号； 2、 所有的业务都放在 /app/ 下「yun用户的家目录」，避免业务数据乱放； 3、 该用户也被 ansible 使用，因为几乎所有的生产环境都是禁止 root 远程登录的（因此该 yun 用户也进行了 sudo 提权）。 1234567# 使用一个专门的用户，避免直接使用root用户# 添加用户、指定家目录并指定用户密码# sudo提权# 让其它普通用户可以进入该目录查看信息useradd -u 1050 -d /app yun &amp;&amp; echo &apos;123456&apos; | /usr/bin/passwd --stdin yunecho &quot;yun ALL=(ALL) NOPASSWD: ALL&quot; &gt;&gt; /etc/sudoerschmod 755 /app/ Ansible 部署流程添加 epel 源「如果没有的话」添加阿里云 epel 源 1https://opsx.alibaba.com/mirror Ansible 安装与版本信息查看12345678910111213[root@ansi-manager ~]# yum install -y ansible [root@ansi-manager ~]# whereis ansible # ansible 位置信息ansible: /usr/bin/ansible /etc/ansible /usr/share/ansible /usr/share/man/man1/ansible.1.gz[root@ansi-manager ~]# ansible --version # 版本信息查看ansible 2.8.1 # ansible 版本 config file = /etc/ansible/ansible.cfg # 使用的配置文件 configured module search path = [u&apos;/root/.ansible/plugins/modules&apos;, u&apos;/usr/share/ansible/plugins/modules&apos;] # 模块查找路径 ansible python module location = /usr/lib/python2.7/site-packages/ansible # ansible Python 模块位置，使用 Python 2.7 executable location = /bin/ansible # ansible 执行文件的位置 python version = 2.7.5 (default, Apr 11 2018, 07:36:10) [GCC 4.8.5 20150623 (Red Hat 4.8.5-28)] # Python 版本信息[yun@ansi-manager ~]$ ll /usr/bin/ansible /bin/ansible # ansible 命令位置lrwxrwxrwx 1 root root 20 Jun 24 14:14 /bin/ansible -&gt; /usr/bin/ansible-2.7lrwxrwxrwx 1 root root 20 Jun 24 14:14 /usr/bin/ansible -&gt; /usr/bin/ansible-2.7 Ansible 配置文件讲解Ansible配置文件查找顺序ansible 将从多个地方查找配置文件，顺序如下： 1、从环境变量 ANSIBLE_CONFIG 中查找，如果该环境变量有值的话； 2、当前目录的 ansible.cfg 文件；「每个项目都可以有一个该配置文件，这样可以更好的管理项目，移植时也更方便。」 3、当前用户家目录的 .ansible.cfg 文件； 4、/etc/ansible/ansible.cfg 文件。 可以使用 ansible –version 命令查看使用的配置文件。 1在 /etc/ansible/ansible.cfg 配置文件中有该说明 Ansible 部分配置文件讲解实际生产中可以无需做任何修改。 1234567891011121314151617181920212223242526272829[yun@ansi-manager ansible]$ pwd/etc/ansible[yun@ansi-manager ansible]$ vim ansible.cfg#inventory = /etc/ansible/hosts # 受控端主机资源清单#library = /usr/share/my_modules/ # 所需依赖库路径#remote_tmp = ~/.ansible/tmp # 远端机器，临时文件存放位置#local_tmp = ~/.ansible/tmp # 本机临时文件存放位置#forks = 5 # 默认并发数#poll_interval = 15 # 默认轮询时间间隔(单位秒)#sudo_user = root # 默认sudo后的用户#ask_sudo_pass = True # 使用sudo，是否需要输入密码#ask_pass = True # 是否需要输入密码#transport = smart # 传输方式#remote_port = 22 # 默认远程主机的端口号#module_lang = C # 模块和系统之间通信的语言#module_set_locale = False………………# uncomment this to disable SSH key host checking 取消注释以禁用SSH key主机检查 【默认是注释掉的，要检查主机指纹】host_key_checking = False # 解开注释，即跳过检查主机指纹 【只有 root 用户执行时才有该取消指纹检测的权限】………………# logging is off by default unless this path is defined# if so defined, consider logrotate#log_path = /var/log/ansible.log # 开启ansible日志………………[privilege_escalation] # 普通用户提权配置「使用地方：普通用户远程提权使用」#become=True#become_method=sudo#become_user=root#become_ask_pass=False 上述的 [privilege_escalation] 配置，可在 ansible -h 中查看如何使用。如下： 123456789101112131415[yun@ansi-manager ~]$ ansible -h ……………… Privilege Escalation Options: # 权限提升选项 control how and which user you become as on target hosts -b, --become run operations with become (does not imply password prompting) --become-method=BECOME_METHOD privilege escalation method to use (default=sudo), use `ansible-doc -t become -l` to list valid choices. --become-user=BECOME_USER run operations as this user (default=root) -K, --ask-become-pass ask for privilege escalation password………………]]></content>
      <categories>
        <category>ansbile</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ansible-基本概述]]></title>
    <url>%2F2020%2F01%2F01%2Fansible-01%2F</url>
    <content type="text"><![CDATA[为什么要自动化运维纯手动软件安装部署方式我们以 10 台机器部署 Nginx 为例。部署步骤如下： 1、通过 ssh 登录一台机器； 2、yum install -y nginx 或者 获取安装包自行编译安装； 3、配置 Nginx 4、启动 Nginx ，如有必要加入开机自启动； 5、退出登录 上面步骤重复 10 次，即可完成我们的部署要求。 痛点： 1、重复操作频繁，增加了人工成本和后续维护成本； 2、机器太多时，容易落下某些机器且未操作，进而产生后续影响； 3、人工频繁操作时可能有操作步骤不完整的情况，造成该机器和其他机器状态不一致。 自动化运维软件安装部署方式我们还是以 10 台机器部署 Nginx 为例。部署步骤如下： 1、在控制机或者称为管理机的机器上写好相关脚本。「当然该脚本我们是测试通过的，脚本中包括安装、配置、启动等等」 2、将写好的脚本从控制机推送到受控机； 3、在受控机执行相关脚本，根据脚本部署我们需要的 Nginx。 好处 1、减少了重复操作，提高了工作效率； 2、减小了出错几率，提高了准确率； 3、所有机器状态一致，降低了后续维护成本。 自动化运维使用场景软件安装部署 配置同步 代码变更 命令执行 任务执行 Ansible 介绍Ansible是什么Ansible 是基于Python开发，集合了众多运维工具（puppet、cfengine、chef、func、fabric）的优点，实现了批量系统配置、批量程序部署、批量运行命令等功能的自动化管理工具。 Ansible 是基于模块工作的，本身没有批量部署的能力。真正具有批量部署的是 Ansible 所运行的模块，Ansible 只是提供一种框架。进而能减少我们的重复操作，提高工作效率。 Ansible 不需要在远程主机上安装 client/agents，因为它们是基于 SSH 来和远程主机通讯的。 Ansible 目前已经已经被红帽官方收购，是自动化运维工具中大家认可度最高的，并且上手容易，学习简单。是每位运维工程师必须掌握的技能之一。 有哪些功能1、批量执行远程命令：可以对 N 台主机同时进行命令的执行； 2、批量配置软件服务：可以用自动化的方式管理配置和服务； 3、实现软件开发功能：例如 jumpserver 底层使用 Ansible 来实现自动化管理； 4、编排高级的 IT 任务：Ansible 的 Playbooks 是一门编程语言，可以用来描绘一套 IT 架构。 Ansible 特点1、部署简单，只需在主控端部署 Ansible 环境，被控端无需做任何操作； 2、安全可靠，默认使用 SSH 协议对被控端进行管理； 3、有大量的常规运维操作模块，可实现日常绝大部分操作； 4、配置简单、功能强大、扩展性强； 5、支持 API 及自定义模块，可通过 Python 轻松扩展； 6、通过 Playbooks 来定制强大的配置、状态管理； 7、轻量级，无需在客户端安装 Agent，更新时只需在操作机上进行一次更新即可。 Ansible 架构 模块说明如下： Ansible：Ansible 核心程序。 Host Inventory：记录由 Ansible 管理的主机信息清单，包括端口、密码、IP 等。 Playbooks：“剧本” YAML 格式文件，多个任务定义在一个文件中，定义主机需要调用哪些模块来完成的功能。 Core Modules：核心模块，主要操作是通过调用核心模块来完成管理任务。 Custom Modules：自定义模块，完成核心模块无法完成的功能，支持多种语言。 Connection Plugins：连接插件，Ansible 和 Host 通信使用 Ansible 任务执行Ansible 任务执行模式Ansible 控制主机对被管节点的操作方式可分为两类，即 ad-hoc 和 playbook： ad-hoc 模式(点对点模式) 使用单个模块，支持批量执行单条命令。ad-hoc 命令是一种可以快速输入的命令，而且不需要保存起来的命令。就相当于 bash 中的一句 shell。 playbook 模式(剧本模式) 是 Ansible 主要管理方式，也是 Ansible 功能强大的关键所在。playbook 通过多个 tasks 集合完成一类功能，如 Web 服务的安装部署、数据库服务器的批量备份等。可以简单地把 playbook 理解为通过组合多条 ad-hoc 操作的配置文件。 Ansible 执行流程 简单理解就是：Ansible 在运行时，首先读取 ansible.cfg 中的配置，根据规则获取 Inventory 中的管理主机列表，并行的在这些主机中执行配置的任务，最后等待执行返回的结果。 Ansible 命令执行过程1、加载自己的配置文件，默认 /etc/ansible/ansible.cfg； 2、查找对应的主机配置文件，找到要执行的主机或者组； 3、加载自己对应的模块文件，如 command； 4、通过 Ansible 将模块或命令生成对应的临时py文件(python脚本)，并将该文件传输至远程服务器； 5、对应执行用户的家目录的 .ansible/tmp/XXX/XXX.PY文件； 6、给文件 +x 执行权限； 7、执行并返回结果； 8、删除临时py文件，sleep 0退出；]]></content>
      <categories>
        <category>ansbile</category>
      </categories>
      <tags>
        <tag>ansible</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019年总结]]></title>
    <url>%2F2019%2F12%2F31%2Fsummary-2019%2F</url>
    <content type="text"><![CDATA[第一次写总结，相信以后每年的总结都少不了我。 不管是忙碌还是闲散，不管是淡如止水还是心有瑟瑟，2019年都已经过去，双腿都已迈进了2020。 过去一年，互联网行业里，有太多听过的或是认识的人或公司，或倒下，或进去，或饱受嘲讽。没出事的，也正遭遇到巨大的压力。 太多的困难，太多的艰难，太多的难过，太多的困境，也有太多的冲突…… 「以上两句摘自别处，强烈共鸣」 今年的行情我相信大伙儿无论怎样都嗨不起来。对于很多公司活着就是万幸，对于打工者来说有一份稳定的工作就是最大的幸福。 不知道2020年情况如何，但是有太多的人不想继续停留在2019年。 那今年的我干了些什么呢？生活上：1）结婚了：虽然不到三十而立但也不小了，成了家结了婚，同时感谢另一半的相互陪伴。 2）房子装修好了：跟绝大多数人一样第一次装修没什么经验，踩了一些坑，装出来总是差那么点意思，没那么满意。不过想想整体还行吧，就这么招吧，不能哪里不满意就拆哪里，然后重装。一来没精力折腾「人在外地，老妈带我侄儿，老爸在老家挣钱」；二来也没这个经济实力。 3）新房入住：入住之前陆陆续续买了家具家电，十月份入住的新房，今年的团年也在新房团年。所有东西都买了就差空调没买，由于自身经济状态，预计在2020年的三四月份买吧。一个客厅三间卧室估摸着要1.5W左右。 事业上：这个标题有点大，应该是工作上而不是事业上，但是人嘛总得有追求，即使当前还是小虾米一个。 说句心里话，事业上我也想蒸蒸日上，挣大钱，过上富足的生活。但是现实我就是一个打工仔、技术者，每月的薪酬就是我全部的经济来源。没有任何多余的管道流入我的池子里。呜呜呜………… 今年最大的感慨就是幸好我是在一家金融公司上班，虽然受国家政策某些业务进行了调整，但整体上公司业务还很稳定，因此自己的工作也算稳定。 印象最深刻的就是今年年后回来那一两个月，晚上系统升级每次到很晚，但第二天就是有问题然后回滚版本。卧槽，白白熬了个长夜，第二天还有故障，最后还背事件。连续这样搞得我都快怀疑人生了，反正结果并不好。还好后来版本又稳定了，心中松了口气。 除了这个不好的事儿当然还有好事儿啦，但由于都是公司层面的我就不说了。 成长上：从我的博客或公众号文章可知，本人是一个技术者。因此会持续不断地学习必要的技术。 这里得说明一下，我写的博客分两种：一是新学的的技术然后记录下来，方便以后用到能够极速上手；二是以前学过的，但是当时没有做笔记，现在温习后记录的。俗话说的好：好记性不如烂笔头。 如果看到了同一技术有多篇博文，那么就是我系统性研究学习后记录的，如果你恰好也在学习该技术，可以把相关博文从头篇看到尾篇，相信对你很有帮助。 如果想要知道我今年学了哪些技术请自行去看我的博客或者公众号。相关地址如下： 1234CSDN博客：https://blog.csdn.net/woshizhangliang999个人博客：http://zhangblog.com/博客园：https://www.cnblogs.com/zhanglianghhh/GitHub：https://github.com/zhanglianghhh 现在CSDN上总访问量近65W，争取2020年末能到90W+甚至突破100W+。个人博客访问量争取2020年末突破10W+。 争取在2020年能写出更好的文章，帮助到更多的人。 再说明一下：虽然之前有写博客的习惯，但还是比较零散，也没有什么统一。2019年下半年才开始重视这个问题。原有的文章尽量在各个平台同步，当然新的文章都会在各个平台发布。 上面都是说的好听的，那么下面开始说不好的。 自己对时间的有效利用还是比较低，虽然2019年有成长，但是也有大量的时间浪费。已经下定决心在2020年花更多的时间在自身成长上，而不是在娱乐上。 2020年规划：1）自己就是吃技术饭的，因此必要的技术该学就学并记录下来，以前的知识也要梳理总结出来。主职工作要做好。 2）考虑到自己的社交能力、书写能力、口头表达能力比较弱，这方面也会加强。下半年可能会录制些学习视频，相信能够帮助所需之人，也可加强自己口头表达能力。 3）加强身体锻炼，一副好的身体能活的更好，活的更久。 4）扩展自己的知识面；群里也不要总是潜水，要多活跃下让更多人知道自己；最后就是多交流多分享。 5）平时多出去走走，太宅也不好，多认识点朋友多点人生精彩。 写在最后：往昔的2019已经过去历史不可更改，迎来的2020未来共同展望。祝大家在新的一年硕果累累。]]></content>
      <categories>
        <category>总结</category>
      </categories>
      <tags>
        <tag>总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[亿级数据DB秒级平滑扩容！！！]]></title>
    <url>%2F2019%2F12%2F30%2Fdb-switch%2F</url>
    <content type="text"><![CDATA[文章转载自微信公众号：「架构师之路」，作者： 58沈剑 一步一步，娓娓道来。 一般来说，并发量大，吞吐量大的互联网分层架构是怎么样的？ 数据库上层都有一个微服务，服务层记录“业务库”与“数据库实例配置”的映射关系，通过数据库连接池向数据库路由sql语句。 如上图所示，服务层配置用户库user对应的数据库实例ip。 画外音：其实是一个内网域名/IP。 该分层架构，如何应对数据库的高可用？ 数据库高可用，很常见的一种方式，使用双主同步+keepalived+虚ip的方式进行。 如上图所示，两个相互同步的主库使用相同的虚ip。 当主库挂掉的时候，虚ip自动漂移到另一个主库，整个过程对调用方透明，通过这种方式保证数据库的高可用。 画外音：关于高可用，《互联网分层架构如何保证“高可用“？》专题介绍过，本文不再展开。 该分层架构，如何应对数据量的暴增？ 随着数据量的增大，数据库要进行水平切分，分库后将数据分布到不同的数据库实例（甚至物理机器）上，以达到降低数据量，增强性能的扩容目的。 如上图所示，用户库user分布在两个实例上，ip0和ip1，服务层通过用户标识uid取模的方式进行寻库路由，模2余0的访问ip0上的user库，模2余1的访问ip1上的user库。 画外音：此时，水平切分集群的读写实例加倍，单个实例的数据量减半，性能增长可不止一倍。 综上三点所述，大数据量，高可用的互联网微服务分层的架构如下： 既有水平切分，又保证高可用。 如果数据量持续增大，2个库性能扛不住了，该怎么办呢？ 此时，需要继续水平拆分，拆成更多的库，降低单库数据量，增加库主库实例（机器）数量，提高性能。 新的问题来了，分成n个库后，随着数据量的增加，要增加到2*n个库，数据库如何扩容，数据能否平滑迁移，能够持续对外提供服务，保证服务的可用性？ 画外音：你遇到过类似的问题么？ 停服扩容，是最容易想到的方案？ 在讨论秒级平滑扩容方案之前，先简要说明下停服务扩容的方案的步骤： （1）站点挂一个公告“为了为广大用户提供更好的服务，本站点/游戏将在今晚00:00-2:00之间升级，届时将不能登录，用户周知”； 画外音：见过这样的公告么，实际上在迁移数据。 （2）微服务停止服务，数据库不再有流量写入； （3）新建2*n个新库，并做好高可用； （4）写一个小脚本进行数据迁移，把数据从n个库里select出来，insert到2*n个库里； （5）修改微服务的数据库路由配置，模n变为模2*n； （6）微服务重启，连接新库重新对外提供服务； 整个过程中，最耗时的是第四步数据迁移。 如果出现问题，如何进行回滚？ 如果数据迁移失败，或者迁移后测试失败，则将配置改回旧库，恢复服务即可。 停服方案有什么优劣？ 优点：简单。 缺点： （1）需要停止服务，方案不高可用； （2）技术同学压力大，所有工作要在规定时间内完成，根据经验，压力越大约容易出错； 画外音：这一点很致命。 （3）如果有问题第一时间没检查出来，启动了服务，运行一段时间后再发现有问题，则难以回滚，如果回档会，丢失一部分数据； 有没有秒级实施、更平滑、更帅气的方案呢？ 再次看一眼扩容前的架构，分两个库，假设每个库1亿数据量，如何平滑扩容，增加实例数，降低单库数据量呢？三个简单步骤搞定。 步骤一：修改配置 主要修改两处： 数据库实例所在的机器做双虚ip： （1）原%2=0的库是虚ip0，现增加一个虚ip00； （2）原%2=1的库是虚ip1，现增加一个虚ip11； 修改服务的配置，将2个库的数据库配置，改为4个库的数据库配置，修改的时候要注意旧库与新库的映射关系： （1）%2=0的库，会变为%4=0与%4=2； （2）%2=1的部分，会变为%4=1与%4=3； 画外音：这样能够保证，依然路由到正确的数据。 步骤二：reload配置，实例扩容 服务层reload配置，reload可能是这么几种方式： （a）比较原始的，重启服务，读新的配置文件； （b）高级一点的，配置中心给服务发信号，重读配置文件，重新初始化数据库连接池； 不管哪种方式，reload之后，数据库的实例扩容就完成了，原来是2个数据库实例提供服务，现在变为4个数据库实例提供服务，这个过程一般可以在秒级完成。 整个过程可以逐步重启，对服务的正确性和可用性完全没有影响： （a）即使%2寻库和%4寻库同时存在，也不影响数据的正确性，因为此时仍然是双主数据同步的； （b）即使%4=0与%4=2的寻库落到同一个数据库实例上，也不影响数据的正确性，因为此时仍然是双主数据同步的； 完成了实例的扩展，会发现每个数据库的数据量依然没有下降，所以第三个步骤还要做一些收尾工作。 画外音：这一步，数据库实例个数加倍了。 步骤三：收尾工作，数据收缩 有这些一些收尾工作： （a）把双虚ip修改回单虚ip； （b）解除旧的双主同步，让成对库的数据不再同步增加； （c）增加新的双主同步，保证高可用； （d）删除掉冗余数据，例如：ip0里%4=2的数据全部删除，只为%4=0的数据提供服务； 画外音：这一步，数据库单实例数据量减半了。 总结 互联网大数据量，高吞吐量，高可用微服务分层架构，数据库实现秒级平滑扩容的三个步骤为： （1）修改配置（双虚ip，微服务数据库路由）； （2）reload配置，实例增倍完成； （3）删除冗余数据等收尾工作，数据量减半完成； 思路比结论重要，希望大家有收获。]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper常用命令行操作]]></title>
    <url>%2F2019%2F12%2F24%2Fzookeeper03%2F</url>
    <content type="text"><![CDATA[常见的zookeeper命令行操作。在3台机器分别部署了zookeeper-3.4.5【集群】，本文操作是在此基础上进行的。部署详情参见上一篇文章 客户端登录与帮助查看1234567891011121314151617181920212223242526272829303132333435363738394041# 由于是集群模式，所以可以在3台机器的其中任意一台进行登录，操作结果和数据也会同步到其他机器 [root@docker01 bin]# pwd/app/zookeeper-3.4.5/bin[root@docker01 bin]# lltotal 64-rwxr-xr-x 1 501 games 238 Oct 1 2012 README.txt-rwxr-xr-x 1 501 games 1909 Oct 1 2012 zkCleanup.sh-rwxr-xr-x 1 501 games 1049 Oct 1 2012 zkCli.cmd-rwxr-xr-x 1 501 games 1512 Oct 1 2012 zkCli.sh-rwxr-xr-x 1 501 games 1333 Oct 1 2012 zkEnv.cmd-rwxr-xr-x 1 501 games 2599 Oct 1 2012 zkEnv.sh-rwxr-xr-x 1 501 games 1084 Oct 1 2012 zkServer.cmd-rwxr-xr-x 1 501 games 5467 Oct 1 2012 zkServer.sh-rw-r--r-- 1 root root 25108 May 26 14:51 zookeeper.out[root@docker01 bin]# ./zkCli.sh # 客户端登录 ……………… # 一些摘要信息[zk: localhost:2181(CONNECTED) 0] [zk: localhost:2181(CONNECTED) 0] help # 客户端帮助信息 ZooKeeper -server host:port cmd args stat path [watch] # 查看路径【节点】属性信息 [watch] 观察模式，有改变则会被通知，watch一次有效一次 set path data [version] # 设置/修改节点信息 ls path [watch] # 查看路径【节点】 [watch] 观察模式，有改变则会被通知，watch一次有效一次 delquota [-n|-b] path ls2 path [watch] setAcl path acl setquota -n|-b val path history redo cmdno printwatches on|off delete path [version] # 删除节点 sync path listquota path rmr path # 递归删除 get path [watch] # 更具路径【节点】得到信息 [watch] 观察模式，有改变则会被通知，watch一次有效一次 create [-s] [-e] path data acl # 创建节点和数据 -s 序列化【避免重复】 -e 临时数据【常用】 addauth scheme auth quit # 退出客户端 getAcl path close connect host:port # 可以连接到其他客户端，前提在一个集群内[zk: localhost:2181(CONNECTED) 1] 数据的属性说明 属性 描述 czxid 节点被创建的Zxid值 mzxid 节点被修改的Zxid值 ctime 节点被创建的时间 mtime 节点最后一次被修改的时间 versoin 节点被修改的版本号 cversion 节点的所拥有子节点被修改的版本号 aversion 节点的ACL被修改的版本号 emphemeralOwner 如果此节点为临时节点，那么它的值为这个节点拥有者的会话ID；否则，它的值为0 dataLength 节点数据域的长度 numChildren 节点拥有的子节点个数 常用操作切换到其他客户端1234567[zk: localhost:2181(CONNECTED) 1] connect 172.16.1.13:2181 ……………… # 一些摘要信息WATCHER::WatchedEvent state:SyncConnected type:None path:null[zk: 172.16.1.13:2181(CONNECTED) 2] 节点的增删改查增加节点123456789[zk: 172.16.1.13:2181(CONNECTED) 2] ls /[zookeeper][zk: 172.16.1.13:2181(CONNECTED) 3] create /zhang01/yang01 test0001 # 保证父节点存在 Node does not exist: /zhang01/yang01[zk: 172.16.1.13:2181(CONNECTED) 4] [zk: 172.16.1.13:2181(CONNECTED) 4] create /zhang01 test01 Created /zhang01[zk: 172.16.1.13:2181(CONNECTED) 5] ls /[zookeeper, zhang01] 查询节点123456789101112131415[zk: 172.16.1.13:2181(CONNECTED) 7] ls / [zookeeper, zhang01][zk: 172.16.1.13:2181(CONNECTED) 8] get /zhang01 test01 # 内容cZxid = 0x100000007 # 创建数据时的事物编号ctime = Sat May 26 15:11:40 CST 2018 # 创建时间mZxid = 0x100000007 # 修改数据时的事物编号mtime = Sat May 26 15:11:40 CST 2018 # 修改时间pZxid = 0x100000007 # 持久化事物编号cversion = 0 # 创建版本号dataVersion = 0 # 数据版本aclVersion = 0 # 权限版本ephemeralOwner = 0x0 # 持久接待dataLength = 6 # 数据长度numChildren = 0 # 子节点数 修改节点123456789101112131415161718192021222324252627[zk: 172.16.1.13:2181(CONNECTED) 9] ls /[zookeeper, zhang01][zk: 172.16.1.13:2181(CONNECTED) 10] set /zhang01 1111111 # 修改节点信息 cZxid = 0x100000007ctime = Sat May 26 15:11:40 CST 2018mZxid = 0x100000009mtime = Sat May 26 15:15:38 CST 2018pZxid = 0x100000007cversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x0dataLength = 7numChildren = 0[zk: 172.16.1.13:2181(CONNECTED) 11] get /zhang01 1111111cZxid = 0x100000007ctime = Sat May 26 15:11:40 CST 2018mZxid = 0x100000009mtime = Sat May 26 15:15:38 CST 2018pZxid = 0x100000007cversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x0dataLength = 7numChildren = 0 节点常规删除12345678910111213[zk: localhost:2181(CONNECTED) 1] ls /zhang01[test04, test03, test02, test01][zk: localhost:2181(CONNECTED) 3] ls /zhang01/test04 # 没有子节点 [][zk: localhost:2181(CONNECTED) 4] get /zhang01/test04date_test04cZxid = 0x10000000d………………[zk: localhost:2181(CONNECTED) 5] delete /zhang01/test04 # 删除子节点[zk: localhost:2181(CONNECTED) 6] ls /zhang01 # 经查看删除成功 [test03, test02, test01][zk: localhost:2181(CONNECTED) 7] delete /zhang01 # 如果有子节点那么不能删除 Node not empty: /zhang01 递归删除节点1234567[zk: localhost:2181(CONNECTED) 8] ls /zhang01[test03, test02, test01][zk: localhost:2181(CONNECTED) 9] rmr /zhang01 # 递归删除 [zk: localhost:2181(CONNECTED) 10] ls /zhang01 # 删除成功 Node does not exist: /zhang01[zk: localhost:2181(CONNECTED) 11] ls /[zookeeper] 持久节点和临时节点第一台客户端 1234567891011121314151617181920212223242526272829303132333435[zk: localhost:2181(CONNECTED) 14] ls / [zookeeper][zk: localhost:2181(CONNECTED) 15] create /zhang01 yang01 # 默认持久节点 Created /zhang01[zk: localhost:2181(CONNECTED) 16] create -e /zhang02 yang02 # 创建临时节点 Created /zhang02[zk: localhost:2181(CONNECTED) 17] ls /[zookeeper, zhang02, zhang01][zk: localhost:2181(CONNECTED) 18] get /zhang01yang01cZxid = 0x100000018ctime = Sat May 26 15:33:47 CST 2018mZxid = 0x100000018mtime = Sat May 26 15:33:47 CST 2018pZxid = 0x100000018cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0 # 代表持久节点 dataLength = 6numChildren = 0[zk: localhost:2181(CONNECTED) 19] get /zhang02yang02cZxid = 0x100000019ctime = Sat May 26 15:33:55 CST 2018mZxid = 0x100000019mtime = Sat May 26 15:33:55 CST 2018pZxid = 0x100000019cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x1639b3087ac0002 # 代表临时节点 dataLength = 6numChildren = 0[zk: localhost:2181(CONNECTED) 20] quit # 退出当前客户端【那么当前这个客户端创建的临时节点会被自动删除】 第二台客户端 123# 之前可以正常查到，但是当上面的客户端退出后，节点/zhang02 被自动删除了 [zk: localhost:2181(CONNECTED) 4] ls /[zookeeper, zhang01] 节点序列化123456789101112[zk: localhost:2181(CONNECTED) 14] ls /[zookeeper, zhang01][zk: localhost:2181(CONNECTED) 15] ls /zhang01[][zk: localhost:2181(CONNECTED) 16] [zk: localhost:2181(CONNECTED) 16] create -s /zhang01/test01 test01 Created /zhang01/test010000000002[zk: localhost:2181(CONNECTED) 17] create -s -e /zhang01/test02 test02 Created /zhang01/test020000000003[zk: localhost:2181(CONNECTED) 18] [zk: localhost:2181(CONNECTED) 18] ls /zhang01[test020000000003, test010000000002] watch模式节点操作watch监听有不同的类型，有监听状态的stat ，内容的get，目录结构的ls。 watch节点属性监听到自身属性改变则被通知 12345678910111213141516171819202122232425# 第一台客户端监听[zk: localhost:2181(CONNECTED) 4] ls / [zookeeper, zhang01][zk: localhost:2181(CONNECTED) 5] stat /zhang01 watchcZxid = 0x100000018ctime = Sat May 26 15:33:47 CST 2018mZxid = 0x10000001emtime = Sat May 26 15:44:20 CST 2018pZxid = 0x100000018cversion = 0dataVersion = 2aclVersion = 0ephemeralOwner = 0x0dataLength = 8numChildren = 0# 第二台客户端操作[zk: localhost:2181(CONNECTED) 10] set /zhang01 yyyywwwww # 修改节点信息，节点属性被改变……………………# 第一台客户端自动响应[zk: localhost:2181(CONNECTED) 6] WATCHER::WatchedEvent state:SyncConnected type:NodeDataChanged path:/zhang01 watch路径123456789101112131415# 第一台客户端[zk: localhost:2181(CONNECTED) 6] ls /zhang01 watch []# 第二台客户端操作[zk: localhost:2181(CONNECTED) 11] create -e /zhang01/test01 test01Created /zhang01/test01# 第一台客户端自动响应[zk: localhost:2181(CONNECTED) 7] WATCHER::WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/zhang01[zk: localhost:2181(CONNECTED) 7] watch节点内容节点自身内容改变和节点被删除都会被通知 自身属性改变 123456789101112131415161718192021222324# 第一台客户端[zk: localhost:2181(CONNECTED) 7] ls /[zookeeper, zhang01][zk: localhost:2181(CONNECTED) 8] ls /zhang01[test01][zk: localhost:2181(CONNECTED) 9] [zk: localhost:2181(CONNECTED) 9] get /zhang01/test01 watchtest01cZxid = 0x100000020………………# 第二台客户端操作[zk: localhost:2181(CONNECTED) 12] set /zhang01/test01 kkkkcZxid = 0x100000020ctime = Sat May 26 15:49:30 CST 2018………………# 第一台客户端自动响应[zk: localhost:2181(CONNECTED) 10] WATCHER::WatchedEvent state:SyncConnected type:NodeDataChanged path:/zhang01/test01[zk: localhost:2181(CONNECTED) 10] 节点删除 12345678910111213141516# 第一台客户端[zk: localhost:2181(CONNECTED) 10] get /zhang01/test01 watchkkkkcZxid = 0x100000020………………# 第二台客户端操作[zk: localhost:2181(CONNECTED) 14] delete /zhang01/test01 # 第一台客户端自动响应[zk: localhost:2181(CONNECTED) 11] WATCHER::WatchedEvent state:SyncConnected type:NodeDeleted path:/zhang01/test01[zk: localhost:2181(CONNECTED) 11]]]></content>
      <categories>
        <category>ZooKeeper</category>
      </categories>
      <tags>
        <tag>ZooKeeper</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper部署]]></title>
    <url>%2F2019%2F12%2F24%2Fzookeeper02%2F</url>
    <content type="text"><![CDATA[讲述zookeeper的单机部署、伪集群部署与集群部署 主机规划 主机名称 外网IP【外部访问】 内网IP 操作系统 安装软件 docker01 10.0.0.11 172.16.1.11 CentOS 7.2 zookeeper-3.4.5【集群】 docker02 10.0.0.12 172.16.1.12 CentOS 7.2 zookeeper-3.4.5【集群】 docker03 10.0.0.13 172.16.1.13 CentOS 7.2 zookeeper-3.4.5【集群】 docker04 10.0.0.14 172.16.1.14 CentOS 7.2 zookeeper-3.4.5【单独/伪集群】 备注： 12345其中zoo.cfg 的连接数如下：maxClientCnxns=1500 具体就不在每个配置文件中写了如果使用，请加上这个配置 Jdk【java8】软件安装123456789101112131415161718[root@zhang tools]# pwd/root/tools[root@zhang tools]# tar xf jdk-8u162-linux-x64.tar.gz [root@zhang tools]# lltotal 201392drwxr-xr-x 8 10 143 4096 Dec 20 13:27 jdk1.8.0_162-rw-r--r-- 1 root root 189815615 Mar 12 16:47 jdk-8u162-linux-x64.tar.gz-rw-r--r-- 1 root root 16402010 May 14 00:26 zookeeper-3.4.5.tar.gz[root@zhang tools]# mv jdk1.8.0_162/ /app/[root@zhang tools]# cd /app/[root@zhang app]# lltotal 8drwxr-xr-x 8 10 143 4096 Dec 20 13:27 jdk1.8.0_162[root@zhang app]# ln -s jdk1.8.0_162/ jdk[root@zhang app]# lltotal 8lrwxrwxrwx 1 root root 13 May 16 23:19 jdk -&gt; jdk1.8.0_162/drwxr-xr-x 8 10 143 4096 Dec 20 13:27 jdk1.8.0_162 环境变量12345678910111213[yun@iZ2zefjy4361ppunik5222Z ~]$ pwd/app[yun@iZ2zefjy4361ppunik5222Z ~]$ ll -d jdk* # 可以根据实际情况选择jdk版本，其中jdk1.8 可以兼容 jdk1.7 lrwxrwxrwx 1 yun yun 11 Mar 15 14:58 jdk -&gt; jdk1.8.0_162/drwxr-xr-x 8 yun yun 4096 Sep 27 2014 jdk1.7.0_71drwxr-xr-x 8 yun yun 4096 Dec 20 13:27 jdk1.8.0_162[yun@iZ2zefjy4361ppunik5222Z profile.d]$ pwd/etc/profile.d[yun@iZ2zefjy4361ppunik5222Z profile.d]$ cat jdk.sh # java环境变量 export JAVA_HOME=/app/jdkexport JRE_HOME=/app/jdk/jreexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib:$CLASSPATHexport PATH=$JAVA_HOME/bin:$PATH zookeeper【单机部署】配置信息123456789101112131415161718192021222324252627282930313233343536373839# 路径[root@zhang app]# pwd/app[root@zhang app]# lltotal 4lrwxrwxrwx 1 root root 13 May 26 06:55 jdk -&gt; jdk1.8.0_112/drwxr-xr-x 8 10 143 255 Sep 23 2016 jdk1.8.0_112drwxr-xr-x 10 501 games 4096 Nov 5 2012 zookeeper-3.4.5# 配置文件[root@zhang conf]# pwd/app/zookeeper-3.4.5/conf[root@zhang conf]# cp -a zoo_sample.cfg zoo.cfg[root@zhang conf]# vim zoo.cfg # The number of milliseconds of each tick #心跳间隔时间，zookeeper中使用的基本时间单位，毫秒值。每隔2秒发送一个心跳tickTime=2000# The number of ticks that the initial # synchronization phase can takeinitLimit=10# The number of ticks that can pass between # sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just # example sakes. 数据目录 单独目录存放dataDir=/app/bigdata/zookeeper/data# the port at which the clients will connectclientPort=2181## Be sure to read the maintenance section of the # administrator guide before turning on autopurge.## http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance## The number of snapshots to retain in dataDir#autopurge.snapRetainCount=3# Purge task interval in hours# Set to &quot;0&quot; to disable auto purge feature#autopurge.purgeInterval=1 服务启动与停止12345678910111213141516171819202122# 启动服务[root@zhang bin]# pwd/app/zookeeper-3.4.5/bin[root@zhang bin]# ./zkServer.sh startJMX enabled by defaultUsing config: /app/zookeeper-3.4.5/bin/../conf/zoo.cfgStarting zookeeper ... STARTED[root@zhang bin]# [root@zhang bin]# ./zkServer.sh statusJMX enabled by defaultUsing config: /app/zookeeper-3.4.5/bin/../conf/zoo.cfgMode: standalone# 停止服务[root@zhang bin]# ./zkServer.sh stopJMX enabled by defaultUsing config: /app/zookeeper-3.4.5/bin/../conf/zoo.cfgStopping zookeeper ... STOPPED[root@zhang bin]# ./zkServer.sh statusJMX enabled by defaultUsing config: /app/zookeeper-3.4.5/bin/../conf/zoo.cfgError contacting service. It is probably not running. zookeeper【单机伪集群】一台机器，在该台机器上运行多个ZooKeeper Server进程。 软件基本信息12345678910[root@zhang app]# pwd/app[root@zhang app]# lltotal 16drwxr-xr-x 3 root root 23 May 26 07:01 bigdatalrwxrwxrwx 1 root root 13 May 26 06:55 jdk -&gt; jdk1.8.0_112/drwxr-xr-x 8 10 143 255 Sep 23 2016 jdk1.8.0_112drwxr-xr-x 11 501 games 4096 May 26 07:04 zookeeper-3.4.5_01drwxr-xr-x 11 501 games 4096 May 26 07:04 zookeeper-3.4.5_02drwxr-xr-x 11 501 games 4096 May 26 07:04 zookeeper-3.4.5_03 配置文件zookeeper-3.4.5_01 配置信息123456789101112131415161718192021222324252627282930313233343536373839404142[root@zhang conf]# pwd/app/zookeeper-3.4.5_01/conf [root@zhang conf]# lltotal 20-rw-r--r-- 1 501 games 535 Oct 1 2012 configuration.xsl-rw-r--r-- 1 501 games 2161 Oct 1 2012 log4j.properties-rw-r--r-- 1 501 games 927 May 26 07:06 zoo.cfg-rw-r--r-- 1 501 games 808 Oct 1 2012 zoo_sample.cfg[root@zhang conf]# vim zoo.cfg # The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial # synchronization phase can takeinitLimit=10# The number of ticks that can pass between # sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just # example sakes. 数据目录 单独目录存放 确保该目录存在dataDir=/app/bigdata/zookeeper1/data# the port at which the clients will connect# 客户端端口 多个实例的端口配置不可重复clientPort=2181## Be sure to read the maintenance section of the # administrator guide before turning on autopurge.## http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance## The number of snapshots to retain in dataDir#autopurge.snapRetainCount=3# Purge task interval in hours# Set to &quot;0&quot; to disable auto purge feature#autopurge.purgeInterval=1#server.NUM=IP:port1:port2 NUM表示本机为第几号服务器；IP为本机ip地址；#port1为leader与follower通信端口；port2为参与竞选leader的通信端口#多个实例的端口配置不能重复，如下：server.1=172.16.1.14:2222:2225server.2=172.16.1.14:3333:3335server.3=172.16.1.14:4444:4445 zookeeper-3.4.5_02 配置信息123456789101112131415161718192021222324252627282930313233343536373839404142[root@zhang conf]# pwd/app/zookeeper-3.4.5_02/conf[root@zhang conf]# lltotal 16-rw-r--r-- 1 501 games 535 Oct 1 2012 configuration.xsl-rw-r--r-- 1 501 games 2161 Oct 1 2012 log4j.properties-rw-r--r-- 1 501 games 1269 May 26 10:43 zoo.cfg-rw-r--r-- 1 501 games 808 Oct 1 2012 zoo_sample.cfg[root@zhang conf]# vim zoo.cfg # The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial # synchronization phase can takeinitLimit=10# The number of ticks that can pass between # sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just # example sakes. 数据目录 单独目录存放 确保该目录存在dataDir=/app/bigdata/zookeeper2/data# the port at which the clients will connect# 客户端端口 多个实例的端口配置不可重复clientPort=2182## Be sure to read the maintenance section of the # administrator guide before turning on autopurge.## http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance## The number of snapshots to retain in dataDir#autopurge.snapRetainCount=3# Purge task interval in hours# Set to &quot;0&quot; to disable auto purge feature#autopurge.purgeInterval=1#server.NUM=IP:port1:port2 NUM表示本机为第几号服务器；IP为本机ip地址；#port1为leader与follower通信端口；port2为参与竞选leader的通信端口#多个实例的端口配置不能重复，如下：server.1=172.16.1.14:2222:2225server.2=172.16.1.14:3333:3335server.3=172.16.1.14:4444:4445 zookeeper-3.4.5_03 配置信息123456789101112131415161718192021222324252627282930313233343536373839404142[root@zhang conf]# pwd/app/zookeeper-3.4.5_03/conf[root@zhang conf]# lltotal 16-rw-r--r-- 1 501 games 535 Oct 1 2012 configuration.xsl-rw-r--r-- 1 501 games 2161 Oct 1 2012 log4j.properties-rw-r--r-- 1 501 games 1268 May 25 23:39 zoo.cfg-rw-r--r-- 1 501 games 808 Oct 1 2012 zoo_sample.cfg[root@zhang conf]# vim zoo.cfg # The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial # synchronization phase can takeinitLimit=10# The number of ticks that can pass between # sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just # example sakes. 数据目录 单独目录存放dataDir=/app/bigdata/zookeeper3/data# the port at which the clients will connect# 客户端端口 多个实例的端口配置不可重复clientPort=2183## Be sure to read the maintenance section of the # administrator guide before turning on autopurge.## http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance## The number of snapshots to retain in dataDir#autopurge.snapRetainCount=3# Purge task interval in hours# Set to &quot;0&quot; to disable auto purge feature#autopurge.purgeInterval=1#server.NUM=IP:port1:port2 NUM表示本机为第几号服务器；IP为本机ip地址；#port1为leader与follower通信端口；port2为参与竞选leader的通信端口#多个实例的端口配置不能重复，如下：server.1=172.16.1.14:2222:2225server.2=172.16.1.14:3333:3335server.3=172.16.1.14:4444:4445 添加myid文件123456[root@zhang bigdata]# cat /app/bigdata/zookeeper1/data/myid 1[root@zhang bigdata]# cat /app/bigdata/zookeeper2/data/myid 2[root@zhang bigdata]# cat /app/bigdata/zookeeper3/data/myid 3 启动与查看状态依次启动1234567891011121314151617181920212223# 第一台[root@zhang bin]# pwd/app/zookeeper-3.4.5_01/bin[root@zhang bin]# ./zkServer.sh startJMX enabled by defaultUsing config: /app/zookeeper-3.4.5_01/bin/../conf/zoo.cfgStarting zookeeper ... STARTED# 第二台[root@zhang bin]# pwd/app/zookeeper-3.4.5_02/bin[root@zhang bin]# ./zkServer.sh startJMX enabled by defaultUsing config: /app/zookeeper-3.4.5_02/bin/../conf/zoo.cfgStarting zookeeper ... STARTED# 第三台[root@zhang bin]# pwd/app/zookeeper-3.4.5_03/bin[root@zhang bin]# ./zkServer.sh startJMX enabled by defaultUsing config: /app/zookeeper-3.4.5_03/bin/../conf/zoo.cfgStarting zookeeper ... STARTED 查看状态1234567891011121314151617181920212223# 第一台[root@zhang bin]# pwd/app/zookeeper-3.4.5_01/bin[root@zhang bin]# ./zkServer.sh statusJMX enabled by defaultUsing config: /app/zookeeper-3.4.5_01/bin/../conf/zoo.cfgMode: follower# 第二台[root@zhang bin]# pwd/app/zookeeper-3.4.5_02/bin[root@zhang bin]# ./zkServer.sh statusJMX enabled by defaultUsing config: /app/zookeeper-3.4.5_02/bin/../conf/zoo.cfgMode: leader# 第三台[root@zhang bin]# pwd/app/zookeeper-3.4.5_03/bin[root@zhang bin]# ./zkServer.sh statusJMX enabled by defaultUsing config: /app/zookeeper-3.4.5_03/bin/../conf/zoo.cfgMode: follower zookeeper【集群】配置信息123456789101112131415161718192021222324252627282930313233[root@docker01 conf]# pwd/app/zookeeper-3.4.5/conf[root@docker01 conf]# vim zoo.cfg # The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial # synchronization phase can takeinitLimit=10# The number of ticks that can pass between # sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just # example sakes.dataDir=/app/bigdata/zookeeper/data# the port at which the clients will connectclientPort=2181## Be sure to read the maintenance section of the # administrator guide before turning on autopurge.## http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance## The number of snapshots to retain in dataDir#autopurge.snapRetainCount=3# Purge task interval in hours# Set to &quot;0&quot; to disable auto purge feature#autopurge.purgeInterval=1# leader和follow通信端口和投票选举端口 server.1=172.16.1.11:2888:3888 server.2=172.16.1.12:2888:3888server.3=172.16.1.13:2888:3888 添加myid文件123[root@docker01 conf]# cd /app/bigdata/zookeeper/data # [root@docker01 data]# vim myid # 3台机器的myid分别为 1、2、3 # 见开头的主机规划 1 启动zk服务1234567891011121314151617##### 第1台、2台、3台 依次启动[root@docker01 bin]# pwd/app/zookeeper-3.4.5/bin[root@docker01 bin]# lltotal 60-rwxr-xr-x 1 501 games 238 Oct 1 2012 README.txt-rwxr-xr-x 1 501 games 1909 Oct 1 2012 zkCleanup.sh-rwxr-xr-x 1 501 games 1049 Oct 1 2012 zkCli.cmd-rwxr-xr-x 1 501 games 1512 Oct 1 2012 zkCli.sh-rwxr-xr-x 1 501 games 1333 Oct 1 2012 zkEnv.cmd-rwxr-xr-x 1 501 games 2599 Oct 1 2012 zkEnv.sh-rwxr-xr-x 1 501 games 1084 Oct 1 2012 zkServer.cmd-rwxr-xr-x 1 501 games 5467 Oct 1 2012 zkServer.sh[root@docker01 bin]# ./zkServer.sh start JMX enabled by defaultUsing config: /app/zookeeper-3.4.5/bin/../conf/zoo.cfgStarting zookeeper ... STARTED 查询运行状态1234567891011121314151617# 第一台[root@docker01 bin]# ./zkServer.sh status JMX enabled by defaultUsing config: /app/zookeeper-3.4.5/bin/../conf/zoo.cfgMode: follower# 第二台[root@docker02 bin]# ./zkServer.sh status JMX enabled by defaultUsing config: /app/zookeeper-3.4.5/bin/../conf/zoo.cfgMode: leader# 第三台[root@docker03 bin]# ./zkServer.sh status JMX enabled by defaultUsing config: /app/zookeeper-3.4.5/bin/../conf/zoo.cfgMode: follower]]></content>
      <categories>
        <category>ZooKeeper</category>
      </categories>
      <tags>
        <tag>ZooKeeper</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zookeeper入门概述]]></title>
    <url>%2F2019%2F12%2F19%2Fzookeeper01%2F</url>
    <content type="text"><![CDATA[Zookeeper概念简介和应用场景概念简介Zookeeper是一个分布式协调服务；就是为用户的分布式应用程序提供协调服务。 A、zookeeper是为别的分布式程序服务的 B、Zookeeper本身就是一个分布式程序（只要有半数以上节点存活，zk就能正常服务） C、Zookeeper所提供的服务涵盖：主从协调、服务器节点动态上下线、统一配置管理、分布式共享锁、统一名称服务…… D、虽然说可以提供各种服务，但是zookeeper在底层其实只提供了两个功能： &nbsp;&nbsp;&nbsp;&nbsp;1)、管理(存储，读取)用户程序提交的数据； &nbsp;&nbsp;&nbsp;&nbsp;2)、并为用户程序提供数据节点监听服务； Zookeeper集群的角色： Leader 和 follower 只要集群中有半数以上节点存活，集群就能提供服务 分布式场景 高可用场景【应用服务主从选举】 配置管理 Zookeeper特性和数据结构zookeeper特性1、Zookeeper：一个leader，多个follower组成的集群 2、全局数据一致：每个server保存一份相同的数据副本，client无论连接到哪个server，数据都是一致的 3、分布式读写，更新请求转发，由leader实施 4、更新请求顺序进行，来自同一个client的更新请求按其发送顺序依次执行 5、数据更新原子性，一次数据更新要么成功，要么失败 6、实时性，在一定时间范围内，client能读到最新数据 zookeeper数据结构1、层次化的目录结构，命名符合常规文件系统规范(见下图) 2、每个节点在zookeeper中叫做znode,并且只有一个唯一的路径标识 3、节点Znode可以包含数据和子节点（但是EPHEMERAL类型的节点不能有子节点） 4、客户端应用可以在节点上设置监视器（后续详细讲解） 节点类型1、Znode有两种类型： 短暂（ephemeral）（断开连接zookeeper自己删除记录） 持久（persistent）（断开连接不删除） 2、Znode有四种形式的目录节点（默认是persistent ） PERSISTENT PERSISTENT_SEQUENTIAL（持久序列/test0000000019 ） EPHEMERAL EPHEMERAL_SEQUENTIAL 3、创建znode时设置顺序标识，znode名称后会附加一个值，顺序号是一个单调递增的计数器，由父节点维护 4、在分布式系统中，顺序号可以被用于为所有的事件进行全局排序，这样客户端可以通过顺序号推断事件的顺序 zookeeper-api应用基本使用org.apache.zookeeper.Zookeeper是客户端入口主类，负责建立与server的会话。 它提供了下表所示的几类主要方法 ： 功能 描述 create 在目录树中创建一个节点 delete 删除一个节点 exists 测试本地是否存在目标节点 get/set data 从目标节点上读取 / 写数据 get/set ACL 获取 / 设置目标节点访问控制列表信息 get children 检索一个子节点上的列表 sync 等待要被传送的数据 Zookeeper的监听器工作机制 1、在Zookeeper的API操作中，创建main()主方法即主线程； 2、在main线程中创建Zookeeper客户端（zkClient），这时会创建两个线程： &nbsp;&nbsp;&nbsp;&nbsp;线程connet负责网络通信连接，连接服务器； &nbsp;&nbsp;&nbsp;&nbsp;线程Listener负责监听； 3、客户端通过connet线程连接服务器； &nbsp;&nbsp;&nbsp;&nbsp;图中getChildren(“/“ , true) ，” / “表示监听的是根目录，true表示监听，false不监听 4、在Zookeeper的注册监听列表中将注册的监听事件添加到列表中，表示这个服务器中的/path，即根目录这个路径被客户端监听了； 5、一旦被监听的服务器根目录下，数据或路径发生改变，Zookeeper就会将这个消息发送给Listener线程； 6、Listener线程内部调用process方法，采取相应的措施，例如更新服务器列表等。 常见的监听类型 getData(path,watch?)监听的事件是：节点数据变化事件 getChildren(path,watch?)监听的事件是：节点下的子节点增减变化事件 zookeeper原理Zookeeper虽然在配置文件中并没有指定master和slave 但是，zookeeper工作时，是有一个节点为leader，其他则为follower Leader是通过内部的选举机制临时产生的 zookeeper的选举机制（全新集群paxos）以一个简单的例子来说明整个选举的过程. 假设有五台服务器组成的zookeeper集群,它们的id从1-5,同时它们都是最新启动的,也就是没有历史数据,在存放数据量这一点上,都是一样的.假设这些服务器依序启动,来看看会发生什么. 1、服务器1启动,此时只有它一台服务器启动了,它发出去的报没有任何响应,所以它的选举状态一直是LOOKING状态 2、 服务器2启动,它与最开始启动的服务器1进行通信,互相交换自己的选举结果,由于两者都没有历史数据,所以id值较大的服务器2胜出,但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是3),所以服务器1,2还是继续保持LOOKING状态. 3、服务器3启动,根据前面的理论分析,服务器3成为服务器1,2,3中的老大,而与上面不同的是,此时有三台服务器选举了它,所以它成为了这次选举的leader. 4、服务器4启动,根据前面的分析,理论上服务器4应该是服务器1,2,3,4中最大的,但是由于前面已经有半数以上的服务器选举了服务器3,所以它只能接收当小弟的命了. 5、服务器5启动,同4一样,当小弟. 非全新集群的选举机制(数据恢复)那么，初始化的时候，是按照上述的说明进行选举的，但是当zookeeper运行了一段时间之后，有机器down掉，重新选举时，选举过程就相对复杂了。 需要加入数据id、leader id和逻辑时钟。 数据id：数据新的id就大，数据每次更新都会更新id。 Leader id：就是我们配置的myid中的值，每个机器一个。 逻辑时钟：这个值从0开始递增,每次选举对应一个值,也就是说: 如果在同一次选举中,那么这个值应该是一致的 ; 逻辑时钟值越大,说明这一次选举leader的进程更新. 选举的标准就变成： &nbsp;&nbsp;&nbsp;&nbsp;1、逻辑时钟小的选举结果被忽略，重新投票 &nbsp;&nbsp;&nbsp;&nbsp;2、统一逻辑时钟后，数据id大的胜出 &nbsp;&nbsp;&nbsp;&nbsp;3、数据id相同的情况下，leader id大的胜出 根据这个规则选出leader。 zookeeper应用案例（分布式应用HA与分布式锁）实现分布式应用的(主节点HA)同时客户端动态更新主节点状态 某分布式系统中，主节点可以有多台，可以动态上下线 任意一台客户端都能实时感知到主节点服务器的上下线 分布式共享锁的简单实现]]></content>
      <categories>
        <category>ZooKeeper</category>
      </categories>
      <tags>
        <tag>ZooKeeper</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux 用户名、主机添加背景色]]></title>
    <url>%2F2019%2F12%2F15%2Fbg-color%2F</url>
    <content type="text"><![CDATA[背景色添加Linux 用户名、主机添加背景色，用于生产环境，这样可以减少人为的误操作。 123[root@zhang ~]# tail /etc/bashrc………………export PS1=&quot;\[\e[37;40m\][\[\e[37;41m\]\u\[\e[37;41m\]@\h\[\e[37;40m\] \W\[\e[0m\]]\\$ &quot; 结果如下： PS1【了解】PS1是Linux终端用户的一个环境变量，用来定义命令行提示符的参数。默认如下： 12[root@zhang ~]# echo $PS1[\u@\h \W]\$ 意思就是：[当前用户的账号名称@主机名的第一个名字 工作目录的最后一层目录名] PS1的常用参数以及含义: \d ：代表日期，格式为weekday month date，例如：”Mon Aug 1” \H ：完整的主机名称 \h ：仅取主机名中的第一个名字 \t ：显示时间为24小时格式，如：HH：MM：SS \T ：显示时间为12小时格式 \A ：显示时间为24小时格式：HH：MM \u ：当前用户的账号名称 \v ：BASH的版本信息 \w ：完整的工作目录名称 \W ：利用basename取得工作目录名称，只显示最后一个目录名 # ：下达的第几个命令 $ ：提示字符，如果是root用户，提示符为 # ，普通用户则为 $ 颜色设置参数在PS1中设置字符颜色的格式为：[\e[F;Bm]……..[\e[0m]，其中“F“为字体颜色，编号为30-37，“B”为背景颜色，编号为40-47,[\e[0m]作为颜色设定的结束。 颜色对照表： F&nbsp;&nbsp;&nbsp;&nbsp;B 30&nbsp;&nbsp;40 黑色 31&nbsp;&nbsp;41 红色 32&nbsp;&nbsp;42 绿色 33&nbsp;&nbsp;43 黄色 34&nbsp;&nbsp;44 蓝色 35&nbsp;&nbsp;45 紫红色 36&nbsp;&nbsp;46 青蓝色 37&nbsp;&nbsp;47 白色 只需将对应数字套入设置格式中即可。]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Rsync 服务部署与参数详解]]></title>
    <url>%2F2019%2F09%2F26%2Frsync01%2F</url>
    <content type="text"><![CDATA[Rsync 简介rsync 是一款开源的、快速的、多功能的、可实现全量及增量的本地或远程数据同步备份的优秀工具。Rsync软件适用于unix/linux/windows等多种操作系统平台。 传统的 scp 和 cp 工具拷贝每次均为完整拷贝，而rsync除了可以完整拷贝外，还具备增量拷贝功能。因此，从同步数据的性能及效率上，Rsync工具更胜一筹。 官网地址： 123https://download.samba.org/pub/rsync/rsync.html# 或者https://www.samba.org/ftp/rsync/rsync.html 版本查看 123456789101112[yun@backup ~]$ rsync --versionrsync version 3.1.2 protocol version 31Copyright (C) 1996-2015 by Andrew Tridgell, Wayne Davison, and others.Web site: http://rsync.samba.org/Capabilities: 64-bit files, 64-bit inums, 64-bit timestamps, 64-bit long ints, socketpairs, hardlinks, symlinks, IPv6, batchfiles, inplace, append, ACLs, xattrs, iconv, symtimes, preallocrsync comes with ABSOLUTELY NO WARRANTY. This is free software, and youare welcome to redistribute it under certain conditions. See the GNUGeneral Public Licence for details. Rsync 增量复制的原理Rsync 通过 “quick check” 算法 (默认情况) 找到要传输的文件，该算法会查找大小已改变或最后修改时间已改变的文件。 当 “quick check” 指示不需要更新文件的数据 (即：文件的内容) 时，会直接在目标文件上更改其他属性（如权限信息、属主属组信息、时间戳信息）「根据相应的选项参数」。 rsync2.x对比方法，把所有文件比对一遍，然后进行同步。 rsync3.x对比方法，一边比对差异，一边对差异的部分进行同步。 Rsync 软件功能介绍rsync == cp 1234# 拷贝 /etc/rpm 目录[root@back ~]# cp -a /etc/rpm /tmp/# 等价于[root@back ~]# rsync -a /etc/rpm /tmp/ 注意：cp -a 的参数意义和 rsync -a 的参数意义不一样。 rsync == scp 123[yun@back tmp]$ scp -pr yun@172.16.1.182:/etc/xml /tmp/ # 等价于[yun@back tmp]$ rsync -pr yun@172.16.1.182:/etc/xml /tmp/ rsync == rm删除 /tmp/yum/ 目录下的所有文件和目录。【千万不要搞错目录】 123[yun@back tmp]$ rm -fr /tmp/yum/*# 等价于[yun@back tmp]$ rsync -r --delete /tmp/zhang/ /tmp/yum/ 其中 /tmp/zhang/ 目录下无任何文件 rsync == ls -l 12345[yun@backup tmp]$ ls -l /tmp/xml/catalog -rw-r--r-- 1 yun yun 1171 Sep 18 16:08 /tmp/xml/catalog# 类似于[yun@backup tmp]$ rsync /tmp/xml/catalog -rw-r--r-- 1,171 2019/09/18 16:08:41 catalog Rsync 特性介绍1、支持拷贝特殊文件。如链接文件、设备等。2、可以排除指定文件或目录的同步功能，相当于打包tar的排除功能。3、可以做到保持源文件或目录的权限、时间、软链接、属主、属组等所有属性均不改变。4、可实现增量同步。既只同步发生变化的数据，因此传输效率很高，tar -N。5、可以使用 rcp、rsh、ssh 等方式来配合传输文件（rsync本身不对数据加密）。6、可以通过socket（进程方式）传输文件和数据（服务端和客户端）【重点掌握】。7、支持匿名或认证（无需系统用户）的进程传输，可以实现方便安全的进行数据备份及镜像。 Rsync 运用场景数据备份 使用方式：cron + rsync 比如数据库备份，除了本地需要备份外，还需要通过 rsync 在专门的备份服务器上备份一份。 实时同步 使用方式：rsync + inotify 或 sersync 比如为了缓解服务器压力，我们需要将用户上传的图片放在多台服务器上【如果没有上 CDN】，这样高并发访问的时候可以分发到多台机器，减轻服务器压力。 又比如敏感数据不能丢失，那必须做实时备份。 Rsync 使用说明Rsync 传输方式分三种1、单个主机本地内部之间的数据传输（此时类似于 cp 的功能） 2、借助rcp，ssh等通道来传输数据（此时类似于 scp 的功能） 3、以守护进程方式传输数据（rsync 自身的重要功能） 1234567891011121314# 本地数据同步方式Local: rsync [OPTION...] SRC... [DEST]# 远程数据同步方式Access via remote shell: Pull: rsync [OPTION...] [USER@]HOST:SRC... [DEST] Push: rsync [OPTION...] SRC... [USER@]HOST:DEST# 守护进程方式同步数据Access via rsync daemon: Pull: rsync [OPTION...] [USER@]HOST::SRC... [DEST] rsync [OPTION...] rsync://[USER@]HOST[:PORT]/SRC... [DEST] Push: rsync [OPTION...] SRC... [USER@]HOST::DEST rsync [OPTION...] SRC... rsync://[USER@]HOST[:PORT]/DEST 源目录后面无 “/“ 和有 “/“ 的区别将 /etc/yum 目录复制到 /tmp/zhang/ 目录下。 12345# 源目录后面无 &quot;/&quot;[yun@backup ~]$ rsync -avz /etc/yum /tmp/zhang/[yun@backup ~]$ ll /tmp/zhang/total 0drwxr-xr-x 6 yun yun 100 Nov 14 2018 yum 将 /etc/yum/ 目录下的所有文件和目录，复制到 /tmp/zhang/ 目录下。 123456789# 源目录后面有 &quot;/&quot;[yun@backup ~]$ rsync -avz /etc/yum/ /tmp/zhang/[yun@backup ~]$ ll /tmp/zhang/total 4drwxr-xr-x 2 yun yun 6 Apr 13 2018 fssnap.ddrwxr-xr-x 2 yun yun 54 Nov 14 2018 pluginconf.ddrwxr-xr-x 2 yun yun 26 Nov 14 2018 protected.ddrwxr-xr-x 2 yun yun 37 Apr 13 2018 vars-rw-r--r-- 1 yun yun 444 Apr 13 2018 version-groups.conf 常用参数说明 参数 参数说明 -v, --verbose 显示传输了哪些文件 ★★★★★ -z, --compress 传输时进行压缩以提高传输效率，--compress-level=NUM 指定加压缩级别。★★★★★ -P, --progress 显示同步的过程及传输时进度等信息 ★★★★★ -a, --archive 归档模式，表示以递归方式传输文件，并保持文件属性。等价于 -rlptgoD ★★★★★ -r, --recursive 对子目录以递归模式，即目录下的所有目录都同样传输【归档于-a】 -t, --times 保持文件时间信息【归档于-a】 -o, --owner 保持文件属主信息【归档于-a】 -g, --group 保持文件属组信息【归档于-a】 -p, --perms 保持文件权限信息【归档于-a】 -D, --devieves 保持设备文件信息【归档于-a】 -l, --linkd 保持软链接信息【归档于-a】 -L, --copy-links 如果是链接文件那么转为源文件复制【复制软连接的源文件】 -e, --rsh=COMMAND 使用信道协议，指定替代rsh的shell程序 --exclude=PATTERN 指出哪些文件或目录不需要传输，支持通配符 --exclude-from=FILE 在 FILE 文件中指定哪些文件或目录不需要传输 --include=PATTERN 指出哪些文件或目录不被排除要传输，支持通配符；通常配合 --exclude 或 --exclude-from 使用 --include-from=FILE 在 FILE 文件中指定哪些文件或目录不被排除需要传输；通常配合 --exclude 或 --exclude-from 使用 --bwlimit=RATE 限速 限制 I/O 带宽；K字节每秒 --delete 让目标DEST目录与SRC目录数据一致 【慎用】 部分参数示例说明 123456# --exclude=PATTERN 示例[yun@backup tmp]$ rsync -avz --exclude=systemd.conf /etc/yum /tmp/zhang01# 或者[yun@backup tmp]$ rsync -avz --exclude=systemd.* /etc/yum /tmp/zhang01# 或者 多个条件使用多个 --exclude=PATTERN[yun@backup tmp]$ rsync -avz --exclude=systemd.conf --exclude=fastestmirror.conf /etc/yum /tmp/zhang05 12345678# --exclude-from=FILE 示例[yun@backup tmp]$ cat exclude.info fastestmirror.confsystemd.confvars# 那么此时 fastestmirror.conf、systemd.conf 文件 和 vars 目录不会被拷贝[yun@backup tmp]$ rsync -avz --exclude-from=exclude.info /etc/yum /tmp/zhang01 123# --include=PATTERN 示例## 注意参数顺序：--include 或 --include-from 必须要在 --exclude 或 --exclude-from 之前[yun@backup tmp]$ rsync -avz --include=systemd* --exclude=*.conf /etc/yum /tmp/zhang02 Rsync 本地模式实践1Local: rsync [OPTION...] SRC... [DEST] 示例如下： 1rsync -avz /etc/yum /tmp/zhang01 Rsync 使用远程 SSH 通道实践123Access via remote shell: Pull: rsync [OPTION...] [USER@]HOST:SRC... [DEST] Push: rsync [OPTION...] SRC... [USER@]HOST:DEST 拉取数据：将远程机器的数据复制到本地 1[yun@backup ~]$ rsync -avzP -e &apos;ssh -p 22&apos; yun@172.16.1.182:/etc/yum /tmp/zhang01 推送数据：将本地的数据复制到远程机器 1[yun@backup ~]$ rsync -avzP -e &apos;ssh -p 22&apos; /etc/yum yun@172.16.1.182:/tmp/zhang Rsync daemon 服务模式实践12345Access via rsync daemon: Pull: rsync [OPTION...] [USER@]HOST::SRC... [DEST] rsync [OPTION...] rsync://[USER@]HOST[:PORT]/SRC... [DEST] Push: rsync [OPTION...] SRC... [USER@]HOST::DEST rsync [OPTION...] SRC... rsync://[USER@]HOST[:PORT]/DEST 使用规划 1、backup 服务器作为 rsync 服务端 2、以 ansi-haproxy02 作为客户端，进行数据推拉。 rsync 服务端配置系统环境信息123456789101112131415161718[yun@backup ~]$ cat /etc/redhat-release CentOS Linux release 7.5.1804 (Core) [yun@backup ~]$ uname -aLinux backup 3.10.0-862.el7.x86_64 #1 SMP Fri Apr 20 16:44:24 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux[root@backup ~]# rpm -qa | grep &apos;rsync&apos;rsync-3.1.2-4.el7.x86_64[yun@backup ~]$ rsync --version # 系统已默认安装，如果没有那自行安装下rsync version 3.1.2 protocol version 31Copyright (C) 1996-2015 by Andrew Tridgell, Wayne Davison, and others.Web site: http://rsync.samba.org/Capabilities: 64-bit files, 64-bit inums, 64-bit timestamps, 64-bit long ints, socketpairs, hardlinks, symlinks, IPv6, batchfiles, inplace, append, ACLs, xattrs, iconv, symtimes, preallocrsync comes with ABSOLUTELY NO WARRANTY. This is free software, and youare welcome to redistribute it under certain conditions. See the GNUGeneral Public Licence for details. 服务配置文件注意：配置后面不要添加任何信息和空格，不然可能被解析，导致后续一系列问题。 123456789101112131415161718192021222324252627282930313233[root@backup ~]# cat /etc/rsyncd.conf# 备注：更多参数与更多详解，参见 man rsyncd.conf#rsync_config---------------startuid = rootgid = rootuse chroot = falsemax connections = 200timeout = 100pid file = /var/run/rsyncd.pidlock file = /var/run/rsync.locklog file = /var/log/rsyncd.logdont compress = *.gz *.tgz *.zip *.z *.Z *.rpm *.deb *.bz2ignore errors = trueread only = falselist = false## 注意为了避免困惑 hosts allow 和 hosts deny 请二选其一hosts allow = 172.16.1.0/24,10.9.0.0/16,120.27.48.179# hosts deny = 10.0.0.0/16# 支持多个认证账号auth users = rsync_backup,rsync_db_backsecrets file = /etc/rsync.password# 数据备份 注意 path 目录的权限信息[back_data_module]path = /backup/busi_data/# 数据库备份 注意 path 目录的权限信息[back_db_module]path = /backup/database/#rsync_config---------------end 配置文件详解更多参数与详解，参见 man rsyncd.conf 123456789101112131415161718192021222324252627282930313233343536373839404142# 备注：更多参数与更多详解，参见 man rsyncd.conf# rsync_config---------------start# 可以是其他用户# 超级用户运行时的默认设置是切换到系统的“nobody”用户# 如果配置为 root 用户，这时可以同步属主信息# 非超级用户是不能同步属主信息的uid = root # 其他主机通过 rsync 实现推拉时使用什么用户# 可以是其他用户组# 超级用户组运行时的默认设置是切换到系统的“nobody”用户组# 如果配置为 root 用户组，这时可以同步属组信息# 非超级用户组是不能同步属组信息的gid = root # 其他主机通过 rsync 实现推拉时使用什么用户组# 如果为 true，安全性更高，但软连接文件「可能」同步不了# 这是 rsync 的一个安全配置，由于我们大多数都是内网使用，所以可以不配置。建议 falseuse chroot = falsemax connections = 200 # 最大连接数timeout = 100 # 超时时间pid file = /var/run/rsyncd.pid # 进程号文件lock file = /var/run/rsync.lock # 锁文件，防止文件不一致log file = /var/log/rsyncd.log # 日志文件dont compress = *.gz *.tgz *.zip *.z *.Z *.rpm *.deb *.bz2 # 对哪些文件传输时不压缩【如果传输时我们指定了压缩参数】ignore errors = true # 忽略错误read only = false # 在服务端可读写list = false # 不让列表，安全考虑## 注意为了避免困惑 hosts allow 和 hosts deny 请二选其一hosts allow = 172.16.1.0/24,10.9.0.0/16,120.27.48.179 # 可访问的网段或IP，多个使用逗号分隔；其他的都不可访问# hosts deny = 10.0.0.0/16 # 不可访问的网段或IP，多个使用逗号分隔；其他的都可以访问# 支持多个认证账号auth users = rsync_backup,rsync_db_back # 其他主机连接时，校验的用户【虚拟用户】secrets file = /etc/rsync.password # 虚拟用户账号及密码# 数据备份 注意 path 目录的权限信息[back_data_module]path = /backup/busi_data/# 数据库备份 注意 path 目录的权限信息[back_db_module]path = /backup/database/# rsync_config---------------end uid、gid 详解 当 rsync 服务端的 uid、gid 用的是 rsync 用户和用户组【或其他非超级用户、用户组】时，客户端同步时只能同步数据信息、权限信息、时间戳信息，但不能同步属主、属组信息。此时客户端若要同步的数据属主、属组，那么客户端数据的属主、属组必须也是 rsync【即客户端与服务端的 uid、gid 相同】；或者不要同步属主、属组信息。 如果需要把多个文件且属主、属组不同的属性信息也同步到 rsync 服务端，那么在 rsync 服务端的 uid、gid 配置都应该是 root。 use chroot 详解 use chroot = true 需要 root 权限，且 rsync 在传输文件之前首先 chroot 到 path 参数所指定的目录下，然后再开始与客户端进行文件传输。优点：可以提供额外的保护，防止可能的出现的安全漏洞缺点：如果同步的数据有软连接，那么软连接「可能」同步不了 use chroot = false 出于安全原因，默认情况下使用 munge 符号链接，即所有的软连接前面多会加 /rsyncd-munged/，如下： 其他必要设置与配置1、创建对应的目录 12345[root@backup ~]# mkdir -p /backup/&#123;busi_data,database&#125; [root@backup ~]# ll /backup/total 0drwxr-xr-x 2 root root 6 Sep 20 14:54 busi_datadrwxr-xr-x 2 root root 6 Sep 20 14:54 database 2、rsync虚拟用户配置 123456[root@backup ~]# ll /etc/rsync.password # 注意文件的权限信息 600-rw------- 1 root root 63 Sep 20 15:03 /etc/rsync.password[root@backup ~]# [root@backup ~]# cat /etc/rsync.password rsync_backup:rsync_backup_pwdrsync_db_back:rsync_db_back_pwd 启动/停止 Rsync 守护进程服务1234567891011[root@backup ~]# systemctl start rsyncd.service[root@backup ~]# netstat -lntup | grep &apos;rsync&apos;tcp 0 0 0.0.0.0:873 0.0.0.0:* LISTEN 2366/rsynctcp6 0 0 :::873 :::* LISTEN 2366/rsync[root@backup ~]# lsof -i :873COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMErsync 2347 root 3u IPv4 31366 0t0 TCP *:rsync (LISTEN)rsync 2347 root 5u IPv6 31367 0t0 TCP *:rsync (LISTEN)[root@backup ~]# ps -ef | grep &apos;rsync&apos;root 2366 1 0 10:59 ? 00:00:00 /usr/bin/rsync --daemon --no-detachroot 2571 1684 0 11:08 pts/0 00:00:00 grep --color=auto rsync 停止 rsync 服务 1[root@backup ~]# systemctl stop rsyncd.service 加入开机自启动123456[root@backup ~]# systemctl enable rsyncd.service # 加入开机自启动Created symlink from /etc/systemd/system/multi-user.target.wants/rsyncd.service to /usr/lib/systemd/system/rsyncd.service.[root@backup ~]# systemctl status rsyncd.service ● rsyncd.service - fast remote file copy program daemon Loaded: loaded (/usr/lib/systemd/system/rsyncd.service; enabled; vendor preset: disabled) Active: inactive (dead) Rsync 客户端配置系统环境信息123456789101112131415161718[root@ansi-haproxy02 ~]# cat /etc/redhat-release CentOS Linux release 7.5.1804 (Core) [root@ansi-haproxy02 ~]# uname -aLinux ansi-haproxy02 3.10.0-862.el7.x86_64 #1 SMP Fri Apr 20 16:44:24 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux[root@ansi-haproxy02 ~]# rpm -qa | grep &apos;rsync&apos;rsync-3.1.2-4.el7.x86_64[root@ansi-haproxy02 ~]# rsync --versionrsync version 3.1.2 protocol version 31Copyright (C) 1996-2015 by Andrew Tridgell, Wayne Davison, and others.Web site: http://rsync.samba.org/Capabilities: 64-bit files, 64-bit inums, 64-bit timestamps, 64-bit long ints, socketpairs, hardlinks, symlinks, IPv6, batchfiles, inplace, append, ACLs, xattrs, iconv, symtimes, preallocrsync comes with ABSOLUTELY NO WARRANTY. This is free software, and youare welcome to redistribute it under certain conditions. See the GNUGeneral Public Licence for details. 密码文件注意密码文件只能填写对应密码信息，不要有其他多余的字符，rsync 在读取时仅读取该文件的第一行信息。 1234567[root@ansi-haproxy02 ~]# ll /etc/rsync.password* # 注意权限信息 600 或 400-rw------- 1 root root 36 Sep 20 16:28 /etc/rsync.password-rw------- 1 root root 19 Sep 24 11:30 /etc/rsync.password2[root@ansi-haproxy02 ~]# cat /etc/rsync.passwordrsync_backup_pwd[root@ansi-haproxy02 ~]# cat /etc/rsync.password2rsync_db_back_pwd 注意：客户端一个 password 文件不能存放多个密码，rsync 在读取文件的时候只会读取第一个密码。 客户端准备数据123456789101112131415[root@ansi-haproxy02 zhang]# pwd/tmp/zhang[root@ansi-haproxy02 zhang]# cp -a /etc/yum ./[root@ansi-haproxy02 zhang]# lltotal 0drwxr-xr-x. 6 root root 100 Nov 14 2018 yum [root@ansi-haproxy02 zhang]# touch stu&#123;01..100&#125;[root@ansi-haproxy02 zhang]# lsstu001 stu008 stu015 stu022 stu029 stu036 stu043 stu050 stu057 stu064 stu071 stu078 stu085 stu092 stu099stu002 stu009 stu016 stu023 stu030 stu037 stu044 stu051 stu058 stu065 stu072 stu079 stu086 stu093 stu100stu003 stu010 stu017 stu024 stu031 stu038 stu045 stu052 stu059 stu066 stu073 stu080 stu087 stu094 yumstu004 stu011 stu018 stu025 stu032 stu039 stu046 stu053 stu060 stu067 stu074 stu081 stu088 stu095stu005 stu012 stu019 stu026 stu033 stu040 stu047 stu054 stu061 stu068 stu075 stu082 stu089 stu096stu006 stu013 stu020 stu027 stu034 stu041 stu048 stu055 stu062 stu069 stu076 stu083 stu090 stu097stu007 stu014 stu021 stu028 stu035 stu042 stu049 stu056 stu063 stu070 stu077 stu084 stu091 stu098 推拉数据客户端向服务端推送数据使用 rsync_backup 账号测试 情况1、需要校验用户密码 123456[root@ansi-haproxy02 zhang]# rsync -avz /tmp/zhang rsync_backup@172.16.1.181::back_data_modulePassword:sending incremental file list………………sent 6,202 bytes received 2,070 bytes 1,504.00 bytes/sectotal size is 1,117 speedup is 0.14 情况2、不要密码【密码文件】 123456# 方式一# 从本地推送到服务端 back_data_module 模块路径下[root@ansi-haproxy02 zhang]# rsync -avz --password-file=/etc/rsync.password /etc/yum rsync_backup@172.16.1.181::back_data_module # 方式二# 从本地推送到服务端 back_data_module 模块路径下的 zhang 目录下[root@ansi-haproxy02 zhang]# rsync -avz --password-file=/etc/rsync.password /etc/systemd rsync://rsync_backup@172.16.1.181/back_data_module/zhang/ 其中：back_data_module 为 rsync 配置中的模块名 客户端向服务端拉取数据使用 rsync_db_back 账号测试 情况1：需要校验用户密码 1234[root@ansi-haproxy02 test]# rsync -avz rsync_db_back@172.16.1.181::back_data_module/zhang /tmp/test/Password: receiving incremental file list………… 情况2、不要密码【密码文件】 123456# 方式一# 从服务端 back_data_module 模块路径下的yum目录，拉取到本地 /tmp/test1/ 目录下[root@ansi-haproxy02 kkkk]# rsync -avz --password-file=/etc/rsync.password2 rsync_db_back@172.16.1.181::back_data_module/yum /tmp/test1/ # 方式二# 从服务端 back_data_module 模块路径下的yum目录，拉取到本地 /tmp/test2/ 目录下[root@ansi-haproxy02 tmp]# rsync -avz --password-file=/etc/rsync.password2 rsync://rsync_db_back@172.16.1.181/back_data_module/yum /tmp/test2/ 其中：back_data_module 为 rsync 配置中的模块名 注意点：客户端路径末尾是否有 / “/” 有：则表示推送该目录下的所有文件 “/” 无：则表示推送该目录，与该目录下所有文件 目标一端可以加一个不存在的目录类似 cp 命令一样，我们可以在目标一端指定一个目录 推送时，目标一端有该目录则直接使用；没有则创建 Rsync 常见问题故障点1：客户端密码文件属主不对客户端以哪个用户执行命令，那么 password-file 文件的属主就是哪个用户。当然为了安全起见，客户端的 password-file 文件属主最好是 root ，这时我们执行命令也用 root 用户。 123[root@ansi-haproxy02 ~]# rsync -avz --password-file=/etc/rsync.password2 /etc/yum rsync_db_back@172.16.1.181::back_db_moduleERROR: password file must be owned by root when running as rootrsync error: syntax or usage error (code 1) at authenticate.c(200) [sender=3.1.2] 故障点2：客户端密码文件权限不对客户端的 password-file 文件权限是 600 或 400。 123[root@ansi-haproxy02 ~]# rsync -avz --password-file=/etc/rsync.password2 /etc/yum rsync_db_back@172.16.1.181::back_db_moduleERROR: password file must not be other-accessiblersync error: syntax or usage error (code 1) at authenticate.c(196) [sender=3.1.2] 故障点3：客户端用户执行权限不足客户端用户执行权限不足，不能读取 password-file 文件信息。当前使用 yun 用户，实际需要 root 用户。 123[yun@ansi-haproxy02 ~]$ rsync -avz --password-file=/etc/rsync.password2 /etc/yum rsync_db_back@172.16.1.181::back_db_modulersync: could not open password file /etc/rsync.password2: Permission denied (13)rsync error: syntax or usage error (code 1) at authenticate.c(187) [sender=3.1.2] 故障点4：服务端没有对应的模块目录服务端 back_db_module 模块的 path 路径不存在。 123[root@ansi-haproxy02 ~]# rsync -avz --password-file=/etc/rsync.password2 /etc/yum rsync_db_back@172.16.1.181::back_db_module@ERROR: chdir failedrsync error: error starting client-server protocol (code 5) at main.c(1648) [sender=3.1.2] 故障点5：客户端密码不正确客户端 password-file 文件的密码信息与服务端的密码不匹配，不能通过验证。 123[root@ansi-haproxy02 ~]# rsync -avz --password-file=/etc/rsync.password2 /etc/yum rsync_db_back@172.16.1.181::back_db_module@ERROR: auth failed on module back_db_modulersync error: error starting client-server protocol (code 5) at main.c(1648) [sender=3.1.2] 故障点6：服务端目录不存在服务端只能创建一级不存在的目录，不能创建二级及以上不存在的目录。 1234[root@ansi-haproxy02 ~]# rsync -avz --password-file=/etc/rsync.password2 /etc/yum rsync_db_back@172.16.1.181::back_db_module/aaa/bbbsending incremental file listrsync: mkdir &quot;aaa/bbb&quot; (in back_db_module) failed: No such file or directory (2)rsync error: error in file IO (code 11) at main.c(657) [Receiver=3.1.2]]]></content>
      <categories>
        <category>Rsync</category>
      </categories>
      <tags>
        <tag>Rsync</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux 平均负载 load average 的含义]]></title>
    <url>%2F2019%2F09%2F14%2Flinux-load%2F</url>
    <content type="text"><![CDATA[load average 的含义平均负载是指单位时间内，系统处于可运行状态和不可中断状态的平均进程数。和 CPU 使用率并没有直接的关系 一般的进程需要消耗 CPU、内存、磁盘I/O、网络I/O等资源，在这种情况下，平均负载就不是单独指的CPU使用情况。即内存、磁盘、网络等因素也可以影响系统的平均负载值。不过影响最大的是 CPU 使用率、CPU 等待和磁盘I/O。 一个机器的负载情况通常是通过 CPU 核数来判断的。当平均负载比 CPU 核数还大的时候，系统已经出现了过载。 如在单核处理器中，平均负载值为 1 或者小于 1 的时候，系统处理进程会非常轻松，即负载很低。当达到 3 的时候，就会显得很忙，达到 5 或者 8 的时候就不能很好的处理进程了（其中 5 和 8 目前还是个争议的阈值，为了保守起见，建议选择低的）。 查看load average 数据下面几个命令都可以看到 load average 123# top # uptime # w 截图如下： top 命令: uptime 命令: w 命令: 这里的 load average 的三个值分别指系统在最后 1/5/15 分钟 的平均负载值。 根据经验：我们应该把重点放在5/15分钟的平均负载，因为 1 分钟的平均负载太频繁，一瞬间的高并发就会导致该值的大幅度改变。 平均负载与 CPU 使用率在日常使用中，我们经常容易把平均负载和 CPU 使用率混淆，这里我们做下区分。 可能我们会有疑惑，既然平均负载代表的是活跃进程数，那么平均负载高了，不就意味着 CPU 使用率高了吗？ 这里我们还得回到平均负载的含义上来，平均负载是指单位时间内，处于可运行状态和不可中断状态的进程数。所以，他不仅包扩了正在使用CPU的进程，还包括等待 CPU 和等待磁盘I/O的进程。 而 CPU 使用率，是单位时间内 CPU 繁忙情况的统计，和平均负载并不一定完全对应。比如： CPU 密集型进程，使用大量 CPU 会导致平均负载升高，此时这两者是一致的。 I/O 密集型进程， 等待 I/O 也会导致平均负载升高，但是 CPU 使用率不一定很高。 大量等待CPU的进程调用也会导致平均负载升高，此时的 CPU 使用率也会比较高。 平均负载案例分析机器是一个 16 核 CPU 的。 这里会用到 2 个工具，stress 和 sysstat。 stress 是一个 Linux 系统压力测试工具，这里我们用作异常进程模拟平均负载升高的场景。 sysstat 是一个 Linux 性能工具，用来监控和分析系统的性能，以下案例中会用到这个包的 2 个命令 mpstat 和 pidstat。 mpstat 是一个常用的多核 CPU 性能分析工具，用来实时查看每个 CPU 的性能指标，以及所有 CPU 的平均指标。 pidstat 是一个常用的进程性能分析工具，用来实时查看进程的 CPU、内存、I/O 以及上下文切换等性能指标。 场景1：CPU密集型进程我们打开终端一运行 stree 命令，模拟一个 CPU 使用率 100% 的场景 12[root@localhost ~]# stress --cpu 1 --timeout 600stress: info: [5399] dispatching hogs: 1 cpu, 0 io, 0 vm, 0 hdd 我们打开终端二，查看 CPU 负载的上升状态 12345678910[root@localhost ~]# uptime 01:50:42 up 1 day, 1:42, 3 users, load average: 0.68, 0.22, 0.11[root@localhost ~]# uptime 01:50:45 up 1 day, 1:42, 3 users, load average: 0.71, 0.23, 0.12[root@localhost ~]# uptime 01:51:10 up 1 day, 1:43, 3 users, load average: 0.81, 0.29, 0.14[root@localhost ~]# uptime 01:54:58 up 1 day, 1:47, 4 users, load average: 1.03, 0.68, 0.33# 一段时间后，我们发现1分钟的平均 load 值超过了1，为啥？ 设备上还有些其他进程运行啊。 打开终端三，查看 CPU 使用状态 1234567891011121314151617181920212223[root@localhost ~]# mpstat -P ALL 5Linux 3.10.0-514.16.1.el7.x86_64 (localhost.localdomain) 11/24/2018 _x86_64_ (16 CPU)01:53:08 AM CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle01:53:13 AM all 6.24 0.00 0.01 0.00 0.00 0.00 0.01 0.00 0.00 93.7301:53:13 AM 0 0.00 0.00 0.00 0.00 0.00 0.00 0.20 0.00 0.00 99.8001:53:13 AM 1 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0001:53:13 AM 2 0.20 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 99.8001:53:13 AM 3 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0001:53:13 AM 4 0.00 0.00 0.20 0.00 0.00 0.00 0.00 0.00 0.00 99.8001:53:13 AM 5 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0001:53:13 AM 6 0.00 0.00 0.20 0.00 0.00 0.00 0.00 0.00 0.00 99.8001:53:13 AM 7 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0001:53:13 AM 8 0.20 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 99.8001:53:13 AM 9 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0001:53:13 AM 10 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0001:53:13 AM 11 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0001:53:13 AM 12 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0001:53:13 AM 13 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0001:53:13 AM 14 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0001:53:13 AM 15 100.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00# 这里我们可以看到，在CPU15上 CPU的使用率一直处于100%状态，使用这个工具可以持续看到状态的变化。 从终端二中可以看到，1 分钟的平均负载慢慢会增加到 1；而从终端三中可以看到，正好有一个 CPU 的使用率为 100%，但他的 iowait 为 0。这说明，平均负载的升高正是由于 CPU 使用率为 100% 导致的。 那么，到底是哪个进程导致了 CPU 使用率为 100% 呢？ 你可以使用 pidstat 来查询： 123456789101112[root@localhost ~]# pidstat -u 5 1Linux 3.10.0-514.16.1.el7.x86_64 (localhost.localdomain) 11/24/2018 _x86_64_ (16 CPU)02:00:20 AM UID PID %usr %system %guest %CPU CPU Command02:00:25 AM 0 8451 100.00 0.00 0.00 100.00 2 stress02:00:25 AM 0 8456 0.00 0.20 0.00 0.20 3 pidstat02:00:25 AM 0 8457 0.20 0.20 0.00 0.40 15 clientAverage: UID PID %usr %system %guest %CPU CPU CommandAverage: 0 8451 100.00 0.00 0.00 100.00 - stressAverage: 0 8456 0.00 0.20 0.00 0.20 - pidstatAverage: 0 8457 0.20 0.20 0.00 0.40 - client 从这里，可以明显看到，stress 进程的 CPU 使用率为 100%。 场景二：I/O 密集型进程首先还是运行 stress 命令，但这次模拟 I/O 压力，即不停的执行 sync。 打开终端一，执行 stress 12[root@localhost ~]# stress -i 1 --timeout 3600stress: info: [8817] dispatching hogs: 0 cpu, 1 io, 0 vm, 0 hdd 打开终端二 123456[root@localhost ~]# uptime 02:02:36 up 1 day, 1:54, 4 users, load average: 0.83, 0.85, 0.56[root@localhost ~]# uptime 02:05:27 up 1 day, 1:57, 4 users, load average: 0.99, 0.92, 0.63# 这里，也会看到，load会不断的升高 打开终端三 123456789101112131415161718192021222324[root@localhost ~]# mpstat -P ALL 5Linux 3.10.0-514.16.1.el7.x86_64 (localhost.localdomain) 11/24/2018 _x86_64_ (16 CPU)Average: CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idleAverage: all 0.05 0.00 5.93 0.34 0.00 0.00 0.05 0.00 0.00 93.63Average: 0 0.16 0.00 0.48 0.00 0.00 0.00 0.14 0.00 0.00 99.22Average: 1 0.03 0.00 0.09 0.01 0.00 0.00 0.03 0.00 0.00 99.84Average: 2 0.03 0.00 0.09 0.00 0.00 0.00 0.01 0.00 0.00 99.88Average: 3 0.09 0.00 0.23 0.00 0.00 0.00 0.03 0.00 0.00 99.65Average: 4 0.13 0.00 0.53 0.00 0.00 0.00 0.05 0.00 0.00 99.29Average: 5 0.02 0.00 0.05 0.00 0.00 0.00 0.05 0.00 0.00 99.88Average: 6 0.02 0.00 0.35 0.00 0.00 0.00 0.08 0.00 0.00 99.56Average: 7 0.02 0.00 0.04 0.00 0.00 0.00 0.03 0.00 0.00 99.90Average: 8 0.02 0.00 0.14 0.00 0.00 0.00 0.04 0.00 0.00 99.80Average: 9 0.10 0.00 0.28 0.00 0.00 0.00 0.03 0.00 0.00 99.59Average: 10 0.09 0.00 0.34 0.00 0.00 0.00 0.05 0.00 0.00 99.52Average: 11 0.01 0.00 0.06 0.00 0.00 0.00 0.03 0.00 0.00 99.90Average: 12 0.03 0.00 33.73 1.96 0.00 0.00 0.05 0.00 0.00 64.23Average: 13 0.02 0.00 0.04 0.00 0.00 0.00 0.02 0.00 0.00 99.92Average: 14 0.03 0.00 2.43 0.12 0.00 0.00 0.04 0.00 0.00 97.37Average: 15 0.04 0.00 56.38 3.30 0.00 0.00 0.17 0.00 0.00 40.12# 这里看到，CPU 的 use 使用不是很高，反而 sys 使用的比较高，分布在了 2 个 CPU 上，约等于 100%# 同时可以看到 iowait 的值也升高了一些，由于我的设备全是 ssd 磁盘，所以这个 io 的性能可能会稍微好一些。 从以上操作中，我们看到 1 分钟的平均负载会慢慢的增加，其中一个 CPU 的系统 CPU 使用率提升到了 56 ，同时 iowait 也提升到了 3，这说明平均负载的升高是由于系统资源使用和 iowait 导致。 这里更新 sysstat 包的版本 12[root@localhost ~]# wget http://pagesperso-orange.fr/sebastien.godard/sysstat-12.1.1-1.x86_64.rpm[root@localhost ~]# rpm -Uvh sysstat-12.1.1-1.x86_64.rpm 那么到底是哪个进程，导致系统 CPU 使用率特别高，及 CPU 的等待 wait 情况 12345678910111213[root@localhost ~]# pidstat -u 5 1Linux 3.10.0-514.16.1.el7.x86_64 (localhost.localdomain) 11/24/2018 _x86_64_ (16 CPU)02:34:53 AM UID PID %usr %system %guest %wait %CPU CPU Command02:34:58 AM 0 730 0.00 0.20 0.00 0.00 0.20 12 xfsaild/vda602:34:58 AM 0 1471 0.00 0.20 0.00 0.00 0.20 10 kworker/10:202:34:58 AM 0 3042 0.00 0.40 0.00 0.00 0.40 7 kworker/7:1H02:34:58 AM 0 11617 0.00 1.59 0.00 0.00 1.59 2 kworker/u32:102:34:58 AM 0 15272 0.00 91.43 0.00 0.40 91.43 7 stress02:34:58 AM 0 15273 0.00 0.20 0.00 0.00 0.20 14 kworker/u32:002:34:58 AM 0 15274 0.20 0.40 0.00 0.00 0.60 5 pidstat# %wait：表示等待运行时任务占用 CPU 百分比。 通过以上的信息，可以很清晰的看到，是由于 stress 进程出现了大量的系统使用。 场景三：大量进程的场景当系统中运行进程超出CPU运行能力时，就会出现等待CPU的进程。 我们打开终端一：使用 stress 模拟 24 个进程 12[root@localhost ~]# stress -c 24 --timeout 3600stress: info: [11726] dispatching hogs: 24 cpu, 0 io, 0 vm, 0 hdd 打开终端二：看下当前的负载值 123456[root@localhost ~]# uptime 02:20:36 up 1 day, 2:12, 4 users, load average: 17.22, 5.98, 2.61[root@localhost ~]# uptime 02:20:52 up 1 day, 2:13, 4 users, load average: 18.72, 6.86, 2.95[root@localhost ~]# uptime 02:24:03 up 1 day, 2:16, 4 users, load average: 23.77, 14.94, 6.85 打开终端三：看下进程的资源使用信息 12345678910111213141516171819202122232425262728293031[root@localhost ~]# pidstat -u 5 1Linux 3.10.0-514.16.1.el7.x86_64 (localhost.localdomain) 11/24/2018 _x86_64_ (16 CPU)02:28:14 AM UID PID %usr %system %guest %wait %CPU CPU Command02:28:19 AM 0 43 0.00 0.20 0.00 0.00 0.20 7 ksoftirqd/702:28:19 AM 0 2292 0.20 0.00 0.00 0.00 0.20 11 dstat02:28:19 AM 0 11727 48.81 0.00 0.00 44.05 48.81 5 stress02:28:19 AM 0 11728 44.64 0.00 0.00 0.00 44.64 12 stress02:28:19 AM 0 11729 41.27 0.00 0.00 49.60 41.27 11 stress02:28:19 AM 0 11730 46.03 0.00 0.00 41.27 46.03 2 stress02:28:19 AM 0 11731 59.92 0.00 0.00 30.16 59.92 15 stress02:28:19 AM 0 11732 47.62 0.00 0.00 25.60 47.62 13 stress02:28:19 AM 0 11733 65.67 0.00 0.00 22.02 65.67 2 stress02:28:19 AM 0 11734 41.67 0.00 0.00 50.40 41.67 10 stress02:28:19 AM 0 11735 54.17 0.00 0.00 32.34 54.17 15 stress02:28:19 AM 0 11736 42.06 0.00 0.00 50.20 42.06 6 stress02:28:19 AM 0 11737 35.91 0.00 0.00 29.96 35.91 3 stress02:28:19 AM 0 11738 50.20 0.00 0.00 5.16 50.20 10 stress02:28:19 AM 0 11739 42.06 0.00 0.00 49.60 42.06 6 stress02:28:19 AM 0 11740 58.73 0.00 0.00 34.92 58.73 4 stress02:28:19 AM 0 11741 46.63 0.00 0.00 13.49 46.63 1 stress02:28:19 AM 0 11742 43.45 0.00 0.00 50.79 43.45 14 stress02:28:19 AM 0 11743 44.05 0.00 0.00 45.24 44.05 7 stress02:28:19 AM 0 11744 56.55 0.00 0.00 12.70 56.55 0 stress02:28:19 AM 0 11745 46.23 0.00 0.00 49.80 46.23 5 stress02:28:19 AM 0 11746 49.40 0.00 0.00 41.27 49.40 11 stress02:28:19 AM 0 11747 43.65 0.00 0.00 49.40 43.65 14 stress02:28:19 AM 0 11748 59.33 0.00 0.00 0.99 59.33 8 stress02:28:19 AM 0 11749 46.43 0.00 0.00 45.24 46.43 4 stress02:28:19 AM 0 11750 51.19 0.00 0.00 24.60 51.19 9 stress02:28:19 AM 0 14276 0.00 0.40 0.00 0.20 0.40 10 pidstat 我们发现，运行的 24 个 stress 进程，出现了资源争抢的问题，既然出现了资源争抢，就会出现等待时间 wait。 注意事项1、iowait 不等于 cpu wait。 2、iowait 多少算高。 123一般 iowait 达 30% 就算高了，需要关注。使用：iostat -x 1 10其中如果 %util 到 70%，那么磁盘IO 就很频繁了，需要重点关注。 参考文章1、如何理解linux的平均负载？]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[percona-toolkit大表操作DDL使用]]></title>
    <url>%2F2019%2F09%2F02%2Fpercona01%2F</url>
    <content type="text"><![CDATA[操作系统与安装数据库1234567891011121314[root@zhang ~]# cat /etc/redhat-release # 也可以使用其他版本 CentOS Linux release 7.4.1708 (Core)[root@zhang ~]# yum install -y mariadb mariadb-server # CentOS7的mysql数据库为mariadb ………………[root@zhang ~]# systemctl enable mariadb.service # 开机自启动mariadb Created symlink from /etc/systemd/system/multi-user.target.wants/mariadb.service to /usr/lib/systemd/system/mariadb.service.[root@zhang ~]# systemctl start mariadb.service # 启动mariadb [root@zhang ~]# systemctl status mariadb.service # 查看mariadb服务状态 ● mariadb.service - MariaDB database server Loaded: loaded (/usr/lib/systemd/system/mariadb.service; enabled; vendor preset: disabled) Active: active (running) since Wed 2018-05-23 17:13:35 CST; 6s ago Process: 1755 ExecStartPost=/usr/libexec/mariadb-wait-ready $MAINPID (code=exited, status=0/SUCCESS) Process: 1675 ExecStartPre=/usr/libexec/mariadb-prepare-db-dir %n (code=exited, status=0/SUCCESS)……………… 数据库准备工作数据库字符集修改数据库版本信息1234567MariaDB [(none)]&gt; select version();+----------------+| version() |+----------------+| 5.5.56-MariaDB |+----------------+row in set (0.00 sec) 支持哪些字符集12345678910111213141516171819202122232425262728293031323334353637383940414243444546MariaDB [(none)]&gt; show CHARACTER SET; ## 字符集 描述 默认校对规则 最大长度 +----------+-----------------------------+---------------------+--------+| Charset | Description | Default collation | Maxlen |+----------+-----------------------------+---------------------+--------+| big5 | Big5 Traditional Chinese | big5_chinese_ci | 2 || dec8 | DEC West European | dec8_swedish_ci | 1 || cp850 | DOS West European | cp850_general_ci | 1 || hp8 | HP West European | hp8_english_ci | 1 || koi8r | KOI8-R Relcom Russian | koi8r_general_ci | 1 || latin1 | cp1252 West European | latin1_swedish_ci | 1 || latin2 | ISO 8859-2 Central European | latin2_general_ci | 1 || swe7 | 7bit Swedish | swe7_swedish_ci | 1 || ascii | US ASCII | ascii_general_ci | 1 || ujis | EUC-JP Japanese | ujis_japanese_ci | 3 || sjis | Shift-JIS Japanese | sjis_japanese_ci | 2 || hebrew | ISO 8859-8 Hebrew | hebrew_general_ci | 1 || tis620 | TIS620 Thai | tis620_thai_ci | 1 || euckr | EUC-KR Korean | euckr_korean_ci | 2 || koi8u | KOI8-U Ukrainian | koi8u_general_ci | 1 || gb2312 | GB2312 Simplified Chinese | gb2312_chinese_ci | 2 || greek | ISO 8859-7 Greek | greek_general_ci | 1 || cp1250 | Windows Central European | cp1250_general_ci | 1 || gbk | GBK Simplified Chinese | gbk_chinese_ci | 2 || latin5 | ISO 8859-9 Turkish | latin5_turkish_ci | 1 || armscii8 | ARMSCII-8 Armenian | armscii8_general_ci | 1 || utf8 | UTF-8 Unicode | utf8_general_ci | 3 || ucs2 | UCS-2 Unicode | ucs2_general_ci | 2 || cp866 | DOS Russian | cp866_general_ci | 1 || keybcs2 | DOS Kamenicky Czech-Slovak | keybcs2_general_ci | 1 || macce | Mac Central European | macce_general_ci | 1 || macroman | Mac West European | macroman_general_ci | 1 || cp852 | DOS Central European | cp852_general_ci | 1 || latin7 | ISO 8859-13 Baltic | latin7_general_ci | 1 || utf8mb4 | UTF-8 Unicode | utf8mb4_general_ci | 4 || cp1251 | Windows Cyrillic | cp1251_general_ci | 1 || utf16 | UTF-16 Unicode | utf16_general_ci | 4 || cp1256 | Windows Arabic | cp1256_general_ci | 1 || cp1257 | Windows Baltic | cp1257_general_ci | 1 || utf32 | UTF-32 Unicode | utf32_general_ci | 4 || binary | Binary pseudo charset | binary | 1 || geostd8 | GEOSTD8 Georgian | geostd8_general_ci | 1 || cp932 | SJIS for Windows Japanese | cp932_japanese_ci | 2 || eucjpms | UJIS for Windows Japanese | eucjpms_japanese_ci | 3 |+----------+-----------------------------+---------------------+--------+39 rows in set (0.00 sec) 当前数据库默认字符集1234567891011121314MariaDB [(none)]&gt; show variables like &apos;%character_set%&apos;;+--------------------------+----------------------------+| Variable_name | Value |+--------------------------+----------------------------+| character_set_client | utf8 | ## 客户端来源数据使用的字符集| character_set_connection | utf8 | ## 连接层字符集| character_set_database | latin1 | ## 当前选中数据库的默认字符集| character_set_filesystem | binary || character_set_results | utf8 | ## 查询结果返回字符集| character_set_server | latin1 | ## 默认的内部操作字符集【服务端（数据库）字符】| character_set_system | utf8 | ## 系统元数据(字段名等)字符集【Linux系统字符集】| character_sets_dir | /usr/share/mysql/charsets/ |+--------------------------+----------------------------+rows in set (0.00 sec) 修改字符集为utf81234567891011121314151617181920212223242526272829303132333435363738394041[root@zhang ~]# vim /etc/my.cnf [client]default-character-set=utf8[mysqld]character-set-server=utf8datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.sock# Disabling symbolic-links is recommended to prevent assorted security riskssymbolic-links=0# Settings user and group are ignored when systemd is used.# If you need to run mysqld under a different user or group,# customize your systemd unit file for mariadb according to the# instructions in http://fedoraproject.org/wiki/Systemd[mysqld_safe]log-error=/var/log/mariadb/mariadb.logpid-file=/var/run/mariadb/mariadb.pid## include all files from the config directory#!includedir /etc/my.cnf.d[root@zhang ~]# systemctl restart mariadb.service # 重启mariadb# 字符集查看 MariaDB [(none)]&gt; show variables like &apos;%character_set%&apos;; +--------------------------+----------------------------+| Variable_name | Value |+--------------------------+----------------------------+| character_set_client | utf8 || character_set_connection | utf8 || character_set_database | utf8 || character_set_filesystem | binary || character_set_results | utf8 || character_set_server | utf8 || character_set_system | utf8 || character_sets_dir | /usr/share/mysql/charsets/ |+--------------------------+----------------------------+8 rows in set (0.00 sec) 数据库建库、授权操作创建数据库12345678910111213141516171819202122MariaDB [(none)]&gt; create database zhangtest01; Query OK, 1 row affected (0.00 sec)MariaDB [(none)]&gt; show create database zhangtest01; +-------------+----------------------------------------------------------------------+| Database | Create Database |+-------------+----------------------------------------------------------------------+| zhangtest01 | CREATE DATABASE `zhangtest01` /*!40100 DEFAULT CHARACTER SET utf8 */ |+-------------+----------------------------------------------------------------------+row in set (0.00 sec)MariaDB [(none)]&gt; show databases; +--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || test || zhangtest01 |+--------------------+rows in set (0.00 sec) 授权123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354MariaDB [(none)]&gt; grant all on zhangtest01.* to zhang01@&apos;%&apos; identified by &apos;zhang01&apos;; # 错误授权【大表操作时会失败】 Query OK, 0 rows affected (0.00 sec)MariaDB [(none)]&gt; grant all on *.* to zhang06@&apos;%&apos; identified by &apos;zhang06&apos;; # 正确授权 Query OK, 0 rows affected (0.00 sec)MariaDB [(none)]&gt; flush privileges; # 刷新权限 Query OK, 0 rows affected (0.00 sec)MariaDB [(none)]&gt; show grants for zhang01@&apos;%&apos; ; +--------------------------------------------------------------------------------------------------------+| Grants for zhang01@% |+--------------------------------------------------------------------------------------------------------+| GRANT USAGE ON *.* TO &apos;zhang01&apos;@&apos;%&apos; IDENTIFIED BY PASSWORD &apos;*4D6E977808109CE3DEEDEDA4E3EA17CE0F9CC8C1&apos; || GRANT ALL PRIVILEGES ON `zhangtest01`.* TO &apos;zhang01&apos;@&apos;%&apos; |+--------------------------------------------------------------------------------------------------------+2 rows in set (0.00 sec)MariaDB [(none)]&gt; show grants for zhang06@&apos;%&apos; ; +-----------------------------------------------------------------------------------------------------------------+| Grants for zhang06@% |+-----------------------------------------------------------------------------------------------------------------+| GRANT ALL PRIVILEGES ON *.* TO &apos;zhang06&apos;@&apos;%&apos; IDENTIFIED BY PASSWORD &apos;*45D6EF2FFF78EB89123D0056C9AE2FC6BA6DA0E7&apos; |+-----------------------------------------------------------------------------------------------------------------+1 row in set (0.00 sec)mysql&gt; show variables like &apos;%connect%&apos;; +--------------------------+-----------------+| Variable_name | Value |+--------------------------+-----------------+| character_set_connection | utf8 || collation_connection | utf8_general_ci || connect_timeout | 10 || extra_max_connections | 1 || init_connect | || max_connect_errors | 10 || max_connections | 151 || max_user_connections | 0 |+--------------------------+-----------------+8 rows in set (0.00 sec)mysql&gt; show status like &apos;%connect%&apos;; # 连接信息 +--------------------------+-------+| Variable_name | Value |+--------------------------+-------+| Aborted_connects | 11 || Connections | 36 || Max_used_connections | 9 || Ssl_client_connects | 0 || Ssl_connect_renegotiates | 0 || Ssl_finished_connects | 0 || Threads_connected | 5 |+--------------------------+-------+7 rows in set (0.00 sec) 数据库建表、插入数据建表语句123456CREATE TABLE `zhang_test` ( `id` int(10) NOT NULL, `name` varchar(50) NOT NULL, `address` varchar(255) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 插入语句示例1INSERT INTO `zhangtest01`.`zhang_test` (`id`, `name`, `address`) VALUES (&apos;0&apos;, &apos;test0&apos;, &apos;中国XX省XX市0&apos;); 对应的批量SQL语句脚本1234567[root@zhang database]# vim batch_insert.sh #!/bin/shfor i in `echo &#123;1..2000000&#125;`;do echo &quot;INSERT INTO zhangtest01.zhang_test (id, name, address) VALUES (&apos;$&#123;i&#125;&apos;, &apos;test$&#123;i&#125;&apos;, &apos;中国XX省XX市$&#123;i&#125;&apos;); &quot;done 执行脚本将插入数据放到一个文本中，之后导入数据库即可 percona-toolkit安装【可以在另外一台机器】1234567# 官网下载[root@zhang tools]# wget https://www.percona.com/downloads/percona-toolkit/3.0.10/binary/redhat/7/x86_64/percona-toolkit-3.0.10-1.el7.x86_64.rpm ………………[root@zhang tools]# yum install -y percona-toolkit-3.0.10-1.el7.x86_64.rpm ………………[root@docker01 tools]# pt-online-schema-change --help # 帮助文档……………… 大表DDL操作添加表字段【并保存原始表】1234567891011121314151617181920212223242526[root@docker01 tools]# pt-online-schema-change -h172.16.1.14 -uzhang06 -pzhang06 --nocheck-replication-filters --nodrop-old-table --charset=UTF8 --max-load=&quot;Threads_running=1000&quot; --alter &quot;add uuid varchar(100) NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;UUID&apos; after id&quot; D=zhangtest01,t=zhang_test --execute # 语句No slaves found. See --recursion-method if host zhang has slaves.Not checking slave lag because no slaves were found and --check-slave-lag was not specified.Operation, tries, wait: analyze_table, 10, 1 copy_rows, 10, 0.25 create_triggers, 10, 1 drop_triggers, 10, 1 swap_tables, 10, 1 update_foreign_keys, 10, 1Altering `zhangtest01`.`zhang_test`...Creating new table...Created new table zhangtest01._zhang_test_new OK.Altering new table...Altered `zhangtest01`.`_zhang_test_new` OK.2018-05-23T13:02:13 Creating triggers...2018-05-23T13:02:13 Created triggers OK.2018-05-23T13:02:13 Copying approximately 2006480 rows...Copying `zhangtest01`.`zhang_test`: 74% 00:10 remain2018-05-23T13:02:21 Copied rows OK.2018-05-23T13:02:21 Swapping tables...2018-05-23T13:02:21 Swapped original and new tables OK.Not dropping old table because --no-drop-old-table was specified.2018-05-23T13:02:21 Dropping triggers...2018-05-23T13:02:21 Dropped triggers OK.Successfully altered `zhangtest01`.`zhang_test`. 修改后的表结构1234567CREATE TABLE `zhang_test` ( `id` int(10) NOT NULL, `uuid` varchar(100) NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;UUID&apos;, `name` varchar(50) NOT NULL, `address` varchar(255) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 命令参数说明1234567891011121314151617181920--nocheck-replication-filters不检查复制过滤器【有主从复制也照样执行】--[no]drop-old-table 操作完后是否删除原始表【默认TRUE】--critical-load=&quot;Threads_running=1000&quot; # 终止拷贝【不优先使用】 类似于--max-load，不同的是检测到超高负载时会直接中断OSC进程而不是暂停--max-load=&quot;Threads_running=1000&quot; # 暂停拷贝【优先使用】 默认如果检测到服务器负载过重会暂停操作 检查每个块后显示全局状态，如果负载太高暂停（默认Threads_running＝25）D=zhangtest01 操作的哪个数据库t=zhang_test 操作哪张表--execute 执行操作 修改表字段123456789101112131415161718192021222324252627[root@docker01 ~]# pt-online-schema-change -h172.16.1.14 -uzhang06 -pzhang06 --nocheck-replication-filters --charset=UTF8 --max-load=&quot;Threads_running=1000&quot; --alter &quot;MODIFY uuid varchar(80)&quot; D=zhangtest01,t=zhang_test --execute [root@docker01 ~]# pt-online-schema-change -h172.16.1.14 -uzhang06 -pzhang06 --nocheck-replication-filters --charset=UTF8 --max-load=&quot;Threads_running=1000&quot; --alter &quot;MODIFY uuid int(11)&quot; D=zhangtest01,t=zhang_test --execute No slaves found. See --recursion-method if host zhang has slaves.Not checking slave lag because no slaves were found and --check-slave-lag was not specified.Operation, tries, wait: analyze_table, 10, 1 copy_rows, 10, 0.25 create_triggers, 10, 1 drop_triggers, 10, 1 swap_tables, 10, 1 update_foreign_keys, 10, 1Altering `zhangtest01`.`zhang_test`...Creating new table...Created new table zhangtest01._zhang_test_new OK.Altering new table...Altered `zhangtest01`.`_zhang_test_new` OK.2018-05-23T22:11:15 Creating triggers...2018-05-23T22:11:15 Created triggers OK.2018-05-23T22:11:15 Copying approximately 2005915 rows...2018-05-23T22:11:22 Copied rows OK.2018-05-23T22:11:22 Swapping tables...2018-05-23T22:11:22 Swapped original and new tables OK.2018-05-23T22:11:22 Dropping old table...2018-05-23T22:11:22 Dropped old table `zhangtest01`.`__zhang_test_old` OK.2018-05-23T22:11:22 Dropping triggers...2018-05-23T22:11:22 Dropped triggers OK.Successfully altered `zhangtest01`.`zhang_test`. 删除表字段1234567891011121314151617181920212223242526[root@docker01 ~]# pt-online-schema-change -h172.16.1.14 -uzhang06 -pzhang06 --nocheck-replication-filters --charset=UTF8 --max-load=&quot;Threads_running=1000&quot; --alter &quot;DROP uuid&quot; D=zhangtest01,t=zhang_test --execute No slaves found. See --recursion-method if host zhang has slaves.Not checking slave lag because no slaves were found and --check-slave-lag was not specified.Operation, tries, wait: analyze_table, 10, 1 copy_rows, 10, 0.25 create_triggers, 10, 1 drop_triggers, 10, 1 swap_tables, 10, 1 update_foreign_keys, 10, 1Altering `zhangtest01`.`zhang_test`...Creating new table...Created new table zhangtest01._zhang_test_new OK.Altering new table...Altered `zhangtest01`.`_zhang_test_new` OK.2018-05-23T22:04:38 Creating triggers...2018-05-23T22:04:38 Created triggers OK.2018-05-23T22:04:38 Copying approximately 1996965 rows...2018-05-23T22:04:47 Copied rows OK.2018-05-23T22:04:47 Swapping tables...2018-05-23T22:04:47 Swapped original and new tables OK.2018-05-23T22:04:47 Dropping old table...2018-05-23T22:04:47 Dropped old table `zhangtest01`.`__zhang_test_old` OK.2018-05-23T22:04:47 Dropping triggers...2018-05-23T22:04:47 Dropped triggers OK.Successfully altered `zhangtest01`.`zhang_test`. 添加表索引1234567891011121314151617181920212223242526[root@docker01 ~]# pt-online-schema-change -h172.16.1.14 -uzhang06 -pzhang06 --nocheck-replication-filters --charset=UTF8 --max-load=&quot;Threads_running=1000&quot; --alter &quot;ADD INDEX index_name(name)&quot; D=zhangtest01,t=zhang_test --execute # ADD INDEX indexName(columnName) No slaves found. See --recursion-method if host zhang has slaves.Not checking slave lag because no slaves were found and --check-slave-lag was not specified.Operation, tries, wait: analyze_table, 10, 1 copy_rows, 10, 0.25 create_triggers, 10, 1 drop_triggers, 10, 1 swap_tables, 10, 1 update_foreign_keys, 10, 1Altering `zhangtest01`.`zhang_test`...Creating new table...Created new table zhangtest01._zhang_test_new OK.Altering new table...Altered `zhangtest01`.`_zhang_test_new` OK.2018-05-23T22:16:59 Creating triggers...2018-05-23T22:16:59 Created triggers OK.2018-05-23T22:16:59 Copying approximately 2013664 rows...2018-05-23T22:17:12 Copied rows OK.2018-05-23T22:17:12 Swapping tables...2018-05-23T22:17:12 Swapped original and new tables OK.2018-05-23T22:17:12 Dropping old table...2018-05-23T22:17:12 Dropped old table `zhangtest01`.`__zhang_test_old` OK.2018-05-23T22:17:12 Dropping triggers...2018-05-23T22:17:12 Dropped triggers OK.Successfully altered `zhangtest01`.`zhang_test`. 修改后的表结构12345678CREATE TABLE `zhang_test` ( `id` int(10) NOT NULL, `uuid` int(11) DEFAULT NULL, `name` varchar(50) NOT NULL, `address` varchar(255) DEFAULT NULL, PRIMARY KEY (`id`), KEY `index_name` (`name`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 删除表索引1234567891011121314151617181920212223242526[root@docker01 ~]# pt-online-schema-change -h172.16.1.14 -uzhang06 -pzhang06 --nocheck-replication-filters --charset=UTF8 --max-load=&quot;Threads_running=1000&quot; --alter &quot;DROP INDEX index_name&quot; D=zhangtest01,t=zhang_test --execute # DROP INDEX indexName No slaves found. See --recursion-method if host zhang has slaves.Not checking slave lag because no slaves were found and --check-slave-lag was not specified.Operation, tries, wait: analyze_table, 10, 1 copy_rows, 10, 0.25 create_triggers, 10, 1 drop_triggers, 10, 1 swap_tables, 10, 1 update_foreign_keys, 10, 1Altering `zhangtest01`.`zhang_test`...Creating new table...Created new table zhangtest01._zhang_test_new OK.Altering new table...Altered `zhangtest01`.`_zhang_test_new` OK.2018-05-23T22:19:31 Creating triggers...2018-05-23T22:19:31 Created triggers OK.2018-05-23T22:19:31 Copying approximately 2005445 rows...2018-05-23T22:19:38 Copied rows OK.2018-05-23T22:19:38 Swapping tables...2018-05-23T22:19:38 Swapped original and new tables OK.2018-05-23T22:19:38 Dropping old table...2018-05-23T22:19:38 Dropped old table `zhangtest01`.`__zhang_test_old` OK.2018-05-23T22:19:38 Dropping triggers...2018-05-23T22:19:38 Dropped triggers OK.Successfully altered `zhangtest01`.`zhang_test`. 添加唯一索引12345678910111213141516171819202122232425262728##### 注意：确保字段中数据的唯一性，不然会丢失数据[root@docker01 ~]# pt-online-schema-change -h172.16.1.14 -uzhang06 -pzhang06 --nocheck-replication-filters --nocheck-unique-key-change --charset=UTF8 --max-load=&quot;Threads_running=1000&quot; --alter &quot;ADD UNIQUE uniq_index_name(name)&quot; D=zhangtest01,t=zhang_test --execute # ADD UNIQUE uniqueName(columnName) No slaves found. See --recursion-method if host zhang has slaves.Not checking slave lag because no slaves were found and --check-slave-lag was not specified.Operation, tries, wait: analyze_table, 10, 1 copy_rows, 10, 0.25 create_triggers, 10, 1 drop_triggers, 10, 1 swap_tables, 10, 1 update_foreign_keys, 10, 1Altering `zhangtest01`.`zhang_test`...Creating new table...Created new table zhangtest01._zhang_test_new OK.Altering new table...Altered `zhangtest01`.`_zhang_test_new` OK.2018-05-23T22:24:31 Creating triggers...2018-05-23T22:24:31 Created triggers OK.2018-05-23T22:24:31 Copying approximately 2005445 rows...Copying `zhangtest01`.`zhang_test`: 69% 00:13 remain2018-05-23T22:24:44 Copied rows OK.2018-05-23T22:24:44 Swapping tables...2018-05-23T22:24:44 Swapped original and new tables OK.2018-05-23T22:24:44 Dropping old table...2018-05-23T22:24:44 Dropped old table `zhangtest01`.`__zhang_test_old` OK.2018-05-23T22:24:44 Dropping triggers...2018-05-23T22:24:44 Dropped triggers OK.Successfully altered `zhangtest01`.`zhang_test`. 修改后的表结构12345678CREATE TABLE `zhang_test` ( `id` int(10) NOT NULL, `uuid` int(11) DEFAULT NULL, `name` varchar(50) NOT NULL, `address` varchar(255) DEFAULT NULL, PRIMARY KEY (`id`), UNIQUE KEY `uniq_index_name` (`name`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 命令参数说明12--nocheck-unique-key-change 添加该参数选项，才可以添加唯一索引 删除唯一索引123456789101112131415161718192021222324252627##### 与删除普通索引一样[root@docker01 ~]# pt-online-schema-change -h172.16.1.14 -uzhang06 -pzhang06 --nocheck-replication-filters --charset=UTF8 --max-load=&quot;Threads_running=1000&quot; --alter &quot;DROP INDEX uniq_index_name&quot; D=zhangtest01,t=zhang_test --execute # DROP UNIQUE uniqueNameNo slaves found. See --recursion-method if host zhang has slaves.Not checking slave lag because no slaves were found and --check-slave-lag was not specified.Operation, tries, wait: analyze_table, 10, 1 copy_rows, 10, 0.25 create_triggers, 10, 1 drop_triggers, 10, 1 swap_tables, 10, 1 update_foreign_keys, 10, 1Altering `zhangtest01`.`zhang_test`...Creating new table...Created new table zhangtest01._zhang_test_new OK.Altering new table...Altered `zhangtest01`.`_zhang_test_new` OK.2018-05-23T22:35:12 Creating triggers...2018-05-23T22:35:12 Created triggers OK.2018-05-23T22:35:12 Copying approximately 2005445 rows...2018-05-23T22:35:19 Copied rows OK.2018-05-23T22:35:19 Swapping tables...2018-05-23T22:35:19 Swapped original and new tables OK.2018-05-23T22:35:19 Dropping old table...2018-05-23T22:35:19 Dropped old table `zhangtest01`.`__zhang_test_old` OK.2018-05-23T22:35:19 Dropping triggers...2018-05-23T22:35:19 Dropped triggers OK.Successfully altered `zhangtest01`.`zhang_test`. 删除表主键重要说明1234567对主键修改的步骤： 1、保证有一个唯一索引【如果没有那么就添加一个唯一索引】 2、删除原主键 3、添加新主键 4、删除之前的唯一索引【可选】原因如下：The new table `zhangtest01`.`_zhang_test_new` does not have a PRIMARY KEY or a unique index which is required for the DELETE trigger. 1234567891011121314151617181920212223242526[root@docker01 ~]# pt-online-schema-change -h172.16.1.14 -uzhang06 -pzhang06 --nocheck-replication-filters --nocheck-alter --charset=UTF8 --max-load=&quot;Threads_running=1000&quot; --alter &quot;DROP PRIMARY KEY&quot; D=zhangtest01,t=zhang_test --execute No slaves found. See --recursion-method if host zhang has slaves.Not checking slave lag because no slaves were found and --check-slave-lag was not specified.Operation, tries, wait: analyze_table, 10, 1 copy_rows, 10, 0.25 create_triggers, 10, 1 drop_triggers, 10, 1 swap_tables, 10, 1 update_foreign_keys, 10, 1Altering `zhangtest01`.`zhang_test`...Creating new table...Created new table zhangtest01._zhang_test_new OK.Altering new table...Altered `zhangtest01`.`_zhang_test_new` OK.2018-05-23T22:45:28 Creating triggers...2018-05-23T22:45:28 Created triggers OK.2018-05-23T22:45:28 Copying approximately 2005445 rows...2018-05-23T22:45:42 Copied rows OK.2018-05-23T22:45:42 Swapping tables...2018-05-23T22:45:42 Swapped original and new tables OK.2018-05-23T22:45:42 Dropping old table...2018-05-23T22:45:42 Dropped old table `zhangtest01`.`__zhang_test_old` OK.2018-05-23T22:45:42 Dropping triggers...2018-05-23T22:45:42 Dropped triggers OK.Successfully altered `zhangtest01`.`zhang_test`. 修改后的表结构【没有主键了，但有唯一索引】1234567CREATE TABLE `zhang_test` ( `id` int(10) NOT NULL, `uuid` int(11) DEFAULT NULL, `name` varchar(50) NOT NULL, `address` varchar(255) DEFAULT NULL, UNIQUE KEY `uniq_index_name` (`name`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 命令参数说明123--[no]check-alter 解析-ALTER指定并尝试警告可能的意外行为（默认为“是”） 如果没有改选项，修改会失败 添加表主键1234567891011121314151617181920212223242526[root@docker01 ~]# pt-online-schema-change -h172.16.1.14 -uzhang06 -pzhang06 --nocheck-replication-filters --nocheck-alter --charset=UTF8 --max-load=&quot;Threads_running=1000&quot; --alter &quot;ADD PRIMARY KEY (id)&quot; D=zhangtest01,t=zhang_test --execute No slaves found. See --recursion-method if host zhang has slaves.Not checking slave lag because no slaves were found and --check-slave-lag was not specified.Operation, tries, wait: analyze_table, 10, 1 copy_rows, 10, 0.25 create_triggers, 10, 1 drop_triggers, 10, 1 swap_tables, 10, 1 update_foreign_keys, 10, 1Altering `zhangtest01`.`zhang_test`...Creating new table...Created new table zhangtest01._zhang_test_new OK.Altering new table...Altered `zhangtest01`.`_zhang_test_new` OK.2018-05-23T22:47:23 Creating triggers...2018-05-23T22:47:23 Created triggers OK.2018-05-23T22:47:23 Copying approximately 2227360 rows...2018-05-23T22:47:36 Copied rows OK.2018-05-23T22:47:36 Swapping tables...2018-05-23T22:47:36 Swapped original and new tables OK.2018-05-23T22:47:36 Dropping old table...2018-05-23T22:47:36 Dropped old table `zhangtest01`.`__zhang_test_old` OK.2018-05-23T22:47:36 Dropping triggers...2018-05-23T22:47:36 Dropped triggers OK.Successfully altered `zhangtest01`.`zhang_test`. 修改后的表结构12345678CREATE TABLE `zhang_test` ( `id` int(10) NOT NULL, `uuid` int(11) DEFAULT NULL, `name` varchar(50) NOT NULL, `address` varchar(255) DEFAULT NULL, PRIMARY KEY (`id`), UNIQUE KEY `uniq_index_name` (`name`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 多个操作合并1234567891011121314151617181920212223242526[root@docker01 ~]# pt-online-schema-change -h172.16.1.14 -uzhang06 -pzhang06 --nocheck-replication-filters --charset=UTF8 --max-load=&quot;Threads_running=1000&quot; --alter &quot;add last_name varchar(20) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;姓名&apos; after name, ADD INDEX index_address(address), add birthday date COMMENT &apos;生日&apos;&quot; D=zhangtest01,t=zhang_test --execute No slaves found. See --recursion-method if host zhang has slaves.Not checking slave lag because no slaves were found and --check-slave-lag was not specified.Operation, tries, wait: analyze_table, 10, 1 copy_rows, 10, 0.25 create_triggers, 10, 1 drop_triggers, 10, 1 swap_tables, 10, 1 update_foreign_keys, 10, 1Altering `zhangtest01`.`zhang_test`...Creating new table...Created new table zhangtest01._zhang_test_new OK.Altering new table...Altered `zhangtest01`.`_zhang_test_new` OK.2018-05-23T22:55:17 Creating triggers...2018-05-23T22:55:17 Created triggers OK.2018-05-23T22:55:17 Copying approximately 1990757 rows...2018-05-23T22:55:39 Copied rows OK.2018-05-23T22:55:39 Swapping tables...2018-05-23T22:55:39 Swapped original and new tables OK.2018-05-23T22:55:39 Dropping old table...2018-05-23T22:55:39 Dropped old table `zhangtest01`.`__zhang_test_old` OK.2018-05-23T22:55:39 Dropping triggers...2018-05-23T22:55:39 Dropped triggers OK.Successfully altered `zhangtest01`.`zhang_test`. 修改前的表结构12345678CREATE TABLE `zhang_test` ( `id` int(10) NOT NULL, `uuid` int(11) DEFAULT NULL, `name` varchar(50) NOT NULL, `address` varchar(255) DEFAULT NULL, PRIMARY KEY (`id`), UNIQUE KEY `uniq_index_name` (`name`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 修改后的表结构1234567891011CREATE TABLE `zhang_test` ( `id` int(10) NOT NULL, `uuid` int(11) DEFAULT NULL, `name` varchar(50) NOT NULL, `last_name` varchar(20) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;姓名&apos;, `address` varchar(255) DEFAULT NULL, `birthday` date DEFAULT NULL COMMENT &apos;生日&apos;, PRIMARY KEY (`id`), UNIQUE KEY `uniq_index_name` (`name`), KEY `index_address` (`address`)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 附录：1、不停机不停服务，MYSQL可以这样修改亿级数据表结构 2、pt-online-schema-change解读 3、Mysql 查看连接数,状态 最大并发数(赞)]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>percona</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[-bash：fork：Cannot allocate memory 问题的处理]]></title>
    <url>%2F2019%2F08%2F30%2Flinux-thread%2F</url>
    <content type="text"><![CDATA[文章来源：fork:cannot allocate memory问题的处理 文章参考：pid max导致fork: Cannot allocate memory 的分析及解决办法 今天遇到服务器无法SSH，VNC操作命令提示fork:cannot allocate memory free查看内存还有（注意，命令可能要多敲几次才会出来） 查看最大进程数 sysctl kernel.pid_max ps -eLf | wc -l查看进程数 确认是进程数满了 修改最大进程数后系统恢复 1echo 1000000 &gt; /proc/sys/kernel/pid_max 永久生效 12echo &quot;kernel.pid_max=1000000 &quot; &gt;&gt; /etc/sysctl.confsysctl -p]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[“反向代理层”绝不能替代“DNS轮询”！]]></title>
    <url>%2F2019%2F08%2F12%2Fproxy_dns01%2F</url>
    <content type="text"><![CDATA[文章转载自微信公众号：「架构师之路」，作者： 58沈剑 有朋友问我，DNS轮询是不是过时的技术了？有了反向代理层（Nginx、LVS、F5等），是不是就不需要DNS轮询了？ 然而，反向代理层绝不能替代 DNS 轮询！ 反向代理层有什么用？架构实现时要注意什么？(1) 作为服务端统一入口，屏蔽后端WEB集群细节，代表整个WEB集群； PS：这就是为啥它叫反向代理。 (2) 保证WEB集群的扩展性，Nginx后端可随时加WEB实例； (3) 实施负载均衡，反向代理层会将请求均匀分发给后端WEB集群的每一个实例； (4) 保证WEB集群的高可用，任何一个WEB实例挂了，服务都不受影响； (5) 注意自身高可用，防止一台Nginx挂了，服务端统一入口受影响； 反向代理层还存在啥问题？反向代理层自身的扩展性问题并没有得到很好的解决，例如当Nginx成为系统瓶颈的时候，无法扩容。 DNS轮询如何解决反向代理层的扩展性问题？通过在DNS-server上对一个域名设置多个IP解析，能够增加入口Nginx实例个数，起到水平扩容的作用，解决反向代理层的扩展性问题。 因此，反向代理和DNS轮询并不是互斥的技术，however，这里详细展开讲一下接入层的架构渐进历程。 裸奔时代（1）单机架构 裸奔时代的架构图如上： (1) 浏览器通过DNS-server，域名解析到ip； (2) 浏览器通过ip访问web-server； 缺点： (1) 非高可用，web-server挂了整个系统就挂了； (2) 扩展性差，当吞吐量达到web-server上限时，无法扩容； PS：单机不涉及负载均衡问题。 简易扩容方案（2）DNS轮询假设tomcat的吞吐量是1000次每秒，当系统总吞吐量达到3000时，如何扩容是首先要解决的问题，DNS轮询是一个很容易想到的方案。 PS：DNS轮询解决扩展性问题。 此时的架构图如上： (1) 多部署几份web-server，1个tomcat抗1000，部署3个tomcat就能抗3000； (2) 在DNS-server层面，域名每次解析到不同的ip； 优点： (1) 零成本：在DNS-server上多配几个ip即可，功能也不收费； (2) 部署简单：多部署几个web-server即可，原系统架构不需要做任何改造； (3) 负载均衡：变成了多机，负载也是均衡的； 缺点： (1) 非高可用：DNS-server只负责域名解析ip，这个ip对应的服务是否可用，DNS-server是不保证的，假设有一个web-server挂了，部分服务会受到影响； (2) 扩容非实时：DNS解析有一个生效周期； (3) 暴露了太多的外网 ip； 简易扩容方案（3）反向代理Nginxtomcat的性能较差，但Nginx作为反向代理的性能就强很多，假设线上跑到1w，就比tomcat高了10倍，可以利用这个特性来做扩容。 此时的架构图如上： (1) 站点层与浏览器层之间加入了一个反向代理层，利用高性能的Nginx来做反向代理； (2) Nginx将http请求分发给后端多个web-server； 优点： (1) DNS-server不需要动； (2) 负载均衡：通过Nginx来保证； (3) 只暴露一个外网 ip，Nginx-&gt;tomcat之间使用内网访问； (4) 扩容实时：Nginx内部可控，随时增加web-server随时实时扩容； (5) 能够保证站点层的可用性：任何一台tomcat挂了，Nginx可以将流量迁移到其他tomcat； PS：反向代理，能够更实时，更方便的扩容了。 缺点： (1) 时延增加 + 架构更复杂了：中间多加了一个反向代理层； (2) 反向代理层成了单点，非高可用：tomcat挂了不影响服务，Nginx挂了怎么办？ 高可用方案（4）keepalived为了解决高可用的问题，keepalived出场了。 (1) 做两台Nginx组成一个集群，分别部署上keepalived，设置成相同的虚IP，保证Nginx的高可用； (2) 当一台Nginx挂了，keepalived能够探测到，并将流量自动迁移到另一台Nginx上，整个过程对调用方透明； 优点： (1) 解决了高可用的问题； PS：反向代理的高可用也解决了。 缺点： (1) 资源利用率只有50%； (2) Nginx仍然是接入单点，如果接入吞吐量超过的Nginx的性能上限怎么办，例如qps达到了50000咧？ scale up扩容方案（5）lvs/f5Nginx是应用软件，性能比tomcat好，但总有个上限，超出了上限，还是扛不住。 lvs就不一样了，它实施在操作系统层面；f5的性能又更好了，它实施在硬件层面；它们性能比Nginx好很多，例如每秒可以抗10w，这样可以利用他们来扩容，常见的架构图如下： (1) 如果通过Nginx可以扩展多个tomcat一样，可以通过lvs来扩展多个Nginx； (2) 通过keepalived+VIP的方案可以保证可用性； 99.9999%的公司到这一步基本就结束了，解决了接入层高可用、扩展性、负载均衡的问题。PS：上游再加一层扩充性能。 完美了嘛，还有什么潜在问题？ 好吧，不管是使用lvs还是f5，这些都是scale up的方案，根本上，lvs/f5还是会有性能上限，假设每秒能处理10w的请求，一天也只能处理80亿的请求（10w秒吞吐量*8w秒），那万一系统的日PV超过80亿怎么办呢？ scale out扩容方案（6）DNS轮询如之前文章所述，水平扩展，才是解决性能问题的根本方案，能够通过加机器扩充性能的方案才具备最好的扩展性。 facebook，google，baidu的PV是不是超过80亿呢，它们的域名只对应一个ip么，终点又是起点，还是得通过DNS轮询来进行扩容。PS：DNS轮询解决扩展性问题。 (1) 通过DNS轮询来线性扩展入口lvs层的性能； (2) 通过keepalived来保证高可用； (3) 通过lvs来扩展多个Nginx； (4) 通过Nginx来做负载均衡，业务七层路由； 总结稍微做一个简要的总结： (1) 接入层架构要考虑的问题域为：高可用、扩展性、反向代理、负载均衡； (2) Nginx、keepalived、lvs、f5可以很好的解决高可用、扩展性、反向代理、负载均衡的问题； (3) 水平扩展 scale out是解决扩展性问题的根本方案，DNS轮询是不能完全被Nginx/lvs/f5所替代的；]]></content>
      <categories>
        <category>DNS</category>
        <category>proxy</category>
      </categories>
      <tags>
        <tag>DNS</tag>
        <tag>proxy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[负载均衡，必须要知道的 5 件事]]></title>
    <url>%2F2019%2F08%2F12%2Fslb01%2F</url>
    <content type="text"><![CDATA[文章转载自微信公众号：「架构师之路」，作者： 58沈剑 什么是负载均衡？负载均衡（Load Balance）是分布式系统架构设计中必须考虑的因素之一，它通常是指，将请求/数据均匀分摊到多个操作单元上执行，负载均衡的关键在于均匀。 常见的负载均衡方案有哪些？ 常见互联网分布式架构如上，分为： 客户端层 反向代理层 站点层 服务层 数据层 可以看到，每一个下游都有多个上游调用，只需要做到，每一个上游都均匀访问每一个下游，就能实现整体的均匀分摊。 第一层：客户端层到反向代理层 客户端层到反向代理层的负载均衡，是通过“DNS轮询”实现的。 DNS-server 对于一个域名配置了多个解析 ip，每次 DNS 解析请求来访问 DNS-server，会轮询返回这些 ip，保证每个 ip 的解析概率是相同的。这些 ip 就是 nginx 的外网 ip，以做到每台 nginx 的请求分配也是均衡的。 第二层：反向代理层到站点层 反向代理层到站点层的负载均衡，是通过“nginx”实现的。 PS：nginx 是反向代理的泛指。 修改nginx.conf，可以实现多种均衡策略： (1) 请求轮询：和DNS轮询类似，请求依次路由到各个 web-server； (2) 最少连接路由：哪个web-server的连接少，路由到哪个 web-server； (3) ip 哈希：按照访问用户的ip哈希值来路由 web-server，只要用户的 ip 分布是均匀的，请求理论上也是均匀的，ip 哈希均衡方法可以做到，同一个用户的请求固定落到同一台 web-server 上，此策略适合有状态服务，例如 session； PS：站点层可以存储 session，但强烈不建议这么做，站点层无状态是分布式架构设计的基本原则之一，session最好放到数据层存储。 (4) … 第三层：站点层到服务层 站点层到服务层的负载均衡，是通过“服务连接池”实现的。 上游连接池会建立与下游服务多个连接，每次请求会“随机”选取连接来访问下游服务。除了负载均衡，服务连接池还能够实现故障转移、超时处理、限流限速、ID串行化等诸多功能。 第四层：访问数据层在数据量很大的情况下，由于数据层（db/cache）涉及数据的水平切分，所以数据层的负载均衡更为复杂一些，它分为“数据的均衡”，与“请求的均衡”。 数据的均衡是指：水平切分后的每个服务（db/cache），数据量是均匀的。 请求的均衡是指：水平切分后的每个服务（db/cache），请求量是均匀的。 业内常见的水平切分方式有这么几种： 按照range水平切分 每一个数据服务，存储一定范围的数据： user0 服务：存储 uid 范围 1-1kw user1 服务：存储 uid 范围 1kw-2kw 这个方案的好处是： 规则简单，service只需判断一下uid范围就能路由到对应的存储服务 数据均衡性较好 比较容易扩展，可以随时加一个uid[2kw,3kw]的数据服务 不足是： 请求的负载不一定均衡，一般来说，新注册的用户会比老用户更活跃，大range的服务请求压力会更大 按照 id 哈希水平切分 每一个数据服务，存储某个key值hash后的部分数据： user0服务：存储偶数uid数据 user1服务：存储奇数uid数据 这个方案的好处是： 规则简单，service只需对uid进行hash能路由到对应的存储服务 数据均衡性较好 请求均匀性较好 不足是： 不容易扩展，扩展一个数据服务，hash方法改变时候，可能需要进行数据迁移 总结负载均衡（Load Balance）是分布式系统架构设计中必须考虑的因素之一，它通常是指，将请求/数据均匀分摊到多个操作单元上执行，其的关键在于均匀： 反向代理层的负载均衡，是通过“DNS轮询”实现的 站点层的负载均衡，是通过“nginx”实现的 服务层的负载均衡，是通过“服务连接池”实现的 数据层的负载均衡，要考虑“数据的均衡”与“请求的均衡”两个点，常见的方式有“按照范围水平切分”与“hash水平切分”]]></content>
      <categories>
        <category>SLB</category>
      </categories>
      <tags>
        <tag>SLB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker Swarm 常用操作]]></title>
    <url>%2F2019%2F08%2F08%2Fdocker-swarm01%2F</url>
    <content type="text"><![CDATA[说明本文档针对docker swarm操作。 针对的系统是以一个本地的测试系统为例。其中机器信息如下，172.16.1.13作为docker swarm的管理机。 本地测试的机器列表信息： 主机名 模拟的外网 内网IP 部署模块 mini01 10.0.0.11 172.16.1.11 tomcat、hadoop-datanode、hbase-regionserver 【swarm 管理】 mini02 10.0.0.12 172.16.1.12 tomcat、hadoop-datanode、hbase-regionserver 【swarm 管理】 mini03 10.0.0.13 172.16.1.13 visualizer、spark、zookeeper、hadoop-namnode、hbase-master【swarm 管理机】 docker swarm初始化根据规划在172.16.1.13这台机器上操作： 12345678910[root@mini03 ~]# docker swarm init # 针对机器只有一个IP的情况 Error response from daemon: could not choose an IP address to advertise since this system has multiple addresses on different interfaces (172.16.1.13 on eth0 and 10.0.0.13 on eth1) - specify one with --advertise-addr[root@mini03 ~]# docker swarm init --advertise-addr 172.16.1.13 # 针对机器有多个IP的情况，需要指定一个IP，一般都是指定内网IPSwarm initialized: current node (yo5f7qb28gf6g38ve4xhcis17) is now a manager.To add a worker to this swarm, run the following command: # 在其他机器上执行，这样可以加入该swarm管理 docker swarm join --token SWMTKN-1-4929ovxh6agko49u0yokrzustjf6yzt30iv1zvwqn8d3pndm92-0kuha3sa80u2u27yca6kzdbnb 172.16.1.13:2377To add a manager to this swarm, run &apos;docker swarm join-token manager&apos; and follow the instructions. 得到加入到该swarm的命令 1234[root@mini03 ~]# docker swarm join-token worker To add a worker to this swarm, run the following command: # 在其他机器上执行，这样可以加入该swarm管理 docker swarm join --token SWMTKN-1-4929ovxh6agko49u0yokrzustjf6yzt30iv1zvwqn8d3pndm92-0kuha3sa80u2u27yca6kzdbnb 172.16.1.13:2377 初始化网络初始化一个swarm网络，让系统组件使用这个指定的网络。 1234567891011121314151617181920212223242526272829303132333435363738[root@mini03 ~]# docker network create -d overlay --attachable zhang vu07em5fvpuojih6wgckdkdzj[root@mini03 docker-swarm]# docker network ls # 查看网络NETWORK ID NAME DRIVER SCOPEfa8a244c6bd5 bridge bridge local51c95dea1e5c docker_gwbridge bridge local7a7e31f4bce8 host host local5hgg372xwxbl ingress overlay swarmlmt3pjswf7l0 zhang overlay swarm5ea08e9a282f none null local[root@mini03 ~]# docker network inspect zhang # 查看网络信息 [ &#123; &quot;Name&quot;: &quot;zhang&quot;, &quot;Id&quot;: &quot;xiykborz8hn2td40ykhi20dck&quot;, &quot;Created&quot;: &quot;0001-01-01T00:00:00Z&quot;, &quot;Scope&quot;: &quot;swarm&quot;, &quot;Driver&quot;: &quot;overlay&quot;, &quot;EnableIPv6&quot;: false, &quot;IPAM&quot;: &#123; &quot;Driver&quot;: &quot;default&quot;, &quot;Options&quot;: null, &quot;Config&quot;: [] &#125;, &quot;Internal&quot;: false, &quot;Attachable&quot;: true, &quot;Ingress&quot;: false, &quot;ConfigFrom&quot;: &#123; &quot;Network&quot;: &quot;&quot; &#125;, &quot;ConfigOnly&quot;: false, &quot;Containers&quot;: null, &quot;Options&quot;: &#123; &quot;com.docker.network.driver.overlay.vxlanid_list&quot;: &quot;4097&quot; &#125;, &quot;Labels&quot;: null &#125;] 删除网络【慎用】 删除docker中的zhang网络 123456789[root@mini03 docker-swarm]# docker network rm zhang zhang[root@mini03 docker-swarm]# docker network lsNETWORK ID NAME DRIVER SCOPEfa8a244c6bd5 bridge bridge local51c95dea1e5c docker_gwbridge bridge local7a7e31f4bce8 host host local5hgg372xwxbl ingress overlay swarm5ea08e9a282f none null local 加入或退出swarm管理在mini01、mini02上执行 如下命令。 1docker swarm join --token SWMTKN-1-4929ovxh6agko49u0yokrzustjf6yzt30iv1zvwqn8d3pndm92-0kuha3sa80u2u27yca6kzdbnb 172.16.1.13:2377 当前swarm有哪些节点12345[root@mini03 ~]# docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION2pfwllgxpajx5aitlvcih9vsq mini01 Ready Active 17.09.0-cezho14u85itt5l2i6cpg8fcd6t mini02 Ready Active 17.09.0-ceyo5f7qb28gf6g38ve4xhcis17 * mini03 Ready Active Leader 17.09.0-ce 退出当前的swarm节点123456789101112# 在swarm管理机mini03上的操作# 其中 2pfwllgxpajx5aitlvcih9vsq 是mini01在swarm机器上的ID，根据docker node ls 获取[root@mini03 ~]# docker node rm --force 2pfwllgxpajx5aitlvcih9vsq # 如果mini01上的docker没有停止服务，那么就需要使用 --force 选项2pfwllgxpajx5aitlvcih9vsq[root@mini03 ~]# docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSIONzho14u85itt5l2i6cpg8fcd6t mini02 Ready Active 17.09.0-ceyo5f7qb28gf6g38ve4xhcis17 * mini03 Ready Active Leader 17.09.0-ce########################################### 需要在mini01上执行的命令，这样mini01才能彻底退出swarm管理[root@mini01 ~]# docker swarm leaveNode left the swarm. swarm管理机退出swarm首先需要删除所有节点，然后强制退出swarm即可 1234567[root@mini03 ~]# docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSIONyo5f7qb28gf6g38ve4xhcis17 * mini03 Ready Active Leader 17.09.0-ce[root@mini03 ~]# docker swarm leave --force # swarm管理机退出swarm，需要 --force 参数Node left the swarm. [root@mini03 ~]# docker node lsError response from daemon: This node is not a swarm manager. Use &quot;docker swarm init&quot; or &quot;docker swarm join&quot; to connect this node to swarm and try again. 当前swarm有哪些服务1234567891011[root@mini03 ~]# docker service ls # 只是示例，不是实际数据ID NAME MODE REPLICAS IMAGE PORTSlq7zkkal6ujt hadoop_datanode global 2/2 bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8 ph2fu37k886b hadoop_namenode replicated 1/1 bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8 *:50070-&gt;50070/tcpca47u5i2ubes hbase-master replicated 1/1 bde2020/hbase-master:1.0.0-hbase1.2.6 *:16010-&gt;16010/tcpmkks4oa2ppcn hbase-regionserver-1 replicated 1/1 bde2020/hbase-regionserver:1.0.0-hbase1.2.6 j4mhizg4j67p hbase-regionserver-2 replicated 1/1 bde2020/hbase-regionserver:1.0.0-hbase1.2.6 yndrkc2bcpra hbase_zoo1 replicated 1/1 zookeeper:3.4.10 *:2181-&gt;2181/tcpr5ycrvo0zout spark_spark replicated 1/1 zhang/spark:latest *:4040-&gt;4040/tcp,*:7777-&gt;7777/tcp,*:8081-&gt;8081/tcp,*:18080-&gt;8080/tcpf2v091nz24rg tomcat_tomcat global 2/2 zhang/tomcat:latest *:6543-&gt;6543/tcp,*:9999-&gt;9999/tcp,*:18081-&gt;8081/tcpclfpryaerq2l visualizer replicated 1/1 dockersamples/visualizer:latest *:8080-&gt;8080/tcp swarm标签管理标签添加根据最开始的主机和组件部署规划，标签规划如下：在swarm管理机mini03上执行 123456789101112131415# 给mini01机器的标签docker node update --label-add tomcat=true mini01docker node update --label-add datanode=true mini01docker node update --label-add hbase-regionserver-1=true mini01# 给mini02机器的标签docker node update --label-add tomcat=true mini02docker node update --label-add datanode=true mini02docker node update --label-add hbase-regionserver-2=true mini02# 给mini03机器的标签docker node update --label-add spark=true mini03docker node update --label-add zookeeper=true mini03docker node update --label-add namenode=true mini03docker node update --label-add hbase-master=true mini03 删除标签在swarm管理机mini03上执行，示例如下： 1docker node update --label-rm zookeeper mini03 查看swarm当前的标签在swarm管理机mini03上执行： 1234[root@mini03 ~]# docker node ls -q | xargs docker node inspect -f &apos;&#123;&#123;.ID&#125;&#125;[&#123;&#123;.Description.Hostname&#125;&#125;]:&#123;&#123;.Spec.Labels&#125;&#125;&apos;6f7dwt47y6qvgs3yc6l00nmjd[mini01]:map[tomcat:true datanode:true hbase-regionserver-1:true]5q2nmm2xaexhkn20z8f8ezglr[mini02]:map[tomcat:true datanode:true hbase-regionserver-2:true]ncppwjknhcwbegmliafut0718[mini03]:map[hbase-master:true namenode:true spark:true zookeeper:true] 查看日志启动容器时，查看相关日志，例如如下： 1234docker stack ps hadoopdocker stack ps hadoop --format &quot;&#123;&#123;.Name&#125;&#125;: &#123;&#123;.Error&#125;&#125;&quot;docker stack ps hadoop --format &quot;&#123;&#123;.Name&#125;&#125;: &#123;&#123;.Error&#125;&#125;&quot; --no-truncdocker stack ps hadoop --no-trunc]]></content>
      <categories>
        <category>Docker Swarm</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Docker Swarm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装指定版本的 Docker 服务]]></title>
    <url>%2F2019%2F08%2F06%2Fdocker-deploy01%2F</url>
    <content type="text"><![CDATA[说明之前部署docker服务的时候都是安装最新的docker版本，并使用docker swarm部署大数据组件。 但是在近期的一次部署发现 docker 18.06.1 版本，在使用docker swarm部署大数据组件的时候namenode存储的datanode信息不正确。原因是 18.06.1 版本中的docker swarm 存在一个LB网络，造成了该问题。 这个问题对于Hadoop本身是没有任何问题的，但是当我们启动hbase的时候却有问题了。通过日志发现hbase找不到datanode的节点信息，因为hbase得到的是LB的IP而不是datanode本身的IP，最终导致hbase启动失败。 最后解决的方案就是docker版本回退到 17.09.0 版本，该版本不存在LB网络。Hadoop的namenode中存储的datanode信息是正确的。 1234567891011121314151617181920212223242526272829[root@mini03 docker-swarm]# docker -vDocker version 18.06.1-ce, build e68fc7a[root@mini03 docker-swarm]# docker network lsNETWORK ID NAME DRIVER SCOPEf28f7ab2d811 bridge bridge local51c95dea1e5c docker_gwbridge bridge local7a7e31f4bce8 host host local3cxch31bl38k ingress overlay swarm5ea08e9a282f none null localpwk7oy2h3gnp zhang overlay swarm # 自己创建的网络[root@mini03 docker-swarm]# docker network inspect zhang ……………… &quot;Containers&quot;: &#123; &quot;a9e2e20c89bb6fbc2984a19c4c8e9f9500f3360f2b0434819fc31a143cbc7fc9&quot;: &#123; &quot;Name&quot;: &quot;visualizer_visualizer.1.0lgaqosyogoy0edkqdakeycz4&quot;, &quot;EndpointID&quot;: &quot;2cae08f3a1a63eadff6fee675e249ce19956dcc1d871329c90056a1829abc1d1&quot;, &quot;MacAddress&quot;: &quot;02:42:0a:00:00:04&quot;, &quot;IPv4Address&quot;: &quot;10.0.0.4/24&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125;, &quot;lb-zhang&quot;: &#123; &quot;Name&quot;: &quot;zhang-endpoint&quot;, &quot;EndpointID&quot;: &quot;44ed04b5768dd4ae9edf2e63bded8d5ab5af7cb43d49a4d0d4fbd999abfd5373&quot;, &quot;MacAddress&quot;: &quot;02:42:0a:00:00:02&quot;, &quot;IPv4Address&quot;: &quot;10.0.0.2/24&quot;, &quot;IPv6Address&quot;: &quot;&quot; &#125; &#125;,……………… docker安装指定版本1234567891011121314151617181920212223242526272829303132# 安装必要的一些系统工具[root@mini02 tools]# yum install -y yum-utils device-mapper-persistent-data lvm2 # 添加软件源信息[root@mini02 tools]# yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo # 查看可安装的版本信息[root@mini02 tools]# yum makecache fast [root@mini02 tools]# yum list docker-ce.x86_64 --showduplicates | sort -r * updates: mirrors.aliyun.comLoading mirror speeds from cached hostfileLoaded plugins: fastestmirror * extras: mirrors.aliyun.com * epel: mirrors.aliyun.comdocker-ce.x86_64 18.06.1.ce-3.el7 docker-ce-stabledocker-ce.x86_64 18.06.0.ce-3.el7 docker-ce-stabledocker-ce.x86_64 18.03.1.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 18.03.0.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.12.1.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.12.0.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.09.1.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.09.0.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.06.2.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.06.1.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.06.0.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.03.3.ce-1.el7 docker-ce-stabledocker-ce.x86_64 17.03.2.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.03.1.ce-1.el7.centos docker-ce-stabledocker-ce.x86_64 17.03.0.ce-1.el7.centos docker-ce-stable# 安装指定版本的docker服务[root@mini02 tools]# yum -y install docker-ce-17.09.0.ce-1.el7.centos # 版本信息查看[root@mini02 tools]# docker -vDocker version 17.09.0-ce, build afdb6d4 加入开机自启动12345678[root@mini02 tools]# systemctl status docker● docker.service - Docker Application Container Engine Loaded: loaded (/usr/lib/systemd/system/docker.service; disabled; vendor preset: disabled) Active: inactive (dead) Docs: https://docs.docker.com………………[root@mini02 tools]# systemctl enable docker.service # 加入开机自启动 Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service. 问题解决123456789101112131415161718[root@mini02 tools]# systemctl start dockerJob for docker.service failed because the control process exited with error code. See &quot;systemctl status docker.service&quot; and &quot;journalctl -xe&quot; for details.[root@mini02 tools]# journalctl -xe # 查询具体信息-- Defined-By: systemd-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel-- -- Unit docker.service has begun starting up.Nov 01 17:40:55 mini02 dockerd[2493]: time=&quot;2018-11-01T17:40:55.181209947+08:00&quot; level=info msg=&quot;libcontainerd: new containerd process, pid: 2501&quot;Nov 01 17:40:56 mini02 dockerd[2493]: time=&quot;2018-11-01T17:40:56.187023899+08:00&quot; level=error msg=&quot;[graphdriver] prior storage driver overlay2 failed: driver not supported&quot;Nov 01 17:40:56 mini02 dockerd[2493]: Error starting daemon: error initializing graphdriver: driver not supportedNov 01 17:40:56 mini02 systemd[1]: docker.service: main process exited, code=exited, status=1/FAILURENov 01 17:40:56 mini02 systemd[1]: Failed to start Docker Application Container Engine.-- Subject: Unit docker.service has failed-- Defined-By: systemd-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel………………# 具体信息如下截图，解决方法如下，之后就可以正常起docker服务了[root@mini02 tools]# mv /var/lib/docker /var/lib/docker.old 另请参考参考博客：Docker CE 镜像源站 参考博客：docker启动异常driver not supported]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VMware 实现 iptables NAT及端口映射]]></title>
    <url>%2F2019%2F08%2F06%2Fiptables01%2F</url>
    <content type="text"><![CDATA[前言本文只讲解实战应用，不会涉及原理讲解。如果想要了解iptables的工作流程或原理可参考如下博文。 具体操作是在PC机的VMware虚拟机上进行的，因此涉及的地址都是内网IP。在实际工作中也是一样的操作流程，只需要把涉及外网的地址改为公网IP即可。 文章参考：iptables nat及端口映射 文章参考：企业软件防火墙iptables 为什么有这篇文章？原因是在日常工作中，我们都会在自己的电脑上安装VMware虚拟机，并由此实现一些业务系统【如：LNMP】或模拟线上的网络环境等。 而本文模拟的就是IDC机房或办公网的环境。机房内网服务器不能上外网，只能通过网关服务器上外网。而外网服务器想要访问机房内部的服务器，也只能通过网关服务器转发实现访问。 iptables表和链的工作流程 常用操作123456789101112## 清空所有规则【默认是filter表】iptables -Fiptables -Xiptables -Ziptables -t nat -Fiptables -t nat -Xiptables -t nat -Z## 查看规则iptables -nLiptables -nL -t nat## 删除指定表指定链的指定行数据iptables -t nat -D POSTROUTING 1 涉及虚拟机网络设置内部服务器node01网络设置内网设置【只有一个网卡】 备注： 使用LAN区段，那么本机登录该虚拟机也不行，也ping不通，不在同一个网段不能互访。只能通过网关服务器ssh跳转登录访问。 eth0配置： 1234567891011121314[root@InnerNode01 network-scripts]# cat ifcfg-eth0 DEVICE=eth0TYPE=EthernetONBOOT=yesNM_CONTROLLED=yesBOOTPROTO=noneIPV6INIT=yesUSERCTL=noIPADDR=172.16.10.10NETMASK=255.255.255.0GATEWAY=172.16.10.5# 阿里云DNSDNS1=223.5.5.5DNS2=223.6.6.6 网关服务器网络设置内网设置 备注：网关服务器的内网地址和内部服务器的地址在同一个网段。因此他们之间可以互访。 eth0配置： 12345678910[zhang@gateway01 network-scripts]$ cat ifcfg-eth0 DEVICE=eth0TYPE=EthernetONBOOT=yesNM_CONTROLLED=yesBOOTPROTO=noneIPV6INIT=yesUSERCTL=noIPADDR=172.16.10.5NETMASK=255.255.255.0 外网设置【模拟的公网】 eth1配置： 1234567891011121314[root@gateway01 network-scripts]# cat ifcfg-eth1 DEVICE=eth1TYPE=EthernetONBOOT=yesNM_CONTROLLED=yesBOOTPROTO=noneIPV6INIT=yesUSERCTL=noIPADDR=10.0.0.5NETMASK=255.255.255.0GATEWAY=10.0.0.2# 阿里云DNSDNS1=223.5.5.5DNS2=223.6.6.6 外网服务器设置外网设置【只有一个网卡】 eth0配置： 1234567891011121314[root@internet01 network-scripts]# cat ifcfg-eth0 DEVICE=eth0TYPE=EthernetONBOOT=yesNM_CONTROLLED=yesBOOTPROTO=noneIPV6INIT=yesUSERCTL=noIPADDR=10.0.0.8NETMASK=255.255.255.0GATEWAY=10.0.0.2# 阿里云DNSDNS1=223.5.5.5DNS2=223.6.6.6 简单的NAT路由器网络架构 NAT需求介绍网关2个网络接口 Lan口: 172.16.10.5/24&ensp;&ensp;&ensp;&ensp;eth0 Wan口: 10.0.0.5/24&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;eth1 目的：实现内网中的节点服务器node01 IP：172.16.10.10（网段：172.16.10.0/24）可控的访问internet。 网关服务器操作1、网关机器开启linux的转发功能 123456789[root@gateway01 ~]# tail /etc/sysctl.conf # 添加如下内容…………net.ipv4.ip_forward = 1[root@gateway01 ~]# sysctl -p # 生效[root@gateway01 ~]# sysctl -a | grep &apos;net.ipv4.ip_forward&apos; net.ipv4.ip_forward = 1net.ipv4.ip_forward_use_pmtu = 0[root@gateway01 ~]# cat /proc/sys/net/ipv4/ip_forward1 2、网关机器iptables操作 1iptables -P FORWARD DROP 将FORWARD链的策略设置为DROP，这样做的目的是做到对内网ip的控制，你允许哪一个访问internet就可以增加一个规则，不在规则中的ip将无法访问internet。 1iptables -A FORWARD -m state --state ESTABLISHED,RELATED -j ACCEPT 这条规则规定允许任何地址到任何地址的确认包和关联包通过。一定要加这一条，否则你只允许lan IP访问没有用。 1iptables -t nat -A POSTROUTING -s 172.16.10.0/24 -j SNAT --to 10.0.0.5 这条规则做了一个SNAT，也就是源地址转换，将来自172.16.10.0/24的地址转换为10.0.0.5。 有这几条规则，一个简单的nat路由器就实现了。这时你可以将允许访问的ip或网段添加至FORWARD链，他们就能访问internet了。 12iptables -A FORWARD -s 172.16.10.10 -j ACCEPT # 允许单个地址 或者如下命令iptables -A FORWARD -s 172.16.10.0/24 -j ACCEPT # 允许该网段 比如我想让172.16.10.10这个地址访问internet，那么你就加如上的命令就可以了。 3、保存iptables规则 1iptables-save &gt; /etc/sysconfig/iptables 内部服务器node01测试1234567891011121314[root@InnerNode01 ~]# ping www.baidu.com # 查看是否可以ping通PING www.a.shifen.com (180.97.33.108) 56(84) bytes of data.64 bytes from 180.97.33.108 (180.97.33.108): icmp_seq=1 ttl=127 time=43.4 ms64 bytes from 180.97.33.108 (180.97.33.108): icmp_seq=2 ttl=127 time=42.6 ms64 bytes from 180.97.33.108 (180.97.33.108): icmp_seq=3 ttl=127 time=42.1 ms^C--- www.a.shifen.com ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 2005msrtt min/avg/max/mdev = 42.114/42.735/43.420/0.561 ms[root@InnerNode01 ~]# [root@InnerNode01 ~]# telnet www.baidu.com 80 # telnet 是否可行Trying 112.34.112.40...Connected to www.baidu.com.Escape character is &apos;^]&apos;. 测验完毕！ 端口转发网络架构 端口转发需求介绍内部机器1个网络接口Lan内web server: 172.16.10.10:80 网关2个网络接口 Lan口:172.16.10.5/24&ensp;&ensp;&ensp;&ensp;eth0 Wan口:10.0.0.5/24&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;eth1 目的：对内部server进行端口转发，实现internet 10.0.0.8（网段：10.0.0.0/24）用户【模拟外网机器】访问内网服务器172.16.10.10:80。 网关服务器操作1、网关机器开启linux的转发功能 1234[root@gateway01 ~]# tail /etc/sysctl.conf # 添加如下内容…………net.ipv4.ip_forward = 1[root@gateway01 ~]# sysctl -p # 生效 2、网关机器iptables操作 1iptables -P FORWARD DROP 将FORWARD链的策略设置为DROP，这样做的目的是做到ip的控制，你允许哪一个访问就可以增加一个规则，不在规则中的ip将无法访问。 1iptables -A FORWARD -m state --state ESTABLISHED,RELATED -j ACCEPT 这条规则规定允许任何地址到任何地址的确认包和关联包通过。一定要加这一条，否则你只允许lan IP访问没有用。 1iptables -t nat -A PREROUTING -d 10.0.0.5 -p tcp --dport 80 -j DNAT --to 172.16.10.10:80 如果你要把访问 10.0.0.5:80 的数据包转发到Lan内web server，用上面的命令。 好了，命令完成了，端口转发也做完了，本例能不能转发呢？不能，为什么呢？我下面分析一下。本例中我们的FORWARD策略是DROP。那么也就是说，没有符合规则的包将被丢弃，不管内到外还是外到内。因此，我们需要加入下面的规则。 1iptables -A FORWARD -d 172.16.10.10 -p tcp --dport 80 -j ACCEPT 3、保存iptables规则 1iptables-save &gt; /etc/sysconfig/iptables 操作验证1、在内部服务器监听80端口 12345678910## xshell标签1操作[root@InnerNode01 ~]# nc -l 80 # 保持持续监听## xshell标签2操作[root@InnerNode01 ~]# netstat -lntupActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:111 0.0.0.0:* LISTEN 808/rpcbind tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 1971/nc ### 可见80端口已经监听成功tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1099/sshd tcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 1355/master 2、在外网服务器Telnet 1234567[zhang@internet01 ~]$ telnet 10.0.0.5 80Trying 10.0.0.5...Connected to 10.0.0.5.Escape character is &apos;^]&apos;.^]telnet&gt; quitConnection closed. 由上可知，外网服务器（10.0.0.8）访问内部服务器（172.16.10.10:80）成功。 测验完毕！]]></content>
      <categories>
        <category>iptables</category>
      </categories>
      <tags>
        <tag>iptables</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从头到尾说一次 Java 垃圾回收]]></title>
    <url>%2F2019%2F07%2F23%2Fjvm01%2F</url>
    <content type="text"><![CDATA[文章转载自微信公众号：「阿里巴巴中间件」，作者：率鸽 之前上学的时候有这个一个梗，说在食堂里吃饭，吃完把餐盘端走清理的，是 C++ 程序员，吃完直接就走的，是 Java 程序员。 确实，在 Java 的世界里，似乎我们不用对垃圾回收那么的专注，很多初学者不懂 GC，也依然能写出一个能用甚至还不错的程序或系统。但其实这并不代表 Java 的 GC 就不重要。相反，它是那么的重要和复杂，以至于出了问题，那些初学者除了打开 GC 日志，看着一堆0101的天文，啥也做不了。 今天我们就从头到尾完整地聊一聊 Java 的垃圾回收。 什么是垃圾回收垃圾回收（Garbage Collection，GC），顾名思义就是释放垃圾占用的空间，防止内存泄露。有效的使用可以使用的内存，对内存堆中已经死亡的或者长时间没有使用的对象进行清除和回收。 Java 语言出来之前，大家都在拼命的写 C 或者 C++ 的程序，而此时存在一个很大的矛盾，C++ 等语言创建对象要不断的去开辟空间，不用的时候又需要不断的去释放控件，既要写构造函数，又要写析构函数，很多时候都在重复的 allocated，然后不停的析构。于是，有人就提出，能不能写一段程序实现这块功能，每次创建，释放控件的时候复用这段代码，而无需重复的书写呢？ 1960年，基于 MIT 的 Lisp 首先提出了垃圾回收的概念，而这时 Java 还没有出世呢！所以实际上 GC 并不是Java的专利，GC 的历史远远大于 Java 的历史！ 怎么定义垃圾既然我们要做垃圾回收，首先我们得搞清楚垃圾的定义是什么，哪些内存是需要回收的。 引用计数算法引用计数算法（Reachability Counting）是通过在对象头中分配一个空间来保存该对象被引用的次数（Reference Count）。如果该对象被其它对象引用，则它的引用计数加1，如果删除对该对象的引用，那么它的引用计数就减1，当该对象的引用计数为0时，那么该对象就会被回收。 1String m = new String(&quot;jack&quot;); 先创建一个字符串，这时候”jack”有一个引用，就是 m。 然后将 m 设置为 null，这时候”jack”的引用次数就等于0了，在引用计数算法中，意味着这块内容就需要被回收了。 1m = null; 引用计数算法是将垃圾回收分摊到整个应用程序的运行当中了，而不是在进行垃圾收集时，要挂起整个应用的运行，直到对堆中所有对象的处理都结束。因此，采用引用计数的垃圾收集不属于严格意义上的 “Stop-The-World” 的垃圾收集机制。 看似很美好，但我们知道JVM的垃圾回收就是 “Stop-The-World” 的，那是什么原因导致我们最终放弃了引用计数算法呢？看下面的例子。 123456789101112131415public class ReferenceCountingGC &#123; public Object instance; public ReferenceCountingGC(String name)&#123;&#125;&#125;public static void testGC()&#123; ReferenceCountingGC a = new ReferenceCountingGC(&quot;objA&quot;); ReferenceCountingGC b = new ReferenceCountingGC(&quot;objB&quot;); a.instance = b; b.instance = a; a = null; b = null;&#125; 定义2个对象 相互引用 置空各自的声明引用 我们可以看到，最后这2个对象已经不可能再被访问了，但由于他们相互引用着对方，导致它们的引用计数永远都不会为0，通过引用计数算法，也就永远无法通知GC收集器回收它们。 可达性分析算法可达性分析算法（Reachability Analysis）的基本思路是，通过一些被称为引用链（GC Roots）的对象作为起点，从这些节点开始向下搜索，搜索走过的路径被称为（Reference Chain)，当一个对象到 GC Roots 没有任何引用链相连时（即从 GC Roots 节点到该节点不可达），则证明该对象是不可用的。 通过可达性算法，成功解决了引用计数所无法解决的问题-“循环依赖”。只要你无法与 GC Root 建立直接或间接的连接，系统就会判定你为可回收对象。那这样就引申出了另一个问题，哪些属于 GC Root。 Java 内存区域在 Java 语言中，可作为 GC Root 的对象包括以下4种： 虚拟机栈（栈帧中的本地变量表）中引用的对象 方法区中类静态属性引用的对象 方法区中常量引用的对象 本地方法栈中 JNI（即一般说的 Native 方法）引用的对象 虚拟机栈（栈帧中的本地变量表）中引用的对象此时的 s，即为 GC Root，当 s 置空时，localParameter 对象也断掉了与 GC Root 的引用链，将被回收。 12345678public class StackLocalParameter &#123; public StackLocalParameter(String name)&#123;&#125;&#125;public static void testGC()&#123; StackLocalParameter s = new StackLocalParameter(&quot;localParameter&quot;); s = null;&#125; 方法区中类静态属性引用的对象s 为 GC Root，s 置为 null，经过 GC 后，s 所指向的 properties 对象由于无法与 GC Root 建立关系被回收。 而 m 作为类的静态属性，也属于 GC Root，parameter 对象依然与 GC root 建立着连接，所以此时 parameter 对象并不会被回收。 12345678910public class MethodAreaStaicProperties &#123; public static MethodAreaStaicProperties m; public MethodAreaStaicProperties(String name)&#123;&#125;&#125;public static void testGC()&#123; MethodAreaStaicProperties s = new MethodAreaStaicProperties(&quot;properties&quot;); s.m = new MethodAreaStaicProperties(&quot;parameter&quot;); s = null;&#125; 方法区中常量引用的对象m 即为方法区中的常量引用，也为 GC Root，s 置为 null 后，final 对象也不会因没有与 GC Root 建立联系而被回收。 123456789public class MethodAreaStaicProperties &#123; public static final MethodAreaStaicProperties m = MethodAreaStaicProperties(&quot;final&quot;); public MethodAreaStaicProperties(String name)&#123;&#125;&#125;public static void testGC()&#123; MethodAreaStaicProperties s = new MethodAreaStaicProperties(&quot;staticProperties&quot;); s = null;&#125; 本地方法栈中引用的对象任何 Native 接口都会使用某种本地方法栈，实现的本地方法接口是使用 C 连接模型的话，那么它的本地方法栈就是 C 栈。当线程调用 Java 方法时，虚拟机会创建一个新的栈帧并压入 Java 栈。然而当它调用的是本地方法时，虚拟机会保持 Java 栈不变，不再在线程的 Java 栈中压入新的帧，虚拟机只是简单地动态连接并直接调用指定的本地方法。 怎么回收垃圾在确定了哪些垃圾可以被回收后，垃圾收集器要做的事情就是开始进行垃圾回收，但是这里面涉及到一个问题是：如何高效地进行垃圾回收。 由于Java虚拟机规范并没有对如何实现垃圾收集器做出明确的规定，因此各个厂商的虚拟机可以采用不同的方式来实现垃圾收集器，这里我们讨论几种常见的垃圾收集算法的核心思想。 标记—清除算法 标记清除算法（Mark-Sweep）是最基础的一种垃圾回收算法，它分为2部分，先把内存区域中的这些对象进行标记，哪些属于可回收标记出来，然后把这些垃圾拎出来清理掉。就像上图一样，清理掉的垃圾就变成未使用的内存区域，等待被再次使用。 这逻辑再清晰不过了，并且也很好操作，但它存在一个很大的问题，那就是内存碎片。 上图中等方块的假设是 2M，小一些的是 1M，大一些的是 4M。等我们回收完，内存就会切成了很多段。我们知道开辟内存空间时，需要的是连续的内存区域，这时候我们需要一个 2M的内存区域，其中有2个 1M 是没法用的。这样就导致，其实我们本身还有这么多的内存的，但却用不了。 复制算法 复制算法（Copying）是在标记清除算法上演化而来，解决标记清除算法的内存碎片问题。它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。保证了内存的连续可用，内存分配时也就不用考虑内存碎片等复杂情况，逻辑清晰，运行高效。 上面的图很清楚，也很明显的暴露了另一个问题，合着我这140平的大三房，只能当70平米的小两房来使？代价实在太高。 标记整理算法 标记整理算法（Mark-Compact）标记过程仍然与标记—清除算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，再清理掉端边界以外的内存区域。 标记整理算法一方面在标记—清除算法上做了升级，解决了内存碎片的问题，也规避了复制算法只能利用一半内存区域的弊端。看起来很美好，但从上图可以看到，它对内存变动更频繁，需要整理所有存活对象的引用地址，在效率上比复制算法要差很多。 分代收集算法分代收集算法（Generational Collection）严格来说并不是一种思想或理论，而是融合上述 3 种基础的算法思想，而产生的针对不同情况所采用不同算法的一套组合拳。 对象存活周期的不同将内存划分为几块。一般是把 Java 堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。 在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用标记—清理或者标记整理算法来进行回收。 so，另一个问题来了，那内存区域到底被分为哪几块，每一块又有什么特别适合什么算法呢？ 内存模型与回收策略 Java 堆（Java Heap）是JVM所管理的内存中最大的一块，堆又是垃圾收集器管理的主要区域，这里我们主要分析一下 Java 堆的结构。 Java 堆主要分为2个区域：年轻代与老年代，其中年轻代又分 Eden 区和 Survivor 区，其中 Survivor 区又分 From 和 To 2个区。可能这时候大家会有疑问，为什么需要 Survivor 区，为什么Survivor 还要分2个区。不着急，我们从头到尾，看看对象到底是怎么来的，而它又是怎么没的。 Eden 区IBM 公司的专业研究表明，有将近98%的对象是朝生夕死，所以针对这一现状，大多数情况下，对象会在新生代 Eden 区中进行分配，当 Eden 区没有足够空间进行分配时，虚拟机会发起一次 Minor GC，Minor GC 相比 Major GC 更频繁，回收速度也更快。 通过 Minor GC 之后，Eden 会被清空，Eden 区中绝大部分对象会被回收，而那些无需回收的存活对象，将会进到 Survivor 的 To 区（若 To 区不够，则直接进入 Old 区）。 Survivor 区Survivor 区相当于是 Eden 区和 Old 区的一个缓冲，类似于我们交通灯中的黄灯。 Survivor 又分为2个区，一个是 From 区，一个是 To 区。每次执行 Minor GC，会将 Eden 区和 From 存活的对象放到 Survivor 的 To 区（如果 To 区不够，则直接进入 Old 区）。 为啥需要？不就是新生代到老年代么，直接 Eden 到 Old 不好了吗，为啥要这么复杂。 想想如果没有 Survivor 区，Eden 区每进行一次 Minor GC，存活的对象就会被送到老年代，老年代很快就会被填满。而有很多对象虽然一次 Minor GC 没有消灭，但其实也并不会蹦跶多久，或许第二次，第三次就需要被清除。这时候移入老年区，很明显不是一个明智的决定。 所以，Survivor 的存在意义就是减少被送到老年代的对象，进而减少 Major GC 的发生。Survivor 的预筛选保证，只有经历16次 Minor GC 还能在新生代中存活的对象，才会被送到老年代。 为啥需要俩？设置两个 Survivor 区最大的好处就是解决内存碎片化。 我们先假设一下，Survivor 如果只有一个区域会怎样。Minor GC 执行后，Eden 区被清空了，存活的对象放到了 Survivor 区，而之前 Survivor 区中的对象，可能也有一些是需要被清除的。问题来了，这时候我们怎么清除它们？在这种场景下，我们只能标记清除，而我们知道标记清除最大的问题就是内存碎片，在新生代这种经常会消亡的区域，采用标记清除必然会让内存产生严重的碎片化。 因此 Survivor 有2个区域，每次 Minor GC，会将之前 Eden 区和 From 区中的存活对象复制到 To 区域。第二次 Minor GC 时，From 与 To 职责兑换，这时候会将 Eden 区和 To 区中的存活对象再复制到 From 区域，以此反复。 如下图：「采集周期：每分钟采集一次」 这种机制最大的好处就是，整个过程中，永远有一个 Survivor space 是空的，另一个非空的 Survivor space 是无碎片的。那么，Survivor 为什么不分更多块呢？比方说分成三个、四个、五个?显然，如果 Survivor 区再细分下去，每一块的空间就会比较小，容易导致 Survivor 区满，两块 Survivor 区可能是经过权衡之后的最佳方案。 Old 区老年代占据着2/3的堆内存空间，只有在 Major GC 的时候才会进行清理，每次 GC 都会触发 “Stop-The-World” 。内存越大，STW 的时间也越长，所以内存也不仅仅是越大就越好。由于复制算法在对象存活率较高的老年代会进行很多次的复制操作，效率很低，所以老年代这里采用的是标记—整理算法。 除了上述所说，在内存担保机制下，无法安置的对象会直接进到老年代，以下几种情况也会进入老年代。 大对象大对象指需要大量连续内存空间的对象，这部分对象不管是不是“朝生夕死”，都会直接进到老年代。这样做主要是为了避免在 Eden 区及2个 Survivor 区之间发生大量的内存复制。当你的系统有非常多“朝生夕死”的大对象时，得注意了。 长期存活对象虚拟机给每个对象定义了一个对象年龄（Age）计数器。正常情况下对象会不断的在 Survivor 的 From 区与 To 区之间移动，对象在 Survivor 区中每经历一次 Minor GC，年龄就增加1岁。当年龄增加到15岁时，这时候就会被转移到老年代。当然，这里的15，JVM 也支持进行特殊设置。 动态对象年龄虚拟机并不重视要求对象年龄必须到15岁，才会放入老年区，如果 Survivor 空间中相同年龄所有对象大小的总合大于 Survivor 空间的一半，年龄大于等于该年龄的对象就可以直接进去老年区，无需等你 “成年”。 这其实有点类似于负载均衡，轮询是负载均衡的一种，保证每台机器都分得同样的请求。看似很均衡，但每台机的硬件不通，健康状况不同，我们还可以基于每台机接受的请求数，或每台机的响应时间等，来调整我们的负载均衡算法。 最后，常用的垃圾收集器有：Serial，Parallel Old，CMS，G1 本文部分内容参考自书籍：《深入理解Java虚拟机》。 本文作者：聂晓龙（花名：率鸽），阿里巴巴高级开发工程。目前团队正在疯狂招聘中，感兴趣的同学可直接邮件 xiaolong.nxl#alibaba-inc.com，fulan.zjf#alibaba-inc.com. 阅读原文]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux curl 表单登录或提交与cookie使用]]></title>
    <url>%2F2019%2F07%2F20%2Fcurl03%2F</url>
    <content type="text"><![CDATA[前言本文主要讲解通过curl 实现表单提交登录。单独的表单提交与表单登录都差不多，因此就不单独说了。 说明：针对curl表单提交实现登录，不是所有网站都适用，原因是有些网站后台做了限制或有其他校验。我们不知道这些网站后台的限制或校验机制具体是什么，因此直接curl表单登录可能是不行的。 当然，如下案例是可以用curl登录的。 案例：LeanCloud登录要求和结果要求：通过curl登录后，能正常访问leancloud的应用页面。 登录页面链接如下： 1https://leancloud.cn/dashboard/login.html#/signin 能正常访问如下页面： 1https://leancloud.cn/dashboard/applist.html#/apps 浏览器访问效果： 无登录直接访问结果浏览器访问结果 上图红框 403 中的访问连接如下： 1https://leancloud.cn/1.1/clients/self/apps 通过curl 验证是否登录123456789101112[root@iZ28xbsfvc4Z ~]# curl -i https://leancloud.cn/1.1/clients/self/appsHTTP/1.1 403 ForbiddenServer: openrestyDate: Sun, 14 Jul 2019 11:35:28 GMTContent-Type: application/json;charset=utf-8Transfer-Encoding: chunkedConnection: keep-aliveVary: Accept-EncodingCache-Control: no-cache,no-storePragma: no-cache&#123;&quot;code&quot;:1,&quot;error&quot;:&quot;User doesn&apos;t sign in.&quot;&#125; 获取表单字段信息 获取表单提交链接通过下图可得到表单提交的链接信息。具体如下： 1https://leancloud.cn/1.1/signin curl 表单登录并保存cookie信息123curl -v -c leancloud1.info -X POST -F &apos;email=yourname&apos; -F &apos;password=yourpassword&apos; https://leancloud.cn/1.1/signin# 或则curl -v -c leancloud3.info -X POST -d &apos;email=yourname&amp;password=yourpassword&apos; https://leancloud.cn/1.1/signin 查看cookie信息123456789101112131415161718[root@iZ28xbsfvc4Z 20190714_02]# lltotal 32-rw-r--r-- 1 root root 337 Jul 14 19:45 leancloud1.info-rw-r--r-- 1 root root 335 Jul 14 19:46 leancloud3.info[root@iZ28xbsfvc4Z 20190714_02]# cat leancloud1.info # Netscape HTTP Cookie File# http://curl.haxx.se/docs/http-cookies.html# This file was generated by libcurl! Edit at your own risk.#HttpOnly_leancloud.cn FALSE / TRUE 1563709522 uluru_user Ff1IPOiMX%2F6ipevuxy0OOg%3D%3Dleancloud.cn FALSE / TRUE 1563709522 XSRF-TOKEN 5647dc84bd6eaea37eca2d07ae0e401cca4ba76803989c8559XXXXX7283da[root@iZ28xbsfvc4Z 20190714_02]# cat leancloud3.info # Netscape HTTP Cookie File# http://curl.haxx.se/docs/http-cookies.html# This file was generated by libcurl! Edit at your own risk.#HttpOnly_leancloud.cn FALSE / TRUE 1563709591 uluru_user arTwQm6JylzLjBaQt7TpiQ%3D%3Dleancloud.cn FALSE / TRUE 1563709591 XSRF-TOKEN 751e12827c7c046408541bc1bf962b5912ac35b0d07f88120XXXXXX40704704 每列字段说明：domain：创建并可以读取变量的域名。flag：一个 TRUE/FALSE 值，表明给定域中的所有机器是否都可以访问该变量。此值由浏览器自动设置，具体取决于你为域设置的值。path：变量在域中有效的路径。secure：一个 TRUE/FALSE 值，表明是否需要与域的安全连接来访问变量。expiration：该变量将过期的UNIX时间。UNIX时间定义为自1970年1月1日00:00:00 GMT开始的秒数。name：变量名称value：变量值 校验是否登录成功直接访问和带有cookie访问，这两种访问方式，请对比查看。 直接访问123456789101112[root@iZ28xbsfvc4Z 20190714_02]# curl -i https://leancloud.cn/1.1/clients/self/appsHTTP/1.1 403 ForbiddenServer: openrestyDate: Sun, 14 Jul 2019 11:52:47 GMTContent-Type: application/json;charset=utf-8Transfer-Encoding: chunkedConnection: keep-aliveVary: Accept-EncodingCache-Control: no-cache,no-storePragma: no-cache&#123;&quot;code&quot;:1,&quot;error&quot;:&quot;User doesn&apos;t sign in.&quot;&#125; 带有cookie文件的访问12345678910111213141516# 使用cookie[root@iZ28xbsfvc4Z 20190714_02]# curl -i -b leancloud1.info https://leancloud.cn/1.1/clients/self/apps ## 或者[root@iZ28xbsfvc4Z 20190714_02]# curl -i -b leancloud3.info https://leancloud.cn/1.1/clients/self/appsHTTP/1.1 200 OKServer: openrestyDate: Sun, 14 Jul 2019 11:53:29 GMTContent-Type: application/json;charset=utf-8Transfer-Encoding: chunkedConnection: keep-aliveVary: Accept-EncodingCache-Control: no-cache,no-storePragma: no-cacheStrict-Transport-Security: max-age=31536000[&#123;&quot;app_domain&quot;:null,&quot;description&quot;:null,&quot;archive_status&quot;:0,&quot;biz_type&quot;:&quot;dev&quot;,&quot;master_key&quot;: ……………… 复制浏览器的cookie访问12345678910111213[root@iZ28xbsfvc4Z 20190720]# curl -i -H &apos;cookie: _ga=GA1.2.2055706705.1560005524; …………&apos; https://leancloud.cn/1.1/clients/self/appsHTTP/1.1 200 OKServer: openrestyDate: Sat, 20 Jul 2019 08:11:37 GMTContent-Type: application/json;charset=utf-8Transfer-Encoding: chunkedConnection: keep-aliveVary: Accept-EncodingCache-Control: no-cache,no-storePragma: no-cacheStrict-Transport-Security: max-age=31536000[&#123;&quot;app_domain&quot;:null,&quot;description&quot;:null,&quot;archive_status&quot;:0,&quot;biz_type&quot;:&quot;dev&quot;,&quot;master_key&quot;: ……………… 由上可知curl登录成功。 推荐阅读Linux curl 命令详解 Linux curl 常用示例 Linux curl 表单登录或提交与cookie使用]]></content>
      <categories>
        <category>curl</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>curl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux curl 常用示例]]></title>
    <url>%2F2019%2F07%2F18%2Fcurl02%2F</url>
    <content type="text"><![CDATA[前言本篇文章包含了curl的常用案例使用。 如果想了解curl选项的详细说明，请参考前一篇文章「Linux curl 命令详解」。 常见网页访问示例基本用法访问一个网页 1curl https://www.baidu.com 执行后，相关的网页信息会打印出来 进度条展示有时候我们不需要进度表展示，而需要进度条展示。比如：下载文件时。 可以通过 -#, --progress-bar 选项实现。 12345678[root@iZ28xbsfvc4Z 20190713]# curl https://www.baidu.com | head -n1 # 进度表显示 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 2443 100 2443 0 0 11662 0 --:--:-- --:--:-- --:--:-- 11688&lt;!DOCTYPE html&gt;[root@iZ28xbsfvc4Z 20190713]# curl -# https://www.baidu.com | head -n1 # 进度条显示######################################################################## 100.0%&lt;!DOCTYPE html&gt; 静默模式与错误信息打印当我们做一些操作时，可能会出现进度表。这时我们可以使用 -s, --silent 静默模式去掉这些不必要的信息。 如果使用 -s, --silent 时，还需要打印错误信息，那么还需要使用 -S, --show-error 选项。 静默模式示例1234567[root@iZ28xbsfvc4Z ~]# curl https://www.baidu.com | head -n1 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 2443 100 2443 0 0 11874 0 --:--:-- --:--:-- --:--:-- 11859&lt;!DOCTYPE html&gt;[root@iZ28xbsfvc4Z ~]# curl -s https://www.baidu.com | head -n1&lt;!DOCTYPE html&gt; 静默模式结合错误信息打印1234[root@iZ28xbsfvc4Z 20190713]# curl -s https://140.205.16.113/ [root@iZ28xbsfvc4Z 20190713]# [root@iZ28xbsfvc4Z 20190713]# curl -sS https://140.205.16.113/ curl: (51) Unable to communicate securely with peer: requested domain name does not match the server&apos;s certificate. 显示详细操作信息使用 -v, --verbose 选项实现。 以 &gt; 开头的行表示curl发送的”header data”；&lt; 表示curl接收到的通常情况下隐藏的”header data”；而以 * 开头的行表示curl提供的附加信息。 12345678910111213141516171819202122232425262728293031323334[root@iZ28xbsfvc4Z 20190712]# curl -v https://www.baidu.com* About to connect() to www.baidu.com port 443 (#0)* Trying 180.101.49.12...* Connected to www.baidu.com (180.101.49.12) port 443 (#0)* Initializing NSS with certpath: sql:/etc/pki/nssdb* CAfile: /etc/pki/tls/certs/ca-bundle.crt CApath: none* SSL connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256* Server certificate:* subject: CN=baidu.com,O=&quot;Beijing Baidu Netcom Science Technology Co., Ltd&quot;,OU=service operation department,L=beijing,ST=beijing,C=CN* start date: May 09 01:22:02 2019 GMT* expire date: Jun 25 05:31:02 2020 GMT* common name: baidu.com* issuer: CN=GlobalSign Organization Validation CA - SHA256 - G2,O=GlobalSign nv-sa,C=BE&gt; GET / HTTP/1.1&gt; User-Agent: curl/7.29.0&gt; Host: www.baidu.com&gt; Accept: */*&gt; &lt; HTTP/1.1 200 OK&lt; Accept-Ranges: bytes&lt; Cache-Control: private, no-cache, no-store, proxy-revalidate, no-transform&lt; Connection: Keep-Alive&lt; Content-Length: 2443&lt; Content-Type: text/html&lt; Date: Fri, 12 Jul 2019 08:26:23 GMT&lt; Etag: &quot;588603eb-98b&quot;&lt; Last-Modified: Mon, 23 Jan 2017 13:23:55 GMT&lt; Pragma: no-cache&lt; Server: bfe/1.0.8.18&lt; Set-Cookie: BDORZ=27315; max-age=86400; domain=.baidu.com; path=/&lt; &lt;!DOCTYPE html&gt;……………… # curl 网页的具体信息 指定访问的请求方法当然curl默认使用GET方式访问。使用了 -d, --data &lt;data&gt; 选项，那么会默认为 POST方法访问。如果此时还想实现 GET 访问，那么可以使用 -G, --get 选项强制curl 使用GET方法访问。 同时 -X, --request &lt;command&gt; 选项也可以指定访问方法。 POST请求和数据传输为了抓包查看信息所以使用了 --local-port &lt;num&gt;[-num] 选项，在实际应用中不需要该选项。 12345678910111213141516171819202122232425[root@iZ28xbsfvc4Z ~]# curl -sv --local-port 9000 -X POST -d &apos;user=zhang&amp;pwd=123456&apos; http://www.zhangblog.com/2019/06/24/domainexpire/ | head -n1 ## 或者[root@iZ28xbsfvc4Z ~]# curl -sv --local-port 9000 -d &apos;user=zhang&amp;pwd=123456&apos; http://www.zhangblog.com/2019/06/24/domainexpire/ | head -n1* About to connect() to www.zhangblog.com port 80 (#0)* Trying 120.27.48.179...* Connected to www.zhangblog.com (120.27.48.179) port 80 (#0)&gt; POST /2019/06/24/domainexpire/ HTTP/1.1 # POST 请求方法&gt; User-Agent: curl/7.29.0&gt; Host: www.zhangblog.com&gt; Accept: */*&gt; Content-Length: 21&gt; Content-Type: application/x-www-form-urlencoded&gt; &#125; [data not shown]* upload completely sent off: 21 out of 21 bytes&lt; HTTP/1.1 405 Not Allowed&lt; Server: nginx/1.14.2&lt; Date: Thu, 18 Jul 2019 07:56:23 GMT&lt; Content-Type: text/html&lt; Content-Length: 173&lt; Connection: keep-alive&lt; &#123; [data not shown]* Connection #0 to host www.zhangblog.com left intact&lt;html&gt; 抓包信息 1[root@iZ28xbsfvc4Z tcpdump]# tcpdump -i any port 9000 -A -s 0 指定请求方法1curl -vs -X POST https://www.baidu.com | head -n1 1curl -vs -X PUT https://www.baidu.com | head -n1 保存访问网页使用linux的重定向功能保存1curl www.baidu.com &gt;&gt; baidu.html 使用curl的大O选项通过 -O, --remote-name 选项实现。 1234567[root@iZ28xbsfvc4Z 20190712]# curl -O https://www.baidu.com # 使用了 -O 选项，必须指定到具体的文件 错误使用curl: Remote file name has no length!curl: try &apos;curl --help&apos; or &apos;curl --manual&apos; for more information[root@iZ28xbsfvc4Z 20190712]# curl -O https://www.baidu.com/index.html # 使用了 -O 选项，必须指定到具体的文件 正确使用 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 2443 100 2443 0 0 13289 0 --:--:-- --:--:-- --:--:-- 13349 使用curl的小o选项通过 -o, --output &lt;file&gt; 选项实现。 12345678910111213141516[root@iZ28xbsfvc4Z 20190713]# curl -o sina.txt https://www.sina.com.cn/ # 单个操作[root@iZ28xbsfvc4Z 20190713]# ll-rw-r--r-- 1 root root 154 Jul 13 21:06 sina.txt[root@iZ28xbsfvc4Z 20190703]# curl &quot;http://www.&#123;baidu,douban&#125;.com&quot; -o &quot;site_#1.txt&quot; # 批量操作，注意curl 的地址需要用引号括起来[1/2]: http://www.baidu.com --&gt; site_baidu.txt % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 2381 100 2381 0 0 46045 0 --:--:-- --:--:-- --:--:-- 46686[2/2]: http://www.douban.com --&gt; site_douban.txt100 162 100 162 0 0 3173 0 --:--:-- --:--:-- --:--:-- 3173[root@iZ28xbsfvc4Z 20190703]# [root@iZ28xbsfvc4Z 20190703]# lltotal 220-rw-r--r-- 1 root root 2381 Jul 4 16:53 site_baidu.txt-rw-r--r-- 1 root root 162 Jul 4 16:53 site_douban.txt 允许不安全访问当我们使用curl进行https访问访问时，如果SSL证书是我们自签发的证书，那么这个时候需要使用 -k, --insecure 选项，允许不安全的访问。 1234567891011[root@iZ28xbsfvc4Z ~]# curl https://140.205.16.113/ # 被拒绝curl: (51) Unable to communicate securely with peer: requested domain name does not match the server&apos;s certificate.[root@iZ28xbsfvc4Z ~]# [root@iZ28xbsfvc4Z ~]# curl -k https://140.205.16.113/ # 允许执行不安全的证书连接&lt;!DOCTYPE HTML PUBLIC &quot;-//IETF//DTD HTML 2.0//EN&quot;&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;403 Forbidden&lt;/title&gt;&lt;/head&gt;&lt;body bgcolor=&quot;white&quot;&gt;&lt;h1&gt;403 Forbidden&lt;/h1&gt;&lt;p&gt;You don&apos;t have permission to access the URL on this server.&lt;hr/&gt;Powered by Tengine&lt;/body&gt;&lt;/html&gt; 获取HTTP响应状态码在脚本中，这是很常见的测试网站是否正常的用法。 通过 -w, --write-out &lt;format&gt; 选项实现。 12345[root@iZ28xbsfvc4Z 20190713]# curl -o /dev/null -s -w %&#123;http_code&#125; https://baidu.com302[root@iZ28xbsfvc4Z 20190713]# [root@iZ28xbsfvc4Z 20190713]# [root@iZ28xbsfvc4Z 20190713]# curl -o /dev/null -s -w %&#123;http_code&#125; https://www.baidu.com200[root@iZ28xbsfvc4Z 20190713]# 指定proxy服务器以及其端口很多时候上网需要用到代理服务器(比如是使用代理服务器上网或者因为使用curl别人网站而被别人屏蔽IP地址的时候)，幸运的是curl通过使用 -x, --proxy &lt;[protocol://][user:password@]proxyhost[:port]&gt; 选项来支持设置代理。 1curl -x 192.168.100.100:1080 https://www.baidu.com 模仿浏览器访问有些网站需要使用特定的浏览器去访问他们，有些还需要使用某些特定的浏览器版本。我们可以通过 -A, --user-agent &lt;agent string&gt; 或者 -H, --header &lt;header&gt; 选项实现模拟浏览器访问。 123curl -A &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/75.0.3770.999&quot; http://www.zhangblog.com/2019/06/24/domainexpire/ 或者curl -H &apos;User-Agent: Mozilla/5.0&apos; http://www.zhangblog.com/2019/06/24/domainexpire/ 伪造referer（盗链）有些网站的网页对http访问的链接来源做了访问限制，这些限制几乎都是通过referer来实现的。 比如：要求是先访问首页，然后再访问首页中的邮箱页面，这时访问邮箱的referer地址就是访问首页成功后的页面地址。如果服务器发现对邮箱页面访问的referer地址不是首页的地址，就断定那是个盗连了。 可以通过 -e, --referer 或则 -H, --header &lt;header&gt; 实现伪造 referer 。 123curl -e &apos;https://www.baidu.com&apos; http://www.zhangblog.com/2019/06/24/domainexpire/或者curl -H &apos;Referer: https://www.baidu.com&apos; http://www.zhangblog.com/2019/06/24/domainexpire/ 构造HTTP请求头可以通过 -H, --header &lt;header&gt; 实现构造http请求头。 1curl -H &apos;Connection: keep-alive&apos; -H &apos;Referer: https://sina.com.cn&apos; -H &apos;User-Agent: Mozilla/1.0&apos; http://www.zhangblog.com/2019/06/24/domainexpire/ 保存响应头信息可以通过 -D, --dump-header &lt;file&gt; 选项实现。 12345[root@iZ28xbsfvc4Z 20190703]# curl -D baidu_header.info www.baidu.com ………………[root@iZ28xbsfvc4Z 20190703]# lltotal 4-rw-r--r-- 1 root root 400 Jul 3 10:11 baidu_header.info # 生成的头文件 限时访问--connect-timeout &lt;seconds&gt; 连接服务端的超时时间。这只限制了连接阶段，一旦curl连接了此选项就不再使用了。 123456# 当前 https://www.zhangXX.com 是国外服务器，访问受限[root@iZ28xbsfvc4Z ~]# curl --connect-timeout 10 https://www.zhangXX.com | head % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0:00:10 --:--:-- 0curl: (28) Connection timed out after 10001 milliseconds -m, --max-time &lt;seconds&gt; 允许整个操作花费的最大时间(以秒为单位)。这对于防止由于网络或链接变慢而导致批处理作业挂起数小时非常有用。 12345678910111213[root@iZ28xbsfvc4Z ~]# curl -m 10 --limit-rate 5 http://www.baidu.com/ | head # 超过10秒后，断开连接 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 2 2381 2 50 0 0 4 0 0:09:55 0:00:10 0:09:45 4curl: (28) Operation timed out after 10103 milliseconds with 50 out of 2381 bytes received&lt;!DOCTYPE html&gt;&lt;!--STATUS OK--&gt;&lt;html&gt; &lt;head&gt;&lt;met### 或[root@iZ28xbsfvc4Z ~]# curl -m 10 https://www.zhangXX.com | head # 超过10秒后，断开连接 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0:00:10 --:--:-- 0curl: (28) Connection timed out after 10001 milliseconds 显示抓取错误当我们请求访问失败时或者没有该网页时，网站一般都会给出一个错误的提示页面。 如果我们不需要这个错误页面，只想得到简洁的错误信息。那么可以通过 -f, --fail 选项实现。 12345678910[root@iZ28xbsfvc4Z 20190713]# curl http://www.zhangblog.com/201912312&lt;html&gt;&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;&lt;body bgcolor=&quot;white&quot;&gt;&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;&lt;hr&gt;&lt;center&gt;nginx/1.14.2&lt;/center&gt;&lt;/body&gt;&lt;/html&gt;[root@iZ28xbsfvc4Z 20190713]# curl -f http://www.zhangblog.com/201912312 # 得到更简洁的错误信息curl: (22) The requested URL returned error: 404 Not Found 表单登录与cookie使用参见：「Linux curl 表单登录或提交与cookie使用」 文件上传与下载涉及 FTP 服务，简单快速搭建可参考：《CentOS7下安装FTP服务》「https://www.cnblogs.com/zhi-leaf/p/5983550.html」 文件下载网页文件下载123# 以进度条展示，而不是进度表展示[root@iZ28xbsfvc4Z 20190715]# curl -# -o tmp.data2 http://www.zhangblog.com/uploads/tmp/tmp.data######################################################################## 100.0% FTP文件下载说明1：其中 ftp1 用户是ftp服务端的账号，具体家目录是：/mnt/ftp1 说明2：当我们使用 curl 通过 FTP 进行下载时，后面跟的路径都是：当前使用的 ftp 账号家目录为基础的相对路径，然后找到的目标文件。 示例1 12345678# 其中 tmp.data 的绝对路径是：/mnt/ftp1/tmpdata/tmp.data ；ftp1 账号的家目录是：/mnt/ftp1# 说明：/tmpdata/tmp.data 这个路径是针对 ftp1 账号的家目录而言的[yun@nginx_proxy01 20190715]$ curl -O ftp://ftp1:123456@172.16.1.195:21/tmpdata/tmp.data # 或者[yun@nginx_proxy01 20190715]$ curl -O -u ftp1:123456 ftp://172.16.1.195:21/tmpdata/tmp.data % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 2048M 100 2048M 0 0 39.5M 0 0:00:51 0:00:51 --:--:-- 143M 示例2 12345678# 其中 nginx-1.14.2.tar.gz 的绝对路径是：/tmp/nginx-1.14.2.tar.gz ；ftp1 账号的家目录是：/mnt/ftp1# 说明：/../../tmp/nginx-1.14.2.tar.gz 这个路径是针对 ftp1 账号的家目录而言的[yun@nginx_proxy01 20190715]$ curl -O ftp://ftp1:123456@172.16.1.195:21/../../tmp/nginx-1.14.2.tar.gz # 或者[yun@nginx_proxy01 20190715]$ curl -O -u ftp1:123456 ftp://172.16.1.195:21/../../tmp/nginx-1.14.2.tar.gz % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 991k 100 991k 0 0 5910k 0 --:--:-- --:--:-- --:--:-- 5937k 文件上传FTP文件上传可以通过 -T, --upload-file &lt;file&gt; 选项实现。 说明1：其中 ftp1 用户是ftp服务端的账号，具体家目录是：/mnt/ftp1 123456789# 其中 tmp_client.data 是客户端本地文件； # /tmpdata/ 这个路径是针对 ftp1 账号的家目录而言的，且上传时该目录必须是存在的，否则上传失败。# 因此上传后文件在ftp服务端的绝对路径是：/mnt/ftp1/tmpdata/tmp_client.data[yun@nginx_proxy01 20190715]$ curl -T tmp_client.data ftp://ftp1:123456@172.16.1.195:21/tmpdata/# 或者[yun@nginx_proxy01 20190715]$ curl -T tmp_client.data -u ftp1:123456 ftp://172.16.1.195:21/tmpdata/ % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 2048M 0 0 100 2048M 0 95.4M 0:00:21 0:00:21 --:--:-- 49.3M 断点续传使用 -C, --continue-at &lt;offset&gt; 选项实现。其中使用 “-C -“「注意有空格和无空格的情况」，告诉curl自动找出在哪里/如何恢复传输。 网页端断点续传下载1curl -C - -o tmp.data http://www.zhangblog.com/uploads/tmp/tmp.data # 下载一个 2G 的文件 FTP断点续传下载细节就不多说了，可参见上面的「FTP文件下载」 123curl -C - -o tmp.data1 ftp://ftp1:123456@172.16.1.195:21/tmpdata/tmp.data # 下载一个 2G 的文件# 或则curl -C - -o tmp.data1 -u ftp1:123456 ftp://172.16.1.195:21/tmpdata/tmp.data # 下载一个 2G 的文件 分段下载有时文件比较大，或者难以迅速传输，而利用分段传输，可以实现稳定、高效并且有保障的传输，更具有实用性，同时容易对差错文件进行更正。 可使用 -r, --range &lt;range&gt; 选项实现。 如下示例使用了同一张图片，大小为 18196 字节。 网页端分段下载分段下载 1234567891011121314[root@iZ28xbsfvc4Z 20190715]# curl -I http://www.zhangblog.com/uploads/hexo/00.jpg # 查看文件大小HTTP/1.1 200 OKServer: nginx/1.14.2Date: Mon, 15 Jul 2019 03:23:44 GMTContent-Type: image/jpegContent-Length: 18196 # 文件大小Last-Modified: Fri, 05 Jul 2019 08:04:58 GMTConnection: keep-aliveETag: &quot;5d1f04aa-4714&quot;Accept-Ranges: bytes### 分段下载一个文件[root@iZ28xbsfvc4Z 20190715]# curl -r 0-499 -o 00-jpg.part1 http://www.zhangblog.com/uploads/hexo/00.jpg[root@iZ28xbsfvc4Z 20190715]# curl -r 500-999 -o 00-jpg.part2 http://www.zhangblog.com/uploads/hexo/00.jpg[root@iZ28xbsfvc4Z 20190715]# curl -r 1000- -o 00-jpg.part3 http://www.zhangblog.com/uploads/hexo/00.jpg 查看下载文件 12345[root@iZ28xbsfvc4Z 20190715]# lltotal 36-rw-r--r-- 1 root root 500 Jul 15 11:25 00-jpg.part1-rw-r--r-- 1 root root 500 Jul 15 11:25 00-jpg.part2-rw-r--r-- 1 root root 17196 Jul 15 11:26 00-jpg.part3 文件合并 1234[root@iZ28xbsfvc4Z 20190715]# cat 00-jpg.part1 00-jpg.part2 00-jpg.part3 &gt; 00.jpg[root@iZ28xbsfvc4Z 20190715]# ll 00.jpgtotal 56-rw-r--r-- 1 root root 18196 Jul 15 11:29 00.jpg FTP分段下载分段下载 123[yun@nginx_proxy01 20190715]$ curl -r 0-499 -o 00-jpg.part1 ftp://ftp1:123456@172.16.1.195:21/tmpdata/00.jpg[yun@nginx_proxy01 20190715]$ curl -r 500-999 -o 00-jpg.part2 ftp://ftp1:123456@172.16.1.195:21/tmpdata/00.jpg[yun@nginx_proxy01 20190715]$ curl -r 1000- -o 00-jpg.part3 ftp://ftp1:123456@172.16.1.195:21/tmpdata/00.jpg 查看下载文件 1234[yun@nginx_proxy01 20190715]$ ll 00-jpg.part*-rw-rw-r-- 1 yun yun 500 Jul 15 17:59 00-jpg.part1-rw-rw-r-- 1 yun yun 500 Jul 15 18:00 00-jpg.part2-rw-rw-r-- 1 yun yun 17196 Jul 15 18:00 00-jpg.part3 文件合并 123[yun@nginx_proxy01 20190715]$ cat 00-jpg.part1 00-jpg.part2 00-jpg.part3 &gt; 00.jpg[yun@nginx_proxy01 20190715]$ ll 00.jpg -rw-rw-r-- 1 yun yun 18196 Jul 15 18:02 00.jpg 推荐阅读Linux curl 命令详解 Linux curl 常用示例 Linux curl 表单登录或提交与cookie使用]]></content>
      <categories>
        <category>curl</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>curl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux curl 命令详解]]></title>
    <url>%2F2019%2F07%2F16%2Fcurl01%2F</url>
    <content type="text"><![CDATA[命令概要该命令设计用于在没有用户交互的情况下工作。 curl 是一个工具，用于传输来自服务器或者到服务器的数据。「向服务器传输数据或者获取来自服务器的数据」 可支持的协议有（DICT、FILE、FTP、FTPS、GOPHER、HTTP、HTTPS、IMAP、IMAPS、LDAP、LDAPS、POP3、POP3S、RTMP、RTSP、SCP、SFTP、SMTP、SMTPS、TELNET和TFTP）。 curl提供了大量有用的技巧，比如代理支持、用户身份验证、FTP上传、HTTP post、SSL连接、cookie、文件断点续传、Metalink等等。正如你将在下面看到的，这些特性的数量会让您头晕目眩！ 访问的URL你可以在命令行上指定任意数量的url。它们将按指定的顺序依次获取。 你可以指定多个url，或url的部分通过在花括号内编写部分集，如： 123http://site.&#123;one,two,three&#125;.com# 参见curl http://www.zhangblog.com/2019/06/16/hexo&#123;04,05,06&#125;/ -I # 查看信息 或者可以使用[]得到字母数字序列的序列，如： 12345ftp://ftp.numericals.com/file[1-100].txtftp://ftp.numericals.com/file[001-100].txt # 前导用零ftp://ftp.letters.com/file[a-z].txt # 参见curl http://www.zhangblog.com/2019/06/16/hexo[04-06]/ -I # 查看信息 不支持嵌套序列，但可以使用几个相邻的序列： 1http://any.org/archive[1996-1999]/vol[1-4]/part&#123;a,b,c&#125;.html 你可以指定一个步长计数器的范围，以获得每第n个数字或字母： 12http://www.numericals.com/file[1-100:10].txt http://www.letters.com/file[a-z:2].txt 如果指定URL而没有protocol:// prefix，默认为HTTP。 常用选项一curl通常在操作过程中显示一个进度表，显示传输的数据量、传输速度和估计的剩余时间等。 -#, --progress-bar将curl进度显示为一个简单的进度条；而不是标准的、具有更多信息的进度表。 1234567[root@iZ28xbsfvc4Z 20190702]# curl -O http://www.zhangblog.com/2019/06/16/hexo04/index.html # 默认的进度表 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 97299 100 97299 0 0 186k 0 --:--:-- --:--:-- --:--:-- 186k[root@iZ28xbsfvc4Z 20190702]# [root@iZ28xbsfvc4Z 20190702]# curl -# -O http://www.zhangblog.com/2019/06/16/hexo04/index.html #简单的进度条######################################################################## 100.0% -0, --http1.0(HTTP)强制curl使用HTTP 1.0发出请求，而不是使用其内部首选的HTTP 1.1。 -1, --tlsv1(SSL)强制curl使用TLS 1.x 版本，当与远程TLS服务进行协商时。可以使用选项 --tlsv1.0、--tlsv1.1和 --tlsv1.2来更精确地控制TLS版本(如果使用的SSL后端支持这种级别的控制)。 -2, --sslv2(SSL)强制curl使用TLS 2 版本，当与远程TLS服务进行协商时。 -3, --sslv3(SSL)强制curl使用TLS 3 版本，当与远程TLS服务进行协商时。 -4, --ipv4如果curl能够将一个地址解析为多个IP版本(比如它支持ipv4和ipv6)，那么这个选项告诉curl只将名称解析为IPv4地址。 -6, --ipv6如果curl能够将一个地址解析为多个IP版本(比如它支持ipv4和ipv6)，那么这个选项告诉curl只将名称解析为IPv6地址。 -a, --append(FTP/SFTP)当在上传中使用时，这将告诉curl追加到目标文件而不是覆盖它。如果文件不存在，将创建它。注意，一些SSH服务器(包括OpenSSH)会忽略此标志。 -A, --user-agent &lt;agent string&gt;(HTTP)指定要发送到HTTP服务端的User-Agent字符串。当然也可以使用 -H, --header 选项来设置。用于模拟客户端，如：谷歌浏览器、火狐浏览器、IE 浏览器等等。 如果多次使用此选项，则将使用最后一个选项。 模仿浏览器访问 1curl -A &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/75.0.3770.999&quot; http://www.zhangblog.com/2019/06/24/domainexpire/ --basic(HTTP)告诉curl使用HTTP基本身份验证。这是默认的。 常用选项二-b, --cookie &lt;name=data&gt;(HTTP)将数据作为cookie传递给HTTP服务器。它应该是之前从服务端接收到的“Set-Cookie:”行中的数据。数据格式为“NAME1=VALUE1;NAME2 = VALUE2”。 如果行中没有使用 ‘=’ 符号，则将其视为一个文件名，用于读取先前存储的cookie行，如果它们匹配，则应在此会话中使用。要读取cookie文件的文件格式应该是纯HTTP头文件或Netscape/Mozilla cookie文件格式。 注意：使用 -b, --cookie 指定的文件仅用作输入。文件中不会存储cookies。要存储cookies，可以使用 -c, --cookie-jar 选项，或者您甚至可以使用 -D, --dump-header 将HTTP头保存到文件中。 -c, --cookie-jar &lt;file name&gt;(HTTP)指定希望curl在完成操作后将所有cookie写入哪个文件。Curl写之前从指定文件读取的所有cookie，以及从远程服务端接收的所有cookie。如果没有已知的cookie，则不会写入任何文件。该文件将使用Netscape cookie文件格式编写。如果你将文件名设置为单个破折号 “-” ，cookie将被标准输出。 该命令行选项将激活cookie引擎，使curl记录并使用cookies。激活它的另一种方法是使用 -b, --cookie 选项。 如果不能创建或写入cookie jar，那么整个curl操作就不会失败，甚至不能清楚地报告错误。使用 -v 会得到一个警告，但这是你得到的关于这种可能致命的情况的唯一可见反馈。 如果多次使用此选项，将使用最后指定的文件名。 --connect-timeout &lt;seconds&gt;连接服务端的超时时间。这只限制了连接阶段，一旦curl连接了此选项就不再使用了。 也可参见：-m, --max-time 选项。 123456# 当前 https://www.zhangXX.com 是国外服务器，访问受限[root@iZ28xbsfvc4Z ~]# curl --connect-timeout 10 https://www.zhangXX.com | head % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0:00:10 --:--:-- 0curl: (28) Connection timed out after 10001 milliseconds --create-dirs当与 -o 选项一起使用时，curl将根据需要创建必要的本地目录层次结构。 这个选项只创建与 -o 选项相关的dirs，没有其他内容。如果 -o 文件名没有使用dir，或者其中提到的dir已经存在，则不会创建dir。 示例 1curl -o ./hexo04/index.html --create-dirs http://www.zhangblog.com/2019/06/16/hexo04 -C, --continue-at &lt;offset&gt;按给定偏移量继续/恢复以前的文件传输。给定的偏移量是将被跳过的确切字节数，从源文件的开头开始计算，然后再将其传输到目标文件。 使用 “-C -“「注意有空格和无空格的情况」，告诉curl自动找出在哪里/如何恢复传输。然后，它使用给定的输出/输入文件来解决这个问题。 12# 下载一个 2G 的文件，可以反复测试，查看结果curl -C - -o tmp.data http://www.zhangblog.com/uploads/tmp/tmp.data -d, --data &lt;data&gt;使用该选项，那么默认请求方式为 POST。(HTTP)在POST请求中向HTTP服务器发送指定的数据，与浏览器在用户填写HTML表单并按下submit按钮时所做的相同。这将导致curl使用content-type application/x-www-form-urlencoded将数据传递给服务器。也可参见：-F，-form 。 如果这些命令在同一个命令行使用多次，这些数据片段将使用指定的分隔符 &amp; 合并。因此，使用 ‘-d name=daniel -d skill=lousy’ 将生成一个类似 ‘name=daniel&amp;skill=lousy’ 的post块，也可以直接这样合并使用。 -d, --data 与 --data-ascii 相同。post数据为纯粹的二进制数据时，那么使用 --data-binary 选项。要对表单字段的值进行url编码，可以使用 --data-urlencode。 如果您以字母@开始数据，那么其余的应该是一个文件名，以便从其中读取数据。或者 - 如果您希望curl从stdin【标准输入】读取数据。文件的内容必须已经是url编码的。还可以指定多个文件。因此，Posting数据名为 “foobar” 的文件将使用 --data @foobar 完成。 示例 请求信息: 123456789101112131415161718192021222324[root@iZ28xbsfvc4Z 20190712]# curl -sv --local-port 9000 -d &apos;user=zhang&amp;pwd=123456&apos; http://www.zhangblog.com/2019/06/24/domainexpire/ | head -n1 * About to connect() to www.zhangblog.com port 80 (#0)* Trying 120.27.48.179...* Local port: 9000* Connected to www.zhangblog.com (120.27.48.179) port 80 (#0)&gt; POST /2019/06/24/domainexpire/ HTTP/1.1 # 可见请求方式为POST&gt; User-Agent: curl/7.29.0&gt; Host: www.zhangblog.com&gt; Accept: */*&gt; Content-Length: 21&gt; Content-Type: application/x-www-form-urlencoded&gt; &#125; [data not shown]* upload completely sent off: 21 out of 21 bytes&lt; HTTP/1.1 405 Not Allowed&lt; Server: nginx/1.14.2&lt; Date: Fri, 12 Jul 2019 13:34:20 GMT&lt; Content-Type: text/html&lt; Content-Length: 173&lt; Connection: keep-alive&lt; &#123; [data not shown]* Connection #0 to host www.zhangblog.com left intact&lt;html&gt; 抓包信息 1[root@iZ28xbsfvc4Z tcpdump]# tcpdump -i any port 9000 -A -s 0 --data-ascii &lt;data&gt;参见 -d, --data --data-binary &lt;data&gt;(HTTP) POST数据完全按照指定的方式，没有任何额外的处理。 如果您以字母@开始数据，其余的应该是文件名。数据是以类似于 --data-ascii 的方式发布的，只不过保留了换行，而且永远不会进行转换【数据不转换】。 如果多次使用此选项，第一个选项后面的选项将按照 -d, --data 中的描述追加数据。 --data-urlencode &lt;data&gt;(HTTP)这个Post 数据，与另一个 --data 选项类似，除执行url编码以外。 -D, --dump-header &lt;file&gt;将响应协议头写入指定的文件。 如果多次使用此选项，则将使用最后一个选项。 当你想要存储HTTP站点发送给你的头文件时，使用此选项非常方便。 12345[root@iZ28xbsfvc4Z 20190703]# curl -D baidu_header.info www.baidu.com ………………[root@iZ28xbsfvc4Z 20190703]# lltotal 4-rw-r--r-- 1 root root 400 Jul 3 10:11 baidu_header.info # 生成的头文件 之后第二次curl调用通过 -b, --cookie 选项，可以从头部读取 cookies 。然而 -c, --cookie-jar 选项是存储 cookies 更好的方法。 常用选项三--digest(HTTP)启用HTTP摘要身份验证。这是一种身份验证方案，可以防止密码以明文通过网络发送。将此选项与普通的 -u, --user 选项组合使用，以设置用户名和密码。相关选项请参见 --ntlm, --negotiate 和 --anyauth。 如果多次使用此选项，则只使用第一个选项。 -e, --referer &lt;URL&gt;(HTTP)将 “Referer Page” 【从哪个页面跳转过来的】信息发送到HTTP服务器。当然也可以使用 -H, --header 标志来设置。 如果多次使用此选项，则将使用最后一个选项。 1curl -e &apos;https:www.baidu.com&apos; http://www.zhangblog.com/2019/06/24/domainexpire/ -f, --fail(HTTP)在服务器错误上静默失败(完全没有输出)。这主要是为了使脚本等更好地处理失败的尝试。 在通常情况下，当HTTP服务器无法交付文档时，它会返回一个HTML文档，说明原因(通常还会描述原因)。此标志将阻止curl输出该值并返回错误22。 12345678910[root@iZ28xbsfvc4Z 20190713]# curl http://www.zhangblog.com/201912312&lt;html&gt;&lt;head&gt;&lt;title&gt;404 Not Found&lt;/title&gt;&lt;/head&gt;&lt;body bgcolor=&quot;white&quot;&gt;&lt;center&gt;&lt;h1&gt;404 Not Found&lt;/h1&gt;&lt;/center&gt;&lt;hr&gt;&lt;center&gt;nginx/1.14.2&lt;/center&gt;&lt;/body&gt;&lt;/html&gt;[root@iZ28xbsfvc4Z 20190713]# curl -f http://www.zhangblog.com/201912312curl: (22) The requested URL returned error: 404 Not Found -F, --form &lt;name=content&gt;(HTTP)这允许curl模拟用户按下submit按钮后填充的表单。 该情况让curl 可使用Content-Type multipart/form-data POST数据。也可以上传二进制文件等。 @文件名：使一个文件作为文件上传附加在post中。&lt;文件名：从文件中获取该文本字段的内容。 例如，要将密码文件发送到服务器，其中“password”是表单字段的名称，/etc/passwd将作为输入: 1curl -F password=@/etc/passwd www.mypasswords.com 您还可以使用 ‘type=’ 告诉curl使用什么 Content-Type ，方法类似于： 123curl -F &quot;web=@index.html;type=text/html&quot; url.com或curl -F &quot;name=daniel;type=text/foo&quot; url.com 可以通过设置 filename= 更改本地上传的文件名，如下： 1curl -F &quot;file=@localfile;filename=nameinpost&quot; url.com 上传的文件名从改为了 nameinpost 如果文件名/路径包括 ‘,’ 或 ‘;’ ，必须用双引号括起来： 123curl -F &quot;file=@\&quot;localfile\&quot;;filename=\&quot;nameinpost\&quot;&quot; url.com或curl -F &apos;file=@&quot;localfile&quot;;filename=&quot;nameinpost&quot;&apos; url.com 最外层可用单引号或双引号。 这个选项可以多次使用。 请勿如下使用 1curl -F &apos;user=zhang&amp;password=pwd&apos; url.com # 这种用法是错误的 --form-string &lt;name=string&gt;(HTTP)类似于 --form，只是命名参数的value字符串按字面意思使用。在值中以 ‘@’ 和 ‘&lt;’ 开头的字符，以及 ‘;type=’ 字符串没有特殊的含义。 如果字符串值有可能意外触发 --form 的 “@” 或 “&lt;” 特性，请优先使用此选项。 -g, --globoff这个选项关闭了“URL全局解析器”。当您设置这个选项时，您可以指定包含字母 {}[] 的url，而不需要curl本身来解释它们。 注意，这些字母不是正常的合法URL内容，但是它们应该按照URI标准进行编码。 -G, --get使用此选项时，将使所有使用 -d, --data 或 --data-binary 指定的数据在HTTP GET请求中使用，而不是在POST请求中使用。数据将被追加到URL的一个 ‘?’ 的分隔符后。 如果与 -I 结合使用，POST数据将被替换追加到带有HEAD请求的URL中。 如果多次使用此选项，则只使用第一个选项。 示例 12345678910111213141516171819202122232425[root@iZ28xbsfvc4Z 20190712]# curl -sv -G --local-port 9000 -d &apos;user=zhang&amp;pwd=123456&apos; http://www.zhangblog.com/2019/06/24/domainexpire/ | head -n1 或则[root@iZ28xbsfvc4Z 20190713]# curl -sv --local-port 9000 &quot;http://www.zhangblog.com/2019/06/24/domainexpire/?user=zhang&amp;pwd=123456&quot; | head -n1* About to connect() to www.zhangblog.com port 80 (#0)* Trying 120.27.48.179...* Local port: 9000* Connected to www.zhangblog.com (120.27.48.179) port 80 (#0)&gt; GET /2019/06/24/domainexpire/?user=zhang&amp;pwd=123456 HTTP/1.1 # 可见请求方式为 GET，且参数追加到了URI后&gt; User-Agent: curl/7.29.0&gt; Host: www.zhangblog.com&gt; Accept: */*&gt; &lt; HTTP/1.1 200 OK&lt; Server: nginx/1.14.2&lt; Date: Fri, 12 Jul 2019 14:04:19 GMT&lt; Content-Type: text/html&lt; Content-Length: 51385&lt; Last-Modified: Tue, 09 Jul 2019 13:55:19 GMT&lt; Connection: keep-alive&lt; ETag: &quot;5d249cc7-c8b9&quot;&lt; Accept-Ranges: bytes&lt; &#123; [data not shown]* Connection #0 to host www.zhangblog.com left intact&lt;!DOCTYPE html&gt; 抓包信息 1[root@iZ28xbsfvc4Z tcpdump]# tcpdump -i any port 9000 -A -s 0 -H, --header &lt;header&gt;(HTTP) 要发送到服务端的自定义请求头。 此选项可多次用于添加/替换/删除多个headers。 1curl -H &apos;Connection: keep-alive&apos; -H &apos;Referer: https://sina.com.cn&apos; -H &apos;User-Agent: Mozilla/1.0&apos; http://www.zhangblog.com/2019/06/24/domainexpire/ --ignore-content-length(HTTP)忽略Content-Length 头信息。 -i, --include(HTTP)在输出的内容中包含HTTP 头信息。 1curl -i https://www.baidu.com -I, --head(HTTP/FTP/FILE)只获取HTTP头文件。在FTP或FILE 文件上使用时，curl只显示文件大小和最后修改时间。 1curl -I https://www.baidu.com -k, --insecure(SSL)允许curl执行不安全的SSL连接和传输。所有SSL连接都尝试使用默认安装的CA证书包来确保安全。 示例 1234567891011[root@iZ28xbsfvc4Z ~]# curl https://140.205.16.113/ # 被拒绝curl: (51) Unable to communicate securely with peer: requested domain name does not match the server&apos;s certificate.[root@iZ28xbsfvc4Z ~]# [root@iZ28xbsfvc4Z ~]# curl -k https://140.205.16.113/ # 允许执行不安全的证书连接&lt;!DOCTYPE HTML PUBLIC &quot;-//IETF//DTD HTML 2.0//EN&quot;&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;403 Forbidden&lt;/title&gt;&lt;/head&gt;&lt;body bgcolor=&quot;white&quot;&gt;&lt;h1&gt;403 Forbidden&lt;/h1&gt;&lt;p&gt;You don&apos;t have permission to access the URL on this server.&lt;hr/&gt;Powered by Tengine&lt;/body&gt;&lt;/html&gt; 常用选项四--keepalive-time &lt;seconds&gt;keepalive 时长。如果使用no-keepalive，则此选项无效。 如果多次使用此选项，则将使用最后一个选项。如果未指定，该选项默认为60秒。 --key &lt;key&gt;(SSL/SSH)私钥文件名。允许你在这个单独的文件中提供你的私钥。 对于SSH，如果没有指定，curl尝试如下顺序：’~/.ssh/id_rsa’，’~/.ssh/id_dsa’，’./id_rsa’，’./id_dsa’。 如果多次使用此选项，则将使用最后一个选项。 --key-type &lt;type&gt;(SSL)私钥文件类型。指定 --key 提供的私钥的类型。支持DER、PEM和ENG。如果没有指定，则定为PEM。 如果多次使用此选项，则将使用最后一个选项。 -L, --location(HTTP/HTTPS) 跟踪重定向如果服务器报告请求页面已移动到另一个位置(用location: header和3XX响应代码表示)，此选项将使curl在新位置上重做请求。 如果与 -i, --include 或 -I, --head 一起使用，将显示所有请求页面的标题。 1234567891011121314151617181920[root@iZ28xbsfvc4Z ~]# curl -I -L https://baidu.com/ HTTP/1.1 302 Moved Temporarily # 302 重定向Server: bfe/1.0.8.18Date: Thu, 04 Jul 2019 03:07:15 GMTContent-Type: text/htmlContent-Length: 161Connection: keep-aliveLocation: http://www.baidu.com/HTTP/1.1 200 OKAccept-Ranges: bytesCache-Control: private, no-cache, no-store, proxy-revalidate, no-transformConnection: Keep-AliveContent-Length: 277Content-Type: text/htmlDate: Thu, 04 Jul 2019 03:07:15 GMTEtag: &quot;575e1f60-115&quot;Last-Modified: Mon, 13 Jun 2016 02:50:08 GMTPragma: no-cacheServer: bfe/1.0.8.18 --limit-rate &lt;speed&gt;指定要使用curl的最大传输速率。 如果有一个有限的管道，并且希望传输不要使用您的全部带宽，那么这个特性是非常有用的。 12curl --limit-rate 500 http://www.baidu.com/curl --limit-rate 2k http://www.baidu.com/ 单位：默认字节，除非添加后缀。附加 “k” 或 “K” 表示千字节， “m” 或 “M” 表示兆字节，而 “g” 或 “G” 表示千兆字节。例如:200K, 3m和1G。 给定的速率是整个传输过程中计算的平均速度。这意味着curl可能在短时间内使用更高的传输速度，但是随着时间的推移，它只使用给定的速率。 如果多次使用此选项，则将使用最后一个选项。 --local-port &lt;num&gt;[-num]指定本地的一个端口或端口范围去连接。 请注意，端口号本质上是一种稀缺资源，有时会很忙，因此将此范围设置为太窄可能会导致不必要的连接失败。 12curl --local-port 9000 http://www.baidu.com/curl --local-port 9000-9999 http://www.baidu.com/ -m, --max-time &lt;seconds&gt;允许整个操作花费的最大时间(以秒为单位)。 这对于防止由于网络或链接变慢而导致批处理作业挂起数小时非常有用。 也可参见：--connect-timeout 选项 12345678910111213[root@iZ28xbsfvc4Z ~]# curl -m 10 --limit-rate 5 http://www.baidu.com/ | head # 超过10秒后，断开连接 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 2 2381 2 50 0 0 4 0 0:09:55 0:00:10 0:09:45 4curl: (28) Operation timed out after 10103 milliseconds with 50 out of 2381 bytes received&lt;!DOCTYPE html&gt;&lt;!--STATUS OK--&gt;&lt;html&gt; &lt;head&gt;&lt;met### 或[root@iZ28xbsfvc4Z ~]# curl -m 10 https://www.zhangXX.com | head # 超过10秒后，断开连接 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- 0:00:10 --:--:-- 0curl: (28) Connection timed out after 10001 milliseconds --max-filesize &lt;bytes&gt;指定要下载的文件的最大大小(以字节为单位)。如果请求的文件大于这个值，那么传输将不会启动，curl将返回退出代码63。 示例 123456789101112131415161718192021222324[root@iZ28xbsfvc4Z ~]# curl -I http://www.zhangblog.com/uploads/hexo/00.jpg # 正常HTTP/1.1 200 OKServer: nginx/1.14.2Date: Thu, 04 Jul 2019 07:24:24 GMTContent-Type: image/jpegContent-Length: 18196Last-Modified: Mon, 24 Jun 2019 01:43:02 GMTConnection: keep-aliveETag: &quot;5d102aa6-4714&quot;Accept-Ranges: bytes[root@iZ28xbsfvc4Z ~]# echo $?0[root@iZ28xbsfvc4Z ~]# [root@iZ28xbsfvc4Z ~]# [root@iZ28xbsfvc4Z ~]# curl --max-filesize 1000 -I http://www.zhangblog.com/uploads/hexo/00.jpg # 受限异常HTTP/1.1 200 OKServer: nginx/1.14.2Date: Thu, 04 Jul 2019 07:24:54 GMTContent-Type: image/jpegcurl: (63) Maximum file size exceeded[root@iZ28xbsfvc4Z ~]# [root@iZ28xbsfvc4Z ~]# echo $?63 --max-redirs &lt;num&gt;设置允许的最大重定向跟踪数。 如果也使用了 -L, --location，则此选项可用于防止curl在悖论中无限重定向。默认情况下，限制为50重定向。将此选项设置为-1，使其无限。 --no-keepalive禁用在TCP连接上使用keepalive消息，因为默认情况下curl启用了它们。 注意，这是文档中已否定的选项名。因此，您可以使用 --keepalive 来强制keepalive。 常用选项五-o, --output &lt;file&gt;输出到一个文件，而不是标准输出。 如果使用 {} 或 [] 来获取多个documents。可以使用 ‘#’ 后跟说明符中的一个数字。该变量将替换为正在获取URL的当前字符串。就像： 12curl http://&#123;one,two&#125;.site.com -o &quot;file_#1.txt&quot;curl http://&#123;site,host&#125;.host[1-5].com -o &quot;#1_#2&quot; 示例1 12345678910111213141516171819[root@iZ28xbsfvc4Z 20190703]# curl &quot;http://www.zhangblog.com/2019/06/16/hexo&#123;04,05,06&#125;/&quot; -o &quot;file_#1.info&quot; # 注意curl 的地址需要用引号括起来或[root@iZ28xbsfvc4Z 20190703]# curl &quot;http://www.zhangblog.com/2019/06/16/hexo[04-06]/&quot; -o &quot;file_#1.info&quot; # 注意curl 的地址需要用引号括起来[1/3]: http://www.zhangblog.com/2019/06/16/hexo04/ --&gt; file_04.info % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 97299 100 97299 0 0 1551k 0 --:--:-- --:--:-- --:--:-- 1557k[2/3]: http://www.zhangblog.com/2019/06/16/hexo05/ --&gt; file_05.info100 54409 100 54409 0 0 172M 0 --:--:-- --:--:-- --:--:-- 172M[3/3]: http://www.zhangblog.com/2019/06/16/hexo06/ --&gt; file_06.info100 56608 100 56608 0 0 230M 0 --:--:-- --:--:-- --:--:-- 230M[root@iZ28xbsfvc4Z 20190703]# [root@iZ28xbsfvc4Z 20190703]# lltotal 212-rw-r--r-- 1 root root 97299 Jul 4 16:51 file_04.info-rw-r--r-- 1 root root 54409 Jul 4 16:51 file_05.info-rw-r--r-- 1 root root 56608 Jul 4 16:51 file_06.info 示例2 12345678910111213[root@iZ28xbsfvc4Z 20190703]# curl &quot;http://www.&#123;baidu,douban&#125;.com&quot; -o &quot;site_#1.txt&quot; # 注意curl 的地址需要用引号括起来[1/2]: http://www.baidu.com --&gt; site_baidu.txt % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 2381 100 2381 0 0 46045 0 --:--:-- --:--:-- --:--:-- 46686[2/2]: http://www.douban.com --&gt; site_douban.txt100 162 100 162 0 0 3173 0 --:--:-- --:--:-- --:--:-- 3173[root@iZ28xbsfvc4Z 20190703]# [root@iZ28xbsfvc4Z 20190703]# lltotal 220-rw-r--r-- 1 root root 2381 Jul 4 16:53 site_baidu.txt-rw-r--r-- 1 root root 162 Jul 4 16:53 site_douban.txt -O, --remote-name写入到本地文件，名称与远程文件的名称相同。(只使用远程文件的文件部分，路径被切断。) 用于保存的远程文件名是从给定的URL中提取的，没有其他内容。因此，文件将保存在当前工作目录中。如果希望将文件保存在另一个目录中，请确保在curl调用 -O, --remote-name之前更改当前工作目录! 1234567[root@iZ28xbsfvc4Z 20190712]# curl -O https://www.baidu.com # 使用了 -O 选项，必须指定到具体的文件 错误使用curl: Remote file name has no length!curl: try &apos;curl --help&apos; or &apos;curl --manual&apos; for more information[root@iZ28xbsfvc4Z 20190712]# curl -O https://www.baidu.com/index.html # 使用了 -O 选项，必须指定到具体的文件 正确使用 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 2443 100 2443 0 0 13289 0 --:--:-- --:--:-- --:--:-- 13349 --pass &lt;phrase&gt;(SSL/SSH)私钥密码 如果多次使用此选项，则将使用最后一个选项。 --post301告诉curl当301重定向时，不要将POST请求转换为GET请求。 非rfc行为在web浏览器中无处不在，因此curl在缺省情况下进行转换以保持一致性。但是，服务器可能需要在重定向之后将POST保留为POST。 这个选项只有在使用 -L, --location 时才有意义 --post302告诉curl当302重定向时，不要将POST请求转换为GET请求。 非rfc行为在web浏览器中无处不在，因此curl在缺省情况下进行转换以保持一致性。但是，服务器可能需要在重定向之后将POST保留为POST。 这个选项只有在使用 -L, --location 时才有意义 --post303告诉curl当303重定向时，不要将POST请求转换为GET请求。 非rfc行为在web浏览器中无处不在，因此curl在缺省情况下进行转换以保持一致性。但是，服务器可能需要在重定向之后将POST保留为POST。 这个选项只有在使用 -L, --location 时才有意义 说明：上述三个选项都是为了防止在重定向过程中，原来的 POST 请求，变为 GET请求。为了防止该情况，有两种处理方式。1、使用上述选项可避免；2、使用 -X POST 选项和命令。 示例 1[root@iZ28xbsfvc4Z ~]# curl -Lsv -d &apos;user=zhang&apos; https://baidu.com | head -n1 开始是POST请求，302 重定向后变为了 GET请求。 1[root@iZ28xbsfvc4Z ~]# curl -Lsv -d &apos;user=zhang&apos; --post301 --post302 --post303 https://baidu.com | head -n1 前后都是 POST 请求。但是选项较多。 1[root@iZ28xbsfvc4Z ~]# curl -Lsv -d &apos;user=zhang&apos; -X POST https://baidu.com | head -n1 前后都是 POST 请求。推荐使用此命令。 --pubkey &lt;key&gt;(SSH)公钥文件名。允许在这个单独的文件中提供公钥。 如果多次使用此选项，则将使用最后一个选项。 -r, --range &lt;range&gt;(HTTP/FTP/SFTP/FILE)从HTTP/1.1、FTP或SFTP服务器或本地文件检索字节范围。范围可以通过多种方式指定。用于分段下载。 有时文件比较大，或者难以迅速传输，而利用分段传输，可以实现稳定、高效并且有保障的传输，更具有实用性，同时容易对差错文件进行更正。 0-499：指定前500个字节500-999：指定第二个500字节-500：指定最后500个字节9500-：指定9500字节及之后的字节0-0,-1：指定第一个和最后一个字节500-700,600-799：从偏移量500开始指定300字节100-199,500-599：指定两个单独100字节的范围 分段下载 12345678910111213[root@iZ28xbsfvc4Z 20190715]# curl -I http://www.zhangblog.com/uploads/hexo/00.jpg # 查看文件大小HTTP/1.1 200 OKServer: nginx/1.14.2Date: Mon, 15 Jul 2019 03:23:44 GMTContent-Type: image/jpegContent-Length: 18196 # 文件大小Last-Modified: Fri, 05 Jul 2019 08:04:58 GMTConnection: keep-aliveETag: &quot;5d1f04aa-4714&quot;Accept-Ranges: bytes[root@iZ28xbsfvc4Z 20190715]# curl -r 0-499 -o 00-jpg.part1 http://www.zhangblog.com/uploads/hexo/00.jpg[root@iZ28xbsfvc4Z 20190715]# curl -r 500-999 -o 00-jpg.part2 http://www.zhangblog.com/uploads/hexo/00.jpg[root@iZ28xbsfvc4Z 20190715]# curl -r 1000- -o 00-jpg.part3 http://www.zhangblog.com/uploads/hexo/00.jpg 查看下载文件 12345[root@iZ28xbsfvc4Z 20190715]# lltotal 36-rw-r--r-- 1 root root 500 Jul 15 11:25 00-jpg.part1-rw-r--r-- 1 root root 500 Jul 15 11:25 00-jpg.part2-rw-r--r-- 1 root root 17196 Jul 15 11:26 00-jpg.part3 文件合并 1234[root@iZ28xbsfvc4Z 20190715]# cat 00-jpg.part1 00-jpg.part2 00-jpg.part3 &gt; 00.jpg[root@iZ28xbsfvc4Z 20190715]# lltotal 56-rw-r--r-- 1 root root 18196 Jul 15 11:29 00.jpg -R, --remote-time使curl尝试获取远程文件的时间戳，如果可用，则使本地文件获得相同的时间戳【针对修改时间戳Modify】。 1curl -o nfs1.info -R http://www.zhangblog.com/2019/07/05/nfs1/ --retry &lt;num&gt;传输出现问题时，重试的次数。数字设置为0将使curl不重试(这是缺省值)。 出现的瞬时错误如：timeout、FTP 4xx响应状代码或HTTP 5xx响应状代码。 当curl准备重试传输时，它将首先等待一秒钟，之后对于所有即将到来的重试，它将把等待时间延长一倍，直到达到10分钟，这将是其余重试之间的延迟。 --retry-delay &lt;seconds&gt;传输出现问题时，设置重试间隔时间。将此延迟设置为零将使curl使用默认的延迟时间。 --retry-max-time &lt;seconds&gt;传输出现问题时，设置最大重试时间。将此选项设置为0则不超时重试。 常用选项六-s, --silent静默或静音模式。不显示进度表/条或错误消息。 示例 1234567[root@iZ28xbsfvc4Z 20190713]# curl https://www.baidu.com | head -n1 # 默认有进度表 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 2443 100 2443 0 0 13346 0 --:--:-- --:--:-- --:--:-- 13349&lt;!DOCTYPE html&gt;[root@iZ28xbsfvc4Z 20190713]# curl -s https://www.baidu.com | head -n1&lt;!DOCTYPE html&gt; -S, --show-error当与 -s 一起使用时，如果curl失败，curl将显示一条错误消息。 1234[root@iZ28xbsfvc4Z 20190713]# curl -s https://140.205.16.113/ [root@iZ28xbsfvc4Z 20190713]# [root@iZ28xbsfvc4Z 20190713]# curl -sS https://140.205.16.113/ curl: (51) Unable to communicate securely with peer: requested domain name does not match the server&apos;s certificate. --stderr &lt;file&gt;将错误信息重定向到一个文件。如果文件名是普通的 ‘-‘，则将其写入stdout。 如果多次使用此选项，则将使用最后一个选项。 123456[root@iZ28xbsfvc4Z 20190713]# curl --stderr err.info https://140.205.16.113/ [root@iZ28xbsfvc4Z 20190713]# lltotal 92-rw-r--r-- 1 root root 116 Jul 13 10:19 err.info[root@iZ28xbsfvc4Z 20190713]# cat err.info curl: (51) Unable to communicate securely with peer: requested domain name does not match the server&apos;s certificate. -T, --upload-file &lt;file&gt;这将指定的本地文件传输到远程URL。如果指定的URL中没有文件部分，Curl将附加本地文件名。 注意：必须在最后一个目录上使用尾随 / 来真正证明Curl没有文件名，否则Curl会认为您的最后一个目录名是要使用的远程文件名。这很可能导致上传操作失败。如果在HTTP(S)服务器上使用此命令，则将使用PUT命令。 同时也支持多个文件上传，如下： 123curl -T &quot;&#123;file1,file2&#125;&quot; http://www.uploadtothissite.com或则curl -T &quot;img[1-1000].png&quot; ftp://ftp.picturemania.com/upload/ --trace &lt;file&gt;对指定文件进行debug。包括所有传入和传出数据。 此选项会覆盖之前使用的 -v、--verbose或 --trace-ascii。 如果多次使用此选项，则将使用最后一个选项。 1curl --trace trace.info https://www.baidu.com --trace-ascii &lt;file&gt;对指定文件进行debug。包括所有传入和传出数据。 这非常类似于 --trace，但是省略了十六进制部分，只显示转储的ASCII部分。使它输出更小，对于我们来说可能更容易阅读。 此选项会覆盖之前使用的 -v、--verbose或 --trace。 如果多次使用此选项，则将使用最后一个选项。 1curl --trace-ascii trace2.info https://www.baidu.com --trace-time为curl显示的每个跟踪或冗长的行添加时间戳。 1curl --trace-ascii trace3.info --trace-time https://www.baidu.com -v, --verbose显示详细操作信息。主要用于调试。 以 &gt; 开头的行表示curl发送的”header data”；&lt; 表示curl接收到的通常情况下隐藏的”header data”；而以 * 开头的行表示curl提供的附加信息。 12345678910111213141516171819202122232425262728293031323334[root@iZ28xbsfvc4Z 20190712]# curl -v https://www.baidu.com* About to connect() to www.baidu.com port 443 (#0)* Trying 180.101.49.12...* Connected to www.baidu.com (180.101.49.12) port 443 (#0)* Initializing NSS with certpath: sql:/etc/pki/nssdb* CAfile: /etc/pki/tls/certs/ca-bundle.crt CApath: none* SSL connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256* Server certificate:* subject: CN=baidu.com,O=&quot;Beijing Baidu Netcom Science Technology Co., Ltd&quot;,OU=service operation department,L=beijing,ST=beijing,C=CN* start date: May 09 01:22:02 2019 GMT* expire date: Jun 25 05:31:02 2020 GMT* common name: baidu.com* issuer: CN=GlobalSign Organization Validation CA - SHA256 - G2,O=GlobalSign nv-sa,C=BE&gt; GET / HTTP/1.1&gt; User-Agent: curl/7.29.0&gt; Host: www.baidu.com&gt; Accept: */*&gt; &lt; HTTP/1.1 200 OK&lt; Accept-Ranges: bytes&lt; Cache-Control: private, no-cache, no-store, proxy-revalidate, no-transform&lt; Connection: Keep-Alive&lt; Content-Length: 2443&lt; Content-Type: text/html&lt; Date: Fri, 12 Jul 2019 08:26:23 GMT&lt; Etag: &quot;588603eb-98b&quot;&lt; Last-Modified: Mon, 23 Jan 2017 13:23:55 GMT&lt; Pragma: no-cache&lt; Server: bfe/1.0.8.18&lt; Set-Cookie: BDORZ=27315; max-age=86400; domain=.baidu.com; path=/&lt; &lt;!DOCTYPE html&gt;……………… # curl 网页的具体信息 -w, --write-out &lt;format&gt;在完成和成功操作后要在stdout上显示什么。 支持如下变量，具体含义请自行参见curl文档。 123456789101112131415161718192021222324252627content_typefilename_effectiveftp_entry_pathhttp_codehttp_connectlocal_iplocal_portnum_connectsnum_redirectsredirect_urlremote_ipremote_portsize_downloadsize_headersize_requestsize_uploadspeed_downloadspeed_uploadssl_verify_resulttime_appconnecttime_connecttime_namelookuptime_pretransfertime_redirecttime_starttransfertime_totalurl_effective 示例 1234567[root@iZ28xbsfvc4Z 20190713]# curl -o /dev/null -s -w %&#123;content_type&#125; www.baidu.com # 输出结果没有换行text/html[root@iZ28xbsfvc4Z 20190713]# [root@iZ28xbsfvc4Z 20190713]# curl -o /dev/null -s -w %&#123;http_code&#125; www.baidu.com # 输出结果没有换行200[root@iZ28xbsfvc4Z 20190713]# [root@iZ28xbsfvc4Z 20190713]# curl -o /dev/null -s -w %&#123;local_port&#125; www.baidu.com # 输出结果没有换行37346[root@iZ28xbsfvc4Z 20190713]# [root@iZ28xbsfvc4Z 20190713]# -x, --proxy &lt;[protocol://][user:password@]proxyhost[:port]&gt;使用指定的HTTP代理。如果没有指定端口号，则假定它位于端口1080。 -X, --request &lt;command&gt;(HTTP)指定与HTTP服务器通信时的请求方式。默认GET 1curl -vs -X POST https://www.baidu.com | head -n1 1curl -vs -X PUT https://www.baidu.com | head -n1 推荐阅读Linux curl 命令详解 Linux curl 常用示例 Linux curl 表单登录或提交与cookie使用]]></content>
      <categories>
        <category>curl</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>curl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NFS 服务搭建与配置]]></title>
    <url>%2F2019%2F07%2F05%2Fnfs1%2F</url>
    <content type="text"><![CDATA[本文讲解在 CentOS 5.x、CentOS 6.x 和 CentOS 7.x 环境下，如何安装与部署 NFS 服务。 注意：一台机器不要同时做 NFS 的服务端和 NFS 的客户端。如果同时作了 NFS 的服务端和客户端，那么在关机的时候，会一直夯住，可能十分钟之后甚至更久才能关闭成功。 NFS 工作原理简介 启动 NFS SERVER 之前，首先要启动 RPC 服务（CentOS 5.x 下为 portmap 服务，CentOS 6.x 和 CentOS 7.x 下为 rpcbind 服务，下同），否则 NFS SERVER 就无法向 RPC 服务注册了。 另外，如果 RPC 服务重新启动，原来已经注册好的NFS端口数据就会丢失，因此，此时 RPC 服务管理的NFS程序也需要重新启动以重新向RPC注册。 要特别注意的是：一般修改NFS配置文件后，是不需要重启NFS的，直接在命令行执行 /etc/init.d/nfs reload 「针对CentOS 5.x 或 CentOS 6.x」 或 systemctl reload nfs.service 「针对CentOS 7.x」 或 exportfs -rv 即可使修改的 /etc/exports 生效。 NFS 服务所需的安装包安装 NFS 和 RPC 「服务端、客户端都安装」12345[root@backup ~]# rpm -qa nfs-utils rpcbind[root@backup ~]# yum install nfs-utils rpcbind -y #nfs需要的安装包[root@backup ~]# rpm -qa nfs-utils rpcbindnfs-utils-1.2.3-64.el6.x86_64rpcbind-0.2.0-11.el6_7.x86_64 查看用户信息1234567891011[root@nfs01 ~]# tail /etc/passwdhaldaemon:x:68:68:HAL daemon:/:/sbin/nologinntp:x:38:38::/etc/ntp:/sbin/nologinsaslauth:x:499:76:Saslauthd user:/var/empty/saslauth:/sbin/nologinpostfix:x:89:89::/var/spool/postfix:/sbin/nologinsshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologintcpdump:x:72:72::/:/sbin/nologinoldboy:x:500:500::/home/oldboy:/bin/bashrpc:x:32:32:Rpcbind Daemon:/var/cache/rpcbind:/sbin/nologin #yum安装rpc服务时创建的rpcuser:x:29:29:RPC Service User:/var/lib/nfs:/sbin/nologin #yum安装rpc服务时创建的nfsnobody:x:65534:65534:Anonymous NFS User:/var/lib/nfs:/sbin/nologin #yum安装nfs服务时创建的 NFS 版本查看服务端版本查看1nfsstat -s 客户端版本查看1nfsstat -c NFS服务端搭建配置exports1234567891011[root@nfs01 ~]# mkdir /data[root@nfs01 ~]# ll -d /data/drwxr-xr-x. 3 root root 4096 Apr 11 09:49 /data/[root@nfs01 ~]# chown -R nfsnobody.nfsnobody /data/ [root@nfs01 ~]# ll -d /data/ drwxr-xr-x. 3 nfsnobody nfsnobody 4096 Apr 11 09:49 /data/[root@nfs01 ~]# cat /etc/exports # share /data for web created by zhangliang at 2016-05-21/data 172.16.1.0/24(rw,sync)#172.16.1.0/24(rw,sync) 没有空格#/data 172.16.1.0/24(rw,sync,root_squash,all_squash,anonuid=XXXX,anongid=XXXX) # 推荐配置 其他配置示例： 1234# 指定 IP 配置/opt 192.168.0.1(ro) 192.168.0.2(rw)# 指定 网段/data 172.16.1.0/24(rw,sync) 启动rpcbind服务CentOS 5.x 和 CentOS 6.x 启动方式1234567891011121314151617181920212223242526[root@nfs01 ~]# /etc/init.d/rpcbind start [root@nfs01 ~]# netstat -anp | grep &apos;rpc&apos; tcp 0 0 0.0.0.0:111 0.0.0.0:* LISTEN 1346/rpcbind tcp 0 0 0.0.0.0:38420 0.0.0.0:* LISTEN 1368/rpc.statd tcp 0 0 :::13894 :::* LISTEN 1368/rpc.statd tcp 0 0 :::111 :::* LISTEN 1346/rpcbind udp 0 0 0.0.0.0:673 0.0.0.0:* 1346/rpcbind udp 0 0 127.0.0.1:703 0.0.0.0:* 1368/rpc.statd udp 0 0 0.0.0.0:15306 0.0.0.0:* 1368/rpc.statd udp 0 0 0.0.0.0:111 0.0.0.0:* 1346/rpcbind udp 0 0 :::673 :::* 1346/rpcbind udp 0 0 :::50537 :::* 1368/rpc.statd udp 0 0 :::111 :::* 1346/rpcbind unix 2 [ ACC ] STREAM LISTENING 10120 1346/rpcbind /var/run/rpcbind.sockunix 2 [ ] DGRAM 10207 1368/rpc.statd [root@nfs01 ~]# rpcinfo -p localhost program vers proto port service 100000 4 tcp 111 portmapper 100000 3 tcp 111 portmapper 100000 2 tcp 111 portmapper 100000 4 udp 111 portmapper 100000 3 udp 111 portmapper 100000 2 udp 111 portmapper 100024 1 udp 15306 status 100024 1 tcp 38420 status#### 由上可知，暂时只有自己的端口服务，没有其他的 CentOS 7.x 启动方式1234567891011121314151617[root@nginx_cdn ~]# systemctl start rpcbind.service [root@nginx_cdn ~]# netstat -anp | grep &apos;rpc&apos; tcp 0 0 0.0.0.0:111 0.0.0.0:* LISTEN 1930/rpcbind tcp6 0 0 :::111 :::* LISTEN 1930/rpcbind udp 0 0 0.0.0.0:832 0.0.0.0:* 1930/rpcbind udp 0 0 0.0.0.0:111 0.0.0.0:* 1930/rpcbind udp6 0 0 :::832 :::* 1930/rpcbind udp6 0 0 :::111 :::* 1930/rpcbind unix 2 [ ACC ] STREAM LISTENING 17999 1/systemd /var/run/rpcbind.sock[root@nginx_cdn ~]# rpcinfo -p localhost program vers proto port service 100000 4 tcp 111 portmapper 100000 3 tcp 111 portmapper 100000 2 tcp 111 portmapper 100000 4 udp 111 portmapper 100000 3 udp 111 portmapper 100000 2 udp 111 portmapper 启动NFSCentOS 5.x 和 CentOS 6.x 启动方式123456789101112131415161718192021222324252627282930313233343536373839404142[root@nfs01 ~]# /etc/init.d/nfs start Starting NFS services: [ OK ]Starting NFS quotas: [ OK ]Starting NFS mountd: [ OK ]Starting NFS daemon: [ OK ]正在启动 RPC idmapd： [确定][root@nfs01 ~]# rpcinfo -p localhost program vers proto port service 100000 4 tcp 111 portmapper 100000 3 tcp 111 portmapper 100000 2 tcp 111 portmapper 100000 4 udp 111 portmapper 100000 3 udp 111 portmapper 100000 2 udp 111 portmapper 100024 1 udp 15306 status 100024 1 tcp 38420 status 100011 1 udp 875 rquotad 100011 2 udp 875 rquotad 100011 1 tcp 875 rquotad 100011 2 tcp 875 rquotad 100005 1 udp 11473 mountd 100005 1 tcp 62369 mountd 100005 2 udp 17528 mountd 100005 2 tcp 47308 mountd 100005 3 udp 11312 mountd 100005 3 tcp 51724 mountd 100003 2 tcp 2049 nfs 100003 3 tcp 2049 nfs 100003 4 tcp 2049 nfs 100227 2 tcp 2049 nfs_acl 100227 3 tcp 2049 nfs_acl 100003 2 udp 2049 nfs 100003 3 udp 2049 nfs 100003 4 udp 2049 nfs 100227 2 udp 2049 nfs_acl 100227 3 udp 2049 nfs_acl 100021 1 udp 25181 nlockmgr 100021 3 udp 25181 nlockmgr 100021 4 udp 25181 nlockmgr 100021 1 tcp 20093 nlockmgr 100021 3 tcp 20093 nlockmgr 100021 4 tcp 20093 nlockmgr CentOS 7.x 启动方式1234567891011121314151617181920212223242526272829[root@nginx_cdn ~]# systemctl start nfs.service [root@nginx_cdn ~]# rpcinfo -p localhost program vers proto port service 100000 4 tcp 111 portmapper 100000 3 tcp 111 portmapper 100000 2 tcp 111 portmapper 100000 4 udp 111 portmapper 100000 3 udp 111 portmapper 100000 2 udp 111 portmapper 100024 1 udp 44741 status 100024 1 tcp 23203 status 100005 1 udp 20048 mountd 100005 1 tcp 20048 mountd 100005 2 udp 20048 mountd 100005 2 tcp 20048 mountd 100005 3 udp 20048 mountd 100005 3 tcp 20048 mountd 100003 3 tcp 2049 nfs 100003 4 tcp 2049 nfs 100227 3 tcp 2049 nfs_acl 100003 3 udp 2049 nfs 100003 4 udp 2049 nfs 100227 3 udp 2049 nfs_acl 100021 1 udp 48638 nlockmgr 100021 3 udp 48638 nlockmgr 100021 4 udp 48638 nlockmgr 100021 1 tcp 16146 nlockmgr 100021 3 tcp 16146 nlockmgr 100021 4 tcp 16146 nlockmgr 加入开始自启动CentOS 5.x 和 CentOS 6.x 环境使用 /etc/rc.local123456789[root@nfs01 ~]# tail /etc/rc.local # You can put your own initialization stuff in here if you don&apos;t# want to do the full Sys V style init stuff.touch /var/lock/subsys/local# start rpc and nfs server/etc/init.d/rpcbind start/etc/init.d/nfs start 使用chkconfig123456789[root@nfs01 ~]# chkconfig rpcbind on [root@nfs01 ~]# chkconfig nfs on [root@nfs01 ~]# ls /etc/rc.d/rc3.d/* | grep -E &apos;rpc|nfs&apos; /etc/rc.d/rc3.d/K61nfs-rdma/etc/rc.d/rc3.d/K69rpcsvcgssd/etc/rc.d/rc3.d/S13rpcbind/etc/rc.d/rc3.d/S14nfslock/etc/rc.d/rc3.d/S19rpcgssd/etc/rc.d/rc3.d/S30nfs 查看rpc服务和NFS服务的开机启动顺序 12345678910111213141516171819202122[root@nfs01 ~]# head /etc/init.d/rpcbind #! /bin/sh## rpcbind Start/Stop RPCbind## chkconfig: 2345 13 87 #运行级别 开机顺序 关机顺序【其中 2345 指的是 运行级别】# description: The rpcbind utility is a server that converts RPC program \# numbers into universal addresses. It must be running on the \# host to be able to make RPC calls on a server on that machine.## processname: rpcbind[root@nfs01 ~]# head /etc/init.d/nfs #!/bin/sh## nfs This shell script takes care of starting and stopping# the NFS services.## chkconfig: - 30 60# description: NFS is a popular protocol for file sharing across networks.# This service provides NFS server functionality, which is \# configured via the /etc/exports file.# probe: true CentOS 7.x 环境12[root@nginx_cdn ~]# systemctl enable rpcbind.service [root@nginx_cdn ~]# systemctl enable nfs.service 查看具体状态情况 12345678[root@nginx_cdn ~]# systemctl status rpcbind.service ● rpcbind.service - RPC bind service Loaded: loaded (/usr/lib/systemd/system/rpcbind.service; enabled; vendor preset: enabled)………………[root@nginx_cdn ~]# systemctl status nfs.service ● nfs-server.service - NFS server and services Loaded: loaded (/usr/lib/systemd/system/nfs-server.service; enabled; vendor preset: disabled)……………… 查看有哪些参数生效12[root@nfs01 ~]# cat /var/lib/nfs/etab /data 172.16.1.0/24(rw,sync,wdelay,hide,nocrossmnt,secure,root_squash,no_all_squash,no_subtree_check,secure_locks,acl,anonuid=65534,anongid=65534,sec=sys,rw,root_squash,no_all_squash) 参数说明：ro：只读设置，这样 NFS 客户端只能读、不能写（默认设置）；rw：读写设置，NFS 客户端可读写；sync：将数据同步写入磁盘中，效率低，但可以保证数据的一致性（默认设置）；async：将数据先保存在内存缓冲区中，必要时才写入磁盘；如果服务器重新启动，这种行为可能会导致数据损坏，但效率高；root_squash：当客户端用 root 用户访问该共享文件夹时，将 root 用户映射成匿名用户（默认设置）；no_root_squash：客户端的 root 用户不映射。这样客户端的 root 用户与服务端的 root 用户具有相同的访问权限，这可能会带来严重的安全影响。没有充分的理由，不应该指定此选项；all_squash：客户端所有普通用户及所属组都映射为匿名用户及匿名用户组；「推荐设置」no_all_squash：客户端所有普通用户及所属组不映射（默认设置）；subtree_check：如果共享，如：/usr/bin之类的子目录时，强制NFS检查父目录的权限；no_subtree_check：即使共享 NFS 服务端的子目录时，nfs服务端也不检查其父目录的权限，这样可以提高效率（默认设置）；secure：限制客户端只能从小于1024的tcp/ip端口连接nfs服务器（默认设置）；insecure：允许客户端从大于1024的tcp/ip端口连接服务器；wdelay：检查是否有相关的写操作，如果有则将这些写操作一起执行，这样可以提高效率（默认设置）；no_wdelay：若有写操作则立即执行，当使用async时，无需此设置；anonuid=xxx：将远程访问的所有用户主都映射为匿名用户主账户，并指定该匿名用户主为本地用户主（UID=xxx）；anongid=xxx：将远程访问的所有用户组都映射为匿名用户组账户，并指定该匿名用户组为本地用户组（GID=xxx）； 检查是否成功123[root@nfs01 ~]# showmount -e 172.16.1.31 # 其中 172.16.1.31 为 NFS 服务端IP Export list for 172.16.1.31:/data 172.16.1.0/24 NFS客户端配置启动rpcbind服务CentOS 5.x 和 CentOS 6.x 环境12[root@web01 ~]# /etc/init.d/rpcbind start Starting rpcbind: [ OK ] CentOS 7.x 环境1[root@nginx_proxy01 ~]# systemctl start rpcbind.service 检查共享信息123[root@web01 ~]# showmount -e 172.16.1.31 Export list for 172.16.1.31:/data 172.16.1.0/24 NFS挂载在2台机器都挂载 NFS，好用于后面的测试。 1[root@web01 ~]# mount -t nfs 172.16.1.31:/data /mnt 查看挂载信息查询方式1123456[root@web01 ~]# df -h #有时可能会被卡主Filesystem Size Used Avail Use% Mounted on/dev/sda3 8.8G 1.5G 6.9G 18% /tmpfs 495M 0 495M 0% /dev/shm/dev/sda1 190M 40M 141M 23% /boot172.16.1.31:/data 8.8G 1.5G 6.9G 18% /mnt 查询方式212345678910111213[root@web01 ~]# cat /proc/mounts # 优先使用，监控时使用该命令rootfs / rootfs rw 0 0proc /proc proc rw,relatime 0 0sysfs /sys sysfs rw,relatime 0 0devtmpfs /dev devtmpfs rw,relatime,size=490920k,nr_inodes=122730,mode=755 0 0devpts /dev/pts devpts rw,relatime,gid=5,mode=620,ptmxmode=000 0 0tmpfs /dev/shm tmpfs rw,relatime 0 0/dev/sda3 / ext4 rw,relatime,barrier=1,data=ordered 0 0/proc/bus/usb /proc/bus/usb usbfs rw,relatime 0 0/dev/sda1 /boot ext4 rw,relatime,barrier=1,data=ordered 0 0none /proc/sys/fs/binfmt_misc binfmt_misc rw,relatime 0 0sunrpc /var/lib/nfs/rpc_pipefs rpc_pipefs rw,relatime 0 0172.16.1.31:/data/ /mnt nfs4 rw,relatime,vers=4,rsize=131072,wsize=131072,namlen=255,hard,proto=tcp,port=0,timeo=600,retrans=2,sec=sys,clientaddr=172.16.1.8,minorversion=0,local_lock=none,addr=172.16.1.31 0 0 测试在客户端和服务端之间测试「2个客户端，1个服务端」 1、任意客户端创建文件夹或创建文件并且输入数据，在服务端是否可以查看； 2、服务端创建文件夹或创建文件并且输入数据，在任意客户端是否可以查看； 3、在客户端A 删除客户端B 创建的文件 4、在客户端B 删除客户端A 创建的文件 加入开机自启动如果是 CentOS 7 环境，那么必须保证 /etc/rc.d/rc.local 文件具有可执行权限，否则该脚本不会执行也不会生效。 开机自启动方式1123456[root@web01 mnt]$ ll /etc/rc.local lrwxrwxrwx. 1 root root 13 Nov 14 2018 /etc/rc.local -&gt; rc.d/rc.local[root@web01 mnt]# tail -3 /etc/rc.local # mount nfsmount -t nfs 172.16.1.31:/data /mnt 开机自启动方式2123[root@web01 mnt]$ cat /etc/fstab # 添加如下信息172.16.1.31:/data /mnt nfs defaults 0 0 存在问题加入了开机自启动，当重启 NFS 客户端机器时，如果此时 NFS 服务端机器已关机，或者网络存在问题等等。使 NFS 客户端连接 NFS 服务端失败，那么此时会造成 NFS 客户端机器起不来的情况。 因此为了避免该情况发生，不建议机器开机自启动就挂载 NFS。 如果一台机器必须挂载 NFS，那么我们就做好监控。当该机器未挂载 NFS 时就告警给我们，然后我们去手动挂载。 当然如果实际环境中你们的 NFS 服务极其稳定，且几乎不再改变 NFS 服务端地址，那么此时你也可以加入开机自启动。 这些都是根据实际具体情况具体分析的。]]></content>
      <categories>
        <category>NFS</category>
      </categories>
      <tags>
        <tag>NFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell脚本：通过域名获取证书的过期时间]]></title>
    <url>%2F2019%2F06%2F24%2Fdomainexpire%2F</url>
    <content type="text"><![CDATA[各位读者都是经常上网的人，当今社会我们可以暂时性没有很多东西，但是就是不能没有网络。否则的话可能会产生严重的焦虑，感觉自己突然就和社会脱节了。 那么在访问各网站的时候，不知道你有没有注意几乎所有网站网址的开头都是 https:// 打头的，而不是 http:// 打头。为什么呢？因为 https 协议对比 http 协议而言安全性更有保证。防止你访问过程中产生的敏感信息被第三方人或组织非法获取到，并作他用。 自签发证书和权威证书当然如果一个网站需要提供 https 访问，那么需要 SSL 证书，这个证书有两种途经获取：第一，我们自签发；第二，购买权威证书。 这两个有什么区别呢？前者不被浏览器承认，因为这个证书是自签发的，认为是非法的。谁知道你有没有动过什么手脚呢，对吧。 自签发证书，谷歌浏览器访问结果如下： 如果我们使用权威证书呢，那么需要我们申请域名，然后备案「就是用来做什么的，如果用途不当，也好查你。」，之后就是向 SSL 权威机构购买 SSL 权威证书了。购买完后，就能在你的网站上使用了。 权威证书，谷歌浏览器访问结果如下： 是不是想着这样就万事大吉了，以后这个 SSL 证书就能够一直使用下去了。呵呵……，少年你太单纯了，要是这样怎么符合细水长流的做法，权威机构是不会只做一锤子买卖的，那样岂不亏大了。这个 SSL 证书是有过期时间的，如果过期你还未更新，仍使用老的 SSL 证书，那么浏览器就会提示不安全。那样你的用户还敢访问你的网站吗，哼哼…………。 实际中不管是个人，还是部门，还是一个公司，总不能每天都看看自家网站的 SSL 证书什么时候过期吧。即便每天看，只要是人执行那总有打盹的时候，那就会有疏漏，要是发生证书过期并且未及时更新新证书产生了其他影响，这事儿就难说了。 因此为了避免上述现象，我们应该怎么防范呢。答案：对 SSL 证书过期时间加监控，比如过期时间小于 30 天就告警，这样我们就能从容处理了。大多数公司都有告警系统吧，例如：zabbix 或者 cacti 或者 open-falcon 等等。 所谓监控好加，但是如何获取 SSL 证书的过期时间呢，这才是重点。那么就到了我们今天的正文了。「哎，扯了半天终于到正主了，让各位读者久等了。」 通过域名获取 SSL 证书过期时间需要两个文件，一个用于存储域名和端口信息，另一个是具体检测 SSL 证书过期时间的执行脚本。注意：这两个文件是在一个目录下。 domain_ssl.info「存储域名信息」 123[root@mini05 20180930]# cat domain_ssl.info # 检测百度域名www.baidu.com:443 check_domain_time.sh「SSL 证书过期检测脚本」 123456789101112131415161718192021222324252627282930313233343536373839404142[root@mini05 20180930]# cat check_domain_time.sh #!/bin/bash################ Version Info ################### Create Date: 2018-09-29# Author: Zhang# Mail: zhang@xxxx.com# Version: 1.0# Attention: 通过域名获取证书的过期时间################################################# V1.0.0 2018-09-29 脚本编写 张# 1.通过域名获取证书的过期时间################################################# 加载环境变量. /etc/profile. ~/.bash_profile. /etc/bashrc# 脚本所在目录即脚本名称script_dir=$( cd &quot;$( dirname &quot;$0&quot; )&quot; &amp;&amp; pwd )script_name=$(basename $&#123;0&#125;)readFile=&quot;$&#123;script_dir&#125;/domain_ssl.info&quot;grep -v &apos;^#&apos; $&#123;readFile&#125; | while read line;do # 读取存储了需要监测的域名的文件 # echo &quot;$&#123;line&#125;&quot; get_domain=$(echo &quot;$&#123;line&#125;&quot; | awk -F &apos;:&apos; &apos;&#123;print $1&#125;&apos;) get_port=$(echo &quot;$&#123;line&#125;&quot; | awk -F &apos;:&apos; &apos;&#123;print $2&#125;&apos;) # echo $&#123;get_domain&#125; # echo &quot;$&#123;get_port&#125;&quot; # echo &quot;======&quot; # 使用openssl获取域名的证书情况，然后获取其中的到期时间 END_TIME=$(echo | openssl s_client -servername $&#123;get_domain&#125; -connect $&#123;get_domain&#125;:$&#123;get_port&#125; 2&gt;/dev/null | openssl x509 -noout -dates |grep &apos;After&apos;| awk -F &apos;=&apos; &apos;&#123;print $2&#125;&apos;| awk -F &apos; +&apos; &apos;&#123;print $1,$2,$4 &#125;&apos; ) END_TIME1=$(date +%s -d &quot;$END_TIME&quot;) # 将日期转化为时间戳 NOW_TIME=$(date +%s -d &quot;$(date | awk -F &apos; +&apos; &apos;&#123;print $2,$3,$6&#125;&apos;)&quot;) # 将当前的日期也转化为时间戳 RST=$(($(($END_TIME1-$NOW_TIME))/(60*60*24))) # 到期时间减去目前时间再转化为天数 echo &quot;$&#123;RST&#125;&quot;done 执行结果 12[root@mini05 20180930]# sh check_domain_time.sh 389 有了上面这个结果后，相信你就知道应该怎么对其进行监控了。]]></content>
      <categories>
        <category>Shell</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下使用 github+hexo 搭建个人博客07-next主题接入搜索和站点管理]]></title>
    <url>%2F2019%2F06%2F17%2Fhexo07%2F</url>
    <content type="text"><![CDATA[前言这是搭建个人博客系统系列文章的最后一篇，如果你是从第一篇一路跟下来的，那么恭喜你，即将完成整个博客网站的搭建。OK，话不多说，开始我们的收官之战。 不知你想过没有，如果我们的文章少，一眼看完整个目录，那么还好。但是如果日积月累几年下来，我们的文章增加到 100+ 以上，那么不管是你博主，还是访问用户，如何去快速找到所需的文章呢。这时我们就需要用到搜索了。 还有就是我们的文章最终是要让其他人看的，而不是我们自娱自乐。除了到各大平台引流到自己的博客系统外，最好让搜索引擎也收录我们的文章。常用的搜索引擎有百度和谷歌，因此本文会针对这两者进行讲解。 搜索服务Local Search添加百度/谷歌/本地 自定义站点内容搜索。 12345678910111213141516[root@iZ28xbsfvc4Z hexo]# pwd # 站点目录/app/softinsall/hexo[root@iZ28xbsfvc4Z hexo]# npm install hexo-generator-searchdb --save [root@iZ28xbsfvc4Z hexo]# vim _config.yml # 站点配置文件，追加信息# Local Searchsearch: path: search.xml field: post format: html limit: 10000[root@iZ28xbsfvc4Z hexo]# cd themes/next/ # 到主题目录[root@iZ28xbsfvc4Z next]# vim _config.yml # 修改主题配置文件# Local search# Dependencies: https://github.com/flashlab/hexo-generator-searchlocal_search: enable: true # 从 false 改为 true 然后进入站点目录，清除静态文件和缓存，重新生成，之后再启动服务即可 hexo clean ==&gt; hexo g ==&gt; hexo s -p 80 页面效果 站点管理先确认博客是否被收录在百度或者谷歌上面输入下面格式来判断，如果能搜索到就说明被收录，否则就没有。 1site:zhangblog.com 创建站点地图文件站点地图是一种文件，您可以通过该文件列出您网站上的网页，从而将您网站内容的组织架构告知Google和其他搜索引擎。搜索引擎网页抓取工具会读取此文件，以便更加智能地抓取您的网站。 安装插件在站点目录安装插件，并修改站点配置文件。 1234567891011121314151617181920[root@iZ28xbsfvc4Z hexo]# pwd # 站点目录/app/softinsall/hexo[root@iZ28xbsfvc4Z hexo]# npm install hexo-generator-sitemap --save [root@iZ28xbsfvc4Z hexo]# npm install hexo-generator-baidu-sitemap --save [root@iZ28xbsfvc4Z hexo]# vim _config.yml # 添加如下信息# 站点地图Plugins: - hexo-generator-baidu-sitemap - hexo-generator-sitemapbaidusitemap: path: baidusitemap.xmlsitemap: path: sitemap.xml[root@iZ28xbsfvc4Z hexo]# hexo g # 生成静态文件，可见有 baidusitemap.xml 和 sitemap.xml 文件生成INFO Start processingINFO Files loaded in 1.25 sINFO Generated: baidusitemap.xml # 生成的文件INFO Generated: sitemap.xml # 生成的文件INFO 2 files generated in 1.26 s 百度站点地图 http://www.zhangblog.com/baidusitemap.xml 谷歌站点地图 http://www.zhangblog.com/sitemap.xml 百度收录我们的博客 百度资源平台：https://ziyuan.baidu.com/dashboard/index 添加站点 这里推荐使用文件验证。下载文件放到 hexo\public 目录下即可。 数据引入 ==&gt; 链接提交为了方便我们使用「自动提交」下的「自动推送」和「sitemap」。 自动推送自动推送很简单，就是在你代码里面嵌入自动推送JS代码，在页面被访问时，页面URL将立即被推送给百度。 将复制的 JS 代码，添加到如下文件： 123456789101112131415161718[root@iZ28xbsfvc4Z next]# pwd #主题目录/app/softinsall/hexo/themes/next[root@iZ28xbsfvc4Z next]# vim layout/_partials/footer.swig # 在文件最后面追加&lt;!-- 百度自动推送 --&gt;&lt;script&gt;(function()&#123; var bp = document.createElement(&apos;script&apos;); var curProtocol = window.location.protocol.split(&apos;:&apos;)[0]; if (curProtocol === &apos;https&apos;) &#123; bp.src = &apos;https://zz.bdstatic.com/linksubmit/push.js&apos;; &#125; else &#123; bp.src = &apos;http://push.zhanzhang.baidu.com/push.js&apos;; &#125; var s = document.getElementsByTagName(&quot;script&quot;)[0]; s.parentNode.insertBefore(bp, s);&#125;)();&lt;/script&gt; 代码来源 sitemap提交这个直接提交就行。 得到结果 如何选择链接提交方式1、主动推送：最为快速的提交方式，推荐你将站点当天新产出链接立即通过此方式推送给百度，以保证新链接可以及时被百度收录。2、自动推送：最为便捷的提交方式，请将自动推送的JS代码部署在站点的每一个页面源代码中，部署代码的页面在每次被浏览时，链接会被自动推送给百度。可以与主动推送配合使用。3、sitemap：您可以定期将网站链接放到sitemap中，然后将sitemap提交给百度。百度会周期性的抓取检查您提交的sitemap，对其中的链接进行处理，但收录速度慢于主动推送。4、手动提交：一次性提交链接给百度，可以使用此种方式。 谷歌收录我们的博客谷歌操作比较简单，就是向 Google 站长工具提交 sitemap 就可以了。 谷歌资源地址：https://search.google.com/search-console?hl=zh-CN 得到结果 总结上述这些完成后，搜索引擎不会马上就收录完成。得等一两天后才会完成收录。在站点管理页面中才有数据。 谷歌收录会快些最长几天，百度的话可能要等半个月左右吧。 推荐阅读Linux下使用 github+hexo 搭建个人博客01-hexo搭建 Linux下使用 github+hexo 搭建个人博客02-hexo部署到Github Pages Linux下使用 github+hexo 搭建个人博客03-hexo配置优化 Linux下使用 github+hexo 搭建个人博客04-next主题优化 Linux下使用 github+hexo 搭建个人博客05-next主题接入评论系统 Linux下使用 github+hexo 搭建个人博客06-next主题接入数据统计 Linux下使用 github+hexo 搭建个人博客07-next主题接入搜索和站点管理]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下使用 github+hexo 搭建个人博客06-next主题接入数据统计]]></title>
    <url>%2F2019%2F06%2F16%2Fhexo06%2F</url>
    <content type="text"><![CDATA[前言之前说了 next 主题的优化和接入评论系统。让我们完成了自己所需的页面风格和排版，也可让访问用户在每篇博文评论，完成博主和访问用户的交互。 本章我们继续讲解其他重要功能。 既然是一个网站，那么我们就需要收集网站访问数据，提供流量趋势、来源分析、转化跟踪、页面热力图、访问流等多种统计分析服务；这时我们就需要引入——百度统计。 上述的统计只能在百度统计中查看，但我想在自己的网站页面直接就能看一些简单的数据。比如：网站访问人数，访问次数，每篇文章访问次数，网站总字数，每篇文章字数，阅读时长估算等。那么我们就可以引入不蒜子统计，字数统计，阅读次数统计了。具体那就参见下文了。 百度统计需要在百度统计进行注册，并拿到脚本的 ID。 之后在主题配置文件中修改。 12345[root@iZ28xbsfvc4Z next]# pwd # 主题目录/app/softinsall/hexo/themes/next[root@iZ28xbsfvc4Z next]# vim _config.yml # Baidu Analytics IDbaidu_analytics: 983XXXXXXXXXXXXXXXXXXXXXXXXXX2 访问报告查看过半小时或一小时左右可在百度统计查看报告。 不蒜子统计编辑主题配置文件中的 busuanzi_count 的配置项。 123456789101112131415161718192021[root@iZ28xbsfvc4Z next]# pwd # 主题目录/app/softinsall/hexo/themes/next[root@iZ28xbsfvc4Z next]# vim _config.yml # Show PV/UV of the website/page with busuanzi.# Get more information on http://ibruce.info/2015/04/04/busuanzi/busuanzi_count: # count values only if the other configs are false enable: true # custom uv span for the whole site site_uv: true site_uv_header: 本站访客数 site_uv_footer: 人次 # custom pv span for the whole site site_pv: true site_pv_header: 本站总访问量 site_pv_footer: 次 # custom pv span for one page only # 每篇博文阅读次数，使用 leancloud 统计。原因是在「首页」中，leancloud 统计也能看阅读次数，而不蒜子则不行。 page_pv: false page_pv_header: 本文总阅读量 page_pv_footer: 次 不蒜子域名修改因七牛强制过期『dn-lbstatics.qbox.me』域名，与客服沟通无果，只能更换域名到『busuanzi.ibruce.info』！ 修改如下： 1234567[root@iZ28xbsfvc4Z next]# pwd # 主题目录/app/softinsall/hexo/themes/next[root@iZ28xbsfvc4Z next]# vim layout/_third-party/analytics/busuanzi-counter.swig &#123;% if theme.busuanzi_count.enable %&#125;&lt;div class=&quot;busuanzi-count&quot;&gt; &lt;script async src=&quot;https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js&quot;&gt;&lt;/script&gt;……………… 浏览器访问页面底部 阅读次数统计在 LeanCloud 中创建 Class在之前的评论系统中，已经讲解了 LeanCloud 账号的创建、应用创建、获取App ID 和 App Key 已经安全加固。这里仅对阅读次数的 Class 创建做讲解。 数据栏中，_开头的都是系统预定义好的表。 为了区分，新建一张表来保存数据。为了保证对NexT主题的修改兼容，新建Class名字必须为Counter。 为了避免权限问题导致 次数统计显示不正常，选择无限制，创建Class。 主题配置修改在主题配置文件中修改： 123456789[root@iZ28xbsfvc4Z next]# pwd # 主题目录/app/softinsall/hexo/themes/next[root@iZ28xbsfvc4Z next]# vim _config.yml # Show number of visitors to each article.# You can visit https://leancloud.cn get AppID and AppKey.leancloud_visitors: enable: true app_id: h7YmXXXXXXXXXXXXXX app_key: VhTGXXXXXXXXXX 浏览器访问文章标题 字数统计用于统计文章的字数以及分析出阅读时间。 安装 wordcount 插件需要安装的插件 123[root@iZ28xbsfvc4Z hexo]# pwd # 站点目录/app/softinsall/hexo[root@iZ28xbsfvc4Z hexo]# npm install hexo-wordcount --save 主题配置修改在主题配置文件中修改。 1234567891011121314[root@iZ28xbsfvc4Z next]# pwd # 主题目录/app/softinsall/hexo/themes/next[root@iZ28xbsfvc4Z next]# vim _config.yml # Post wordcount display settings# Dependencies: https://github.com/willin/hexo-wordcountpost_wordcount: item_text: true # 文本显示 wordcount: true # 单篇 字数统计 min2read: true # 单篇 阅读时长 totalcount: true # 网站 字数统计 # 该post_wordcount的所有设置另起一行显示 separated_meta: true[root@iZ28xbsfvc4Z next]# vim languages/zh-Hans.yml # 从英文改为中文 totalcount: 本站总字数 浏览器访问文章标题 页面底部 推荐阅读Linux下使用 github+hexo 搭建个人博客01-hexo搭建 Linux下使用 github+hexo 搭建个人博客02-hexo部署到Github Pages Linux下使用 github+hexo 搭建个人博客03-hexo配置优化 Linux下使用 github+hexo 搭建个人博客04-next主题优化 Linux下使用 github+hexo 搭建个人博客05-next主题接入评论系统 Linux下使用 github+hexo 搭建个人博客06-next主题接入数据统计 Linux下使用 github+hexo 搭建个人博客07-next主题接入搜索和站点管理]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下使用 github+hexo 搭建个人博客05-next主题接入评论系统]]></title>
    <url>%2F2019%2F06%2F16%2Fhexo05%2F</url>
    <content type="text"><![CDATA[前言静态站点拥有一定的局限性，因此我们需要借助于第三方服务来扩展我们站点的功能。 而评论系统是最常用于和网站用户交流的，因此本章讲解在 next 主题，如何接入评论系统。 参考网站：Next 使用文档，第三方服务集成 http://theme-next.iissnan.com/third-party-services.html 常用评论系统考虑到国内整体互联网环境，因此评论系统除了考虑到长期可用外，还需要考虑不会被屏蔽。因此在这里推荐三款评论系统。 来必力虽然是韩国的「撇开政治因素」，但是UI设计和后台管理也比较不错，数据可视化用到了图标来展示，所以有付费和免费两个版本，博客只需要用免费的就行了。当然付费的效果主要体现在数据分析上，还可以取消掉免费版未来所带来的广告。当然由于网站在国外，因此登录该网站有些慢，这个你要理解。 登录后可评论。 畅言随着国内其他家评论系统的停服，畅言的地位比之前更加壮大，也正因此听说广告越来越多，饱受诟病。如果想去掉广告，很简单氪金充钱就行。还有就是该评论系统注册时需要填写备案信息，这点请大家务必知晓。 登录后可评论。 Valine由于我们使用的是Next 5.1.3版本，本身就已经集成了valine，因此正常情况下是按照官方文档走就可以了，5分钟开启你的评论系统。因为我们的评论系统其实是放在 LeanCloud 上的，因此首先需要去 LeanCloud 注册一个账号。 可匿名评论。 综上所述，优先选择来必力或 Valine。 来必力评论获取 livere_uid当然怎么在来必力网站注册之类的我就不多说了。说一点：注册完毕提交信息后，可能会等两三分钟才会成功。 如果遇见韩文，那么可以用有道翻译或百度翻译。 如下页面可获取你的ID 在Next主题配置当前版本的 Next 主题已经集成来必力评论系统，因此只需在主题配置文件中配置 livere_uid 即可。 123456[root@iZ28xbsfvc4Z next]# pwd # 主题目录/app/softinsall/hexo/themes/next[root@iZ28xbsfvc4Z next]# vim _config.yml # Support for LiveRe comments system.# You can get your uid from https://livere.com/insight/myCode (General web site)livere_uid: MTAyMC8XXXXXXXXXXNw== 浏览器访问 数据分析页面 Valine 评论系统LeanCloud 账号注册LeanCloud 官网地址 https://leancloud.cn 请自行注册 创建应用注册完毕后，创建应用，我这创建的的是 zhangblog。 创建Class 数据栏中，_开头的都是系统预定义好的表。 为了区分，新建一张表来保存数据。为了保证对NexT主题的修改兼容，新建Class名字必须为Comment。 为了避免权限问题导致 次数统计显示不正常，选择无限制，创建Class。 获取 App ID 和 App Key 如下：Class创建完成后，选择界面最左侧的设置 → 应用Key，复制App ID和App Key。 安全加固因为AppID以及AppKey是暴露在外的，为了确保只用于我们自己的博客，可设置Web安全域名，填入自己的博客域名。 修改主题配置上文说过，Next 5.1.3版本已经集成 Valine，因此我们只需在主题配置文件中修改即可。 12345678910111213141516[root@iZ28xbsfvc4Z next]# pwd # 主题目录/app/softinsall/hexo/themes/next[root@iZ28xbsfvc4Z next]# vim _config.yml # Valine.# You can get your appid and appkey from https://leancloud.cn# more info please open https://valine.js.orgvaline: enable: true appid: h7YmXXXXXXXXXXXXXXXXXX appkey: VhTXXXXXXXXXXXXXXX notify: false # mail notifier , https://github.com/xCss/Valine/wiki verify: false # Verification code placeholder: 来了，就撩两句呗！ # comment box placeholder avatar: mm # gravatar style guest_info: nick,mail,link # custom comment header pageSize: 20 # pagination size 浏览器访问 推荐阅读Linux下使用 github+hexo 搭建个人博客01-hexo搭建 Linux下使用 github+hexo 搭建个人博客02-hexo部署到Github Pages Linux下使用 github+hexo 搭建个人博客03-hexo配置优化 Linux下使用 github+hexo 搭建个人博客04-next主题优化 Linux下使用 github+hexo 搭建个人博客05-next主题接入评论系统 Linux下使用 github+hexo 搭建个人博客06-next主题接入数据统计 Linux下使用 github+hexo 搭建个人博客07-next主题接入搜索和站点管理]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下使用 github+hexo 搭建个人博客04-next主题优化]]></title>
    <url>%2F2019%2F06%2F16%2Fhexo04%2F</url>
    <content type="text"><![CDATA[前言上篇我们说了 hexo 的优化，针对的站点的优化。 本篇讲解 next 主题的优化，包括：使用语言、前端页面显示宽度、菜单、侧栏、头像、添加或取消动画效果、打赏功能等等。 让页面排版更符合我们所要的功能和所想的风格。 可参考网站 http://theme-next.iissnan.com/getting-started.html 主题设定选择 Scheme修改 next 主题配置文件。 12345678[root@zhangblog next]# pwd/app/softinsall/hexo/themes/next[root@zhangblog next]# vim _config.yml # Schemes#scheme: Muse#scheme: Mistscheme: Pisces#scheme: Gemini 可以自行更换，不用重启 hexo 服务。个人更喜欢 Pisces，将菜单栏放在左侧，而不是原来的顶部。Muse 默认 Scheme，这是 NexT 最初的版本，黑白主调，大量留白Mist Muse 的紧凑版本，整洁有序的单栏外观Pisces 双栏 Scheme，小家碧玉似的清新 页面宽度设置默认情况下，该主题页面两边留白较多，所以如果需要可以把两边留白处减少些。 当然，此步操作可略。 123456789101112131415[root@zhangblog css]# pwd/app/softinsall/hexo/themes/next/source/css[root@zhangblog css]# vim _variables/base.styl # 修改处一$main-desktop = 1160px[root@zhangblog css]# vim _schemes/Pisces/_layout.styl # 修改处二 .content-wrap &#123; float: right; box-sizing: border-box; padding: $content-desktop-padding; /* width: $content-desktop; 改为如下信息 */ width: calc(100% - 252px); background: white; min-height: 700px; box-shadow: $box-shadow-inner; border-radius: $border-radius-inner; 设置语言页面默认为英文，改为中文显示。 首先确定该主题支持哪些语言。 1234567891011121314151617181920[root@zhangblog languages]# pwd # 在主题目录，查看 next 主题支持哪些语言/app/softinsall/hexo/themes/next[root@zhangblog languages]# ll languages/total 64-rw-r--r-- 1 root root 1669 Jun 7 20:13 default.yml-rw-r--r-- 1 root root 1601 Jun 7 20:13 de.yml-rw-r--r-- 1 root root 1712 Jun 7 20:13 en.yml-rw-r--r-- 1 root root 1553 Jun 7 20:13 fr-FR.yml-rw-r--r-- 1 root root 1507 Jun 7 20:13 id.yml-rw-r--r-- 1 root root 1688 Jun 7 20:13 it.yml-rw-r--r-- 1 root root 1573 Jun 7 20:13 ja.yml-rw-r--r-- 1 root root 1596 Jun 7 20:13 ko.yml-rw-r--r-- 1 root root 1729 Jun 7 20:13 nl-NL.yml-rw-r--r-- 1 root root 1545 Jun 7 20:13 pt-BR.yml-rw-r--r-- 1 root root 1583 Jun 7 20:13 pt.yml-rw-r--r-- 1 root root 2632 Jun 7 20:13 ru.yml-rw-r--r-- 1 root root 1997 Jun 7 20:13 vi.yml-rw-r--r-- 1 root root 1781 Jun 7 20:13 zh-Hans.yml # 中文简体，使用该语言-rw-r--r-- 1 root root 1763 Jun 7 20:13 zh-hk.yml-rw-r--r-- 1 root root 1763 Jun 7 20:13 zh-tw.yml 在站点配置文件使用指定语言。 123456[root@zhangblog hexo]# pwd # 站点目录/app/softinsall/hexo[root@zhangblog hexo]# vim _config.yml ………………language: zh-Hanstimezone: 重新生成静态文件，然后重启 hexo 服务，再次访问可见是中文显示了。 设置菜单菜单配置包括三个部分，第一是菜单项（名称和链接），第二是菜单项的显示文本，第三是菜单项对应的图标。 修改主题配置文件。 1234567891011121314[root@zhangblog next]# vim _config.yml menu: home: / || home archives: /archives/ || archive tags: /tags/ || tags categories: /categories/ || th about: /about/ || user #schedule: /schedule/ || calendar sitemap: /sitemap.xml || sitemap commonweal: /404/ || heartbeat# Enable/Disable menu icons.menu_icons: enable: true home 主页archives 归档类tags 标签页categories 分类页about 关于页schedule 时间表sitemap 网站地图commonweal 公益 404 设置侧栏修改主题配置文件。 12345[root@zhangblog next]# vim _config.yml sidebar: # Sidebar Position, available values: left | right (only for Pisces | Gemini). position: left #position: right 默认不用修改。侧边栏位置，可用值:：left | right (仅适用于 Pisces | Gemini)。 设置头像修改主题配置文件。 1234567[root@zhangblog next]# vim _config.yml # Sidebar Avataravatar: # In theme directory (source/images): /images/avatar.gif # In site directory (source/uploads): /uploads/avatar.gif # You can also use other linking images. url: /uploads/avatar.png 如果是站外，完整的互联网 URI 如：http://example.com/avatar.png 如果是站内：1、将头像放置主题目录下的 source/uploads/ （新建 uploads 目录若不存在），配置为：avatar: /uploads/avatar.png2、或者 放置在 source/images/ 目录下，配置为：avatar: /images/avatar.png 图片路径 1234[root@zhangblog next]# pwd # next 主题目录/app/softinsall/hexo/themes/next[root@zhangblog next]# ll source/uploads/avatar.png -rw-r--r-- 1 root root 131807 Apr 30 14:39 source/uploads/avatar.png 主题配置设置「RSS」false：禁用 RSS，不在页面上显示 RSS 连接。留空：使用 Hexo 生成的 Feed 链接。 你可以需要先安装 hexo-generator-feed 插件。 插件地址：https://github.com/hexojs/hexo-generator-feed 安装插件 123[root@iZ28xbsfvc4Z hexo]# pwd # 站点目录/app/softinsall/hexo[root@iZ28xbsfvc4Z hexo]# npm install hexo-generator-feed --save 站点配置文件修改 123456789101112[root@zhangblog hexo]# pwd/app/softinsall/hexo[root@zhangblog hexo]# vim _config.yml #Feed Atomfeed: type: atom path: atom.xml limit: 20 hub: content: content_limit: 140 content_limit_delim: &apos; &apos; 参数讲解：type: RSS 的类型(atom/rss2)path: 文件路径，默认是 atom.xml/rss2.xmllimit: 展示文章的数量，使用 0 或则 false 代表展示全部hub:content: 在RSS文件中是否包含内容，有3个值 true/false 默认不填为 falsecontent_limit: 指定内容的长度作为摘要，仅仅在上面content设置为 false 和没有自定义的描述出现content_limit_delim: 上面截取描述的分隔符，截取内容是以指定的这个分隔符作为截取结束的标志。在达到规定的内容长度之前最后出现的这个分隔符之前的内容，防止从中间截断。 添加「标签」页面新建标签页面 1234[root@zhangblog hexo]# pwd # 定位到 Hexo 站点目录下/app/softinsall/hexo[root@zhangblog hexo]# hexo new page tags INFO Created: /app/softinsall/hexo/source/tags/index.md 标签页面设置 123456789101112[root@zhangblog tags]# pwd/app/softinsall/hexo/source/tags[root@zhangblog tags]# lltotal 4-rw-r--r-- 1 root root 79 Jun 7 10:48 index.md[root@zhangblog tags]# cat index.md ---title: All Tagsdate: 2019-06-07 10:36:52type: &quot;tags&quot;comments: false--- 注意：如果有集成评论服务，页面也会带有评论。 若需要关闭的话，请添加字段 comments 并将值设置为 false。 使用标签在文章中使用标签。 123456789[root@zhangblog hexo]# pwd/app/softinsall/hexo[root@zhangblog hexo]# head source/_posts/MarkDown-新手指南.md # 相关信息如下---title: MarkDown 新手指南date: 2019-06-04 19:28:51tags: - MarkDown--- 浏览器访问 添加「分类」页面新建分类页面 1234[root@zhangblog hexo]# pwd # 定位到 Hexo 站点目录下/app/softinsall/hexo[root@zhangblog hexo]# hexo new page categories INFO Created: /app/softinsall/hexo/source/categories/index.md 分类页面设置 123456789101112[root@zhangblog categories]# pwd/app/softinsall/hexo/source/categories[root@zhangblog categories]# lltotal 4-rw-r--r-- 1 root root 89 Jun 7 11:04 index.md[root@zhangblog categories]# cat index.md ---title: 文章分类date: 2019-06-07 11:00:17type: &quot;categories&quot;comments: false--- 注意：如果有集成评论服务，页面也会带有评论。 若需要关闭的话，请添加字段 comments 并将值设置为 false。 使用分类在文章中使用分类。 1234567891011[root@zhangblog hexo]# pwd/app/softinsall/hexo[root@zhangblog hexo]# head source/_posts/MarkDown-新手指南.md # 相关信息如下---title: MarkDown 新手指南date: 2019-06-04 19:28:51tags: - MarkDowncategories: - MarkDown--- 浏览器访问 添加「关于」页面新建关于页面 1234[root@zhangblog hexo]# pwd # 定位到 Hexo 站点目录下/app/softinsall/hexo[root@zhangblog hexo]# hexo new page aboutINFO Created: /app/softinsall/hexo/source/about/index.md 关于页面编辑 123456789101112131415161718192021222324252627282930313233[root@zhangblog about]# pwd/app/softinsall/hexo/source/about[root@zhangblog about]# lltotal 4-rw-r--r-- 1 root root 47 Jun 7 16:07 index.md[root@zhangblog about]# cat index.md ---title: 关于我date: 2019-06-07 16:07:36---# 关于本博客本博客诞生于 2019-06，虽然 2015-02 就开始在 CSDN 写博客，但是最开始都是作为自己的笔记记录，因此刚开始那段时间也不怎么重视排版。如果在 CSDN 看了我那些早期的博客，发现排版不好，体验性欠缺，还请多多包涵。后来该博客经过几次改版，自己发现不怎么适应。因此就转到了博客园。相比前者的经常改版，甚至有段时间广告频繁，博客园就好很多，页面也非常清爽。………………等等，后期可能还会有其他动作，敬请期待…………# 联系方式邮箱：zhanglianghhh@163.comQQ: 1369929127&lt;/br&gt;&lt;center&gt;**你对本站的捐赠，就是我最大的动力！**&lt;/center&gt;--- 浏览器访问 添加「公益404」页面腾讯公益404页面，寻找丢失儿童，让大家一起关注此项公益事业！ 新建关于页面 1234[root@zhangblog hexo]# pwd # 定位到 Hexo 站点目录下/app/softinsall/hexo[root@zhangblog hexo]# hexo new page 404INFO Created: /app/softinsall/hexo/source/404/index.md 关于页面编辑 1234567891011121314151617181920212223242526272829[root@zhangblog 404]# pwd/app/softinsall/hexo/source/404[root@zhangblog 404]# lltotal 4-rw-r--r-- 1 root root 758 Jun 7 23:19 index.md[root@zhangblog 404]# cat index.md ---title: 404date: 2019-06-07 23:15:22---&lt;!DOCTYPE HTML&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html;charset=utf-8;&quot;/&gt; &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=edge,chrome=1&quot; /&gt; &lt;meta name=&quot;robots&quot; content=&quot;all&quot; /&gt; &lt;meta name=&quot;robots&quot; content=&quot;index,follow&quot;/&gt; &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://qzone.qq.com/gy/404/style/404style.css&quot;&gt;&lt;/head&gt;&lt;body&gt; &lt;script type=&quot;text/plain&quot; src=&quot;http://www.qq.com/404/search_children.js&quot; charset=&quot;utf-8&quot; homePageUrl=&quot;/&quot; homePageName=&quot;回到我的主页&quot;&gt; &lt;/script&gt; &lt;script src=&quot;https://qzone.qq.com/gy/404/data.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://qzone.qq.com/gy/404/page.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 浏览器访问 侧边栏社交链接侧栏社交链接的修改包含两个部分，第一是链接，第二是链接图标。两者配置均在主题配置文件中。 1234567891011121314[root@zhangblog next]# pwd # 主题目录/app/softinsall/hexo/themes/next[root@zhangblog next]# vim _config.ymlsocial: GitHub: https://github.com/zhanglianghhh || github E-Mail: mailto:zhanglianghhh@163.com || envelope 知乎: https://www.zhihu.com/people/lightzhang-23-69/activities || globe CSDN: https://blog.csdn.net/woshizhangliang999 || codiepie 博客园: https://www.cnblogs.com/zhanglianghhh/p/ || rss-squaresocial_icons: enable: true icons_only: false transition: false 以如下配置说明： GitHub: https://github.com/zhanglianghhh || github GitHub： 表示页面显示的文字https://github.com/zhanglianghhh ： 跳转URLgithub： 使用的图标 更多图标参见如下网站： http://www.fontawesome.com.cn/faicons/ 页面效果 开启打赏功能越来越多的平台（微信公众号、新浪博客、简书、百度打赏等）支持打赏功能，付费阅读时代越来越近，因此增加了打赏功能。 支持微信打赏和支付宝打赏，只需在主题配置文件中填入微信和支付宝收款二维码图片地址，即可开启打赏功能。 1234567[root@zhangblog next]# pwd # 在主题目录/app/softinsall/hexo/themes/next[root@zhangblog next]# vim _config.yml # Rewardreward_comment: 坚持原创分享，你的支持就是我最大的动力！wechatpay: /uploads/weixin_cash_code.pngalipay: /uploads/alipay_cash_code.png 图片所在位置 123456[root@zhangblog next]# pwd # 在主题目录/app/softinsall/hexo/themes/next[root@zhangblog next]# ll source/uploads/weixin_cash_code.png -rw-r--r-- 1 root root 27337 Jun 7 19:39 source/uploads/weixin_cash_code.png[root@zhangblog next]# ll source/uploads/alipay_cash_code.png -rw-r--r-- 1 root root 58235 Jun 7 19:37 source/uploads/alipay_cash_code.png 页面效果 友情链接在主题配置文件中修改。 1234567891011[root@zhangblog next]# pwd # 主题目录/app/softinsall/hexo/themes/next[root@zhangblog next]# vim _config.yml# Blog rollslinks_icon: linklinks_title: Links#links_layout: blocklinks_layout: inlinelinks: OpenInfo: http://mp.weixin.qq.com/user1 stormzhang: http://mp.weixin.qq.com/user2 页面效果 站点建立时间这个时间将在站点的底部显示，例如 © 2015- 2019。 编辑主题配置文件，修改字段 since。 12345678910[root@zhangblog next]# pwd # 主题目录/app/softinsall/hexo/themes/next[root@zhangblog next]# vim _config.ymlfooter: # Specify the date when the site was setup. # If not defined, current year will be used. 修改处如下 since: 2015 # Icon between year and copyright info. icon: user 页面效果 订阅微信公众号在每篇文章的末尾默认显示微信公众号二维码，扫一扫，轻松订阅。 编辑主题配置文件，如下： 12345678[root@zhangblog next]# pwd # 在主题目录/app/softinsall/hexo/themes/next[root@zhangblog next]# vim _config.yml# Wechat Subscriberwechat_subscriber: enabled: true qcode: /uploads/weixin_pulic_code.png description: 欢迎扫一扫，订阅我的微信公众号！ 页面样式修改 1234567891011[root@zhangblog next]# pwd # 在主题目录/app/softinsall/hexo/themes/next[root@zhangblog next]# vim layout/_macro/wechat-subscriber.swig &lt;div id=&quot;wechat_subscriber&quot; style=&quot;display: block; padding: 10px 0; margin: 20px auto; width: 100%; text-align: center&quot;&gt;&lt;!-- &lt;img id=&quot;wechat_subscriber_qcode&quot; src=&quot;&#123;&#123; theme.wechat_subscriber.qcode &#125;&#125;&quot; alt=&quot;&#123;&#123; theme.author &#125;&#125; wechat&quot; style=&quot;width: 200px; max-width: 100%;&quot;/&gt; 去掉 style 中的 width: 200px; --&gt; &lt;img id=&quot;wechat_subscriber_qcode&quot; src=&quot;&#123;&#123; theme.wechat_subscriber.qcode &#125;&#125;&quot; alt=&quot;&#123;&#123; theme.author &#125;&#125; wechat&quot; style=&quot;max-width: 100%;&quot;/&gt; &lt;div&gt;&#123;&#123; theme.wechat_subscriber.description &#125;&#125;&lt;/div&gt;&lt;/div&gt; 页面效果 设置「动画效果」Next 主题默认开启动画效果，由于该效果使用 JavaScript 编写，因此只有当 JavaScript 脚本加载完毕后，才会显示页面。如果你对加载速度在乎的话，那么可以关闭动画效果。 编辑主题配置文件，如下： 123456789[root@zhangblog next]# pwd # 在主题目录/app/softinsall/hexo/themes/next[root@zhangblog next]# vim _config.yml# Use velocity to animate everything.motion: # true 开启动画， false 关闭动画 enable: true # true 异步 false 同步 async: true 设置「背景动画」Next 主题自带四种背景动画效果，有兴趣自行体验，不过建议最好别开背景动画，因为会消耗额外的客户端资源。 编辑主题配置文件，如下： 1234567891011121314[root@zhangblog next]# pwd # 在主题目录/app/softinsall/hexo/themes/next[root@zhangblog next]# vim _config.yml# Canvas-nestcanvas_nest: false# three_wavesthree_waves: false# canvas_linescanvas_lines: false# canvas_spherecanvas_sphere: false 底部版权信息修改主题配置文件，如下： 12345678[root@zhangblog next]# pwd # 在主题目录/app/softinsall/hexo/themes/next[root@zhangblog next]# vim _config.yml# Declare license on postspost_copyright: enable: true license: CC BY-NC-SA 3.0 license_url: https://creativecommons.org/licenses/by-nc-sa/3.0/ 页面效果 添加文章更新时间在主题配置文件中进行修改配置。 123456789[root@iZ28xbsfvc4Z next]# pwd # 主题目录/app/softinsall/hexo/themes/next[root@iZ28xbsfvc4Z next]# vim _config.yml # Post meta display settingspost_meta: item_text: true created_at: true updated_at: true # 从 false 改为 true categories: true 浏览器访问 首页不显示全文(只显示预览)在主题配置文件中进行修改配置。 123456789[root@iZ28xbsfvc4Z next]# pwd # 主题目录/app/softinsall/hexo/themes/next[root@iZ28xbsfvc4Z next]# vim _config.yml # Automatically Excerpt. Not recommend.# Please use &lt;!-- more --&gt; in the post to control excerpt accurately.auto_excerpt: # 从 false 改为 true enable: true length: 150 页面效果 文章末尾统一添加“本文结束”标记12345678910111213141516171819[root@iZ28xbsfvc4Z next]# pwd # 主题目录/app/softinsall/hexo/themes/next[root@iZ28xbsfvc4Z next]# vim layout/_macro/passage-end-tag.swig # 增加该文件&lt;div&gt; &#123;% if not is_index %&#125; &lt;div style=&quot;text-align:center;color: #555;font-size:24px;&quot;&gt;&lt;-------------The End-------------&gt;&lt;/div&gt; &#123;% endif %&#125;&lt;/div&gt;[root@iZ28xbsfvc4Z next]# vim layout/_macro/post.swig # 修改该文件，在 &lt;div class=&quot;post-body&gt;…………&lt;/div&gt; 标签后增加如下信息 &lt;!-- 文章末尾统一添加“本文结束”标记 --&gt; &lt;div&gt; &#123;% if not is_index %&#125; &#123;% include &apos;passage-end-tag.swig&apos; %&#125; &#123;% endif %&#125; &lt;/div&gt;[root@iZ28xbsfvc4Z next]# vim _config.yml # 主题配置文件修改# 文章末尾添加“本文结束”标记passage_end_tag: enabled: true 页面效果 推荐阅读Linux下使用 github+hexo 搭建个人博客01-hexo搭建 Linux下使用 github+hexo 搭建个人博客02-hexo部署到Github Pages Linux下使用 github+hexo 搭建个人博客03-hexo配置优化 Linux下使用 github+hexo 搭建个人博客04-next主题优化 Linux下使用 github+hexo 搭建个人博客05-next主题接入评论系统 Linux下使用 github+hexo 搭建个人博客06-next主题接入数据统计 Linux下使用 github+hexo 搭建个人博客07-next主题接入搜索和站点管理]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下使用 github+hexo 搭建个人博客03-hexo配置优化]]></title>
    <url>%2F2019%2F06%2F14%2Fhexo03%2F</url>
    <content type="text"><![CDATA[前言上两张文章，我们说了 hexo 部署、主题的切换、博文的创建、MarkDown 简单使用和 hexo 部署到 GitHub Pages。 也说了我们会使用 next 主题做为我们后期博客的使用和维护。但是该主题的原生态，可能或多或少不满足我们当前的需求，因此需要我们对其进行优化，达到我们想要的效果。 因此这篇文章和下篇文章主要就是针对主题的优化进行书写的。 注意事项 1、优化完毕或者新建博客后需要 hexo g 生成静态文件； 2、然后重新启动服务，使用命令 hexo s -p 80 3、浏览器查看没有问题后，部署到 GitHub，使用命令：hexo d hexo 的 _config.yml优化官网地址 https://hexo.io/zh-cn/docs/configuration.html 网站设置部分123456789[root@zhangblog hexo]# vim _config.yml # Sitetitle: lightzhang博客subtitle:description: lightzhang博客，不止于技术，更记录人生点滴感悟。keywords:author: lightzhanglanguage:timezone: title 网站标题，需要填写subtitle 网站副标题description 网站描述，主要用于SEO，告诉搜索引擎一个关于您站点的简单描述，通常建议在其中包含您网站的关键词。author 您的名字，用于主题显示文章的作者。language 网站使用的语言timezone 网站时区。Hexo 默认使用您电脑的时区。 网址设置部分1234567[root@zhangblog hexo]# vim _config.yml # URL## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;url: http://www.zhangblog.comroot: /permalink: :year/:month/:day/:title/permalink_defaults: url 网址root 网站根目录permalink 文章的永久链接格式 默认格式 :year/:month/:day/:title/permalink_defaults 永久链接中各部分的默认值 网站存放在子目录如果您的网站存放在子目录中，例如 http://yoursite.com/blog，则请将您的 url 设为 http://yoursite.com/blog 并把 root 设为 /blog/。 目录设置部分12345678910[root@zhangblog hexo]# vim _config.yml# Directorysource_dir: sourcepublic_dir: publictag_dir: tagsarchive_dir: archivescategory_dir: categoriescode_dir: downloads/codei18n_dir: :langskip_render: 目录一般不需要修改。source_dir 资源文件夹，这个文件夹用来存放内容。默认：sourcepublic_dir 公共文件夹，这个文件夹用于存放生成的站点文件。默认：publictag_dir 标签文件夹。默认：tagsarchive_dir 归档文件夹。默认：archivescategory_dir 分类文件夹。默认：categoriescode_dir Include code 文件夹 downloads/codei18n_dir 国际化（i18n）文件夹。默认 :langskip_render 跳过指定文件的渲染 文章设置部分12345678910111213141516[root@zhangblog hexo]# vim _config.yml# Writingnew_post_name: :title.md # File name of new postsdefault_layout: posttitlecase: false # Transform title into titlecaseexternal_link: true # Open external links in new tabfilename_case: 0render_drafts: falsepost_asset_folder: falserelative_link: falsefuture: truehighlight: enable: true line_number: true auto_detect: false tab_replace: 一般不用修改new_post_name 新文章的文件名称。默认 :title.mddefault_layout 预设布局，默认 postauto_spacing 在中文和英文之间加入空格，默认 falsetitlecase 把标题转换为 title case，默认 falseexternal_link 在新标签中打开链接，默认 truefilename_case 把文件名称转换为 (1) 小写或 (2) 大写，默认 0render_drafts 显示草稿，默认 falsepost_asset_folder 启动 Asset 文件夹，默认 falserelative_link 把链接改为与根目录的相对位址，默认 falsefuture 显示未来的文章，默认 truehighlight 代码块的设置 分页设置部分12345[root@zhangblog hexo]# vim _config.yml # Pagination## Set per_page to 0 to disable paginationper_page: 10pagination_dir: page per_page 每页显示的文章量 (0 = 关闭分页功能)，默认 10pagination_dir 分页目录，默认 page 扩展部分设置1234[root@zhangblog hexo]# vim _config.yml # Extensions# 使用主题theme: next theme 当前使用主题名称。值为false时禁用主题 部署部分设置1234567[root@zhangblog hexo]# vim _config.yml# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: git@github.com:zhanglianghhh/zhanglianghhh.github.io.git branch: master deploy 部署部分的设置 推荐阅读Linux下使用 github+hexo 搭建个人博客01-hexo搭建 Linux下使用 github+hexo 搭建个人博客02-hexo部署到Github Pages Linux下使用 github+hexo 搭建个人博客03-hexo配置优化 Linux下使用 github+hexo 搭建个人博客04-next主题优化 Linux下使用 github+hexo 搭建个人博客05-next主题接入评论系统 Linux下使用 github+hexo 搭建个人博客06-next主题接入数据统计 Linux下使用 github+hexo 搭建个人博客07-next主题接入搜索和站点管理]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下使用 github+hexo 搭建个人博客02-hexo部署到Github Pages]]></title>
    <url>%2F2019%2F06%2F12%2Fhexo02%2F</url>
    <content type="text"><![CDATA[前言之前的这篇文章《Linux下使用 github+hexo 搭建个人博客01-hexo搭建》，相信大家都知道怎么搭建 hexo ，怎么切换主题，并且完成了一篇博文的创建，以及 MarkDown 标记语法的用法。如果还不清楚或者不知道的，那就先回去看看这篇文章。 那么我们接下来就需要将 hexo 和 GitHub Pages 结合了，为什么要结合呢？因为当前我们的博客还是在本地机器，如果因为我们不小心删了数据，或者购买的云服务因为没有及时续费，导致机器被释放了，那我们就永久失去了这些数据。 因此如果这些数据对我们还有用，并且想永久保存，那么就需要找个类似 SVN 或者 Git 之类的代码版本托管仓库了。那理所当然选 GitHub 了，就当前环境还有比 GitHub 更好的吗。 注册 GitHub 账号GitHub 官网： https://github.com/ 具体注册过程也很简单，这里就不说了。请自行注册。 使用 GitHub Pages创建指定的 GitHub 仓库点击创建按钮 仓库名称和配置选择 开启 GitHub Pages进入 Settings 默认已开启 GitHub Pages 选择主题 浏览器访问 https://zhanglianghhh.github.io/ 在 GitHub 上添加 SSH Keys 信息为了能将个人博客服务器上的博客数据推送到 GitHub，达到数据永久保存效果，我们需要把博客服务器的 SSH keys 信息在 GitHub 上添加信任。 本地服务器创建 ssh-key 信息123456789101112131415161718192021222324252627282930[root@zhangblog ~]# ssh-keygen -t rsa # 如果遇见等待输入的地方，按下 Enter 回车键即可，无需任何其他输入Generating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:73zrQW4LTBgAVqQKvOoTxFrgaGF/sobf643Q+3w7or0 root@zhangblogThe key&apos;s randomart image is:+---[RSA 2048]----+| o++ ||oo . . . ||*oo . . ||.*o+ . o ||+oo + S . . ||oo o. + o ||. +... + + ||.. ...*. = o.o || .. .*+E+.=o+. |+----[SHA256]-----+[root@zhangblog ~]# cd .ssh/[root@zhangblog .ssh]# lltotal 12-rw------- 1 root root 0 Jun 3 17:02 authorized_keys-rw------- 1 root root 1675 Jun 5 14:17 id_rsa-rw-r--r-- 1 root root 396 Jun 5 14:17 id_rsa.pub-rw-r--r-- 1 root root 395 Dec 14 17:15 known_hosts[root@zhangblog .ssh]# cat id_rsa.pub # 具体的公钥信息ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQD4iDDDDDDDDDDgMMutdH7KdI5P7BrtHbfRG+MYyr1/Gtz45hJgbVHBCTFZaTn2+MekFQcZVkyc2kEU7L7mm4ZGWkStgbXkas+uTFwo3kLlX8ozcUC3jM8rhzbWPv8piq58ezBnrMZ0zNsCgHGXpokUmLqYt1mpLXz5rsOzwGgHHkp+Wlr+6tTQxr/+9T4CiE/RkFKi/mehn01rjOcVluYSkwkVii03EzMlMcoyV3ctnWzwyZIWAQAsvDSN2CQAdRtaUHOJOAoRv8/s4jDiWU1ia0JYmm2D/IWcLl2hxNtGeVHTFk9l1djtUQu47zuoOM4y6ySlUx28HNIAMw14gjIv5 root@zhangblog GitHub 添加 SSH Keys OK，这样我们就添加成功了。 GitHub 连接测试1234567[root@zhangblog ~]# ssh -T git@github.comThe authenticity of host &apos;github.com (13.250.177.223)&apos; can&apos;t be established.RSA key fingerprint is SHA256:nThbg6kXUpJWGl7E1IGOCspRomTxdCARLviKw6E5SY8.RSA key fingerprint is MD5:16:27:ac:a5:76:28:2d:36:63:1b:56:4d:eb:df:a6:48.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added &apos;github.com,13.250.177.223&apos; (RSA) to the list of known hosts.Hi zhanglianghhh! You&apos;ve successfully authenticated, but GitHub does not provide shell access. 可见连接 GitHub 成功。 设置你的账号信息12[root@zhangblog hexo]# git config --global user.name &quot;zhanglianghhh&quot; [root@zhangblog hexo]# git config --global user.email &quot;zhanglianghhh@163.com&quot; 这里的用户名和邮箱，应该和Github上的账户邮箱保持一致，防止之后同步的不一致。 Hexo 部署到 GitHub Pages_config.yml 配置修改12345678910[root@zhangblog hexo]# pwd/app/softinsall/hexo[root@zhangblog hexo]# vim _config.yml………………# Deployment## Docs: https://hexo.io/docs/deployment.html # 修改或添加如下信息deploy: type: git repo: git@github.com:zhanglianghhh/zhanglianghhh.github.io.git branch: master 在部署到 GitHub 之前，还需要安装如下扩展： 123[root@iZ28xbsfvc4Z hexo]# pwd # 站点目录/app/softinsall/hexo[root@iZ28xbsfvc4Z hexo]# npm install hexo-deployer-git --save 部署到 GitHub123[root@zhangblog hexo]# pwd/app/softinsall/hexo[root@zhangblog hexo]# hexo d -g # 部署前，先生成静态文件 -g 可选 浏览器访问12https://zhanglianghhh.github.io/ # GitHub Pages 的访问http://www.zhangblog.com/ # 个人网站的访问 推荐阅读Linux下使用 github+hexo 搭建个人博客01-hexo搭建 Linux下使用 github+hexo 搭建个人博客02-hexo部署到Github Pages Linux下使用 github+hexo 搭建个人博客03-hexo配置优化 Linux下使用 github+hexo 搭建个人博客04-next主题优化 Linux下使用 github+hexo 搭建个人博客05-next主题接入评论系统 Linux下使用 github+hexo 搭建个人博客06-next主题接入数据统计 Linux下使用 github+hexo 搭建个人博客07-next主题接入搜索和站点管理]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux下使用 github+hexo 搭建个人博客01-hexo搭建]]></title>
    <url>%2F2019%2F06%2F11%2Fhexo01%2F</url>
    <content type="text"><![CDATA[前言文章首发于：lightzhang博客文章地址：http://www.zhangblog.com微信公众号：OpenInfo 为什么要搭建自己的博客系统？ 原因有好几个吧，归类如下：1、自己搭建博客系统很有成就感，可以自己选定页面风格和页面排版； 2、自己搭建博客系统可以根据自己的需要添加各种插件功能，因此整体上比网上的第三方博客网站更好； 3、hexo 支持 MarkDown 标记语法，我们可以很容易的上手，排版简单明了； 4、网上主流的第三方博客网站，不一定很符合你个人的风格，而且由于网站要持续运营下去，因此在此过程中会不可避免的接入广告。当然这点我们也要理解，要允许网站有盈利，这样才能一直为大家服务下去； 5、第三方博客网站肯定会有系统升级，因此会时有出现各种改版的情况，你之前写的博文可能不符合新版本，造成之前的博文排版变得奇丑无比，简直令人崩溃「当然这种情况很少」。还有就是如果改版后符合你的操作习惯那还好，如果不符合那你就有点方了。 6、第三方博客网站有时会出现其他问题。最常见的就是博客页面改版「上一条说过」和图片加载不出来的情况等等。 大概就是上述几条吧，如果你还有其他的原因，欢迎你在文章底部留言！ 说明：如果要把 hexo 生成的静态文件对外提供访问，那么请使用 Nginx 完成。Nginx的部分配置如下： 1234567891011121314server &#123; listen 80; server_name www.zhangblog.com zhangblog.com 120.27.48.179; access_log logs/access.log main; location / &#123; alias /app/softinsall/hexo/public/; index index.html index.htm; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125;&#125; 本次部署的机器信息12机器系统：CentOS Linux release 7.5IP地址：120.27.48.179 因为该博客本人会长期维护和支持下去，因此我是在阿里云购买的机器。 域名解析【可省略】由于这个博客系统是我以后经常使用并且持续维护，因此我在阿里云购买了域名并且进行了备案。域名为：zhangblog.com 。并将域名 zhangblog 解析到了 120.27.48.179。 如果你只是个人测试使用，或者没有自己的域名，那么该项可省略。 安装 Git1[root@zhangblog ~]# yum install -y git 具体使用处1、后续在 hexo 安装不同的主题时，会使用Git方式获取这些主题；2、将 hexo 与自己的 GitHub Pages 结合时。 node.js 安装nodejs 下载官网地址： 1http://nodejs.cn/download/ 为了方便，我们直接下载二进制版本。这样就省去了编译安装步骤。 二进制安装包下载 123[root@zhangblog software]# pwd/app/software[root@zhangblog software]# wget https://npm.taobao.org/mirrors/node/v10.16.0/node-v10.16.0-linux-x64.tar.xz # 下载二进制安装包 nodejs 部署123456789101112[root@zhangblog software]# pwd/app/software[root@zhangblog software]# tar xf node-v10.16.0-linux-x64.tar.xz [root@zhangblog software]# mv node-v10.16.0-linux-x64 /app/softinsall/[root@zhangblog software]# cd /app/softinsall/[root@zhangblog softinsall]# pwd/app/softinsall[root@zhangblog softinsall]# ln -s node-v10.16.0-linux-x64 nodejs[root@zhangblog softinsall]# lltotal 4lrwxrwxrwx 1 root root 23 Jun 4 15:59 nodejs -&gt; node-v10.16.0-linux-x64drwxrwxr-x 6 500 500 4096 May 29 05:36 node-v10.16.0-linux-x64 版本信息 1234[root@zhangblog bin]# pwd/app/softinsall/nodejs/bin[root@zhangblog bin]# ./node -vv10.16.0 创建软连接 12[root@zhangblog bin]# ln -s /app/softinsall/nodejs/bin/node /usr/local/bin/node [root@zhangblog bin]# ln -s /app/softinsall/nodejs/bin/npm /usr/local/bin/npm hexo 常用操作如果熟悉 hexo 命令，那么可以忽略这一节。 hexo 操作命令官网 1https://hexo.io/docs/commands $ hexo init [folder]初始化一个网站。如果没有提供文件夹，Hexo将在当前目录中创建网站。 $ hexo new [layout] 创建新文章。如果没有提供布局，Hexo 将使用 _config.yml 中的 default_layout 项提供的布局。如果标题包含空格，用引号括起来。 $ hexo generate 1简写：hexo g 生成静态文件。 可选项 1-d, --deploy 生成静态文件完成后部署 $ hexo server 1简写：hexo s 启动本地服务器。默认情况下，这是在 http://localhost:4000/ 可选项 1-p, --port 使用端口，覆盖默认端口 $ hexo deploy 1简写：hexo d 部署你的网站。 可选项 1-g, --generate 完成部署之前，生成静态文件。 $ hexo clean清除缓存文件(db.json)和生成的文件(public)。使用新主题或想重新生成静态文件时可使用 $ hexo version版本信息 12345678910111213141516171819[root@zhangblog hexo]# hexo versionhexo: 3.8.0hexo-cli: 2.0.0os: Linux 3.10.0-862.14.4.el7.x86_64 linux x64http_parser: 2.8.0node: 10.16.0v8: 6.8.275.32-node.52uv: 1.28.0zlib: 1.2.11brotli: 1.0.7ares: 1.15.0modules: 64nghttp2: 1.34.0napi: 4openssl: 1.1.1bicu: 64.2unicode: 12.1cldr: 35.1tz: 2019a hexo 部署1[root@zhangblog ~]# npm install hexo-cli -g # 安装 hexo 将 hexo 命令添加到全局，采用软连接方式。 123[root@zhangblog bin]# pwd # hexo 命令所在目录/app/softinsall/nodejs/lib/node_modules/hexo-cli/bin[root@zhangblog bin]# ln -s /app/softinsall/nodejs/lib/node_modules/hexo-cli/bin/hexo /usr/local/bin/hexo 部署 hexo 博客环境部署 hexo可以放在和 nodejs 同层级的目录。 1234567891011121314151617[root@zhangblog softinsall]# pwd/app/softinsall[root@zhangblog softinsall]# mkdir hexo[root@zhangblog softinsall]# cd hexo/[root@zhangblog hexo]# hexo init # 新建一个网站，默认在目前的文件夹建立网站。[root@zhangblog hexo]# ll total 168-rw-r--r-- 1 root root 1765 Jun 4 16:14 _config.ymldrwxr-xr-x 285 root root 12288 Jun 4 16:15 node_modules-rw-r--r-- 1 root root 443 Jun 4 16:14 package.json-rw-r--r-- 1 root root 138442 Jun 4 16:15 package-lock.jsondrwxr-xr-x 2 root root 4096 Jun 4 16:14 scaffoldsdrwxr-xr-x 3 root root 4096 Jun 4 16:14 sourcedrwxr-xr-x 3 root root 4096 Jun 4 16:14 themes[root@zhangblog hexo]# ll themes/ # 查看自带的主题total 4drwxr-xr-x 6 root root 4096 Jun 4 16:14 landscape 启动环境测试123456789101112131415[root@zhangblog hexo]# hexo g # 生成静态文件[root@zhangblog hexo]# lltotal 200-rw-r--r-- 1 root root 1765 Jun 4 16:14 _config.yml-rw-r--r-- 1 root root 25063 Jun 4 16:26 db.jsondrwxr-xr-x 285 root root 12288 Jun 4 16:15 node_modules-rw-r--r-- 1 root root 447 Jun 4 16:26 package.json-rw-r--r-- 1 root root 138442 Jun 4 16:15 package-lock.jsondrwxr-xr-x 7 root root 4096 Jun 4 16:26 public # 生成的静态文件drwxr-xr-x 2 root root 4096 Jun 4 16:14 scaffoldsdrwxr-xr-x 3 root root 4096 Jun 4 16:14 sourcedrwxr-xr-x 3 root root 4096 Jun 4 16:14 themes[root@zhangblog hexo]# hexo s # 启动服务，默认是 http://localhost:4000INFO Start processingINFO Hexo is running at http://localhost:4000 . Press Ctrl+C to stop. 端口信息12[root@zhangblog ~]# netstat -lntup | grep &apos;hexo&apos;tcp6 0 0 :::4000 :::* LISTEN 7072/hexo 浏览器访问1http://www.zhangblog.com:4000/ 更新 hexo 主题获取NexT主题该主题是我们以后长期使用的主题，后续的优化也是基于该主题进行。由于该主题风格和页面排版都很好，因此推荐大家使用。 123456[root@zhangblog hexo]# pwd/app/softinsall/hexo# 注意当前的目录， themes/next 指定存放的位置[root@zhangblog hexo]# git clone https://github.com/theme-next/hexo-theme-next themes/next # 新地址，当前维护中【但是有些细节不如老版本的】# 或者 ***** [root@zhangblog hexo]# git clone https://github.com/iissnan/hexo-theme-next themes/next # 老地址，没有维护了「推荐使用」 next 主题在 GitHub 有两个地址： 12https://github.com/theme-next/hexo-theme-next # 新地址，当前维护中https://github.com/iissnan/hexo-theme-next # 老地址，没有维护了 使用主题12345678910[root@zhangblog hexo]# hexo clean # 清楚缓存和静态文件目录[root@zhangblog hexo]# vim _config.yml # 修改该配置…………# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: next…………[root@zhangblog hexo]# hexo g # 生成静态文件[root@zhangblog hexo]# hexo s -p 80 # 启动服务，指定端口 浏览器访问1http://www.zhangblog.com/ 新建一篇博客新建博客12345678910[root@zhangblog hexo]# pwd/app/softinsall/hexo[root@zhangblog hexo]# hexo new &apos;MarkDown_Use_Guide&apos;INFO Created: /app/softinsall/hexo/source/_posts/MarkDown_Use_Guide.md[root@zhangblog hexo]# cat source/_posts/MarkDown_Use_Guide.md # 系统生成内容如下---title: MarkDown_Use_Guidedate: 2019-06-04 19:28:51tags:--- 为博客添加内容—- Markdown 新手指南123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115[root@zhangblog hexo]# pwd/app/softinsall/hexo[root@zhangblog hexo]# cat source/_posts/MarkDown_Use_Guide.md ---title: MarkDown 新手指南date: 2019-06-04 19:28:51tags: - MarkDowncategories: - MarkDown---# 标题# 一级标题## 二级标题### 三级标题#### 四级标题##### 五级标题###### 六级标题 ---# 列表## 无序列表- 文本1- 文本2- 文本3## 有序列表1. 文本12. 文本23. 文本3---# 链接、图片和引用## 这是一个超连接[lightzhang 个人博客](http://www.zhangblog.com/)## 这是一个站外图片链接![示例图](https://www.cnblogs.com/images/logo_small.gif)## 这是一个站内图片链接![数字网络](/uploads/weixin_pulic_small.jpg)## 这个一个引用&gt; 宠辱不惊，看庭前花开花落；去留无意，望天上云卷云舒---# 粗体和斜体从最开始的 *wordpress* ,到 *tale* ,到现在的**hexo**,网站变得越来越简单,越来越轻量级,这里主要说说**hexo**的使用。---# 代码引用## 多行代码···「备注：实践中，请把前一行开头的 · 改为 `」#!/bin/bash################ Version Info ################### Create Date: 2018-09-29# Author: Zhang# Mail: zhang@xxxx.com# Version: 1.0# Attention: 脚本描述说明################################################···「备注：实践中，请把前一行开头的 · 改为 `」## 单行代码 【只能一行】`/bin/sh echo &quot;test&quot; &gt;&gt; /dev/null`---# 表格## 书写格式1| Tables | Are | Cool || ------------- |:-------------:| -----:|| col 3 is | right-aligned | $1600 || col 2 is | centered | $12 || zebra stripes | are neat | $1 |## 书写格式2dog | bird | cat----|------|----foo | foo | foobar | bar | barbaz | baz | baz## 书写格式3| 名称 | 系统版本 | 内网IP | Hostname ||--|--|--|--|| salt100 | CentOS7.5 | 172.16.1.100 | 10.0.0.100 || salt01 | CentOS7.5 | 172.16.1.11 | 10.0.0.11 || salt02 | CentOS7.5 | 172.16.1.12 | 10.0.0.12 || salt03 | CentOS7.5 | 172.16.1.13 | 10.0.0.13 |# 字体或图片居中&lt;center&gt;**读万卷书，行万里路**&lt;/center&gt;&lt;center&gt;![数字网络](/uploads/avatar_small.png)&lt;/center&gt;--- 站内图片位置12345678[root@zhangblog hexo]# pwd # 站点位置/app/softinsall/hexo[root@zhangblog hexo]# ll source/uploads/total 388-rw-r--r-- 1 root root 131807 Apr 30 14:39 avatar.png-rw-r--r-- 1 root root 16602 Jun 7 17:39 avatar_small.png-rw-r--r-- 1 root root 209605 Jun 7 17:02 weixin_pulic.jpg-rw-r--r-- 1 root root 19296 Jun 11 14:55 weixin_pulic_small.jpg 生成静态文件123[root@zhangblog hexo]# pwd/app/softinsall/hexo[root@zhangblog hexo]# hexo g # 生成静态文件 浏览器访问12http://www.zhangblog.com/http://www.zhangblog.com/2019/06/04/MarkDown_Use_Guide/ 推荐阅读Linux下使用 github+hexo 搭建个人博客01-hexo搭建 Linux下使用 github+hexo 搭建个人博客02-hexo部署到Github Pages Linux下使用 github+hexo 搭建个人博客03-hexo配置优化 Linux下使用 github+hexo 搭建个人博客04-next主题优化 Linux下使用 github+hexo 搭建个人博客05-next主题接入评论系统 Linux下使用 github+hexo 搭建个人博客06-next主题接入数据统计 Linux下使用 github+hexo 搭建个人博客07-next主题接入搜索和站点管理]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MarkDown 新手指南]]></title>
    <url>%2F2019%2F06%2F04%2FMarkDown_Use_Guide%2F</url>
    <content type="text"><![CDATA[标题一级标题二级标题三级标题四级标题五级标题六级标题 列表无序列表 文本1 文本2 文本3 有序列表 文本1 文本2 文本3 链接、图片和引用这是一个超连接lightzhang 个人博客 这是一个站外图片链接 这是一个站内图片链接 这个一个引用 宠辱不惊，看庭前花开花落；去留无意，望天上云卷云舒 粗体和斜体从最开始的 wordpress ,到 tale ,到现在的hexo,网站变得越来越简单,越来越轻量级,这里主要说说hexo的使用。 代码引用多行代码12345678#!/bin/bash################ Version Info ################### Create Date: 2018-09-29# Author: Zhang# Mail: zhang@xxxx.com# Version: 1.0# Attention: 脚本描述说明################################################ 单行代码 【只能一行】/bin/sh echo &quot;test&quot; &gt;&gt; /dev/null 表格书写格式1 Tables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 书写格式2 dog bird cat foo foo foo bar bar bar baz baz baz 书写格式3 名称 系统版本 内网IP Hostname salt100 CentOS7.5 172.16.1.100 10.0.0.100 salt01 CentOS7.5 172.16.1.11 10.0.0.11 salt02 CentOS7.5 172.16.1.12 10.0.0.12 salt03 CentOS7.5 172.16.1.13 10.0.0.13 字体或图片居中 读万卷书，行万里路]]></content>
      <categories>
        <category>MarkDown</category>
      </categories>
      <tags>
        <tag>MarkDown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F06%2F03%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
